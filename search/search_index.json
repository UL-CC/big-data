{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos. Requisitos Previos Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition 1. Introducci\u00f3n Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark 2. PySpark y SparkSQL Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Fundamentos de DataFrames en Spark Manipulaci\u00f3n y Transformaci\u00f3n de Datos Consultas y SQL en Spark Optimizaci\u00f3n y Rendimiento 3. Arquitectura y Dise\u00f1o de Flujos ETL Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar flujos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables, seguros y resilientes, aplicando buenas pr\u00e1cticas y patrones modernos de ingenier\u00eda de datos. Utilizando herramientas como Apache Spark y Apache Airflow, podr\u00e1 conectar m\u00faltiples fuentes de datos, gestionar la evoluci\u00f3n de esquemas, monitorear el desempe\u00f1o de los pipelines y aplicar estrategias para asegurar la calidad, eficiencia y sostenibilidad de los procesos tanto en entornos locales como en la nube. Dise\u00f1o y Orquestaci\u00f3n de Pipelines ETL Conexi\u00f3n a M\u00faltiples Fuentes de Datos Procesamiento Escalable y Particionamiento Manejo de Esquemas y Calidad de Datos Monitorizaci\u00f3n y Troubleshooting de Pipelines Seguridad en ETL y Protecci\u00f3n de Datos Patrones de Dise\u00f1o y Optimizaci\u00f3n en la Nube 4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow DAGs, operadores y tareas Integraci\u00f3n con ecosistema Big Data Monitoreo y manejo de dependencias 5. Proyecto Integrador y Despliegue Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"Inicio"},{"location":"#metodos-de-procesamiento-y-analisis-de-big-data","text":"Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos.","title":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data"},{"location":"#requisitos-previos","text":"Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition","title":"Requisitos Previos"},{"location":"#1-introduccion","text":"Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark","title":"1. Introducci\u00f3n"},{"location":"#2-pyspark-y-sparksql","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Fundamentos de DataFrames en Spark Manipulaci\u00f3n y Transformaci\u00f3n de Datos Consultas y SQL en Spark Optimizaci\u00f3n y Rendimiento","title":"2. PySpark y SparkSQL"},{"location":"#3-arquitectura-y-diseno-de-flujos-etl","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar flujos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables, seguros y resilientes, aplicando buenas pr\u00e1cticas y patrones modernos de ingenier\u00eda de datos. Utilizando herramientas como Apache Spark y Apache Airflow, podr\u00e1 conectar m\u00faltiples fuentes de datos, gestionar la evoluci\u00f3n de esquemas, monitorear el desempe\u00f1o de los pipelines y aplicar estrategias para asegurar la calidad, eficiencia y sostenibilidad de los procesos tanto en entornos locales como en la nube. Dise\u00f1o y Orquestaci\u00f3n de Pipelines ETL Conexi\u00f3n a M\u00faltiples Fuentes de Datos Procesamiento Escalable y Particionamiento Manejo de Esquemas y Calidad de Datos Monitorizaci\u00f3n y Troubleshooting de Pipelines Seguridad en ETL y Protecci\u00f3n de Datos Patrones de Dise\u00f1o y Optimizaci\u00f3n en la Nube","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"#4-automatizacion-y-orquestacion-con-apache-airflow","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow DAGs, operadores y tareas Integraci\u00f3n con ecosistema Big Data Monitoreo y manejo de dependencias","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"#5-proyecto-integrador-y-despliegue","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"5. Proyecto Integrador y Despliegue"},{"location":"cluster_spark/","text":"\u2190 Volver al Inicio Spark en modo local El modo local es la configuraci\u00f3n m\u00e1s sencilla y se ejecuta en una sola m\u00e1quina sin distribuci\u00f3n de trabajo. Es ideal para desarrollo, pruebas y peque\u00f1os conjuntos de datos, ya que no requiere un entorno distribuido. Caracter\u00edsticas No depende de un gestor de cl\u00faster como YARN o Kubernetes . Puede usar uno o m\u00faltiples n\u00facleos en la m\u00e1quina local ( local[1] para un n\u00facleo, local[*] para todos los n\u00facleos disponibles). Se ejecuta sin necesidad de configurar un nodo maestro ( driver ), ni nodos trabajadores ( worker ). Es \u00fatil para depuraci\u00f3n y desarrollo r\u00e1pido. Ejecuci\u00f3n Descarga la imagen de Docker: docker pull quay.io/jupyter/pyspark-notebook Crea un volumen con el nombre work , para preservar el trabajo realizado con Jupyter: docker volume create work Ejecuta el contenedor en modo interactivo, iniciar\u00e1 Jupyter Notebook y lo expondr\u00e1 en el puerto 8888: docker run -it --rm -p 8888:8888 -v work:/home/jovyan/ quay.io/jupyter/pyspark-notebook Accede a Jupyter Notebook: Abre el link que muestra el LOG de docker al iniciar: http://localhost:8888/... Crea un nuevo notebook Python, e inicia una sesi\u00f3n en modo local con todos los n\u00facleos disponibles ( local[*] ): from pyspark.sql import SparkSession spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate() print(spark) Spark en modo cluster El modo cl\u00faster permite distribuir el procesamiento de datos entre m\u00faltiples m\u00e1quinas. Es esencial cuando se trabaja con grandes vol\u00famenes de datos que no pueden manejarse eficientemente en una sola m\u00e1quina. Caracter\u00edsticas Requiere un gestor de cl\u00faster como Spark Standalone , YARN , Mesos o Kubernetes . Puede escalar horizontalmente agregando m\u00e1s nodos al cl\u00faster. Requiere un nodo maestro ( driver ) que coordina el trabajo, y nodos trabajadores ( worker ) que ejecutan las tareas. Ideal para procesamiento distribuido de Big Data. Ejecuci\u00f3n Opci\u00f3n 1: usando Docker Compose (recomendada) Crea el archivo docker-compose.yml : version: '3' services: spark-master: image: quay.io/jupyter/pyspark-notebook container_name: spark-master ports: - \"7077:7077\" # puerto del nodo maestro - \"8080:8080\" # interfaz web del nodo maestro - \"8888:8888\" # interfaz web de Jupyter environment: - SPARK_MODE=master volumes: - ./data:/home/jovyan/data spark-worker-1: image: quay.io/jupyter/pyspark-notebook container_name: spark-worker-1 depends_on: - spark-master environment: - SPARK_MASTER_URL=spark://spark-master:7077 - SPARK_MODE=worker - SPARK_WORKER_CORES=1 - SPARK_WORKER_MEMORY=1G spark-worker-2: image: quay.io/jupyter/pyspark-notebook container_name: spark-worker-2 depends_on: - spark-master environment: - SPARK_MASTER_URL=spark://spark-master:7077 - SPARK_MODE=worker - SPARK_WORKER_CORES=1 - SPARK_WORKER_MEMORY=1G Ejecutar: docker-compose up -d Acceder a la interfaz Web del nodo maestro: http://localhost:8080 Acceder a Jupyter Notebook: http://localhost:8888 si solicita una contrase\u00f1a, jupyter Utilizando PySpark en el cuaderno: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .master(\"spark://spark-master:7077\") \\ .appName(\"test-cluster\") \\ .getOrCreate()","title":"Cluster spark"},{"location":"cluster_spark/#spark-en-modo-local","text":"El modo local es la configuraci\u00f3n m\u00e1s sencilla y se ejecuta en una sola m\u00e1quina sin distribuci\u00f3n de trabajo. Es ideal para desarrollo, pruebas y peque\u00f1os conjuntos de datos, ya que no requiere un entorno distribuido.","title":"Spark en modo local"},{"location":"cluster_spark/#caracteristicas","text":"No depende de un gestor de cl\u00faster como YARN o Kubernetes . Puede usar uno o m\u00faltiples n\u00facleos en la m\u00e1quina local ( local[1] para un n\u00facleo, local[*] para todos los n\u00facleos disponibles). Se ejecuta sin necesidad de configurar un nodo maestro ( driver ), ni nodos trabajadores ( worker ). Es \u00fatil para depuraci\u00f3n y desarrollo r\u00e1pido.","title":"Caracter\u00edsticas"},{"location":"cluster_spark/#ejecucion","text":"Descarga la imagen de Docker: docker pull quay.io/jupyter/pyspark-notebook Crea un volumen con el nombre work , para preservar el trabajo realizado con Jupyter: docker volume create work Ejecuta el contenedor en modo interactivo, iniciar\u00e1 Jupyter Notebook y lo expondr\u00e1 en el puerto 8888: docker run -it --rm -p 8888:8888 -v work:/home/jovyan/ quay.io/jupyter/pyspark-notebook Accede a Jupyter Notebook: Abre el link que muestra el LOG de docker al iniciar: http://localhost:8888/... Crea un nuevo notebook Python, e inicia una sesi\u00f3n en modo local con todos los n\u00facleos disponibles ( local[*] ): from pyspark.sql import SparkSession spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate() print(spark)","title":"Ejecuci\u00f3n"},{"location":"cluster_spark/#spark-en-modo-cluster","text":"El modo cl\u00faster permite distribuir el procesamiento de datos entre m\u00faltiples m\u00e1quinas. Es esencial cuando se trabaja con grandes vol\u00famenes de datos que no pueden manejarse eficientemente en una sola m\u00e1quina.","title":"Spark en modo cluster"},{"location":"cluster_spark/#caracteristicas_1","text":"Requiere un gestor de cl\u00faster como Spark Standalone , YARN , Mesos o Kubernetes . Puede escalar horizontalmente agregando m\u00e1s nodos al cl\u00faster. Requiere un nodo maestro ( driver ) que coordina el trabajo, y nodos trabajadores ( worker ) que ejecutan las tareas. Ideal para procesamiento distribuido de Big Data.","title":"Caracter\u00edsticas"},{"location":"cluster_spark/#ejecucion_1","text":"","title":"Ejecuci\u00f3n"},{"location":"cluster_spark/#opcion-1-usando-docker-compose-recomendada","text":"Crea el archivo docker-compose.yml : version: '3' services: spark-master: image: quay.io/jupyter/pyspark-notebook container_name: spark-master ports: - \"7077:7077\" # puerto del nodo maestro - \"8080:8080\" # interfaz web del nodo maestro - \"8888:8888\" # interfaz web de Jupyter environment: - SPARK_MODE=master volumes: - ./data:/home/jovyan/data spark-worker-1: image: quay.io/jupyter/pyspark-notebook container_name: spark-worker-1 depends_on: - spark-master environment: - SPARK_MASTER_URL=spark://spark-master:7077 - SPARK_MODE=worker - SPARK_WORKER_CORES=1 - SPARK_WORKER_MEMORY=1G spark-worker-2: image: quay.io/jupyter/pyspark-notebook container_name: spark-worker-2 depends_on: - spark-master environment: - SPARK_MASTER_URL=spark://spark-master:7077 - SPARK_MODE=worker - SPARK_WORKER_CORES=1 - SPARK_WORKER_MEMORY=1G Ejecutar: docker-compose up -d Acceder a la interfaz Web del nodo maestro: http://localhost:8080 Acceder a Jupyter Notebook: http://localhost:8888 si solicita una contrase\u00f1a, jupyter Utilizando PySpark en el cuaderno: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .master(\"spark://spark-master:7077\") \\ .appName(\"test-cluster\") \\ .getOrCreate()","title":"Opci\u00f3n 1: usando Docker Compose (recomendada)"},{"location":"ejemplo_json/","text":"Ejemplo JSON { \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_json/#ejemplo-json","text":"{ \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_log/","text":"Ejemplo LOG de acceso web 192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_log/#ejemplo-log-de-acceso-web","text":"192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_xml/","text":"Ejemplo XML <?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"ejemplo_xml/#ejemplo-xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"tema11/","text":"1. Introducci\u00f3n Tema 1.1. Fundamentos de Big Data Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados. 1.1.1 \u00bfQu\u00e9 es Big Data? Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales. 1.1.2 Las 5 V s del Big Data: Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data: Volumen Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad. Velocidad Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones. Variedad Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas. Veracidad Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados. Valor Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha. 1.1.3 Desaf\u00edos del Big Data: El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos. 1.1.4 Oportunidades del Big Data: A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros. 1.1.5 Tipos de Datos en Big Data: Datos Estructurados Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo . Datos Semi-estructurados Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor. Datos No Estructurados Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre. 1.1.6 Modelos de Procesamiento de Datos Procesamiento Batch (por Lotes) Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa. Procesamiento en Tiempo Real (Streaming) Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real. Procesamiento Interactivo (Ad-Hoc/Exploratorio) Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido. Tarea Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Fundamentos de Big Data"},{"location":"tema11/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema11/#tema-11-fundamentos-de-big-data","text":"Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados.","title":"Tema 1.1. Fundamentos de Big Data"},{"location":"tema11/#111-que-es-big-data","text":"Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales.","title":"1.1.1 \u00bfQu\u00e9 es Big Data?"},{"location":"tema11/#112-las-5-vs-del-big-data","text":"Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data:","title":"1.1.2 Las 5 Vs del Big Data:"},{"location":"tema11/#volumen","text":"Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad.","title":"Volumen"},{"location":"tema11/#velocidad","text":"Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones.","title":"Velocidad"},{"location":"tema11/#variedad","text":"Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas.","title":"Variedad"},{"location":"tema11/#veracidad","text":"Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados.","title":"Veracidad"},{"location":"tema11/#valor","text":"Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha.","title":"Valor"},{"location":"tema11/#113-desafios-del-big-data","text":"El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos.","title":"1.1.3 Desaf\u00edos del Big Data:"},{"location":"tema11/#114-oportunidades-del-big-data","text":"A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros.","title":"1.1.4 Oportunidades del Big Data:"},{"location":"tema11/#115-tipos-de-datos-en-big-data","text":"","title":"1.1.5 Tipos de Datos en Big Data:"},{"location":"tema11/#datos-estructurados","text":"Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo .","title":"Datos Estructurados"},{"location":"tema11/#datos-semi-estructurados","text":"Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor.","title":"Datos Semi-estructurados"},{"location":"tema11/#datos-no-estructurados","text":"Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre.","title":"Datos No Estructurados"},{"location":"tema11/#116-modelos-de-procesamiento-de-datos","text":"","title":"1.1.6 Modelos de Procesamiento de Datos"},{"location":"tema11/#procesamiento-batch-por-lotes","text":"Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa.","title":"Procesamiento Batch (por Lotes)"},{"location":"tema11/#procesamiento-en-tiempo-real-streaming","text":"Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real.","title":"Procesamiento en Tiempo Real (Streaming)"},{"location":"tema11/#procesamiento-interactivo-ad-hocexploratorio","text":"Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido.","title":"Procesamiento Interactivo (Ad-Hoc/Exploratorio)"},{"location":"tema11/#tarea","text":"Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Tarea"},{"location":"tema12/","text":"1. Introducci\u00f3n Tema 1.2 Introducci\u00f3n al ecosistema Spark Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo. 1.2.1 \u00bfQu\u00e9 es Apache Spark? Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos. Procesamiento en memoria Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria. Resilient Distributed Datasets (RDDs) Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave. 1.2.2 Componentes principales de Spark Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero. Spark Core Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia. Spark SQL Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional. Spark Streaming Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos. MLlib MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos. GraphX GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n. 1.2.3 Arquitectura de Spark La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva. Spark Driver El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones. Spark Executor Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos. Cluster Manager El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias. 1.2.4 Interacci\u00f3n con Spark La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente. Spark Shell El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande. Spark Submit spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL. Spark UI La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria. 1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas. Particionamiento de datos El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes. Persistencia de datos (Caching) La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido. Lazy Evaluation (Evaluaci\u00f3n Perezosa) La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia. 1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas. Spark vs. Hadoop MapReduce Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente. Tarea Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema12/#tema-12-introduccion-al-ecosistema-spark","text":"Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo.","title":"Tema 1.2 Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#121-que-es-apache-spark","text":"Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos.","title":"1.2.1 \u00bfQu\u00e9 es Apache Spark?"},{"location":"tema12/#procesamiento-en-memoria","text":"Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria.","title":"Procesamiento en memoria"},{"location":"tema12/#resilient-distributed-datasets-rdds","text":"Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave.","title":"Resilient Distributed Datasets (RDDs)"},{"location":"tema12/#122-componentes-principales-de-spark","text":"Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero.","title":"1.2.2 Componentes principales de Spark"},{"location":"tema12/#spark-core","text":"Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia.","title":"Spark Core"},{"location":"tema12/#spark-sql","text":"Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional.","title":"Spark SQL"},{"location":"tema12/#spark-streaming","text":"Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos.","title":"Spark Streaming"},{"location":"tema12/#mllib","text":"MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos.","title":"MLlib"},{"location":"tema12/#graphx","text":"GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n.","title":"GraphX"},{"location":"tema12/#123-arquitectura-de-spark","text":"La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva.","title":"1.2.3 Arquitectura de Spark"},{"location":"tema12/#spark-driver","text":"El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones.","title":"Spark Driver"},{"location":"tema12/#spark-executor","text":"Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos.","title":"Spark Executor"},{"location":"tema12/#cluster-manager","text":"El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias.","title":"Cluster Manager"},{"location":"tema12/#124-interaccion-con-spark","text":"La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente.","title":"1.2.4 Interacci\u00f3n con Spark"},{"location":"tema12/#spark-shell","text":"El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande.","title":"Spark Shell"},{"location":"tema12/#spark-submit","text":"spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL.","title":"Spark Submit"},{"location":"tema12/#spark-ui","text":"La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria.","title":"Spark UI"},{"location":"tema12/#125-conceptos-fundamentales-de-procesamiento-distribuido-en-spark","text":"El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas.","title":"1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark"},{"location":"tema12/#particionamiento-de-datos","text":"El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes.","title":"Particionamiento de datos"},{"location":"tema12/#persistencia-de-datos-caching","text":"La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido.","title":"Persistencia de datos (Caching)"},{"location":"tema12/#lazy-evaluation-evaluacion-perezosa","text":"La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia.","title":"Lazy Evaluation (Evaluaci\u00f3n Perezosa)"},{"location":"tema12/#126-comparacion-de-spark-con-otras-herramientas-big-data","text":"Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas.","title":"1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data"},{"location":"tema12/#spark-vs-hadoop-mapreduce","text":"Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente.","title":"Spark vs. Hadoop MapReduce"},{"location":"tema12/#tarea","text":"Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Tarea"},{"location":"tema13/","text":"1. Introducci\u00f3n Tema 1.3 RDD, DataFrame y Dataset Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas. 1.3.1 RDD (Resilient Distributed Datasets) Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje. Caracter\u00edsticas clave de los RDDs Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark. Operaciones de transformaci\u00f3n Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo. Operaciones de acci\u00f3n Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido. 1.3.2 DataFrame Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados. Ventajas sobre los RDDs Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos. Creaci\u00f3n y manipulaci\u00f3n de DataFrames Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) . 1.3.3 Dataset El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark. Seguridad de tipos (Type-safety) La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador. Encoders Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado. 1.3.4 Comparaci\u00f3n y Casos de Uso La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n. Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir. Tarea Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"RDD, DataFrame y Dataset"},{"location":"tema13/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema13/#tema-13-rdd-dataframe-y-dataset","text":"Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas.","title":"Tema 1.3 RDD, DataFrame y Dataset"},{"location":"tema13/#131-rdd-resilient-distributed-datasets","text":"Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje.","title":"1.3.1 RDD (Resilient Distributed Datasets)"},{"location":"tema13/#caracteristicas-clave-de-los-rdds","text":"Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark.","title":"Caracter\u00edsticas clave de los RDDs"},{"location":"tema13/#operaciones-de-transformacion","text":"Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo.","title":"Operaciones de transformaci\u00f3n"},{"location":"tema13/#operaciones-de-accion","text":"Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido.","title":"Operaciones de acci\u00f3n"},{"location":"tema13/#132-dataframe","text":"Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados.","title":"1.3.2 DataFrame"},{"location":"tema13/#ventajas-sobre-los-rdds","text":"Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos.","title":"Ventajas sobre los RDDs"},{"location":"tema13/#creacion-y-manipulacion-de-dataframes","text":"Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) .","title":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames"},{"location":"tema13/#133-dataset","text":"El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark.","title":"1.3.3 Dataset"},{"location":"tema13/#seguridad-de-tipos-type-safety","text":"La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador.","title":"Seguridad de tipos (Type-safety)"},{"location":"tema13/#encoders","text":"Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado.","title":"Encoders"},{"location":"tema13/#134-comparacion-y-casos-de-uso","text":"La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n.","title":"1.3.4 Comparaci\u00f3n y Casos de Uso"},{"location":"tema13/#tabla-comparativa-detallada-rdd-vs-dataframe-vs-dataset","text":"Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir.","title":"Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset"},{"location":"tema13/#tarea","text":"Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"Tarea"},{"location":"tema14/","text":"1. Introducci\u00f3n Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente. 1.4.1 Requisitos previos para la instalaci\u00f3n Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo. Java Development Kit (JDK) Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n. Python (para PySpark) Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior. Apache Hadoop (opcional, para HDFS y YARN) Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop). 1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento. Descarga de Spark El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/ Configuraci\u00f3n del modo Standalone El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado. Configuraci\u00f3n con YARN (Yet Another Resource Negotiator) YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos. Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo) La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental. 1.4.3 Spark en entornos de nube (AWS, Azure, GCP) La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos. Servicios gestionados de Spark en la nube Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres. Ventajas de los servicios gestionados Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones. Configuraci\u00f3n de acceso a datos en la nube Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP. 1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas) Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows. Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark) Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal. Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs) Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888 Tarea Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema14/#tema-14-instalacion-y-configuracion-de-spark","text":"Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente.","title":"Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#141-requisitos-previos-para-la-instalacion","text":"Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo.","title":"1.4.1 Requisitos previos para la instalaci\u00f3n"},{"location":"tema14/#java-development-kit-jdk","text":"Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n.","title":"Java Development Kit (JDK)"},{"location":"tema14/#python-para-pyspark","text":"Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior.","title":"Python (para PySpark)"},{"location":"tema14/#apache-hadoop-opcional-para-hdfs-y-yarn","text":"Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop).","title":"Apache Hadoop (opcional, para HDFS y YARN)"},{"location":"tema14/#142-instalacion-de-un-cluster-spark-on-premise","text":"Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento.","title":"1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise"},{"location":"tema14/#descarga-de-spark","text":"El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/","title":"Descarga de Spark"},{"location":"tema14/#configuracion-del-modo-standalone","text":"El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado.","title":"Configuraci\u00f3n del modo Standalone"},{"location":"tema14/#configuracion-con-yarn-yet-another-resource-negotiator","text":"YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos.","title":"Configuraci\u00f3n con YARN (Yet Another Resource Negotiator)"},{"location":"tema14/#optimizacion-de-la-configuracion-memoria-cores-paralelismo","text":"La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental.","title":"Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo)"},{"location":"tema14/#143-spark-en-entornos-de-nube-aws-azure-gcp","text":"La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos.","title":"1.4.3 Spark en entornos de nube (AWS, Azure, GCP)"},{"location":"tema14/#servicios-gestionados-de-spark-en-la-nube","text":"Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres.","title":"Servicios gestionados de Spark en la nube"},{"location":"tema14/#ventajas-de-los-servicios-gestionados","text":"Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones.","title":"Ventajas de los servicios gestionados"},{"location":"tema14/#configuracion-de-acceso-a-datos-en-la-nube","text":"Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP.","title":"Configuraci\u00f3n de acceso a datos en la nube"},{"location":"tema14/#144-instalacion-de-spark-en-windows-para-practicas","text":"Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows.","title":"1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas)"},{"location":"tema14/#uso-de-imagenes-docker-para-spark-ej-bitnamispark","text":"Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal.","title":"Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark)"},{"location":"tema14/#configuracion-de-entorno-de-desarrollo-jupyter-ides","text":"Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888","title":"Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs)"},{"location":"tema14/#tarea","text":"Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Tarea"},{"location":"tema15/","text":"1. Introducci\u00f3n Tema 1.5 Primeros pasos con PySpark Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados. 1.5.1 Entorno de desarrollo para PySpark Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE. Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv) Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows). Instalaci\u00f3n de PySpark ( pip install pyspark ) Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa. Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm) Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente. Acceso a la Spark UI La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores. 1.5.2 Inicializaci\u00f3n de SparkSession La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz. El papel de SparkSession SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL). Creaci\u00f3n de una SparkSession La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts. 1.5.3 Carga de datos con PySpark Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos. Lectura de archivos CSV (inferSchema, header, delimiter) El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema) Lectura de archivos JSON Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea. 1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames. Visualizaci\u00f3n de datos ( show() , printSchema() , describe() ) Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50 Selecci\u00f3n de columnas ( select() ) La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show() Filtrado de filas ( filter() / where() ) Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show() 1.5.5 Operaciones comunes de transformaci\u00f3n Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos. Renombrar y eliminar columnas ( withColumnRenamed() , drop() ) Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show() A\u00f1adir y modificar columnas ( withColumn() ) El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show() Operaciones de agregaci\u00f3n ( groupBy() , agg() , sum() , avg() , min() , max() ) Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show() 1.5.6 Escritura de datos con PySpark Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes. Guardar DataFrames en formato CSV, JSON, Parquet PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\") Modos de escritura (append, overwrite, ignore, errorIfExists) PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists Particionamiento de la salida ( partitionBy() ) El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True) Tarea Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Primeros pasos con PySpark"},{"location":"tema15/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema15/#tema-15-primeros-pasos-con-pyspark","text":"Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados.","title":"Tema 1.5 Primeros pasos con PySpark"},{"location":"tema15/#151-entorno-de-desarrollo-para-pyspark","text":"Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE.","title":"1.5.1 Entorno de desarrollo para PySpark"},{"location":"tema15/#configuracion-de-un-entorno-python-anacondaminiconda-virtualenv","text":"Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows).","title":"Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv)"},{"location":"tema15/#instalacion-de-pyspark-pip-install-pyspark","text":"Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa.","title":"Instalaci\u00f3n de PySpark (pip install pyspark)"},{"location":"tema15/#integracion-con-jupyter-notebooks-o-ides-vs-code-pycharm","text":"Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente.","title":"Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm)"},{"location":"tema15/#acceso-a-la-spark-ui","text":"La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores.","title":"Acceso a la Spark UI"},{"location":"tema15/#152-inicializacion-de-sparksession","text":"La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz.","title":"1.5.2 Inicializaci\u00f3n de SparkSession"},{"location":"tema15/#el-papel-de-sparksession","text":"SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL).","title":"El papel de SparkSession"},{"location":"tema15/#creacion-de-una-sparksession","text":"La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts.","title":"Creaci\u00f3n de una SparkSession"},{"location":"tema15/#153-carga-de-datos-con-pyspark","text":"Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos.","title":"1.5.3 Carga de datos con PySpark"},{"location":"tema15/#lectura-de-archivos-csv-inferschema-header-delimiter","text":"El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema)","title":"Lectura de archivos CSV (inferSchema, header, delimiter)"},{"location":"tema15/#lectura-de-archivos-json","text":"Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea.","title":"Lectura de archivos JSON"},{"location":"tema15/#154-exploracion-basica-de-dataframes","text":"Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames.","title":"1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames"},{"location":"tema15/#visualizacion-de-datos-show-printschema-describe","text":"Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50","title":"Visualizaci\u00f3n de datos (show(), printSchema(), describe())"},{"location":"tema15/#seleccion-de-columnas-select","text":"La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show()","title":"Selecci\u00f3n de columnas (select())"},{"location":"tema15/#filtrado-de-filas-filter-where","text":"Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show()","title":"Filtrado de filas (filter() / where())"},{"location":"tema15/#155-operaciones-comunes-de-transformacion","text":"Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos.","title":"1.5.5 Operaciones comunes de transformaci\u00f3n"},{"location":"tema15/#renombrar-y-eliminar-columnas-withcolumnrenamed-drop","text":"Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show()","title":"Renombrar y eliminar columnas (withColumnRenamed(), drop())"},{"location":"tema15/#anadir-y-modificar-columnas-withcolumn","text":"El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show()","title":"A\u00f1adir y modificar columnas (withColumn())"},{"location":"tema15/#operaciones-de-agregacion-groupby-agg-sum-avg-min-max","text":"Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show()","title":"Operaciones de agregaci\u00f3n (groupBy(), agg(), sum(), avg(), min(), max())"},{"location":"tema15/#156-escritura-de-datos-con-pyspark","text":"Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes.","title":"1.5.6 Escritura de datos con PySpark"},{"location":"tema15/#guardar-dataframes-en-formato-csv-json-parquet","text":"PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\")","title":"Guardar DataFrames en formato CSV, JSON, Parquet"},{"location":"tema15/#modos-de-escritura-append-overwrite-ignore-errorifexists","text":"PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists","title":"Modos de escritura (append, overwrite, ignore, errorIfExists)"},{"location":"tema15/#particionamiento-de-la-salida-partitionby","text":"El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True)","title":"Particionamiento de la salida (partitionBy())"},{"location":"tema15/#tarea","text":"Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Tarea"},{"location":"tema21/","text":"2. PySpark y SparkSQL Tema 2.1 Fundamentos de DataFrames en Spark Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender la estructura y el funcionamiento de los DataFrames de Apache Spark, realizar operaciones fundamentales de manipulaci\u00f3n de datos, gestionar esquemas y tipos de datos complejos, y leer y escribir datos en los formatos m\u00e1s comunes utilizados en entornos de Big Data. Introducci\u00f3n : En el vasto universo del Big Data, la capacidad de procesar y analizar vol\u00famenes masivos de informaci\u00f3n es crucial. Apache Spark, con su motor de procesamiento distribuido, se ha consolidado como una herramienta indispensable para esta tarea. En el coraz\u00f3n de su eficiencia y facilidad de uso se encuentran los DataFrames, una abstracci\u00f3n de datos distribuida que organiza los datos en columnas con nombre, similar a una tabla en una base de datos relacional o una hoja de c\u00e1lculo. Esta estructura permite a los desarrolladores trabajar con datos de forma intuitiva y optimizada, aprovechando el poder de Spark para el procesamiento paralelo y distribuido. Desarrollo : Este tema se centrar\u00e1 en los pilares de la manipulaci\u00f3n de datos en Spark a trav\u00e9s de los DataFrames. Exploraremos c\u00f3mo los DataFrames facilitan las operaciones de transformaci\u00f3n y consulta, abstraen la complejidad de la distribuci\u00f3n de datos y proporcionan un API robusto para interactuar con ellos. Abordaremos desde las operaciones b\u00e1sicas como selecci\u00f3n y filtrado, hasta la comprensi\u00f3n de los esquemas y tipos de datos complejos, y la interacci\u00f3n con una variedad de formatos de archivo est\u00e1ndar de la industria. 2.1.1 Operaciones con DataFrames Las operaciones con DataFrames en Spark son el n\u00facleo de la manipulaci\u00f3n de datos, permitiendo seleccionar, filtrar, agregar, unir y realizar una multitud de transformaciones sobre conjuntos de datos distribuidos de manera eficiente. A diferencia de los RDDs, los DataFrames ofrecen un nivel de abstracci\u00f3n superior, permitiendo a Spark optimizar internamente las operaciones gracias a la informaci\u00f3n del esquema. Creaci\u00f3n de DataFrames La creaci\u00f3n de DataFrames es el primer paso para trabajar con datos en PySpark. Se pueden crear a partir de diversas fuentes, como listas de Python, RDDs existentes, o leyendo directamente desde archivos. Desde una lista de tuplas o diccionarios: from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() # Opci\u00f3n 1: Usando una lista de tuplas y definiendo el esquema data_tuples = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] schema_tuples = StructType([ StructField(\"name\", StringType(), True), StructField(\"id\", IntegerType(), True), StructField(\"city\", StringType(), True) ]) df_from_tuples = spark.createDataFrame(data_tuples, schema=schema_tuples) df_from_tuples.show() # Resultado: # +-------+---+----+ # | name| id|city| # +-------+---+----+ # | Alice| 1| NY| # | Bob| 2| LA| # |Charlie| 3| CHI| # +-------+---+----+ # Opci\u00f3n 2: Usando una lista de diccionarios (Spark infiere el esquema) data_dicts = [{\"name\": \"Alice\", \"id\": 1, \"city\": \"NY\"}, {\"name\": \"Bob\", \"id\": 2, \"city\": \"LA\"}, {\"name\": \"Charlie\", \"id\": 3, \"city\": \"CHI\"}] df_from_dicts = spark.createDataFrame(data_dicts) df_from_dicts.show() # Resultado: # +-------+---+----+ # | city| id|name| # +-------+---+----+ # | NY| 1|Alice| # | LA| 2| Bob| # | CHI| 3|Charlie| # +-------+---+----+ Desde un RDD existente: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]) df_from_rdd = spark.createDataFrame(rdd, [\"name\", \"id\"]) df_from_rdd.show() # Resultado: # +-------+---+ # | name| id| # +-------+---+ # | Alice| 1| # | Bob| 2| # |Charlie| 3| # +-------+---+ Desde un archivo (se ver\u00e1 en 2.1.3): # df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True) # df.show() Transformaciones de DataFrames Las transformaciones en DataFrames son operaciones lazy (perezosas), lo que significa que no se ejecutan hasta que se invoca una acci\u00f3n. Esto permite a Spark optimizar el plan de ejecuci\u00f3n. Selecci\u00f3n de columnas ( select y withColumn ): from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate() data = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"id\", \"city\"]) # Seleccionar columnas espec\u00edficas df.select(\"name\", \"city\").show() # Resultado: # +-------+----+ # | name|city| # +-------+----+ # | Alice| NY| # | Bob| LA| # |Charlie| CHI| # +-------+----+ # Renombrar una columna al seleccionar df.select(col(\"name\").alias(\"full_name\"), \"city\").show() # Resultado: # +---------+----+ # |full_name|city| # +---------+----+ # | Alice| NY| # | Bob| LA| # | Charlie| CHI| # +---------+----+ # A\u00f1adir una nueva columna df.withColumn(\"country\", lit(\"USA\")).show() # Resultado: # +-------+---+----+-------+ # | name| id|city|country| # +-------+---+----+-------+ # | Alice| 1| NY| USA| # | Bob| 2| LA| USA| # |Charlie| 3| CHI| USA| # +-------+---+----+-------+ # Modificar una columna existente (ejemplo: incrementar id) df.withColumn(\"id_plus_10\", col(\"id\") + 10).show() # Resultado: # +-------+---+----+----------+ # | name| id|city|id_plus_10| # +-------+---+----+----------+ # | Alice| 1| NY| 11| # | Bob| 2| LA| 12| # |Charlie| 3| CHI| 13| # +-------+---+----+----------+ Filtrado de filas ( filter o where ): from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"DataFrameFiltering\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\"), (\"David\", 35, \"NY\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Filtrar por una condici\u00f3n simple df.filter(col(\"age\") > 25).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # | Bob| 30| LA| # |David| 35| NY| # +-----+---+----+ # Filtrar por m\u00faltiples condiciones df.filter((col(\"age\") > 20) & (col(\"city\") == \"NY\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # |David| 35| NY| # +-----+---+----+ # Usando el m\u00e9todo where (alias de filter) df.where(col(\"name\").like(\"A%\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # +-----+---+----+ Agregaciones ( groupBy y funciones de agregaci\u00f3n): from pyspark.sql import SparkSession from pyspark.sql.functions import avg, count, sum, min, max spark = SparkSession.builder.appName(\"DataFrameAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000), (\"Dept1\", \"Bob\", 1200), (\"Dept2\", \"Charlie\", 900), (\"Dept2\", \"David\", 1500)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"]) # Contar empleados por departamento df.groupBy(\"department\").count().show() # Resultado: # +----------+-----+ # |department|count| # +----------+-----+ # | Dept1| 2| # | Dept2| 2| # +----------+-----+ # Calcular salario promedio y m\u00e1ximo por departamento df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"), max(\"salary\").alias(\"max_salary\")).show() # Resultado: # +----------+----------+----------+ # |department|avg_salary|max_salary| # +----------+----------+----------+ # | Dept1| 1100.0| 1200| # | Dept2| 1200.0| 1500| # +----------+----------+----------+ # Sumar salarios por departamento df.groupBy(\"department\").agg(sum(\"salary\").alias(\"total_salary\")).show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | Dept1| 2200| # | Dept2| 2400| # +----------+------------+ Acciones de DataFrames Las acciones son operaciones que disparan la ejecuci\u00f3n del plan de transformaciones y devuelven un resultado a la aplicaci\u00f3n del controlador o escriben datos en un sistema de almacenamiento. Mostrar datos ( show ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Mostrar las primeras filas del DataFrame df.show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ # Mostrar un n\u00famero espec\u00edfico de filas y truncar el contenido de las columnas si es largo df.show(numRows=1, truncate=False) # Resultado: # +-----+---+ # |name |age| # +-----+---+ # |Alice|25 | # +-----+---+ Contar filas ( count ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Contar el n\u00famero total de filas en el DataFrame num_rows = df.count() print(f\"N\u00famero de filas: {num_rows}\") # Resultado: N\u00famero de filas: 3 Recopilar datos ( collect , take , first ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Recopilar todos los datos del DataFrame en una lista de Rows en el driver all_data = df.collect() print(f\"Todos los datos: {all_data}\") # Resultado: Todos los datos: [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=22)] # Tomar las primeras N filas first_two = df.take(2) print(f\"Primeras 2 filas: {first_two}\") # Resultado: Primeras 2 filas: [Row(name='Alice', age=25), Row(name='Bob', age=30)] # Obtener la primera fila first_row = df.first() print(f\"Primera fila: {first_row}\") # Resultado: Primera fila: Row(name='Alice', age=25) 2.1.2 Esquemas y tipos de datos complejos El esquema de un DataFrame es una estructura fundamental que define los nombres de las columnas y sus tipos de datos correspondientes. Esta metadata es crucial para la optimizaci\u00f3n de Spark, ya que le permite saber c\u00f3mo se organizan los datos y aplicar optimizaciones de tipo de datos y de columna. La inferencia de esquema y la definici\u00f3n expl\u00edcita son dos formas de manejarlo, y Spark tambi\u00e9n soporta tipos de datos complejos como ArrayType , MapType y StructType para manejar estructuras anidadas. Inferencia y Definici\u00f3n Expl\u00edcita de Esquemas La forma en que Spark determina el esquema de un DataFrame es vital para la integridad y eficiencia del procesamiento de datos. Inferencia de esquema ( inferSchema=True ): Spark puede intentar adivinar el esquema de un archivo de datos (CSV, JSON, Parquet, etc.) leyendo una muestra. Esto es conveniente para la exploraci\u00f3n inicial, pero puede ser propenso a errores, especialmente con datos inconsistentes o tipos ambiguos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SchemaInference\").getOrCreate() # Creando un archivo CSV de ejemplo data_csv = \"\"\"name,age,city Alice,25,NY Bob,30,LA Charlie,null,CHI David,35,NY\"\"\" with open(\"data.csv\", \"w\") as f: f.write(data_csv) # Inferencia de esquema al leer un CSV df_inferred = spark.read.csv(\"data.csv\", header=True, inferSchema=True) df_inferred.printSchema() # Resultado (ejemplo): # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # Observar que \"age\" se infiri\u00f3 como IntegerType, lo cual es correcto si no hay valores no num\u00e9ricos. # Si hubiera un valor no num\u00e9rico, podr\u00eda inferirse como StringType o fallar la inferencia. Definici\u00f3n expl\u00edcita de esquema ( StructType y StructField ): Es la forma m\u00e1s robusta y recomendada para entornos de producci\u00f3n. Permite controlar con precisi\u00f3n los tipos de datos y la nulabilidad, evitando problemas de inferencia y mejorando el rendimiento. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType spark = SparkSession.builder.appName(\"ExplicitSchema\").getOrCreate() # Definir un esquema expl\u00edcito custom_schema = StructType([ StructField(\"employee_name\", StringType(), True), StructField(\"employee_id\", IntegerType(), False), # Not nullable StructField(\"salary\", DoubleType(), True), StructField(\"is_active\", BooleanType(), True) ]) data = [(\"Alice\", 1, 50000.0, True), (\"Bob\", 2, 60000.50, False), (\"Charlie\", 3, 75000.0, True)] df_explicit = spark.createDataFrame(data, schema=custom_schema) df_explicit.printSchema() # Resultado: # root # |-- employee_name: string (nullable = true) # |-- employee_id: integer (nullable = false) # |-- salary: double (nullable = true) # |-- is_active: boolean (nullable = true) # Intentar insertar un valor nulo en una columna no nula causar\u00eda un error o comportamiento inesperado # data_error = [(\"David\", None, 80000.0, True)] # Esto generar\u00eda un error si intentas crear el DF # df_error = spark.createDataFrame(data_error, schema=custom_schema) Acceder y manipular el esquema ( df.schema ): El esquema de un DataFrame es accesible a trav\u00e9s del atributo .schema , que devuelve un objeto StructType . from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"SchemaAccess\").getOrCreate() data = [(\"Alice\", 1), (\"Bob\", 2)] df = spark.createDataFrame(data, [\"name\", \"id\"]) # Acceder al esquema print(df.schema) # Resultado: StructType([StructField('name', StringType(), True), StructField('id', LongType(), True)]) # Iterar sobre los campos del esquema for field in df.schema: print(f\"Nombre de columna: {field.name}, Tipo: {field.dataType}, Nulable: {field.nullable}\") # Resultado: # Nombre de columna: name, Tipo: StringType, Nulable: True # Nombre de columna: id, Tipo: LongType, Nulable: True Tipos de Datos Complejos Spark permite manejar estructuras de datos m\u00e1s all\u00e1 de los tipos at\u00f3micos, lo que es fundamental para trabajar con datos semi-estructurados y anidados como JSON. StructType (Estructuras Anidadas/Registros): Representa una estructura similar a un objeto o un registro, donde cada campo tiene un nombre y un tipo de datos. Permite modelar objetos complejos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypes\").getOrCreate() # Definir un esquema con un StructType anidado address_schema = StructType([ StructField(\"street\", StringType(), True), StructField(\"city\", StringType(), True), StructField(\"zip\", StringType(), True) ]) person_schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"address\", address_schema, True) # Columna de tipo StructType ]) data = [ (\"Alice\", 25, (\"123 Main St\", \"NY\", \"10001\")), (\"Bob\", 30, (\"456 Oak Ave\", \"LA\", \"90001\")) ] df = spark.createDataFrame(data, schema=person_schema) df.printSchema() # Resultado: # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- address: struct (nullable = true) # | |-- street: string (nullable = true) # | |-- city: string (nullable = true) # | |-- zip: string (nullable = true) df.show(truncate=False) # Resultado: # +-----+---+-------------------------+ # |name |age|address | # +-----+---+-------------------------+ # |Alice|25 |{123 Main St, NY, 10001} | # |Bob |30 |{456 Oak Ave, LA, 90001} | # +-----+---+-------------------------+ # Acceder a campos anidados df.select(\"name\", \"address.city\").show() # Resultado: # +-----+----+ # |name |city| # +-----+----+ # |Alice|NY | # |Bob |LA | # +-----+----+ ArrayType (Arrays/Listas): Representa una colecci\u00f3n de elementos del mismo tipo. \u00datil para modelar listas o arreglos de datos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypesArray\").getOrCreate() # Definir un esquema con un ArrayType course_schema = StructType([ StructField(\"student_name\", StringType(), True), StructField(\"grades\", ArrayType(IntegerType()), True) # Columna de tipo ArrayType ]) data = [ (\"Alice\", [90, 85, 92]), (\"Bob\", [78, 80]), (\"Charlie\", []) ] df = spark.createDataFrame(data, schema=course_schema) df.printSchema() # Resultado: # root # |-- student_name: string (nullable = true) # |-- grades: array (nullable = true) # | |-- element: integer (containsNull = true) df.show(truncate=False) # Resultado: # +------------+----------+ # |student_name|grades | # +------------+----------+ # |Alice |[90, 85, 92]| # |Bob |[78, 80] | # |Charlie |[] | # +------------+----------+ # Acceder a elementos de array (requiere funciones de Spark) from pyspark.sql.functions import size, array_contains df.select(\"student_name\", size(\"grades\").alias(\"num_grades\")).show() # Resultado: # +------------+----------+ # |student_name|num_grades| # +------------+----------+ # | Alice| 3| # | Bob| 2| # | Charlie| 0| # +------------+----------+ df.filter(array_contains(\"grades\", 90)).show() # Resultado: # +------------+----------+ # |student_name| grades| # +------------+----------+ # | Alice|[90, 85, 92]| # +------------+----------+ MapType (Mapas/Diccionarios): Representa una colecci\u00f3n de pares clave-valor. \u00datil para datos que se asemejan a diccionarios o JSON con claves din\u00e1micas. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, MapType spark = SparkSession.builder.appName(\"ComplexTypesMap\").getOrCreate() # Definir un esquema con un MapType product_schema = StructType([ StructField(\"product_id\", StringType(), True), StructField(\"attributes\", MapType(StringType(), StringType()), True) # Columna de tipo MapType ]) data = [ (\"P101\", {\"color\": \"red\", \"size\": \"M\", \"material\": \"cotton\"}), (\"P102\", {\"color\": \"blue\", \"size\": \"L\"}), (\"P103\", {}) ] df = spark.createDataFrame(data, schema=product_schema) df.printSchema() # Resultado: # root # |-- product_id: string (nullable = true) # |-- attributes: map (nullable = true) # | |-- key: string # | |-- value: string (containsNull = true) df.show(truncate=False) # Resultado: # +----------+-----------------------------------+ # |product_id|attributes | # +----------+-----------------------------------+ # |P101 |{color -> red, size -> M, material -> cotton}| # |P102 |{color -> blue, size -> L} | # |P103 |{} | # +----------+-----------------------------------+ # Acceder a elementos de mapa (se usa con `getItem` o notaci\u00f3n de corchetes) from pyspark.sql.functions import col df.select(\"product_id\", col(\"attributes\")[\"color\"].alias(\"product_color\")).show() # Resultado: # +----------+-------------+ # |product_id|product_color| # +----------+-------------+ # | P101| red| # | P102| blue| # | P103| null| # +----------+-------------+ 2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON) Spark es vers\u00e1til en la lectura y escritura de datos, soportando una amplia gama de formatos de archivo. La elecci\u00f3n del formato adecuado es crucial para el rendimiento y la eficiencia del almacenamiento en entornos de Big Data. Los formatos columnares como Parquet y ORC son altamente recomendados para el an\u00e1lisis debido a su eficiencia en la lectura y compresi\u00f3n. Lectura de Datos La lectura de datos es la base para cualquier an\u00e1lisis. Spark proporciona un API spark.read muy flexible para cargar datos desde diversas fuentes. Lectura de archivos CSV: Ideal para datos tabulares simples. Es importante configurar header=True si el archivo tiene encabezados y inferSchema=True para que Spark intente adivinar los tipos de datos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteCSV\").getOrCreate() # Crear un archivo CSV de ejemplo data_csv = \"\"\"id,name,age,city 1,Alice,25,New York 2,Bob,30,Los Angeles 3,Charlie,22,Chicago\"\"\" with open(\"users.csv\", \"w\") as f: f.write(data_csv) # Leer un archivo CSV df_csv = spark.read.csv(\"users.csv\", header=True, inferSchema=True) df_csv.printSchema() df_csv.show() # Resultado: # root # |-- id: integer (nullable = true) # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # +---+-------+---+----------+ # | id| name|age| city| # +---+-------+---+----------+ # | 1| Alice| 25| New York| # | 2| Bob| 30|Los Angeles| # | 3|Charlie| 22| Chicago| # +---+-------+---+----------+ Lectura de archivos JSON: \u00datil para datos semi-estructurados. Spark puede inferir el esquema autom\u00e1ticamente. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteJSON\").getOrCreate() # Crear un archivo JSON de ejemplo data_json = \"\"\" {\"id\": 1, \"name\": \"Alice\", \"hobbies\": [\"reading\", \"hiking\"]} {\"id\": 2, \"name\": \"Bob\", \"hobbies\": [\"gaming\"]} {\"id\": 3, \"name\": \"Charlie\", \"hobbies\": []} \"\"\" with open(\"users.json\", \"w\") as f: f.write(data_json) # Leer un archivo JSON df_json = spark.read.json(\"users.json\") df_json.printSchema() df_json.show(truncate=False) # Resultado: # root # |-- hobbies: array (nullable = true) # | |-- element: string (containsNull = true) # |-- id: long (nullable = true) # |-- name: string (nullable = true) # +--------------------+---+-------+ # |hobbies |id |name | # +--------------------+---+-------+ # |[reading, hiking] |1 |Alice | # |[gaming] |2 |Bob | # |[] |3 |Charlie| # +--------------------+---+-------+ Lectura de archivos Parquet: Formato columnar altamente eficiente para Big Data. Spark lo usa como formato por defecto y es altamente optimizado para el rendimiento. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteParquet\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Parquet data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.write.mode(\"overwrite\").parquet(\"users.parquet\") # Leer un archivo Parquet df_parquet = spark.read.parquet(\"users.parquet\") df_parquet.printSchema() df_parquet.show() # Resultado: # root # |-- name: string (nullable = true) # |-- age: long (nullable = true) # |-- city: string (nullable = true) # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ Lectura de archivos ORC: Otro formato columnar optimizado para Big Data, desarrollado por Apache Hive. Ofrece compresi\u00f3n y rendimiento similares a Parquet. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteORC\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como ORC data = [(\"ProductA\", 100, \"Electronics\"), (\"ProductB\", 50, \"Books\")] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\"]) df.write.mode(\"overwrite\").orc(\"products.orc\") # Leer un archivo ORC df_orc = spark.read.orc(\"products.orc\") df_orc.printSchema() df_orc.show() # Resultado: # root # |-- product_name: string (nullable = true) # |-- price: long (nullable = true) # |-- category: string (nullable = true) # +------------+-----+-----------+ # |product_name|price| category| # +------------+-----+-----------+ # | ProductA| 100|Electronics| # | ProductB| 50| Books| # +------------+-----+-----------+ Lectura de archivos Avro: Formato de serializaci\u00f3n de datos basado en esquema. Requiere el paquete spark-avro . from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteAvro\") \\ .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\ .getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Avro data = [(\"Event1\", \"typeA\", 1678886400), (\"Event2\", \"typeB\", 1678886460)] df = spark.createDataFrame(data, [\"event_id\", \"event_type\", \"timestamp\"]) df.write.mode(\"overwrite\").format(\"avro\").save(\"events.avro\") # Leer un archivo Avro df_avro = spark.read.format(\"avro\").load(\"events.avro\") df_avro.printSchema() df_avro.show() # Resultado: # root # |-- event_id: string (nullable = true) # |-- event_type: string (nullable = true) # |-- timestamp: long (nullable = true) # +--------+----------+----------+ # |event_id|event_type| timestamp| # +--------+----------+----------+ # | Event1| typeA|1678886400| # | Event2| typeB|1678886460| # +--------+----------+----------+ Escritura de Datos La escritura de DataFrames a diferentes formatos es tan importante como su lectura, ya que permite persistir los resultados de las transformaciones y compartirlos con otras aplicaciones o sistemas. Modos de escritura ( mode ): Cuando se escribe un DataFrame, es fundamental especificar el modo de escritura para evitar p\u00e9rdidas de datos o errores. overwrite : Sobrescribe los datos existentes en la ubicaci\u00f3n de destino. append : A\u00f1ade los datos al final de los datos existentes. ignore : Si los datos ya existen, la operaci\u00f3n de escritura no hace nada. error (o errorIfExists ): Lanza un error si los datos ya existen (modo por defecto). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WriteModes\").getOrCreate() data = [(\"A\", 1), (\"B\", 2)] df = spark.createDataFrame(data, [\"col1\", \"col2\"]) # Escribir en modo 'overwrite' df.write.mode(\"overwrite\").parquet(\"output_data.parquet\") # Escribir en modo 'append' data_new = [(\"C\", 3)] df_new = spark.createDataFrame(data_new, [\"col1\", \"col2\"]) df_new.write.mode(\"append\").parquet(\"output_data.parquet\") # Verificar el contenido spark.read.parquet(\"output_data.parquet\").show() # Resultado: # +----+----+ # |col1|col2| # +----+----+ # | A| 1| # | B| 2| # | C| 3| # +----+----+ # Escribir en modo 'ignore' (si el archivo ya existe, no har\u00e1 nada) df.write.mode(\"ignore\").csv(\"output_csv\", header=True) Particionamiento de salida ( partitionBy ): Permite organizar los datos en el sistema de archivos subyacente (HDFS, S3, ADLS) en directorios basados en el valor de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por las columnas de partici\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionBy\").getOrCreate() data = [(\"Sales\", 2023, 100), (\"Sales\", 2024, 120), (\"Marketing\", 2023, 80), (\"Marketing\", 2024, 90)] df = spark.createDataFrame(data, [\"department\", \"year\", \"revenue\"]) # Escribir con particionamiento por 'department' y 'year' df.write.mode(\"overwrite\").partitionBy(\"department\", \"year\").parquet(\"department_yearly_revenue.parquet\") # Esto crear\u00e1 una estructura de directorios como: # department_yearly_revenue.parquet/department=Sales/year=2023/part-....parquet # department_yearly_revenue.parquet/department=Sales/year=2024/part-....parquet # ... Manejo de directorios de salida: Spark crea un directorio para cada operaci\u00f3n de escritura. Dentro de este directorio, se encuentran los archivos de datos (partes) y un archivo _SUCCESS si la operaci\u00f3n fue exitosa. # Despu\u00e9s de ejecutar una escritura, puedes explorar la estructura de directorios. # Por ejemplo, para el caso de Parquet sin particionamiento: # ls -R users.parquet/ # Resultado (ejemplo): # users.parquet/: # _SUCCESS/ # part-00000-....snappy.parquet/ Tarea Ejercicios con PySpark : Crea un DataFrame a partir de la siguiente lista de tuplas: [(\"Juan\", \"Perez\", 30, \"Ingeniero\"), (\"Maria\", \"Lopez\", 25, \"Doctora\"), (\"Carlos\", \"Gomez\", 35, \"Abogado\")] . Define el esquema expl\u00edcitamente con las columnas nombre , apellido , edad (entero) y profesion . Luego, muestra el esquema y las primeras filas del DataFrame. Dado el DataFrame del ejercicio 1, selecciona \u00fanicamente las columnas nombre y profesion . Adem\u00e1s, renombra la columna nombre a primer_nombre en el DataFrame resultante. Al DataFrame original del ejercicio 1, a\u00f1ade una nueva columna llamada salario_base con un valor fijo de 50000 . Luego, crea otra columna salario_ajustado que sea salario_base m\u00e1s edad * 100 . Filtra el DataFrame resultante del ejercicio 3 para mostrar solo las personas cuya edad sea mayor a 28 Y su profesion sea Ingeniero o Abogado . Utilizando el DataFrame original del ejercicio 1, calcula el promedio de edad y la cantidad total de personas. Crea un DataFrame de empleados que incluya una columna contacto de tipo StructType con email y telefono como subcampos. Los datos de ejemplo podr\u00edan ser: [(\"Alice\", {\"email\": \"alice@example.com\", \"telefono\": \"123-456-7890\"})] . Muestra el esquema y accede al email de Alice. Crea un DataFrame de estudiantes con una columna cursos_inscritos de tipo ArrayType(StringType()) . Ejemplo de datos: [(\"Bob\", [\"Matem\u00e1ticas\", \"F\u00edsica\"]), (\"Eve\", [\"Qu\u00edmica\"])] . Muestra el esquema y filtra los estudiantes que est\u00e9n inscritos en Matem\u00e1ticas . Crea un archivo CSV llamado productos.csv con los siguientes datos (incluye encabezado): producto_id,nombre,precio,cantidad 1,Laptop,1200.50,10 2,Mouse,25.00,50 3,Teclado,75.99,30 Lee este archivo en un DataFrame, infiriendo el esquema y mostrando el esquema y el contenido. Crea un DataFrame con columnas region , mes y ventas . Los datos de ejemplo: [(\"Norte\", \"Enero\", 1000), (\"Sur\", \"Enero\", 800), (\"Norte\", \"Febrero\", 1100), (\"Sur\", \"Febrero\", 900)] . Guarda este DataFrame como archivos Parquet, particionando por region y mes . Luego, lee solo las ventas de la regi\u00f3n Norte en Enero para verificar la partici\u00f3n. Crea un archivo JSON llamado config.json con los siguientes datos (cada objeto en una l\u00ednea): {\"id\": 1, \"settings\": {\"theme\": \"dark\", \"notifications\": true}} {\"id\": 2, \"settings\": {\"theme\": \"light\", \"notifications\": false}} Lee este archivo en un DataFrame y muestra el theme para cada ID. Ejercicios con SparkSQL : Crea el mismo DataFrame del Ejercicio 1 de PySpark (empleados). Registra este DataFrame como una vista temporal llamada empleados_temp . Luego, ejecuta una consulta SQL para seleccionar todos los empleados. Usando la vista empleados_temp , escribe una consulta SparkSQL para seleccionar nombre , apellido y profesion de los empleados con edad menor a 30 . Sobre la vista empleados_temp , realiza una consulta SQL que seleccione nombre como primer_nombre y profesion como ocupacion . Utilizando empleados_temp , a\u00f1ade una columna calculada llamada edad_futura que sea la edad actual m\u00e1s 5 . Crea una vista temporal a partir de un DataFrame de ventas con columnas producto , region y cantidad . Datos de ejemplo: [(\"Laptop\", \"Norte\", 5), (\"Mouse\", \"Norte\", 10), (\"Laptop\", \"Sur\", 3)] . Calcula la SUM de cantidad por producto usando SparkSQL. Partiendo del DataFrame de empleados con contacto (email y telefono) del Ejercicio 6 de PySpark, crea una vista temporal. Luego, usa SparkSQL para seleccionar el nombre del empleado y su contacto.email . Utilizando el DataFrame de estudiantes con cursos_inscritos del Ejercicio 7 de PySpark, crea una vista temporal. Escribe una consulta SparkSQL para seleccionar los estudiantes que tienen Matem\u00e1ticas en su lista de cursos_inscritos (puedes necesitar una funci\u00f3n SQL de Spark para arrays). Crea el archivo productos.csv del Ejercicio 8 de PySpark. Luego, usando spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"productos.csv\") , crea un DataFrame y reg\u00edstralo como vista temporal productos_temp . Finalmente, selecciona todos los productos con un precio mayor a 50 . Guarda un DataFrame (por ejemplo, el de ventas del Ejercicio 9 de PySpark) como archivos Parquet en una ubicaci\u00f3n espec\u00edfica (ej: \"data/ventas_particionadas\" ). Luego, crea una tabla externa de SparkSQL apuntando a esa ubicaci\u00f3n ( CREATE TABLE ... USING PARQUET LOCATION ... ). Finalmente, consulta las ventas de una region espec\u00edfica directamente desde la tabla SQL. Usando el archivo config.json del Ejercicio 10 de PySpark, lee el JSON y crea una vista temporal config_temp . Escribe una consulta SparkSQL para extraer el valor del theme de la columna settings para cada id (esto requerir\u00e1 desanidaci\u00f3n o funciones JSON de SparkSQL).","title":"Fundamentos de DataFrames en Spark"},{"location":"tema21/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema21/#tema-21-fundamentos-de-dataframes-en-spark","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender la estructura y el funcionamiento de los DataFrames de Apache Spark, realizar operaciones fundamentales de manipulaci\u00f3n de datos, gestionar esquemas y tipos de datos complejos, y leer y escribir datos en los formatos m\u00e1s comunes utilizados en entornos de Big Data. Introducci\u00f3n : En el vasto universo del Big Data, la capacidad de procesar y analizar vol\u00famenes masivos de informaci\u00f3n es crucial. Apache Spark, con su motor de procesamiento distribuido, se ha consolidado como una herramienta indispensable para esta tarea. En el coraz\u00f3n de su eficiencia y facilidad de uso se encuentran los DataFrames, una abstracci\u00f3n de datos distribuida que organiza los datos en columnas con nombre, similar a una tabla en una base de datos relacional o una hoja de c\u00e1lculo. Esta estructura permite a los desarrolladores trabajar con datos de forma intuitiva y optimizada, aprovechando el poder de Spark para el procesamiento paralelo y distribuido. Desarrollo : Este tema se centrar\u00e1 en los pilares de la manipulaci\u00f3n de datos en Spark a trav\u00e9s de los DataFrames. Exploraremos c\u00f3mo los DataFrames facilitan las operaciones de transformaci\u00f3n y consulta, abstraen la complejidad de la distribuci\u00f3n de datos y proporcionan un API robusto para interactuar con ellos. Abordaremos desde las operaciones b\u00e1sicas como selecci\u00f3n y filtrado, hasta la comprensi\u00f3n de los esquemas y tipos de datos complejos, y la interacci\u00f3n con una variedad de formatos de archivo est\u00e1ndar de la industria.","title":"Tema 2.1 Fundamentos de DataFrames en Spark"},{"location":"tema21/#211-operaciones-con-dataframes","text":"Las operaciones con DataFrames en Spark son el n\u00facleo de la manipulaci\u00f3n de datos, permitiendo seleccionar, filtrar, agregar, unir y realizar una multitud de transformaciones sobre conjuntos de datos distribuidos de manera eficiente. A diferencia de los RDDs, los DataFrames ofrecen un nivel de abstracci\u00f3n superior, permitiendo a Spark optimizar internamente las operaciones gracias a la informaci\u00f3n del esquema.","title":"2.1.1 Operaciones con DataFrames"},{"location":"tema21/#creacion-de-dataframes","text":"La creaci\u00f3n de DataFrames es el primer paso para trabajar con datos en PySpark. Se pueden crear a partir de diversas fuentes, como listas de Python, RDDs existentes, o leyendo directamente desde archivos. Desde una lista de tuplas o diccionarios: from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() # Opci\u00f3n 1: Usando una lista de tuplas y definiendo el esquema data_tuples = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] schema_tuples = StructType([ StructField(\"name\", StringType(), True), StructField(\"id\", IntegerType(), True), StructField(\"city\", StringType(), True) ]) df_from_tuples = spark.createDataFrame(data_tuples, schema=schema_tuples) df_from_tuples.show() # Resultado: # +-------+---+----+ # | name| id|city| # +-------+---+----+ # | Alice| 1| NY| # | Bob| 2| LA| # |Charlie| 3| CHI| # +-------+---+----+ # Opci\u00f3n 2: Usando una lista de diccionarios (Spark infiere el esquema) data_dicts = [{\"name\": \"Alice\", \"id\": 1, \"city\": \"NY\"}, {\"name\": \"Bob\", \"id\": 2, \"city\": \"LA\"}, {\"name\": \"Charlie\", \"id\": 3, \"city\": \"CHI\"}] df_from_dicts = spark.createDataFrame(data_dicts) df_from_dicts.show() # Resultado: # +-------+---+----+ # | city| id|name| # +-------+---+----+ # | NY| 1|Alice| # | LA| 2| Bob| # | CHI| 3|Charlie| # +-------+---+----+ Desde un RDD existente: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]) df_from_rdd = spark.createDataFrame(rdd, [\"name\", \"id\"]) df_from_rdd.show() # Resultado: # +-------+---+ # | name| id| # +-------+---+ # | Alice| 1| # | Bob| 2| # |Charlie| 3| # +-------+---+ Desde un archivo (se ver\u00e1 en 2.1.3): # df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True) # df.show()","title":"Creaci\u00f3n de DataFrames"},{"location":"tema21/#transformaciones-de-dataframes","text":"Las transformaciones en DataFrames son operaciones lazy (perezosas), lo que significa que no se ejecutan hasta que se invoca una acci\u00f3n. Esto permite a Spark optimizar el plan de ejecuci\u00f3n. Selecci\u00f3n de columnas ( select y withColumn ): from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate() data = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"id\", \"city\"]) # Seleccionar columnas espec\u00edficas df.select(\"name\", \"city\").show() # Resultado: # +-------+----+ # | name|city| # +-------+----+ # | Alice| NY| # | Bob| LA| # |Charlie| CHI| # +-------+----+ # Renombrar una columna al seleccionar df.select(col(\"name\").alias(\"full_name\"), \"city\").show() # Resultado: # +---------+----+ # |full_name|city| # +---------+----+ # | Alice| NY| # | Bob| LA| # | Charlie| CHI| # +---------+----+ # A\u00f1adir una nueva columna df.withColumn(\"country\", lit(\"USA\")).show() # Resultado: # +-------+---+----+-------+ # | name| id|city|country| # +-------+---+----+-------+ # | Alice| 1| NY| USA| # | Bob| 2| LA| USA| # |Charlie| 3| CHI| USA| # +-------+---+----+-------+ # Modificar una columna existente (ejemplo: incrementar id) df.withColumn(\"id_plus_10\", col(\"id\") + 10).show() # Resultado: # +-------+---+----+----------+ # | name| id|city|id_plus_10| # +-------+---+----+----------+ # | Alice| 1| NY| 11| # | Bob| 2| LA| 12| # |Charlie| 3| CHI| 13| # +-------+---+----+----------+ Filtrado de filas ( filter o where ): from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"DataFrameFiltering\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\"), (\"David\", 35, \"NY\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Filtrar por una condici\u00f3n simple df.filter(col(\"age\") > 25).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # | Bob| 30| LA| # |David| 35| NY| # +-----+---+----+ # Filtrar por m\u00faltiples condiciones df.filter((col(\"age\") > 20) & (col(\"city\") == \"NY\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # |David| 35| NY| # +-----+---+----+ # Usando el m\u00e9todo where (alias de filter) df.where(col(\"name\").like(\"A%\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # +-----+---+----+ Agregaciones ( groupBy y funciones de agregaci\u00f3n): from pyspark.sql import SparkSession from pyspark.sql.functions import avg, count, sum, min, max spark = SparkSession.builder.appName(\"DataFrameAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000), (\"Dept1\", \"Bob\", 1200), (\"Dept2\", \"Charlie\", 900), (\"Dept2\", \"David\", 1500)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"]) # Contar empleados por departamento df.groupBy(\"department\").count().show() # Resultado: # +----------+-----+ # |department|count| # +----------+-----+ # | Dept1| 2| # | Dept2| 2| # +----------+-----+ # Calcular salario promedio y m\u00e1ximo por departamento df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"), max(\"salary\").alias(\"max_salary\")).show() # Resultado: # +----------+----------+----------+ # |department|avg_salary|max_salary| # +----------+----------+----------+ # | Dept1| 1100.0| 1200| # | Dept2| 1200.0| 1500| # +----------+----------+----------+ # Sumar salarios por departamento df.groupBy(\"department\").agg(sum(\"salary\").alias(\"total_salary\")).show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | Dept1| 2200| # | Dept2| 2400| # +----------+------------+","title":"Transformaciones de DataFrames"},{"location":"tema21/#acciones-de-dataframes","text":"Las acciones son operaciones que disparan la ejecuci\u00f3n del plan de transformaciones y devuelven un resultado a la aplicaci\u00f3n del controlador o escriben datos en un sistema de almacenamiento. Mostrar datos ( show ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Mostrar las primeras filas del DataFrame df.show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ # Mostrar un n\u00famero espec\u00edfico de filas y truncar el contenido de las columnas si es largo df.show(numRows=1, truncate=False) # Resultado: # +-----+---+ # |name |age| # +-----+---+ # |Alice|25 | # +-----+---+ Contar filas ( count ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Contar el n\u00famero total de filas en el DataFrame num_rows = df.count() print(f\"N\u00famero de filas: {num_rows}\") # Resultado: N\u00famero de filas: 3 Recopilar datos ( collect , take , first ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Recopilar todos los datos del DataFrame en una lista de Rows en el driver all_data = df.collect() print(f\"Todos los datos: {all_data}\") # Resultado: Todos los datos: [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=22)] # Tomar las primeras N filas first_two = df.take(2) print(f\"Primeras 2 filas: {first_two}\") # Resultado: Primeras 2 filas: [Row(name='Alice', age=25), Row(name='Bob', age=30)] # Obtener la primera fila first_row = df.first() print(f\"Primera fila: {first_row}\") # Resultado: Primera fila: Row(name='Alice', age=25)","title":"Acciones de DataFrames"},{"location":"tema21/#212-esquemas-y-tipos-de-datos-complejos","text":"El esquema de un DataFrame es una estructura fundamental que define los nombres de las columnas y sus tipos de datos correspondientes. Esta metadata es crucial para la optimizaci\u00f3n de Spark, ya que le permite saber c\u00f3mo se organizan los datos y aplicar optimizaciones de tipo de datos y de columna. La inferencia de esquema y la definici\u00f3n expl\u00edcita son dos formas de manejarlo, y Spark tambi\u00e9n soporta tipos de datos complejos como ArrayType , MapType y StructType para manejar estructuras anidadas.","title":"2.1.2 Esquemas y tipos de datos complejos"},{"location":"tema21/#inferencia-y-definicion-explicita-de-esquemas","text":"La forma en que Spark determina el esquema de un DataFrame es vital para la integridad y eficiencia del procesamiento de datos. Inferencia de esquema ( inferSchema=True ): Spark puede intentar adivinar el esquema de un archivo de datos (CSV, JSON, Parquet, etc.) leyendo una muestra. Esto es conveniente para la exploraci\u00f3n inicial, pero puede ser propenso a errores, especialmente con datos inconsistentes o tipos ambiguos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SchemaInference\").getOrCreate() # Creando un archivo CSV de ejemplo data_csv = \"\"\"name,age,city Alice,25,NY Bob,30,LA Charlie,null,CHI David,35,NY\"\"\" with open(\"data.csv\", \"w\") as f: f.write(data_csv) # Inferencia de esquema al leer un CSV df_inferred = spark.read.csv(\"data.csv\", header=True, inferSchema=True) df_inferred.printSchema() # Resultado (ejemplo): # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # Observar que \"age\" se infiri\u00f3 como IntegerType, lo cual es correcto si no hay valores no num\u00e9ricos. # Si hubiera un valor no num\u00e9rico, podr\u00eda inferirse como StringType o fallar la inferencia. Definici\u00f3n expl\u00edcita de esquema ( StructType y StructField ): Es la forma m\u00e1s robusta y recomendada para entornos de producci\u00f3n. Permite controlar con precisi\u00f3n los tipos de datos y la nulabilidad, evitando problemas de inferencia y mejorando el rendimiento. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType spark = SparkSession.builder.appName(\"ExplicitSchema\").getOrCreate() # Definir un esquema expl\u00edcito custom_schema = StructType([ StructField(\"employee_name\", StringType(), True), StructField(\"employee_id\", IntegerType(), False), # Not nullable StructField(\"salary\", DoubleType(), True), StructField(\"is_active\", BooleanType(), True) ]) data = [(\"Alice\", 1, 50000.0, True), (\"Bob\", 2, 60000.50, False), (\"Charlie\", 3, 75000.0, True)] df_explicit = spark.createDataFrame(data, schema=custom_schema) df_explicit.printSchema() # Resultado: # root # |-- employee_name: string (nullable = true) # |-- employee_id: integer (nullable = false) # |-- salary: double (nullable = true) # |-- is_active: boolean (nullable = true) # Intentar insertar un valor nulo en una columna no nula causar\u00eda un error o comportamiento inesperado # data_error = [(\"David\", None, 80000.0, True)] # Esto generar\u00eda un error si intentas crear el DF # df_error = spark.createDataFrame(data_error, schema=custom_schema) Acceder y manipular el esquema ( df.schema ): El esquema de un DataFrame es accesible a trav\u00e9s del atributo .schema , que devuelve un objeto StructType . from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"SchemaAccess\").getOrCreate() data = [(\"Alice\", 1), (\"Bob\", 2)] df = spark.createDataFrame(data, [\"name\", \"id\"]) # Acceder al esquema print(df.schema) # Resultado: StructType([StructField('name', StringType(), True), StructField('id', LongType(), True)]) # Iterar sobre los campos del esquema for field in df.schema: print(f\"Nombre de columna: {field.name}, Tipo: {field.dataType}, Nulable: {field.nullable}\") # Resultado: # Nombre de columna: name, Tipo: StringType, Nulable: True # Nombre de columna: id, Tipo: LongType, Nulable: True","title":"Inferencia y Definici\u00f3n Expl\u00edcita de Esquemas"},{"location":"tema21/#tipos-de-datos-complejos","text":"Spark permite manejar estructuras de datos m\u00e1s all\u00e1 de los tipos at\u00f3micos, lo que es fundamental para trabajar con datos semi-estructurados y anidados como JSON. StructType (Estructuras Anidadas/Registros): Representa una estructura similar a un objeto o un registro, donde cada campo tiene un nombre y un tipo de datos. Permite modelar objetos complejos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypes\").getOrCreate() # Definir un esquema con un StructType anidado address_schema = StructType([ StructField(\"street\", StringType(), True), StructField(\"city\", StringType(), True), StructField(\"zip\", StringType(), True) ]) person_schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"address\", address_schema, True) # Columna de tipo StructType ]) data = [ (\"Alice\", 25, (\"123 Main St\", \"NY\", \"10001\")), (\"Bob\", 30, (\"456 Oak Ave\", \"LA\", \"90001\")) ] df = spark.createDataFrame(data, schema=person_schema) df.printSchema() # Resultado: # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- address: struct (nullable = true) # | |-- street: string (nullable = true) # | |-- city: string (nullable = true) # | |-- zip: string (nullable = true) df.show(truncate=False) # Resultado: # +-----+---+-------------------------+ # |name |age|address | # +-----+---+-------------------------+ # |Alice|25 |{123 Main St, NY, 10001} | # |Bob |30 |{456 Oak Ave, LA, 90001} | # +-----+---+-------------------------+ # Acceder a campos anidados df.select(\"name\", \"address.city\").show() # Resultado: # +-----+----+ # |name |city| # +-----+----+ # |Alice|NY | # |Bob |LA | # +-----+----+ ArrayType (Arrays/Listas): Representa una colecci\u00f3n de elementos del mismo tipo. \u00datil para modelar listas o arreglos de datos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypesArray\").getOrCreate() # Definir un esquema con un ArrayType course_schema = StructType([ StructField(\"student_name\", StringType(), True), StructField(\"grades\", ArrayType(IntegerType()), True) # Columna de tipo ArrayType ]) data = [ (\"Alice\", [90, 85, 92]), (\"Bob\", [78, 80]), (\"Charlie\", []) ] df = spark.createDataFrame(data, schema=course_schema) df.printSchema() # Resultado: # root # |-- student_name: string (nullable = true) # |-- grades: array (nullable = true) # | |-- element: integer (containsNull = true) df.show(truncate=False) # Resultado: # +------------+----------+ # |student_name|grades | # +------------+----------+ # |Alice |[90, 85, 92]| # |Bob |[78, 80] | # |Charlie |[] | # +------------+----------+ # Acceder a elementos de array (requiere funciones de Spark) from pyspark.sql.functions import size, array_contains df.select(\"student_name\", size(\"grades\").alias(\"num_grades\")).show() # Resultado: # +------------+----------+ # |student_name|num_grades| # +------------+----------+ # | Alice| 3| # | Bob| 2| # | Charlie| 0| # +------------+----------+ df.filter(array_contains(\"grades\", 90)).show() # Resultado: # +------------+----------+ # |student_name| grades| # +------------+----------+ # | Alice|[90, 85, 92]| # +------------+----------+ MapType (Mapas/Diccionarios): Representa una colecci\u00f3n de pares clave-valor. \u00datil para datos que se asemejan a diccionarios o JSON con claves din\u00e1micas. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, MapType spark = SparkSession.builder.appName(\"ComplexTypesMap\").getOrCreate() # Definir un esquema con un MapType product_schema = StructType([ StructField(\"product_id\", StringType(), True), StructField(\"attributes\", MapType(StringType(), StringType()), True) # Columna de tipo MapType ]) data = [ (\"P101\", {\"color\": \"red\", \"size\": \"M\", \"material\": \"cotton\"}), (\"P102\", {\"color\": \"blue\", \"size\": \"L\"}), (\"P103\", {}) ] df = spark.createDataFrame(data, schema=product_schema) df.printSchema() # Resultado: # root # |-- product_id: string (nullable = true) # |-- attributes: map (nullable = true) # | |-- key: string # | |-- value: string (containsNull = true) df.show(truncate=False) # Resultado: # +----------+-----------------------------------+ # |product_id|attributes | # +----------+-----------------------------------+ # |P101 |{color -> red, size -> M, material -> cotton}| # |P102 |{color -> blue, size -> L} | # |P103 |{} | # +----------+-----------------------------------+ # Acceder a elementos de mapa (se usa con `getItem` o notaci\u00f3n de corchetes) from pyspark.sql.functions import col df.select(\"product_id\", col(\"attributes\")[\"color\"].alias(\"product_color\")).show() # Resultado: # +----------+-------------+ # |product_id|product_color| # +----------+-------------+ # | P101| red| # | P102| blue| # | P103| null| # +----------+-------------+","title":"Tipos de Datos Complejos"},{"location":"tema21/#213-lectura-y-escritura-en-formatos-populares-parquet-avro-orc-csv-json","text":"Spark es vers\u00e1til en la lectura y escritura de datos, soportando una amplia gama de formatos de archivo. La elecci\u00f3n del formato adecuado es crucial para el rendimiento y la eficiencia del almacenamiento en entornos de Big Data. Los formatos columnares como Parquet y ORC son altamente recomendados para el an\u00e1lisis debido a su eficiencia en la lectura y compresi\u00f3n.","title":"2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON)"},{"location":"tema21/#lectura-de-datos","text":"La lectura de datos es la base para cualquier an\u00e1lisis. Spark proporciona un API spark.read muy flexible para cargar datos desde diversas fuentes. Lectura de archivos CSV: Ideal para datos tabulares simples. Es importante configurar header=True si el archivo tiene encabezados y inferSchema=True para que Spark intente adivinar los tipos de datos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteCSV\").getOrCreate() # Crear un archivo CSV de ejemplo data_csv = \"\"\"id,name,age,city 1,Alice,25,New York 2,Bob,30,Los Angeles 3,Charlie,22,Chicago\"\"\" with open(\"users.csv\", \"w\") as f: f.write(data_csv) # Leer un archivo CSV df_csv = spark.read.csv(\"users.csv\", header=True, inferSchema=True) df_csv.printSchema() df_csv.show() # Resultado: # root # |-- id: integer (nullable = true) # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # +---+-------+---+----------+ # | id| name|age| city| # +---+-------+---+----------+ # | 1| Alice| 25| New York| # | 2| Bob| 30|Los Angeles| # | 3|Charlie| 22| Chicago| # +---+-------+---+----------+ Lectura de archivos JSON: \u00datil para datos semi-estructurados. Spark puede inferir el esquema autom\u00e1ticamente. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteJSON\").getOrCreate() # Crear un archivo JSON de ejemplo data_json = \"\"\" {\"id\": 1, \"name\": \"Alice\", \"hobbies\": [\"reading\", \"hiking\"]} {\"id\": 2, \"name\": \"Bob\", \"hobbies\": [\"gaming\"]} {\"id\": 3, \"name\": \"Charlie\", \"hobbies\": []} \"\"\" with open(\"users.json\", \"w\") as f: f.write(data_json) # Leer un archivo JSON df_json = spark.read.json(\"users.json\") df_json.printSchema() df_json.show(truncate=False) # Resultado: # root # |-- hobbies: array (nullable = true) # | |-- element: string (containsNull = true) # |-- id: long (nullable = true) # |-- name: string (nullable = true) # +--------------------+---+-------+ # |hobbies |id |name | # +--------------------+---+-------+ # |[reading, hiking] |1 |Alice | # |[gaming] |2 |Bob | # |[] |3 |Charlie| # +--------------------+---+-------+ Lectura de archivos Parquet: Formato columnar altamente eficiente para Big Data. Spark lo usa como formato por defecto y es altamente optimizado para el rendimiento. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteParquet\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Parquet data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.write.mode(\"overwrite\").parquet(\"users.parquet\") # Leer un archivo Parquet df_parquet = spark.read.parquet(\"users.parquet\") df_parquet.printSchema() df_parquet.show() # Resultado: # root # |-- name: string (nullable = true) # |-- age: long (nullable = true) # |-- city: string (nullable = true) # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ Lectura de archivos ORC: Otro formato columnar optimizado para Big Data, desarrollado por Apache Hive. Ofrece compresi\u00f3n y rendimiento similares a Parquet. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteORC\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como ORC data = [(\"ProductA\", 100, \"Electronics\"), (\"ProductB\", 50, \"Books\")] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\"]) df.write.mode(\"overwrite\").orc(\"products.orc\") # Leer un archivo ORC df_orc = spark.read.orc(\"products.orc\") df_orc.printSchema() df_orc.show() # Resultado: # root # |-- product_name: string (nullable = true) # |-- price: long (nullable = true) # |-- category: string (nullable = true) # +------------+-----+-----------+ # |product_name|price| category| # +------------+-----+-----------+ # | ProductA| 100|Electronics| # | ProductB| 50| Books| # +------------+-----+-----------+ Lectura de archivos Avro: Formato de serializaci\u00f3n de datos basado en esquema. Requiere el paquete spark-avro . from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteAvro\") \\ .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\ .getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Avro data = [(\"Event1\", \"typeA\", 1678886400), (\"Event2\", \"typeB\", 1678886460)] df = spark.createDataFrame(data, [\"event_id\", \"event_type\", \"timestamp\"]) df.write.mode(\"overwrite\").format(\"avro\").save(\"events.avro\") # Leer un archivo Avro df_avro = spark.read.format(\"avro\").load(\"events.avro\") df_avro.printSchema() df_avro.show() # Resultado: # root # |-- event_id: string (nullable = true) # |-- event_type: string (nullable = true) # |-- timestamp: long (nullable = true) # +--------+----------+----------+ # |event_id|event_type| timestamp| # +--------+----------+----------+ # | Event1| typeA|1678886400| # | Event2| typeB|1678886460| # +--------+----------+----------+","title":"Lectura de Datos"},{"location":"tema21/#escritura-de-datos","text":"La escritura de DataFrames a diferentes formatos es tan importante como su lectura, ya que permite persistir los resultados de las transformaciones y compartirlos con otras aplicaciones o sistemas. Modos de escritura ( mode ): Cuando se escribe un DataFrame, es fundamental especificar el modo de escritura para evitar p\u00e9rdidas de datos o errores. overwrite : Sobrescribe los datos existentes en la ubicaci\u00f3n de destino. append : A\u00f1ade los datos al final de los datos existentes. ignore : Si los datos ya existen, la operaci\u00f3n de escritura no hace nada. error (o errorIfExists ): Lanza un error si los datos ya existen (modo por defecto). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WriteModes\").getOrCreate() data = [(\"A\", 1), (\"B\", 2)] df = spark.createDataFrame(data, [\"col1\", \"col2\"]) # Escribir en modo 'overwrite' df.write.mode(\"overwrite\").parquet(\"output_data.parquet\") # Escribir en modo 'append' data_new = [(\"C\", 3)] df_new = spark.createDataFrame(data_new, [\"col1\", \"col2\"]) df_new.write.mode(\"append\").parquet(\"output_data.parquet\") # Verificar el contenido spark.read.parquet(\"output_data.parquet\").show() # Resultado: # +----+----+ # |col1|col2| # +----+----+ # | A| 1| # | B| 2| # | C| 3| # +----+----+ # Escribir en modo 'ignore' (si el archivo ya existe, no har\u00e1 nada) df.write.mode(\"ignore\").csv(\"output_csv\", header=True) Particionamiento de salida ( partitionBy ): Permite organizar los datos en el sistema de archivos subyacente (HDFS, S3, ADLS) en directorios basados en el valor de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por las columnas de partici\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionBy\").getOrCreate() data = [(\"Sales\", 2023, 100), (\"Sales\", 2024, 120), (\"Marketing\", 2023, 80), (\"Marketing\", 2024, 90)] df = spark.createDataFrame(data, [\"department\", \"year\", \"revenue\"]) # Escribir con particionamiento por 'department' y 'year' df.write.mode(\"overwrite\").partitionBy(\"department\", \"year\").parquet(\"department_yearly_revenue.parquet\") # Esto crear\u00e1 una estructura de directorios como: # department_yearly_revenue.parquet/department=Sales/year=2023/part-....parquet # department_yearly_revenue.parquet/department=Sales/year=2024/part-....parquet # ... Manejo de directorios de salida: Spark crea un directorio para cada operaci\u00f3n de escritura. Dentro de este directorio, se encuentran los archivos de datos (partes) y un archivo _SUCCESS si la operaci\u00f3n fue exitosa. # Despu\u00e9s de ejecutar una escritura, puedes explorar la estructura de directorios. # Por ejemplo, para el caso de Parquet sin particionamiento: # ls -R users.parquet/ # Resultado (ejemplo): # users.parquet/: # _SUCCESS/ # part-00000-....snappy.parquet/","title":"Escritura de Datos"},{"location":"tema21/#tarea","text":"Ejercicios con PySpark : Crea un DataFrame a partir de la siguiente lista de tuplas: [(\"Juan\", \"Perez\", 30, \"Ingeniero\"), (\"Maria\", \"Lopez\", 25, \"Doctora\"), (\"Carlos\", \"Gomez\", 35, \"Abogado\")] . Define el esquema expl\u00edcitamente con las columnas nombre , apellido , edad (entero) y profesion . Luego, muestra el esquema y las primeras filas del DataFrame. Dado el DataFrame del ejercicio 1, selecciona \u00fanicamente las columnas nombre y profesion . Adem\u00e1s, renombra la columna nombre a primer_nombre en el DataFrame resultante. Al DataFrame original del ejercicio 1, a\u00f1ade una nueva columna llamada salario_base con un valor fijo de 50000 . Luego, crea otra columna salario_ajustado que sea salario_base m\u00e1s edad * 100 . Filtra el DataFrame resultante del ejercicio 3 para mostrar solo las personas cuya edad sea mayor a 28 Y su profesion sea Ingeniero o Abogado . Utilizando el DataFrame original del ejercicio 1, calcula el promedio de edad y la cantidad total de personas. Crea un DataFrame de empleados que incluya una columna contacto de tipo StructType con email y telefono como subcampos. Los datos de ejemplo podr\u00edan ser: [(\"Alice\", {\"email\": \"alice@example.com\", \"telefono\": \"123-456-7890\"})] . Muestra el esquema y accede al email de Alice. Crea un DataFrame de estudiantes con una columna cursos_inscritos de tipo ArrayType(StringType()) . Ejemplo de datos: [(\"Bob\", [\"Matem\u00e1ticas\", \"F\u00edsica\"]), (\"Eve\", [\"Qu\u00edmica\"])] . Muestra el esquema y filtra los estudiantes que est\u00e9n inscritos en Matem\u00e1ticas . Crea un archivo CSV llamado productos.csv con los siguientes datos (incluye encabezado): producto_id,nombre,precio,cantidad 1,Laptop,1200.50,10 2,Mouse,25.00,50 3,Teclado,75.99,30 Lee este archivo en un DataFrame, infiriendo el esquema y mostrando el esquema y el contenido. Crea un DataFrame con columnas region , mes y ventas . Los datos de ejemplo: [(\"Norte\", \"Enero\", 1000), (\"Sur\", \"Enero\", 800), (\"Norte\", \"Febrero\", 1100), (\"Sur\", \"Febrero\", 900)] . Guarda este DataFrame como archivos Parquet, particionando por region y mes . Luego, lee solo las ventas de la regi\u00f3n Norte en Enero para verificar la partici\u00f3n. Crea un archivo JSON llamado config.json con los siguientes datos (cada objeto en una l\u00ednea): {\"id\": 1, \"settings\": {\"theme\": \"dark\", \"notifications\": true}} {\"id\": 2, \"settings\": {\"theme\": \"light\", \"notifications\": false}} Lee este archivo en un DataFrame y muestra el theme para cada ID. Ejercicios con SparkSQL : Crea el mismo DataFrame del Ejercicio 1 de PySpark (empleados). Registra este DataFrame como una vista temporal llamada empleados_temp . Luego, ejecuta una consulta SQL para seleccionar todos los empleados. Usando la vista empleados_temp , escribe una consulta SparkSQL para seleccionar nombre , apellido y profesion de los empleados con edad menor a 30 . Sobre la vista empleados_temp , realiza una consulta SQL que seleccione nombre como primer_nombre y profesion como ocupacion . Utilizando empleados_temp , a\u00f1ade una columna calculada llamada edad_futura que sea la edad actual m\u00e1s 5 . Crea una vista temporal a partir de un DataFrame de ventas con columnas producto , region y cantidad . Datos de ejemplo: [(\"Laptop\", \"Norte\", 5), (\"Mouse\", \"Norte\", 10), (\"Laptop\", \"Sur\", 3)] . Calcula la SUM de cantidad por producto usando SparkSQL. Partiendo del DataFrame de empleados con contacto (email y telefono) del Ejercicio 6 de PySpark, crea una vista temporal. Luego, usa SparkSQL para seleccionar el nombre del empleado y su contacto.email . Utilizando el DataFrame de estudiantes con cursos_inscritos del Ejercicio 7 de PySpark, crea una vista temporal. Escribe una consulta SparkSQL para seleccionar los estudiantes que tienen Matem\u00e1ticas en su lista de cursos_inscritos (puedes necesitar una funci\u00f3n SQL de Spark para arrays). Crea el archivo productos.csv del Ejercicio 8 de PySpark. Luego, usando spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"productos.csv\") , crea un DataFrame y reg\u00edstralo como vista temporal productos_temp . Finalmente, selecciona todos los productos con un precio mayor a 50 . Guarda un DataFrame (por ejemplo, el de ventas del Ejercicio 9 de PySpark) como archivos Parquet en una ubicaci\u00f3n espec\u00edfica (ej: \"data/ventas_particionadas\" ). Luego, crea una tabla externa de SparkSQL apuntando a esa ubicaci\u00f3n ( CREATE TABLE ... USING PARQUET LOCATION ... ). Finalmente, consulta las ventas de una region espec\u00edfica directamente desde la tabla SQL. Usando el archivo config.json del Ejercicio 10 de PySpark, lee el JSON y crea una vista temporal config_temp . Escribe una consulta SparkSQL para extraer el valor del theme de la columna settings para cada id (esto requerir\u00e1 desanidaci\u00f3n o funciones JSON de SparkSQL).","title":"Tarea"},{"location":"tema22/","text":"2. PySpark y SparkSQL Tema 2.2 Manipulaci\u00f3n y Transformaci\u00f3n de Datos Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de aplicar t\u00e9cnicas avanzadas de manipulaci\u00f3n y transformaci\u00f3n sobre DataFrames de Apache Spark, utilizando un amplio rango de funciones integradas, creando funciones personalizadas (UDFs) para l\u00f3gica espec\u00edfica, y comprendiendo c\u00f3mo el particionamiento y el paralelismo influyen en el procesamiento eficiente de grandes vol\u00famenes de datos distribuidos. Introducci\u00f3n : La capacidad de transformar datos brutos en informaci\u00f3n valiosa es el coraz\u00f3n del an\u00e1lisis de Big Data. En Apache Spark, los DataFrames no solo ofrecen una interfaz intuitiva para trabajar con datos estructurados y semi-estructurados, sino que tambi\u00e9n proporcionan un conjunto robusto de operaciones y funciones para la manipulaci\u00f3n y limpieza de datos a escala. Desde simples selecciones y filtrados hasta complejas agregaciones y uniones, Spark permite a los usuarios moldear sus datos para satisfacer las necesidades de an\u00e1lisis, modelado o visualizaci\u00f3n, todo ello aprovechando su arquitectura distribuida subyacente. Desarrollo : Este tema profundiza en las capacidades de manipulaci\u00f3n de DataFrames de Spark. Retomaremos y ampliaremos las operaciones fundamentales, exploraremos el vasto cat\u00e1logo de funciones integradas de Spark SQL y aprenderemos a extender esta funcionalidad creando nuestras propias funciones definidas por el usuario (UDFs). Adem\u00e1s, abordaremos conceptos cruciales como el particionamiento y el paralelismo, fundamentales para optimizar el rendimiento y escalar el procesamiento de datos distribuidos de manera efectiva. Comprender estos conceptos es clave para escribir c\u00f3digo Spark eficiente y robusto en escenarios de Big Data. 2.2.1 Operaciones con DataFrames (selecci\u00f3n, filtrado, agregaciones) Si bien ya se introdujeron las operaciones b\u00e1sicas en el tema 2.1, esta secci\u00f3n se enfoca en profundizar y mostrar ejemplos m\u00e1s avanzados y combinaciones de estas operaciones, destacando su poder para la limpieza y preparaci\u00f3n de datos. La flexibilidad del API de DataFrames permite encadenar m\u00faltiples transformaciones, construyendo flujos de trabajo de datos complejos de manera legible y eficiente. Selecci\u00f3n Avanzada de Columnas M\u00e1s all\u00e1 de simplemente elegir columnas por nombre, Spark ofrece potentes capacidades para manipular columnas existentes o crear nuevas basadas en expresiones complejas. Seleccionar y Renombrar M\u00faltiples Columnas din\u00e1micamente: Es com\u00fan necesitar seleccionar un subconjunto de columnas y, al mismo tiempo, renombrarlas. Esto se puede hacer de forma program\u00e1tica utilizando listas de columnas y aplicando aliases. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedSelection\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\"), (\"Charlie\", 22, \"CHI\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Seleccionar y renombrar m\u00faltiples columnas selected_cols = [col(\"name\").alias(\"full_name\"), col(\"age\"), col(\"city\").alias(\"location\")] df.select(*selected_cols).show() # Resultado: # +---------+---+--------+ # |full_name|age|location| # +---------+---+--------+ # | Alice| 25| NY| # | Bob| 30| LA| # | Charlie| 22| CHI| # +---------+---+--------+ Uso de expresiones SQL en select : Spark permite incrustar expresiones SQL directamente dentro de la funci\u00f3n select para mayor flexibilidad, especialmente cuando se trabaja con funciones complejas o l\u00f3gicas condicionales. from pyspark.sql import SparkSession from pyspark.sql.functions import expr spark = SparkSession.builder.appName(\"SqlExpressions\").getOrCreate() data = [(\"Alice\", 25, 50000), (\"Bob\", 30, 60000), (\"Charlie\", 22, 45000)] df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"]) # Calcular un bono basado en el salario usando una expresi\u00f3n SQL df.select(\"name\", \"salary\", expr(\"salary * 0.10 AS bonus\")).show() # Resultado: # +-------+------+-------+ # | name|salary| bonus| # +-------+------+-------+ # | Alice| 50000| 5000.0| # | Bob| 60000| 6000.0| # |Charlie| 45000| 4500.0| # +-------+------+-------+ Eliminar columnas ( drop ): Es una operaci\u00f3n com\u00fan para limpiar DataFrames, eliminando columnas que no son relevantes para el an\u00e1lisis posterior. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropColumn\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Eliminar una sola columna df.drop(\"country\").show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ # Eliminar m\u00faltiples columnas df.drop(\"age\", \"city\").show() # Resultado: # +-----+-------+ # | name|country| # +-----+-------+ # |Alice| USA| # | Bob| USA| # +-----+-------+ Filtrado Avanzado de Filas Las condiciones de filtrado pueden ser muy complejas, combinando m\u00faltiples operadores l\u00f3gicos y funciones. Combinar m\u00faltiples condiciones con & (AND), | (OR), ~ (NOT): Permite construir filtros sofisticados para aislar subconjuntos de datos espec\u00edficos. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedFiltering\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\"), (\"Mouse\", 25, \"Electronics\"), (\"Book\", 15, \"Books\"), (\"Monitor\", 300, \"Electronics\"), (\"Pen\", 2, \"Office\")] df = spark.createDataFrame(data, [\"product\", \"price\", \"category\"]) # Filtrar productos de \"Electronics\" con precio > 100 O productos de \"Books\" df.filter( (col(\"category\") == \"Electronics\") & (col(\"price\") > 100) | (col(\"category\") == \"Books\") ).show() # Resultado: # +--------+-----+-----------+ # | product|price| category| # +--------+-----+-----------+ # | Laptop| 1200|Electronics| # | Book| 15| Books| # | Monitor| 300|Electronics| # +--------+-----+-----------+ Uso de isin para filtrar por m\u00faltiples valores en una columna: Una forma concisa de filtrar filas donde una columna toma uno de varios valores posibles. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"FilteringIsIn\").getOrCreate() data = [(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"CHI\"), (\"David\", \"NY\")] df = spark.createDataFrame(data, [\"name\", \"city\"]) # Filtrar por ciudades espec\u00edficas df.filter(col(\"city\").isin(\"NY\", \"LA\")).show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # |David| NY| # +-----+----+ Manejo de valores nulos en filtros ( isNull , isNotNull , na.drop ): Es crucial manejar los valores nulos al filtrar para evitar resultados inesperados o errores. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"HandlingNullsFiltering\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 22), (\"David\", None)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Filtrar filas donde 'age' no es nulo df.filter(col(\"age\").isNotNull()).show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ # Filtrar filas donde 'age' es nulo df.filter(col(\"age\").isNull()).show() # Resultado: # +-----+----+ # | name| age| # +-----+----+ # | Bob|null| # |David|null| # +-----+----+ # Eliminar filas con cualquier valor nulo df.na.drop().show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ Agregaciones Avanzadas y Agrupamiento Las agregaciones son potentes para resumir datos. Spark permite agregaciones por m\u00faltiples columnas, con funciones de ventana y pivoteo. Agregaciones sobre m\u00faltiples columnas con agg : Permite aplicar m\u00faltiples funciones de agregaci\u00f3n a diferentes columnas en una sola operaci\u00f3n de groupBy . from pyspark.sql import SparkSession from pyspark.sql.functions import avg, sum, count, col spark = SparkSession.builder.appName(\"MultiAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000, 5), (\"Dept1\", \"Bob\", 1200, 8), (\"Dept2\", \"Charlie\", 900, 3), (\"Dept2\", \"David\", 1500, 10)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\", \"projects_completed\"]) # Agregaciones m\u00faltiples por departamento df.groupBy(\"department\").agg( avg(col(\"salary\")).alias(\"avg_salary\"), sum(col(\"projects_completed\")).alias(\"total_projects\"), count(col(\"name\")).alias(\"num_employees\") ).show() # Resultado: # +----------+----------+--------------+-------------+ # |department|avg_salary|total_projects|num_employees| # +----------+----------+--------------+-------------+ # | Dept1| 1100.0| 13| 2| # | Dept2| 1200.0| 13| 2| # +----------+----------+--------------+-------------+ Pivoteo de datos ( pivot ): Transforma filas en columnas, muy \u00fatil para an\u00e1lisis de series temporales o reportes. from pyspark.sql import SparkSession from pyspark.sql.functions import sum spark = SparkSession.builder.appName(\"PivotOperation\").getOrCreate() data = [(\"Sales\", \"Jan\", 100), (\"Sales\", \"Feb\", 120), (\"Marketing\", \"Jan\", 80), (\"Marketing\", \"Feb\", 90), (\"Sales\", \"Mar\", 150)] df = spark.createDataFrame(data, [\"department\", \"month\", \"revenue\"]) # Pivoteo de ingresos por departamento y mes df.groupBy(\"department\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).agg(sum(\"revenue\")).show() # Resultado: # +----------+---+---+---+ # |department|Jan|Feb|Mar| # +----------+---+---+---+ # | Sales|100|120|150| # | Marketing| 80| 90|null| # +----------+---+---+---+ Uniones de DataFrames ( join ): Combina dos DataFrames en funci\u00f3n de una o m\u00e1s claves comunes. Spark soporta varios tipos de uniones (inner, outer, left, right, anti, semi). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameJoins\").getOrCreate() # DataFrame de empleados employees_data = [(\"Alice\", 1, \"Sales\"), (\"Bob\", 2, \"HR\"), (\"Charlie\", 3, \"IT\")] employees_df = spark.createDataFrame(employees_data, [\"name\", \"emp_id\", \"dept_id\"]) # DataFrame de departamentos departments_data = [(1, \"Sales\", \"NY\"), (2, \"HR\", \"LA\"), (4, \"Finance\", \"CHI\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"]) # Inner Join: solo filas que coinciden en ambas tablas employees_df.join(departments_df, on=\"dept_id\", how=\"inner\").show() # Resultado: # +-------+-----+---------+---------+--------+ # |dept_id| name| emp_id|dept_name|location| # +-------+-----+---------+---------+--------+ # | 1|Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # +-------+-----+---------+---------+--------+ # Left Outer Join: todas las filas de la izquierda, y las que coinciden de la derecha employees_df.join(departments_df, on=\"dept_id\", how=\"left_outer\").show() # Resultado: # +-------+-------+------+---------+---------+--------+ # |dept_id| name|emp_id|dept_name|location| # +-------+-------+------+---------+---------+--------+ # | 1| Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # | 3|Charlie| 3| null| null| # +-------+-------+------+---------+---------+--------+ 2.2.2 Funciones integradas y definidas por el usuario (UDFs) Spark proporciona una rica biblioteca de funciones integradas ( pyspark.sql.functions ) que cubren una amplia gama de transformaciones de datos. Sin embargo, cuando la l\u00f3gica de negocio es muy espec\u00edfica y no est\u00e1 cubierta por las funciones existentes, las Funciones Definidas por el Usuario (UDFs) permiten extender la funcionalidad de Spark utilizando c\u00f3digo Python. Funciones Integradas de Spark SQL Estas funciones son altamente optimizadas y deben ser la primera opci\u00f3n para cualquier transformaci\u00f3n. Cubren operaciones num\u00e9ricas, de cadena, de fecha y hora, y de manipulaci\u00f3n de arrays y mapas. Funciones de cadena ( substring , concat_ws , length , lower , upper ): \u00datiles para manipular texto en columnas. from pyspark.sql import SparkSession from pyspark.sql.functions import col, substring, concat_ws, length, lower, upper spark = SparkSession.builder.appName(\"StringFunctions\").getOrCreate() data = [(\"john doe\",), (\"JANE SMITH\",)] df = spark.createDataFrame(data, [\"full_name\"]) # Extraer subcadena df.withColumn(\"first_3_chars\", substring(col(\"full_name\"), 1, 3)).show() # Resultado: # +----------+-------------+ # | full_name|first_3_chars| # +----------+-------------+ # | john doe| joh| # |JANE SMITH| JAN| # +----------+-------------+ # Concatenar con separador (necesita m\u00e1s columnas para ser \u00fatil, ejemplo conceptual) df.withColumn(\"formatted_name\", concat_ws(\", \", lower(col(\"full_name\")))).show() # En este caso, solo convierte a min\u00fasculas # Resultado: # +----------+--------------+ # | full_name|formatted_name| # +----------+--------------+ # | john doe| john doe| # |JANE SMITH| jane smith| # +----------+--------------+ # Obtener longitud y convertir a may\u00fasculas/min\u00fasculas df.select(col(\"full_name\"), length(col(\"full_name\")).alias(\"name_length\"), lower(col(\"full_name\")).alias(\"lower_name\"), upper(col(\"full_name\")).alias(\"upper_name\")).show() # Resultado: # +----------+-----------+----------+----------+ # | full_name|name_length|lower_name|upper_name| # +----------+-----------+----------+----------+ # | john doe| 8| john doe| JOHN DOE| # |JANE SMITH| 10|jane smith|JANE SMITH| # +----------+-----------+----------+----------+ Funciones de fecha y hora ( current_date , datediff , year , month , to_date , to_timestamp ): Esenciales para el procesamiento de datos temporales. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_date, datediff, year, month, to_date, to_timestamp spark = SparkSession.builder.appName(\"DateFunctions\").getOrCreate() data = [(\"2023-01-15\",), (\"2024-03-01\",)] df = spark.createDataFrame(data, [\"event_date\"]) # Convertir a tipo Date y obtener el a\u00f1o y mes df.withColumn(\"event_date_parsed\", to_date(col(\"event_date\"))) \\ .withColumn(\"current_date\", current_date()) \\ .withColumn(\"days_since_event\", datediff(col(\"current_date\"), col(\"event_date_parsed\"))) \\ .withColumn(\"event_year\", year(col(\"event_date_parsed\"))) \\ .withColumn(\"event_month\", month(col(\"event_date_parsed\"))) \\ .show() # Resultado (valores de d\u00edas_since_event variar\u00e1n con la fecha actual): # +----------+-----------------+------------+----------------+----------+-----------+ # |event_date|event_date_parsed|current_date|days_since_event|event_year|event_month| # +----------+-----------------+------------+----------------+----------+-----------+ # |2023-01-15| 2023-01-15| 2025-05-31| 867| 2023| 1| # |2024-03-01| 2024-03-01| 2025-05-31| 457| 2024| 3| # +----------+-----------------+------------+----------------+----------+-----------+ # Convertir a timestamp df_ts = spark.createDataFrame([(\"2023-01-15 10:30:00\",)], [\"datetime_str\"]) df_ts.withColumn(\"parsed_timestamp\", to_timestamp(col(\"datetime_str\"))).show() # Resultado: # +-------------------+--------------------+ # | datetime_str| parsed_timestamp| # +-------------------+--------------------+ # |2023-01-15 10:30:00|2023-01-15 10:30:00| # +-------------------+--------------------+ Funciones condicionales ( when , otherwise ): Permiten aplicar l\u00f3gica condicional para crear nuevas columnas o modificar existentes. from pyspark.sql import SparkSession from pyspark.sql.functions import col, when spark = SparkSession.builder.appName(\"ConditionalFunctions\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 35, \"LA\"), (\"Charlie\", 17, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Clasificar edad en categor\u00edas df.withColumn(\"age_group\", when(col(\"age\") < 18, \"Minor\") .when(col(\"age\") >= 18, \"Adult\") .otherwise(\"Unknown\") ).show() # Resultado: # +-------+---+----+---------+ # | name|age|city|age_group| # +-------+---+----+---------+ # | Alice| 25| NY| Adult| # | Bob| 35| LA| Adult| # |Charlie| 17| CHI| Minor| # +-------+---+----+---------+ Funciones Definidas por el Usuario (UDFs) Las UDFs permiten a los desarrolladores de Python extender la funcionalidad de Spark implementando l\u00f3gica personalizada. Aunque potentes, pueden tener un impacto en el rendimiento debido a la serializaci\u00f3n y deserializaci\u00f3n de datos entre la JVM (donde Spark se ejecuta) y el proceso Python. Creaci\u00f3n de UDFs simples: Se definen como funciones Python normales y luego se registran en Spark usando udf de pyspark.sql.functions . Es crucial especificar el tipo de retorno. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import StringType spark = SparkSession.builder.appName(\"SimpleUDF\").getOrCreate() data = [(\"alice\",), (\"BOB\",), (\"charlie\",)] df = spark.createDataFrame(data, [\"name\"]) # Definir una funci\u00f3n Python para capitalizar la primera letra def capitalize_name(name): return name.capitalize() if name else None # Registrar la UDF con el tipo de retorno capitalize_udf = udf(capitalize_name, StringType()) # Aplicar la UDF al DataFrame df.withColumn(\"capitalized_name\", capitalize_udf(col(\"name\"))).show() # Resultado: # +-------+----------------+ # | name|capitalized_name| # +-------+----------------+ # | alice| Alice| # | BOB| Bob| # |charlie| Charlie| # +-------+----------------+ UDFs con m\u00faltiples argumentos: Las UDFs pueden aceptar m\u00faltiples columnas como entrada. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import DoubleType spark = SparkSession.builder.appName(\"MultiArgUDF\").getOrCreate() data = [(1000, 0.10), (1200, 0.05), (800, 0.15)] df = spark.createDataFrame(data, [\"base_salary\", \"bonus_rate\"]) # Funci\u00f3n Python para calcular el salario total def calculate_total_salary(base_salary, bonus_rate): if base_salary is None or bonus_rate is None: return None return base_salary * (1 + bonus_rate) # Registrar la UDF total_salary_udf = udf(calculate_total_salary, DoubleType()) # Aplicar la UDF df.withColumn(\"total_salary\", total_salary_udf(col(\"base_salary\"), col(\"bonus_rate\"))).show() # Resultado: # +-----------+----------+------------+ # |base_salary|bonus_rate|total_salary| # +-----------+----------+------------+ # | 1000| 0.10| 1100.0| # | 1200| 0.05| 1260.0| # | 800| 0.15| 920.0| # +-----------+----------+------------+ Consideraciones de rendimiento de UDFs (Vectorized UDFs con Pandas): Para mitigar el costo de serializaci\u00f3n/deserializaci\u00f3n, Spark 2.3+ introdujo las UDFs vectorizadas (anteriormente \"Pandas UDFs\"). Estas UDFs operan en pandas.Series o pandas.DataFrame en lugar de una fila a la vez, lo que reduce la sobrecarga y mejora significativamente el rendimiento para ciertas operaciones. from pyspark.sql import SparkSession from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import DoubleType, StringType spark = SparkSession.builder.appName(\"PandasUDF\").getOrCreate() data = [(1, 10.0), (2, 20.0), (3, 30.0)] df = spark.createDataFrame(data, [\"id\", \"value\"]) # Pandas UDF para escalar valores (Series a Series) @pandas_udf(DoubleType(), PandasUDFType.SCALAR) def multiply_by_two(value: float) -> float: return value * 2 df.withColumn(\"value_doubled\", multiply_by_two(col(\"value\"))).show() # Resultado: # +---+-----+-------------+ # | id|value|value_doubled| # +---+-----+-------------+ # | 1| 10.0| 20.0| # | 2| 20.0| 40.0| # | 3| 30.0| 60.0| # +---+-----+-------------+ # Pandas UDF para agregaci\u00f3n (Series a escalar) @pandas_udf(StringType(), PandasUDFType.GROUPED_AGG) def concat_strings(col_series): return \"_\".join(col_series.astype(str)) df_agg = spark.createDataFrame([(\"A\", \"x\"), (\"A\", \"y\"), (\"B\", \"z\")], [\"group\", \"value\"]) df_agg.groupBy(\"group\").agg(concat_strings(col(\"value\")).alias(\"concatenated_values\")).show() # Resultado: # +-----+-------------------+ # |group|concatenated_values| # +-----+-------------------+ # | A| x_y| # | B| z| # +-----+-------------------+ 2.2.3 Particionamiento y paralelismo El particionamiento es un concepto fundamental en Spark que define c\u00f3mo se distribuyen los datos en el cl\u00faster. Un particionamiento adecuado es clave para optimizar el rendimiento de las operaciones, especialmente las que involucran shuffles (intercambio de datos entre nodos). El paralelismo se refiere a la cantidad de tareas que Spark puede ejecutar simult\u00e1neamente. Conceptos de Particionamiento Los datos en Spark se dividen en \"particiones\" l\u00f3gicas, cada una de las cuales es procesada por una tarea. La forma en que los datos se particionan afecta el rendimiento y la eficiencia de la computaci\u00f3n. \u00bfQu\u00e9 es una partici\u00f3n en Spark?: Una partici\u00f3n es una divisi\u00f3n l\u00f3gica de los datos de un RDD o DataFrame. Cada partici\u00f3n se puede almacenar en un nodo diferente del cl\u00faster y se procesa de forma independiente y paralela. M\u00e1s particiones no siempre es mejor; el n\u00famero \u00f3ptimo depende del tama\u00f1o de los datos, el n\u00famero de n\u00facleos del cl\u00faster y la naturaleza de las operaciones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionsConcept\").getOrCreate() data = [(i,) for i in range(100)] df = spark.createDataFrame(data, [\"value\"]) # Obtener el n\u00famero de particiones actual (por defecto Spark usa el n\u00famero de n\u00facleos disponibles o spark.sql.shuffle.partitions) print(f\"N\u00famero inicial de particiones: {df.rdd.getNumPartitions()}\") # Podemos re-particionar un DataFrame (esto implica un shuffle) df_repartitioned = df.repartition(10) print(f\"N\u00famero de particiones despu\u00e9s de repartition: {df_repartitioned.rdd.getNumPartitions()}\") # Una partici\u00f3n es como un bloque de trabajo para un n\u00facleo de CPU. # Si tenemos 100 particiones y 20 n\u00facleos, cada n\u00facleo procesar\u00e1 5 particiones en paralelo (idealmente). Visualizaci\u00f3n del n\u00famero de particiones ( df.rdd.getNumPartitions() ): Es importante saber cu\u00e1ntas particiones tiene un DataFrame para entender c\u00f3mo se distribuir\u00e1 el trabajo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CheckPartitions\").getOrCreate() # Leer un archivo de ejemplo para ver sus particiones # Si no tienes un archivo grande, crea uno peque\u00f1o y l\u00e9elo data = [(i, f\"Name_{i}\") for i in range(1000)] df_large = spark.createDataFrame(data, [\"id\", \"name\"]) df_large.write.mode(\"overwrite\").parquet(\"large_data.parquet\") # Leer el DataFrame df_read = spark.read.parquet(\"large_data.parquet\") # Ver el n\u00famero de particiones print(f\"N\u00famero de particiones del DataFrame le\u00eddo: {df_read.rdd.getNumPartitions()}\") # Generalmente, Spark intenta que el n\u00famero de particiones sea cercano al tama\u00f1o de bloque del HDFS (128MB por defecto) # o al n\u00famero de n\u00facleos en el cluster. Impacto del particionamiento en el rendimiento (Shuffle): Las operaciones que requieren agrupar o unir datos (como groupBy , join , orderBy ) a menudo implican un \"shuffle\", donde los datos se mueven entre los nodos del cl\u00faster. Un n\u00famero incorrecto de particiones puede llevar a un shuffle ineficiente, causando cuellos de botella. Demasiadas particiones peque\u00f1as pueden generar mucha sobrecarga, mientras que muy pocas pueden limitar el paralelismo. Skewed data: Si los datos est\u00e1n muy desequilibrados en las particiones (algunas particiones tienen muchos m\u00e1s datos que otras), esto puede causar cuellos de botella donde una o pocas tareas tardan mucho m\u00e1s en completarse. Small files problem: Si hay muchas particiones muy peque\u00f1as, la sobrecarga de gestionar cada una puede ser mayor que el tiempo de procesamiento real. Control del Particionamiento y Paralelismo Spark ofrece mecanismos para controlar c\u00f3mo se particionan los datos y el nivel de paralelismo. repartition() y coalesce() : repartition() crea un nuevo RDD/DataFrame con un n\u00famero especificado de particiones, distribuyendo los datos uniformemente. Esto siempre implica un shuffle completo. coalesce() reduce el n\u00famero de particiones sin shuffle si es posible (solo reduce el n\u00famero de particiones en el mismo nodo), o con un shuffle m\u00ednimo si se requiere. Es m\u00e1s eficiente para reducir particiones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"RepartitionCoalesce\").getOrCreate() data = [(i,) for i in range(1000)] df = spark.createDataFrame(data, [\"value\"]) print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\") # Var\u00eda seg\u00fan la configuraci\u00f3n local # Reparticionar a 4 particiones (siempre con shuffle) df_repartitioned = df.repartition(4) print(f\"Particiones despu\u00e9s de repartition(4): {df_repartitioned.rdd.getNumPartitions()}\") # Coalesce a 2 particiones (puede evitar shuffle si los datos ya est\u00e1n en menos de 2) df_coalesced = df_repartitioned.coalesce(2) print(f\"Particiones despu\u00e9s de coalesce(2): {df_coalesced.rdd.getNumPartitions()}\") # Coalesce a 1 partici\u00f3n (siempre implicar\u00e1 un shuffle si hay m\u00e1s de 1 partici\u00f3n) df_single_partition = df.coalesce(1) print(f\"Particiones despu\u00e9s de coalesce(1): {df_single_partition.rdd.getNumPartitions()}\") Configuraci\u00f3n de spark.sql.shuffle.partitions : Este par\u00e1metro controla el n\u00famero de particiones que Spark utiliza por defecto despu\u00e9s de una operaci\u00f3n de shuffle. Un valor bien ajustado puede mejorar dr\u00e1sticamente el rendimiento de las operaciones de agregaci\u00f3n y uni\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import count # Configurar el n\u00famero de particiones de shuffle antes de crear la SparkSession spark = SparkSession.builder \\ .appName(\"ShufflePartitions\") \\ .config(\"spark.sql.shuffle.partitions\", \"8\") \\ .getOrCreate() print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\") data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4), (\"B\", 5)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # La operaci\u00f3n groupBy implicar\u00e1 un shuffle, y el resultado tendr\u00e1 8 particiones. df_agg = df.groupBy(\"key\").agg(count(\"*\")).repartition(1) # Repartition para mostrar en una sola salida df_agg.show() # Para verificar el n\u00famero de particiones de la salida de la agregaci\u00f3n (antes del repartition) # print(df_agg.rdd.getNumPartitions()) Estrategias para optimizar particiones en Joins: Broadcast Join: Cuando un DataFrame es peque\u00f1o (por defecto, menos de spark.sql.autoBroadcastJoinThreshold ), Spark puede \"broadcast\" (transmitir) el DataFrame peque\u00f1o a todos los nodos del cl\u00faster, evitando el shuffle de la tabla grande. Es muy eficiente. Hash Join: Cuando la clave de uni\u00f3n est\u00e1 particionada de forma similar en ambos DataFrames, Spark puede realizar un Hash Join, que es eficiente ya que los datos con la misma clave ya est\u00e1n en las mismas particiones. Sort-Merge Join: El join por defecto si las tablas no se pueden transmitir y no est\u00e1n co-ubicadas. Implica ordenar y fusionar las particiones, lo que puede ser costoso. from pyspark.sql import SparkSession from pyspark.sql.functions import broadcast # Para forzar un broadcast join spark = SparkSession.builder \\ .appName(\"JoinStrategies\") \\ .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") # Configurar el umbral de broadcast (por defecto 10MB) .getOrCreate() # DataFrame peque\u00f1o (menos de 10MB de datos) small_df = spark.createDataFrame([(1, \"DeptA\"), (2, \"DeptB\")], [\"dept_id\", \"dept_name\"]) # DataFrame grande large_df = spark.createDataFrame([(101, \"Alice\", 1), (102, \"Bob\", 2), (103, \"Charlie\", 1)], [\"emp_id\", \"emp_name\", \"dept_id\"]) # Spark autom\u00e1ticamente intentar\u00e1 un Broadcast Join si small_df est\u00e1 por debajo del umbral large_df.join(small_df, \"dept_id\", \"inner\").explain() # Ver el plan de ejecuci\u00f3n para confirmar BroadcastHashJoin # Resultado de explain() deber\u00eda mostrar \"*BroadcastHashJoin\" # Forzar un Broadcast Join (\u00fatil si Spark no lo infiere autom\u00e1ticamente o para depuraci\u00f3n) large_df.join(broadcast(small_df), \"dept_id\", \"inner\").explain() 2.2.4 Manejo de datos distribuidos Trabajar con datos distribuidos implica m\u00e1s que solo particionar. Se trata de entender c\u00f3mo Spark gestiona la memoria, el almacenamiento en cach\u00e9 y la persistencia para optimizar el acceso a los datos, y c\u00f3mo maneja los \"shuffles\" que son costosos. Persistencia y Almacenamiento en Cach\u00e9 Para evitar recalcular DataFrames que se utilizan m\u00faltiples veces, Spark permite persistirlos en memoria o en disco. cache() y persist() : cache() es un alias de persist() con el nivel de almacenamiento por defecto ( MEMORY_AND_DISK ). Almacena el DataFrame en la memoria del cl\u00faster para un acceso r\u00e1pido en operaciones futuras. persist() permite especificar diferentes niveles de almacenamiento (solo memoria, solo disco, memoria y disco, con o sin serializaci\u00f3n, con o sin replicaci\u00f3n). from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"Persistence\").getOrCreate() data = [(i, f\"Item_{i}\") for i in range(100000)] df = spark.createDataFrame(data, [\"id\", \"description\"]) # Persistir en memoria y disco (comportamiento por defecto de cache()) df.cache() # Equivalente a df.persist(StorageLevel.MEMORY_AND_DISK) # La primera acci\u00f3n dispara la carga y el almacenamiento en cach\u00e9 df.count() print(f\"DataFrame cached. Number of partitions: {df.rdd.getNumPartitions()}\") # Las acciones subsiguientes ser\u00e1n m\u00e1s r\u00e1pidas df.filter(df.id > 50000).show(5) # Persistir solo en memoria (m\u00e1s r\u00e1pido si los datos caben en memoria) df_mem_only = df.persist(StorageLevel.MEMORY_ONLY) df_mem_only.count() print(f\"DataFrame persisted in MEMORY_ONLY. Number of partitions: {df_mem_only.rdd.getNumPartitions()}\") # Despersistir (liberar los datos de cach\u00e9) df.unpersist() df_mem_only.unpersist() Niveles de almacenamiento ( StorageLevel ): Permiten un control granular sobre c\u00f3mo se almacenan los datos persistidos, equilibrando velocidad y tolerancia a fallos. MEMORY_ONLY : Guarda el RDD deserializado como objetos Python en la JVM. Si no cabe, recalcula. MEMORY_AND_DISK : Guarda en memoria; si no cabe, se desborda a disco. MEMORY_ONLY_SER : Igual que MEMORY_ONLY, pero los datos est\u00e1n serializados (ahorra espacio, pero m\u00e1s lento de acceder). MEMORY_AND_DISK_SER : Igual que MEMORY_AND_DISK, pero serializado. DISK_ONLY : Solo guarda en disco. Versiones con _2 al final (ej. MEMORY_ONLY_2 ): Replica la partici\u00f3n en dos nodos, para tolerancia a fallos. from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"StorageLevels\").getOrCreate() data = [(i,) for i in range(1000000)] df = spark.createDataFrame(data, [\"value\"]) # Persistir en disco para mayor confiabilidad en caso de poca memoria df.persist(StorageLevel.DISK_ONLY) df.count() # Dispara la persistencia print(\"DataFrame persisted to DISK_ONLY.\") # Persistir con replicaci\u00f3n (para tolerancia a fallos) df.persist(StorageLevel.MEMORY_AND_DISK_2) df.count() # Dispara la persistencia print(\"DataFrame persisted to MEMORY_AND_DISK_2 (replicated).\") df.unpersist() \u00bfCu\u00e1ndo usar cache() / persist() ?: Cuando un DataFrame se usa en m\u00faltiples acciones (ej. count() , show() , luego filter() , join() ). Cuando un DataFrame es el resultado de transformaciones costosas (ej. join complejos, agregaciones pesadas). Antes de aplicar algoritmos iterativos (ej. Machine Learning), donde los datos se leen repetidamente. En puntos intermedios de un flujo de trabajo de ETL complejo que se reutilizan. Comprensi\u00f3n y Optimizaci\u00f3n de Shuffles Los shuffles son la operaci\u00f3n m\u00e1s costosa en Spark porque implican la transferencia de datos a trav\u00e9s de la red entre diferentes nodos. Minimizar o optimizar los shuffles es clave para el rendimiento. Identificaci\u00f3n de operaciones que causan Shuffle: Cualquier operaci\u00f3n que requiera que Spark reorganice los datos a trav\u00e9s del cl\u00faster, como: groupBy() join() (excepto Broadcast Joins) orderBy() y sort() repartition() Ventanas anal\u00edticas (ciertas operaciones) from pyspark.sql import SparkSession from pyspark.sql.functions import count spark = SparkSession.builder.appName(\"IdentifyShuffles\").getOrCreate() data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # explain() muestra el plan de ejecuci\u00f3n y ayuda a identificar shuffles df.groupBy(\"key\").agg(count(\"value\")).explain() # En el plan de ejecuci\u00f3n, buscar etapas como \"Exchange\", \"Sort\", \"HashPartitioning\" # Estas indican una operaci\u00f3n de shuffle. # Ejemplo de salida parcial de explain: # == Physical Plan == # *(2) HashAggregate(keys=[key#123], functions=[count(value#124)]) # +- Exchange hashpartitioning(key#123, 200), [id=#12] <--- Esto es un shuffle # +- *(1) HashAggregate(keys=[key#123], functions=[partial_count(value#124)]) # +- *(1) Project [key#123, value#124] # +- *(1) Scan ExistingRDD Mitigaci\u00f3n de Shuffles (ej. Broadcast Join, Co-locaci\u00f3n de datos): Broadcast Join: Como se mencion\u00f3, usar broadcast() para DataFrames peque\u00f1os evita el shuffle de la tabla grande. Co-locaci\u00f3n de datos: Si los datos que se van a unir o agrupar ya est\u00e1n particionados de forma similar en el sistema de archivos (ej. Parquet particionado por la clave de uni\u00f3n), Spark puede evitar shuffles completos. Esto se logra mediante la escritura de datos particionados ( df.write.partitionBy(...) ). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ShuffleMitigation\").getOrCreate() # Peque\u00f1o DataFrame de lookup lookup_data = [(1, \"RegionA\"), (2, \"RegionB\")] lookup_df = spark.createDataFrame(lookup_data, [\"region_id\", \"region_name\"]) # DataFrame grande sales_data = [(101, 1, 100), (102, 2, 150), (103, 1, 200)] sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"region_id\", \"amount\"]) # Forzar Broadcast Join para evitar shuffle de sales_df from pyspark.sql.functions import broadcast sales_df.join(broadcast(lookup_df), \"region_id\").explain() # Deber\u00edas ver BroadcastHashJoin en el plan # Ejemplo de co-locaci\u00f3n con escritura particionada (requiere planificaci\u00f3n previa) # df_large.write.partitionBy(\"join_key\").parquet(\"path/to/partitioned_data\") # df_other_large.write.partitionBy(\"join_key\").parquet(\"path/to/other_partitioned_data\") # Luego, al unirlos, si Spark detecta que est\u00e1n co-ubicados, puede usar un SortMergeJoin m\u00e1s eficiente Manejo de datos desequilibrados (Skewed Data): Cuando un valor de clave tiene significativamente m\u00e1s filas que otros, la partici\u00f3n correspondiente se convierte en un cuello de botella. Salting: A\u00f1adir un sufijo aleatorio a la clave desequilibrada en el DataFrame grande y replicar las filas de la clave desequilibrada en el DataFrame peque\u00f1o con los mismos sufijos. Spark 3.x Adaptive Query Execution (AQE): AQE puede detectar y manejar skew de forma autom\u00e1tica durante la ejecuci\u00f3n de los joins, dividiendo las particiones grandes en subparticiones m\u00e1s peque\u00f1as. Habilitar spark.sql.adaptive.enabled a true (es por defecto en Spark 3.2+). from pyspark.sql import SparkSession from pyspark.sql.functions import lit, rand, concat spark = SparkSession.builder \\ .appName(\"SkewedJoin\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") # Habilitar AQE .getOrCreate() # Simular datos desequilibrados: 'HotKey' tiene muchos m\u00e1s registros data_large = [(i, \"NormalKey\") for i in range(1000)] + \\ [(i, \"HotKey\") for i in range(9000)] large_df = spark.createDataFrame(data_large, [\"id\", \"join_key\"]) data_small = [(\"NormalKey\", \"ValueA\"), (\"HotKey\", \"ValueB\")] small_df = spark.createDataFrame(data_small, [\"join_key\", \"value\"]) # Uni\u00f3n est\u00e1ndar (puede ser lenta debido a HotKey si AQE no est\u00e1 activo o no es suficiente) result_df = large_df.join(small_df, \"join_key\", \"inner\") result_df.explain() # Observar si AQE detecta el skew (si est\u00e1 habilitado) # Ejemplo de Salting (manual): # A\u00f1adir un \"salt\" aleatorio a las claves num_salt_buckets = 10 salted_large_df = large_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (rand() * num_salt_buckets).cast(\"int\"))) salted_small_df = small_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (lit(0) + rand() * num_salt_buckets).cast(\"int\"))) # Si 'HotKey' tiene mucho skew, se replicar\u00eda 'HotKey' en small_df para cada sufijo de salt. # Luego, se unir\u00eda por (join_key, salted_key) # Uni\u00f3n con salting: # result_salted = salted_large_df.join(salted_small_df, on=\"salted_key\", how=\"inner\") # result_salted.show() Tarea Crea un DataFrame de pedidos con las columnas order_id , customer_id , order_date (formato 'YYYY-MM-DD'), y amount (valor num\u00e9rico). Datos de ejemplo: [(\"ORD001\", \"C001\", \"2023-01-10\", 150.75), (\"ORD002\", \"C002\", \"2023-01-15\", 200.00), (\"ORD003\", \"C001\", \"2023-02-01\", 50.25), (\"ORD004\", \"C003\", \"2023-02-05\", 300.00)] Realiza las siguientes transformaciones: * A\u00f1ade una columna order_year que contenga solo el a\u00f1o de order_date . * A\u00f1ade una columna order_month que contenga el n\u00famero del mes de order_date . * A\u00f1ade una columna is_high_value que sea True si amount es mayor o igual a 100 , de lo contrario False . * Muestra el DataFrame resultante. Usando el DataFrame de pedidos del ejercicio 1, calcula el total_amount_spent y el num_orders por customer_id . Muestra los resultados. Crea un DataFrame de productos con columnas product_name y category . Datos de ejemplo: [(\"laptop Dell\", \"electronics\"), (\"teclado logitech\", \"electronics\"), (\"libro de cocina\", \"books\"), (\"Auriculares sony\", \"electronics\")] Realiza las siguientes transformaciones: * Normaliza la columna product_name a min\u00fasculas. * A\u00f1ade una columna brand que extraiga la primera palabra de product_name . * A\u00f1ade una columna product_type basada en la category : si es \"electronics\", \"Tech Gadget\"; si es \"books\", \"Reading Material\"; de lo contrario \"Other\". * Muestra el DataFrame resultante. Define una UDF de PySpark llamada classify_amount que tome un amount num\u00e9rico y devuelva una cadena: \"Peque\u00f1o\" si amount < 50 , \"Mediano\" si 50 <= amount < 200 , y \"Grande\" si amount >= 200 . Aplica esta UDF al DataFrame de pedidos del ejercicio 1 para crear una nueva columna amount_category . Muestra el DataFrame. Crea un segundo DataFrame de clientes con las columnas customer_id y customer_name . Datos de ejemplo: [(\"C001\", \"Ana Garcia\"), (\"C002\", \"Pedro Ruiz\"), (\"C003\", \"Laura Sanz\"), (\"C004\", \"Diego Marin\")] Realiza un INNER JOIN entre el DataFrame de pedidos (ejercicio 1) y el DataFrame de clientes para mostrar los order_id , customer_name y amount de cada pedido. Realiza un LEFT OUTER JOIN de clientes (izquierda) con pedidos (derecha). Muestra todos los clientes y sus pedidos (si los tienen). Filtra el resultado para mostrar solo los clientes que no han realizado ning\u00fan pedido (es decir, donde order_id es nulo). Crea un DataFrame de ventas_regionales con columnas region , product_category y sales_value . Datos de ejemplo: [(\"Norte\", \"Electronics\", 1000), (\"Norte\", \"Books\", 500), (\"Sur\", \"Electronics\", 1200), (\"Sur\", \"Books\", 600), (\"Centro\", \"Electronics\", 800)] Pivotea este DataFrame para mostrar sales_value por region (filas) y product_category (columnas). Crea un DataFrame con 1,000,000 de filas y dos columnas: id (entero secuencial) y random_value (n\u00famero aleatorio). Guarda este DataFrame en formato Parquet en un directorio temporal ( /tmp/large_data.parquet ). Lee el DataFrame de nuevo y averigua su n\u00famero de particiones. Reparticiona el DataFrame a 10 particiones y persist\u00e9lo en memoria ( MEMORY_AND_DISK ). Realiza una operaci\u00f3n de conteo y luego despersiste el DataFrame. Configura spark.sql.shuffle.partitions a un valor bajo (ej. 2) y luego a un valor m\u00e1s alto (ej. 20). Crea un DataFrame con una columna category que tenga 5 valores \u00fanicos y una columna value . Realiza una operaci\u00f3n groupBy por category y suma value . * Usa explain() para observar los planes de ejecuci\u00f3n y c\u00f3mo el n\u00famero de particiones de shuffle cambia. * (Opcional, para un entorno de cl\u00faster) Intenta medir el tiempo de ejecuci\u00f3n en ambos casos para ver el impacto. Crea un DataFrame dim_customers muy peque\u00f1o (ej. 10 filas, customer_id , customer_name ). Crea un DataFrame fact_transactions muy grande (ej. 1,000,000 filas, transaction_id , customer_id , amount ). Realiza un INNER JOIN entre fact_transactions y dim_customers en customer_id . * Usa .explain() para verificar si Spark ha utilizado autom\u00e1ticamente un BroadcastHashJoin . * Intenta forzar un BroadcastHashJoin si no se aplica autom\u00e1ticamente utilizando broadcast(dim_customers) en la uni\u00f3n, y verifica de nuevo el plan de ejecuci\u00f3n.","title":"Manipulaci\u00f3n y Transformaci\u00f3n de Datos"},{"location":"tema22/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema22/#tema-22-manipulacion-y-transformacion-de-datos","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de aplicar t\u00e9cnicas avanzadas de manipulaci\u00f3n y transformaci\u00f3n sobre DataFrames de Apache Spark, utilizando un amplio rango de funciones integradas, creando funciones personalizadas (UDFs) para l\u00f3gica espec\u00edfica, y comprendiendo c\u00f3mo el particionamiento y el paralelismo influyen en el procesamiento eficiente de grandes vol\u00famenes de datos distribuidos. Introducci\u00f3n : La capacidad de transformar datos brutos en informaci\u00f3n valiosa es el coraz\u00f3n del an\u00e1lisis de Big Data. En Apache Spark, los DataFrames no solo ofrecen una interfaz intuitiva para trabajar con datos estructurados y semi-estructurados, sino que tambi\u00e9n proporcionan un conjunto robusto de operaciones y funciones para la manipulaci\u00f3n y limpieza de datos a escala. Desde simples selecciones y filtrados hasta complejas agregaciones y uniones, Spark permite a los usuarios moldear sus datos para satisfacer las necesidades de an\u00e1lisis, modelado o visualizaci\u00f3n, todo ello aprovechando su arquitectura distribuida subyacente. Desarrollo : Este tema profundiza en las capacidades de manipulaci\u00f3n de DataFrames de Spark. Retomaremos y ampliaremos las operaciones fundamentales, exploraremos el vasto cat\u00e1logo de funciones integradas de Spark SQL y aprenderemos a extender esta funcionalidad creando nuestras propias funciones definidas por el usuario (UDFs). Adem\u00e1s, abordaremos conceptos cruciales como el particionamiento y el paralelismo, fundamentales para optimizar el rendimiento y escalar el procesamiento de datos distribuidos de manera efectiva. Comprender estos conceptos es clave para escribir c\u00f3digo Spark eficiente y robusto en escenarios de Big Data.","title":"Tema 2.2 Manipulaci\u00f3n y Transformaci\u00f3n de Datos"},{"location":"tema22/#221-operaciones-con-dataframes-seleccion-filtrado-agregaciones","text":"Si bien ya se introdujeron las operaciones b\u00e1sicas en el tema 2.1, esta secci\u00f3n se enfoca en profundizar y mostrar ejemplos m\u00e1s avanzados y combinaciones de estas operaciones, destacando su poder para la limpieza y preparaci\u00f3n de datos. La flexibilidad del API de DataFrames permite encadenar m\u00faltiples transformaciones, construyendo flujos de trabajo de datos complejos de manera legible y eficiente.","title":"2.2.1 Operaciones con DataFrames (selecci\u00f3n, filtrado, agregaciones)"},{"location":"tema22/#seleccion-avanzada-de-columnas","text":"M\u00e1s all\u00e1 de simplemente elegir columnas por nombre, Spark ofrece potentes capacidades para manipular columnas existentes o crear nuevas basadas en expresiones complejas. Seleccionar y Renombrar M\u00faltiples Columnas din\u00e1micamente: Es com\u00fan necesitar seleccionar un subconjunto de columnas y, al mismo tiempo, renombrarlas. Esto se puede hacer de forma program\u00e1tica utilizando listas de columnas y aplicando aliases. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedSelection\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\"), (\"Charlie\", 22, \"CHI\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Seleccionar y renombrar m\u00faltiples columnas selected_cols = [col(\"name\").alias(\"full_name\"), col(\"age\"), col(\"city\").alias(\"location\")] df.select(*selected_cols).show() # Resultado: # +---------+---+--------+ # |full_name|age|location| # +---------+---+--------+ # | Alice| 25| NY| # | Bob| 30| LA| # | Charlie| 22| CHI| # +---------+---+--------+ Uso de expresiones SQL en select : Spark permite incrustar expresiones SQL directamente dentro de la funci\u00f3n select para mayor flexibilidad, especialmente cuando se trabaja con funciones complejas o l\u00f3gicas condicionales. from pyspark.sql import SparkSession from pyspark.sql.functions import expr spark = SparkSession.builder.appName(\"SqlExpressions\").getOrCreate() data = [(\"Alice\", 25, 50000), (\"Bob\", 30, 60000), (\"Charlie\", 22, 45000)] df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"]) # Calcular un bono basado en el salario usando una expresi\u00f3n SQL df.select(\"name\", \"salary\", expr(\"salary * 0.10 AS bonus\")).show() # Resultado: # +-------+------+-------+ # | name|salary| bonus| # +-------+------+-------+ # | Alice| 50000| 5000.0| # | Bob| 60000| 6000.0| # |Charlie| 45000| 4500.0| # +-------+------+-------+ Eliminar columnas ( drop ): Es una operaci\u00f3n com\u00fan para limpiar DataFrames, eliminando columnas que no son relevantes para el an\u00e1lisis posterior. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropColumn\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Eliminar una sola columna df.drop(\"country\").show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ # Eliminar m\u00faltiples columnas df.drop(\"age\", \"city\").show() # Resultado: # +-----+-------+ # | name|country| # +-----+-------+ # |Alice| USA| # | Bob| USA| # +-----+-------+","title":"Selecci\u00f3n Avanzada de Columnas"},{"location":"tema22/#filtrado-avanzado-de-filas","text":"Las condiciones de filtrado pueden ser muy complejas, combinando m\u00faltiples operadores l\u00f3gicos y funciones. Combinar m\u00faltiples condiciones con & (AND), | (OR), ~ (NOT): Permite construir filtros sofisticados para aislar subconjuntos de datos espec\u00edficos. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedFiltering\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\"), (\"Mouse\", 25, \"Electronics\"), (\"Book\", 15, \"Books\"), (\"Monitor\", 300, \"Electronics\"), (\"Pen\", 2, \"Office\")] df = spark.createDataFrame(data, [\"product\", \"price\", \"category\"]) # Filtrar productos de \"Electronics\" con precio > 100 O productos de \"Books\" df.filter( (col(\"category\") == \"Electronics\") & (col(\"price\") > 100) | (col(\"category\") == \"Books\") ).show() # Resultado: # +--------+-----+-----------+ # | product|price| category| # +--------+-----+-----------+ # | Laptop| 1200|Electronics| # | Book| 15| Books| # | Monitor| 300|Electronics| # +--------+-----+-----------+ Uso de isin para filtrar por m\u00faltiples valores en una columna: Una forma concisa de filtrar filas donde una columna toma uno de varios valores posibles. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"FilteringIsIn\").getOrCreate() data = [(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"CHI\"), (\"David\", \"NY\")] df = spark.createDataFrame(data, [\"name\", \"city\"]) # Filtrar por ciudades espec\u00edficas df.filter(col(\"city\").isin(\"NY\", \"LA\")).show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # |David| NY| # +-----+----+ Manejo de valores nulos en filtros ( isNull , isNotNull , na.drop ): Es crucial manejar los valores nulos al filtrar para evitar resultados inesperados o errores. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"HandlingNullsFiltering\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 22), (\"David\", None)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Filtrar filas donde 'age' no es nulo df.filter(col(\"age\").isNotNull()).show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ # Filtrar filas donde 'age' es nulo df.filter(col(\"age\").isNull()).show() # Resultado: # +-----+----+ # | name| age| # +-----+----+ # | Bob|null| # |David|null| # +-----+----+ # Eliminar filas con cualquier valor nulo df.na.drop().show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+","title":"Filtrado Avanzado de Filas"},{"location":"tema22/#agregaciones-avanzadas-y-agrupamiento","text":"Las agregaciones son potentes para resumir datos. Spark permite agregaciones por m\u00faltiples columnas, con funciones de ventana y pivoteo. Agregaciones sobre m\u00faltiples columnas con agg : Permite aplicar m\u00faltiples funciones de agregaci\u00f3n a diferentes columnas en una sola operaci\u00f3n de groupBy . from pyspark.sql import SparkSession from pyspark.sql.functions import avg, sum, count, col spark = SparkSession.builder.appName(\"MultiAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000, 5), (\"Dept1\", \"Bob\", 1200, 8), (\"Dept2\", \"Charlie\", 900, 3), (\"Dept2\", \"David\", 1500, 10)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\", \"projects_completed\"]) # Agregaciones m\u00faltiples por departamento df.groupBy(\"department\").agg( avg(col(\"salary\")).alias(\"avg_salary\"), sum(col(\"projects_completed\")).alias(\"total_projects\"), count(col(\"name\")).alias(\"num_employees\") ).show() # Resultado: # +----------+----------+--------------+-------------+ # |department|avg_salary|total_projects|num_employees| # +----------+----------+--------------+-------------+ # | Dept1| 1100.0| 13| 2| # | Dept2| 1200.0| 13| 2| # +----------+----------+--------------+-------------+ Pivoteo de datos ( pivot ): Transforma filas en columnas, muy \u00fatil para an\u00e1lisis de series temporales o reportes. from pyspark.sql import SparkSession from pyspark.sql.functions import sum spark = SparkSession.builder.appName(\"PivotOperation\").getOrCreate() data = [(\"Sales\", \"Jan\", 100), (\"Sales\", \"Feb\", 120), (\"Marketing\", \"Jan\", 80), (\"Marketing\", \"Feb\", 90), (\"Sales\", \"Mar\", 150)] df = spark.createDataFrame(data, [\"department\", \"month\", \"revenue\"]) # Pivoteo de ingresos por departamento y mes df.groupBy(\"department\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).agg(sum(\"revenue\")).show() # Resultado: # +----------+---+---+---+ # |department|Jan|Feb|Mar| # +----------+---+---+---+ # | Sales|100|120|150| # | Marketing| 80| 90|null| # +----------+---+---+---+ Uniones de DataFrames ( join ): Combina dos DataFrames en funci\u00f3n de una o m\u00e1s claves comunes. Spark soporta varios tipos de uniones (inner, outer, left, right, anti, semi). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameJoins\").getOrCreate() # DataFrame de empleados employees_data = [(\"Alice\", 1, \"Sales\"), (\"Bob\", 2, \"HR\"), (\"Charlie\", 3, \"IT\")] employees_df = spark.createDataFrame(employees_data, [\"name\", \"emp_id\", \"dept_id\"]) # DataFrame de departamentos departments_data = [(1, \"Sales\", \"NY\"), (2, \"HR\", \"LA\"), (4, \"Finance\", \"CHI\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"]) # Inner Join: solo filas que coinciden en ambas tablas employees_df.join(departments_df, on=\"dept_id\", how=\"inner\").show() # Resultado: # +-------+-----+---------+---------+--------+ # |dept_id| name| emp_id|dept_name|location| # +-------+-----+---------+---------+--------+ # | 1|Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # +-------+-----+---------+---------+--------+ # Left Outer Join: todas las filas de la izquierda, y las que coinciden de la derecha employees_df.join(departments_df, on=\"dept_id\", how=\"left_outer\").show() # Resultado: # +-------+-------+------+---------+---------+--------+ # |dept_id| name|emp_id|dept_name|location| # +-------+-------+------+---------+---------+--------+ # | 1| Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # | 3|Charlie| 3| null| null| # +-------+-------+------+---------+---------+--------+","title":"Agregaciones Avanzadas y Agrupamiento"},{"location":"tema22/#222-funciones-integradas-y-definidas-por-el-usuario-udfs","text":"Spark proporciona una rica biblioteca de funciones integradas ( pyspark.sql.functions ) que cubren una amplia gama de transformaciones de datos. Sin embargo, cuando la l\u00f3gica de negocio es muy espec\u00edfica y no est\u00e1 cubierta por las funciones existentes, las Funciones Definidas por el Usuario (UDFs) permiten extender la funcionalidad de Spark utilizando c\u00f3digo Python.","title":"2.2.2 Funciones integradas y definidas por el usuario (UDFs)"},{"location":"tema22/#funciones-integradas-de-spark-sql","text":"Estas funciones son altamente optimizadas y deben ser la primera opci\u00f3n para cualquier transformaci\u00f3n. Cubren operaciones num\u00e9ricas, de cadena, de fecha y hora, y de manipulaci\u00f3n de arrays y mapas. Funciones de cadena ( substring , concat_ws , length , lower , upper ): \u00datiles para manipular texto en columnas. from pyspark.sql import SparkSession from pyspark.sql.functions import col, substring, concat_ws, length, lower, upper spark = SparkSession.builder.appName(\"StringFunctions\").getOrCreate() data = [(\"john doe\",), (\"JANE SMITH\",)] df = spark.createDataFrame(data, [\"full_name\"]) # Extraer subcadena df.withColumn(\"first_3_chars\", substring(col(\"full_name\"), 1, 3)).show() # Resultado: # +----------+-------------+ # | full_name|first_3_chars| # +----------+-------------+ # | john doe| joh| # |JANE SMITH| JAN| # +----------+-------------+ # Concatenar con separador (necesita m\u00e1s columnas para ser \u00fatil, ejemplo conceptual) df.withColumn(\"formatted_name\", concat_ws(\", \", lower(col(\"full_name\")))).show() # En este caso, solo convierte a min\u00fasculas # Resultado: # +----------+--------------+ # | full_name|formatted_name| # +----------+--------------+ # | john doe| john doe| # |JANE SMITH| jane smith| # +----------+--------------+ # Obtener longitud y convertir a may\u00fasculas/min\u00fasculas df.select(col(\"full_name\"), length(col(\"full_name\")).alias(\"name_length\"), lower(col(\"full_name\")).alias(\"lower_name\"), upper(col(\"full_name\")).alias(\"upper_name\")).show() # Resultado: # +----------+-----------+----------+----------+ # | full_name|name_length|lower_name|upper_name| # +----------+-----------+----------+----------+ # | john doe| 8| john doe| JOHN DOE| # |JANE SMITH| 10|jane smith|JANE SMITH| # +----------+-----------+----------+----------+ Funciones de fecha y hora ( current_date , datediff , year , month , to_date , to_timestamp ): Esenciales para el procesamiento de datos temporales. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_date, datediff, year, month, to_date, to_timestamp spark = SparkSession.builder.appName(\"DateFunctions\").getOrCreate() data = [(\"2023-01-15\",), (\"2024-03-01\",)] df = spark.createDataFrame(data, [\"event_date\"]) # Convertir a tipo Date y obtener el a\u00f1o y mes df.withColumn(\"event_date_parsed\", to_date(col(\"event_date\"))) \\ .withColumn(\"current_date\", current_date()) \\ .withColumn(\"days_since_event\", datediff(col(\"current_date\"), col(\"event_date_parsed\"))) \\ .withColumn(\"event_year\", year(col(\"event_date_parsed\"))) \\ .withColumn(\"event_month\", month(col(\"event_date_parsed\"))) \\ .show() # Resultado (valores de d\u00edas_since_event variar\u00e1n con la fecha actual): # +----------+-----------------+------------+----------------+----------+-----------+ # |event_date|event_date_parsed|current_date|days_since_event|event_year|event_month| # +----------+-----------------+------------+----------------+----------+-----------+ # |2023-01-15| 2023-01-15| 2025-05-31| 867| 2023| 1| # |2024-03-01| 2024-03-01| 2025-05-31| 457| 2024| 3| # +----------+-----------------+------------+----------------+----------+-----------+ # Convertir a timestamp df_ts = spark.createDataFrame([(\"2023-01-15 10:30:00\",)], [\"datetime_str\"]) df_ts.withColumn(\"parsed_timestamp\", to_timestamp(col(\"datetime_str\"))).show() # Resultado: # +-------------------+--------------------+ # | datetime_str| parsed_timestamp| # +-------------------+--------------------+ # |2023-01-15 10:30:00|2023-01-15 10:30:00| # +-------------------+--------------------+ Funciones condicionales ( when , otherwise ): Permiten aplicar l\u00f3gica condicional para crear nuevas columnas o modificar existentes. from pyspark.sql import SparkSession from pyspark.sql.functions import col, when spark = SparkSession.builder.appName(\"ConditionalFunctions\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 35, \"LA\"), (\"Charlie\", 17, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Clasificar edad en categor\u00edas df.withColumn(\"age_group\", when(col(\"age\") < 18, \"Minor\") .when(col(\"age\") >= 18, \"Adult\") .otherwise(\"Unknown\") ).show() # Resultado: # +-------+---+----+---------+ # | name|age|city|age_group| # +-------+---+----+---------+ # | Alice| 25| NY| Adult| # | Bob| 35| LA| Adult| # |Charlie| 17| CHI| Minor| # +-------+---+----+---------+","title":"Funciones Integradas de Spark SQL"},{"location":"tema22/#funciones-definidas-por-el-usuario-udfs","text":"Las UDFs permiten a los desarrolladores de Python extender la funcionalidad de Spark implementando l\u00f3gica personalizada. Aunque potentes, pueden tener un impacto en el rendimiento debido a la serializaci\u00f3n y deserializaci\u00f3n de datos entre la JVM (donde Spark se ejecuta) y el proceso Python. Creaci\u00f3n de UDFs simples: Se definen como funciones Python normales y luego se registran en Spark usando udf de pyspark.sql.functions . Es crucial especificar el tipo de retorno. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import StringType spark = SparkSession.builder.appName(\"SimpleUDF\").getOrCreate() data = [(\"alice\",), (\"BOB\",), (\"charlie\",)] df = spark.createDataFrame(data, [\"name\"]) # Definir una funci\u00f3n Python para capitalizar la primera letra def capitalize_name(name): return name.capitalize() if name else None # Registrar la UDF con el tipo de retorno capitalize_udf = udf(capitalize_name, StringType()) # Aplicar la UDF al DataFrame df.withColumn(\"capitalized_name\", capitalize_udf(col(\"name\"))).show() # Resultado: # +-------+----------------+ # | name|capitalized_name| # +-------+----------------+ # | alice| Alice| # | BOB| Bob| # |charlie| Charlie| # +-------+----------------+ UDFs con m\u00faltiples argumentos: Las UDFs pueden aceptar m\u00faltiples columnas como entrada. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import DoubleType spark = SparkSession.builder.appName(\"MultiArgUDF\").getOrCreate() data = [(1000, 0.10), (1200, 0.05), (800, 0.15)] df = spark.createDataFrame(data, [\"base_salary\", \"bonus_rate\"]) # Funci\u00f3n Python para calcular el salario total def calculate_total_salary(base_salary, bonus_rate): if base_salary is None or bonus_rate is None: return None return base_salary * (1 + bonus_rate) # Registrar la UDF total_salary_udf = udf(calculate_total_salary, DoubleType()) # Aplicar la UDF df.withColumn(\"total_salary\", total_salary_udf(col(\"base_salary\"), col(\"bonus_rate\"))).show() # Resultado: # +-----------+----------+------------+ # |base_salary|bonus_rate|total_salary| # +-----------+----------+------------+ # | 1000| 0.10| 1100.0| # | 1200| 0.05| 1260.0| # | 800| 0.15| 920.0| # +-----------+----------+------------+ Consideraciones de rendimiento de UDFs (Vectorized UDFs con Pandas): Para mitigar el costo de serializaci\u00f3n/deserializaci\u00f3n, Spark 2.3+ introdujo las UDFs vectorizadas (anteriormente \"Pandas UDFs\"). Estas UDFs operan en pandas.Series o pandas.DataFrame en lugar de una fila a la vez, lo que reduce la sobrecarga y mejora significativamente el rendimiento para ciertas operaciones. from pyspark.sql import SparkSession from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import DoubleType, StringType spark = SparkSession.builder.appName(\"PandasUDF\").getOrCreate() data = [(1, 10.0), (2, 20.0), (3, 30.0)] df = spark.createDataFrame(data, [\"id\", \"value\"]) # Pandas UDF para escalar valores (Series a Series) @pandas_udf(DoubleType(), PandasUDFType.SCALAR) def multiply_by_two(value: float) -> float: return value * 2 df.withColumn(\"value_doubled\", multiply_by_two(col(\"value\"))).show() # Resultado: # +---+-----+-------------+ # | id|value|value_doubled| # +---+-----+-------------+ # | 1| 10.0| 20.0| # | 2| 20.0| 40.0| # | 3| 30.0| 60.0| # +---+-----+-------------+ # Pandas UDF para agregaci\u00f3n (Series a escalar) @pandas_udf(StringType(), PandasUDFType.GROUPED_AGG) def concat_strings(col_series): return \"_\".join(col_series.astype(str)) df_agg = spark.createDataFrame([(\"A\", \"x\"), (\"A\", \"y\"), (\"B\", \"z\")], [\"group\", \"value\"]) df_agg.groupBy(\"group\").agg(concat_strings(col(\"value\")).alias(\"concatenated_values\")).show() # Resultado: # +-----+-------------------+ # |group|concatenated_values| # +-----+-------------------+ # | A| x_y| # | B| z| # +-----+-------------------+","title":"Funciones Definidas por el Usuario (UDFs)"},{"location":"tema22/#223-particionamiento-y-paralelismo","text":"El particionamiento es un concepto fundamental en Spark que define c\u00f3mo se distribuyen los datos en el cl\u00faster. Un particionamiento adecuado es clave para optimizar el rendimiento de las operaciones, especialmente las que involucran shuffles (intercambio de datos entre nodos). El paralelismo se refiere a la cantidad de tareas que Spark puede ejecutar simult\u00e1neamente.","title":"2.2.3 Particionamiento y paralelismo"},{"location":"tema22/#conceptos-de-particionamiento","text":"Los datos en Spark se dividen en \"particiones\" l\u00f3gicas, cada una de las cuales es procesada por una tarea. La forma en que los datos se particionan afecta el rendimiento y la eficiencia de la computaci\u00f3n. \u00bfQu\u00e9 es una partici\u00f3n en Spark?: Una partici\u00f3n es una divisi\u00f3n l\u00f3gica de los datos de un RDD o DataFrame. Cada partici\u00f3n se puede almacenar en un nodo diferente del cl\u00faster y se procesa de forma independiente y paralela. M\u00e1s particiones no siempre es mejor; el n\u00famero \u00f3ptimo depende del tama\u00f1o de los datos, el n\u00famero de n\u00facleos del cl\u00faster y la naturaleza de las operaciones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionsConcept\").getOrCreate() data = [(i,) for i in range(100)] df = spark.createDataFrame(data, [\"value\"]) # Obtener el n\u00famero de particiones actual (por defecto Spark usa el n\u00famero de n\u00facleos disponibles o spark.sql.shuffle.partitions) print(f\"N\u00famero inicial de particiones: {df.rdd.getNumPartitions()}\") # Podemos re-particionar un DataFrame (esto implica un shuffle) df_repartitioned = df.repartition(10) print(f\"N\u00famero de particiones despu\u00e9s de repartition: {df_repartitioned.rdd.getNumPartitions()}\") # Una partici\u00f3n es como un bloque de trabajo para un n\u00facleo de CPU. # Si tenemos 100 particiones y 20 n\u00facleos, cada n\u00facleo procesar\u00e1 5 particiones en paralelo (idealmente). Visualizaci\u00f3n del n\u00famero de particiones ( df.rdd.getNumPartitions() ): Es importante saber cu\u00e1ntas particiones tiene un DataFrame para entender c\u00f3mo se distribuir\u00e1 el trabajo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CheckPartitions\").getOrCreate() # Leer un archivo de ejemplo para ver sus particiones # Si no tienes un archivo grande, crea uno peque\u00f1o y l\u00e9elo data = [(i, f\"Name_{i}\") for i in range(1000)] df_large = spark.createDataFrame(data, [\"id\", \"name\"]) df_large.write.mode(\"overwrite\").parquet(\"large_data.parquet\") # Leer el DataFrame df_read = spark.read.parquet(\"large_data.parquet\") # Ver el n\u00famero de particiones print(f\"N\u00famero de particiones del DataFrame le\u00eddo: {df_read.rdd.getNumPartitions()}\") # Generalmente, Spark intenta que el n\u00famero de particiones sea cercano al tama\u00f1o de bloque del HDFS (128MB por defecto) # o al n\u00famero de n\u00facleos en el cluster. Impacto del particionamiento en el rendimiento (Shuffle): Las operaciones que requieren agrupar o unir datos (como groupBy , join , orderBy ) a menudo implican un \"shuffle\", donde los datos se mueven entre los nodos del cl\u00faster. Un n\u00famero incorrecto de particiones puede llevar a un shuffle ineficiente, causando cuellos de botella. Demasiadas particiones peque\u00f1as pueden generar mucha sobrecarga, mientras que muy pocas pueden limitar el paralelismo. Skewed data: Si los datos est\u00e1n muy desequilibrados en las particiones (algunas particiones tienen muchos m\u00e1s datos que otras), esto puede causar cuellos de botella donde una o pocas tareas tardan mucho m\u00e1s en completarse. Small files problem: Si hay muchas particiones muy peque\u00f1as, la sobrecarga de gestionar cada una puede ser mayor que el tiempo de procesamiento real.","title":"Conceptos de Particionamiento"},{"location":"tema22/#control-del-particionamiento-y-paralelismo","text":"Spark ofrece mecanismos para controlar c\u00f3mo se particionan los datos y el nivel de paralelismo. repartition() y coalesce() : repartition() crea un nuevo RDD/DataFrame con un n\u00famero especificado de particiones, distribuyendo los datos uniformemente. Esto siempre implica un shuffle completo. coalesce() reduce el n\u00famero de particiones sin shuffle si es posible (solo reduce el n\u00famero de particiones en el mismo nodo), o con un shuffle m\u00ednimo si se requiere. Es m\u00e1s eficiente para reducir particiones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"RepartitionCoalesce\").getOrCreate() data = [(i,) for i in range(1000)] df = spark.createDataFrame(data, [\"value\"]) print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\") # Var\u00eda seg\u00fan la configuraci\u00f3n local # Reparticionar a 4 particiones (siempre con shuffle) df_repartitioned = df.repartition(4) print(f\"Particiones despu\u00e9s de repartition(4): {df_repartitioned.rdd.getNumPartitions()}\") # Coalesce a 2 particiones (puede evitar shuffle si los datos ya est\u00e1n en menos de 2) df_coalesced = df_repartitioned.coalesce(2) print(f\"Particiones despu\u00e9s de coalesce(2): {df_coalesced.rdd.getNumPartitions()}\") # Coalesce a 1 partici\u00f3n (siempre implicar\u00e1 un shuffle si hay m\u00e1s de 1 partici\u00f3n) df_single_partition = df.coalesce(1) print(f\"Particiones despu\u00e9s de coalesce(1): {df_single_partition.rdd.getNumPartitions()}\") Configuraci\u00f3n de spark.sql.shuffle.partitions : Este par\u00e1metro controla el n\u00famero de particiones que Spark utiliza por defecto despu\u00e9s de una operaci\u00f3n de shuffle. Un valor bien ajustado puede mejorar dr\u00e1sticamente el rendimiento de las operaciones de agregaci\u00f3n y uni\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import count # Configurar el n\u00famero de particiones de shuffle antes de crear la SparkSession spark = SparkSession.builder \\ .appName(\"ShufflePartitions\") \\ .config(\"spark.sql.shuffle.partitions\", \"8\") \\ .getOrCreate() print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\") data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4), (\"B\", 5)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # La operaci\u00f3n groupBy implicar\u00e1 un shuffle, y el resultado tendr\u00e1 8 particiones. df_agg = df.groupBy(\"key\").agg(count(\"*\")).repartition(1) # Repartition para mostrar en una sola salida df_agg.show() # Para verificar el n\u00famero de particiones de la salida de la agregaci\u00f3n (antes del repartition) # print(df_agg.rdd.getNumPartitions()) Estrategias para optimizar particiones en Joins: Broadcast Join: Cuando un DataFrame es peque\u00f1o (por defecto, menos de spark.sql.autoBroadcastJoinThreshold ), Spark puede \"broadcast\" (transmitir) el DataFrame peque\u00f1o a todos los nodos del cl\u00faster, evitando el shuffle de la tabla grande. Es muy eficiente. Hash Join: Cuando la clave de uni\u00f3n est\u00e1 particionada de forma similar en ambos DataFrames, Spark puede realizar un Hash Join, que es eficiente ya que los datos con la misma clave ya est\u00e1n en las mismas particiones. Sort-Merge Join: El join por defecto si las tablas no se pueden transmitir y no est\u00e1n co-ubicadas. Implica ordenar y fusionar las particiones, lo que puede ser costoso. from pyspark.sql import SparkSession from pyspark.sql.functions import broadcast # Para forzar un broadcast join spark = SparkSession.builder \\ .appName(\"JoinStrategies\") \\ .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") # Configurar el umbral de broadcast (por defecto 10MB) .getOrCreate() # DataFrame peque\u00f1o (menos de 10MB de datos) small_df = spark.createDataFrame([(1, \"DeptA\"), (2, \"DeptB\")], [\"dept_id\", \"dept_name\"]) # DataFrame grande large_df = spark.createDataFrame([(101, \"Alice\", 1), (102, \"Bob\", 2), (103, \"Charlie\", 1)], [\"emp_id\", \"emp_name\", \"dept_id\"]) # Spark autom\u00e1ticamente intentar\u00e1 un Broadcast Join si small_df est\u00e1 por debajo del umbral large_df.join(small_df, \"dept_id\", \"inner\").explain() # Ver el plan de ejecuci\u00f3n para confirmar BroadcastHashJoin # Resultado de explain() deber\u00eda mostrar \"*BroadcastHashJoin\" # Forzar un Broadcast Join (\u00fatil si Spark no lo infiere autom\u00e1ticamente o para depuraci\u00f3n) large_df.join(broadcast(small_df), \"dept_id\", \"inner\").explain()","title":"Control del Particionamiento y Paralelismo"},{"location":"tema22/#224-manejo-de-datos-distribuidos","text":"Trabajar con datos distribuidos implica m\u00e1s que solo particionar. Se trata de entender c\u00f3mo Spark gestiona la memoria, el almacenamiento en cach\u00e9 y la persistencia para optimizar el acceso a los datos, y c\u00f3mo maneja los \"shuffles\" que son costosos.","title":"2.2.4 Manejo de datos distribuidos"},{"location":"tema22/#persistencia-y-almacenamiento-en-cache","text":"Para evitar recalcular DataFrames que se utilizan m\u00faltiples veces, Spark permite persistirlos en memoria o en disco. cache() y persist() : cache() es un alias de persist() con el nivel de almacenamiento por defecto ( MEMORY_AND_DISK ). Almacena el DataFrame en la memoria del cl\u00faster para un acceso r\u00e1pido en operaciones futuras. persist() permite especificar diferentes niveles de almacenamiento (solo memoria, solo disco, memoria y disco, con o sin serializaci\u00f3n, con o sin replicaci\u00f3n). from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"Persistence\").getOrCreate() data = [(i, f\"Item_{i}\") for i in range(100000)] df = spark.createDataFrame(data, [\"id\", \"description\"]) # Persistir en memoria y disco (comportamiento por defecto de cache()) df.cache() # Equivalente a df.persist(StorageLevel.MEMORY_AND_DISK) # La primera acci\u00f3n dispara la carga y el almacenamiento en cach\u00e9 df.count() print(f\"DataFrame cached. Number of partitions: {df.rdd.getNumPartitions()}\") # Las acciones subsiguientes ser\u00e1n m\u00e1s r\u00e1pidas df.filter(df.id > 50000).show(5) # Persistir solo en memoria (m\u00e1s r\u00e1pido si los datos caben en memoria) df_mem_only = df.persist(StorageLevel.MEMORY_ONLY) df_mem_only.count() print(f\"DataFrame persisted in MEMORY_ONLY. Number of partitions: {df_mem_only.rdd.getNumPartitions()}\") # Despersistir (liberar los datos de cach\u00e9) df.unpersist() df_mem_only.unpersist() Niveles de almacenamiento ( StorageLevel ): Permiten un control granular sobre c\u00f3mo se almacenan los datos persistidos, equilibrando velocidad y tolerancia a fallos. MEMORY_ONLY : Guarda el RDD deserializado como objetos Python en la JVM. Si no cabe, recalcula. MEMORY_AND_DISK : Guarda en memoria; si no cabe, se desborda a disco. MEMORY_ONLY_SER : Igual que MEMORY_ONLY, pero los datos est\u00e1n serializados (ahorra espacio, pero m\u00e1s lento de acceder). MEMORY_AND_DISK_SER : Igual que MEMORY_AND_DISK, pero serializado. DISK_ONLY : Solo guarda en disco. Versiones con _2 al final (ej. MEMORY_ONLY_2 ): Replica la partici\u00f3n en dos nodos, para tolerancia a fallos. from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"StorageLevels\").getOrCreate() data = [(i,) for i in range(1000000)] df = spark.createDataFrame(data, [\"value\"]) # Persistir en disco para mayor confiabilidad en caso de poca memoria df.persist(StorageLevel.DISK_ONLY) df.count() # Dispara la persistencia print(\"DataFrame persisted to DISK_ONLY.\") # Persistir con replicaci\u00f3n (para tolerancia a fallos) df.persist(StorageLevel.MEMORY_AND_DISK_2) df.count() # Dispara la persistencia print(\"DataFrame persisted to MEMORY_AND_DISK_2 (replicated).\") df.unpersist() \u00bfCu\u00e1ndo usar cache() / persist() ?: Cuando un DataFrame se usa en m\u00faltiples acciones (ej. count() , show() , luego filter() , join() ). Cuando un DataFrame es el resultado de transformaciones costosas (ej. join complejos, agregaciones pesadas). Antes de aplicar algoritmos iterativos (ej. Machine Learning), donde los datos se leen repetidamente. En puntos intermedios de un flujo de trabajo de ETL complejo que se reutilizan.","title":"Persistencia y Almacenamiento en Cach\u00e9"},{"location":"tema22/#comprension-y-optimizacion-de-shuffles","text":"Los shuffles son la operaci\u00f3n m\u00e1s costosa en Spark porque implican la transferencia de datos a trav\u00e9s de la red entre diferentes nodos. Minimizar o optimizar los shuffles es clave para el rendimiento. Identificaci\u00f3n de operaciones que causan Shuffle: Cualquier operaci\u00f3n que requiera que Spark reorganice los datos a trav\u00e9s del cl\u00faster, como: groupBy() join() (excepto Broadcast Joins) orderBy() y sort() repartition() Ventanas anal\u00edticas (ciertas operaciones) from pyspark.sql import SparkSession from pyspark.sql.functions import count spark = SparkSession.builder.appName(\"IdentifyShuffles\").getOrCreate() data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # explain() muestra el plan de ejecuci\u00f3n y ayuda a identificar shuffles df.groupBy(\"key\").agg(count(\"value\")).explain() # En el plan de ejecuci\u00f3n, buscar etapas como \"Exchange\", \"Sort\", \"HashPartitioning\" # Estas indican una operaci\u00f3n de shuffle. # Ejemplo de salida parcial de explain: # == Physical Plan == # *(2) HashAggregate(keys=[key#123], functions=[count(value#124)]) # +- Exchange hashpartitioning(key#123, 200), [id=#12] <--- Esto es un shuffle # +- *(1) HashAggregate(keys=[key#123], functions=[partial_count(value#124)]) # +- *(1) Project [key#123, value#124] # +- *(1) Scan ExistingRDD Mitigaci\u00f3n de Shuffles (ej. Broadcast Join, Co-locaci\u00f3n de datos): Broadcast Join: Como se mencion\u00f3, usar broadcast() para DataFrames peque\u00f1os evita el shuffle de la tabla grande. Co-locaci\u00f3n de datos: Si los datos que se van a unir o agrupar ya est\u00e1n particionados de forma similar en el sistema de archivos (ej. Parquet particionado por la clave de uni\u00f3n), Spark puede evitar shuffles completos. Esto se logra mediante la escritura de datos particionados ( df.write.partitionBy(...) ). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ShuffleMitigation\").getOrCreate() # Peque\u00f1o DataFrame de lookup lookup_data = [(1, \"RegionA\"), (2, \"RegionB\")] lookup_df = spark.createDataFrame(lookup_data, [\"region_id\", \"region_name\"]) # DataFrame grande sales_data = [(101, 1, 100), (102, 2, 150), (103, 1, 200)] sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"region_id\", \"amount\"]) # Forzar Broadcast Join para evitar shuffle de sales_df from pyspark.sql.functions import broadcast sales_df.join(broadcast(lookup_df), \"region_id\").explain() # Deber\u00edas ver BroadcastHashJoin en el plan # Ejemplo de co-locaci\u00f3n con escritura particionada (requiere planificaci\u00f3n previa) # df_large.write.partitionBy(\"join_key\").parquet(\"path/to/partitioned_data\") # df_other_large.write.partitionBy(\"join_key\").parquet(\"path/to/other_partitioned_data\") # Luego, al unirlos, si Spark detecta que est\u00e1n co-ubicados, puede usar un SortMergeJoin m\u00e1s eficiente Manejo de datos desequilibrados (Skewed Data): Cuando un valor de clave tiene significativamente m\u00e1s filas que otros, la partici\u00f3n correspondiente se convierte en un cuello de botella. Salting: A\u00f1adir un sufijo aleatorio a la clave desequilibrada en el DataFrame grande y replicar las filas de la clave desequilibrada en el DataFrame peque\u00f1o con los mismos sufijos. Spark 3.x Adaptive Query Execution (AQE): AQE puede detectar y manejar skew de forma autom\u00e1tica durante la ejecuci\u00f3n de los joins, dividiendo las particiones grandes en subparticiones m\u00e1s peque\u00f1as. Habilitar spark.sql.adaptive.enabled a true (es por defecto en Spark 3.2+). from pyspark.sql import SparkSession from pyspark.sql.functions import lit, rand, concat spark = SparkSession.builder \\ .appName(\"SkewedJoin\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") # Habilitar AQE .getOrCreate() # Simular datos desequilibrados: 'HotKey' tiene muchos m\u00e1s registros data_large = [(i, \"NormalKey\") for i in range(1000)] + \\ [(i, \"HotKey\") for i in range(9000)] large_df = spark.createDataFrame(data_large, [\"id\", \"join_key\"]) data_small = [(\"NormalKey\", \"ValueA\"), (\"HotKey\", \"ValueB\")] small_df = spark.createDataFrame(data_small, [\"join_key\", \"value\"]) # Uni\u00f3n est\u00e1ndar (puede ser lenta debido a HotKey si AQE no est\u00e1 activo o no es suficiente) result_df = large_df.join(small_df, \"join_key\", \"inner\") result_df.explain() # Observar si AQE detecta el skew (si est\u00e1 habilitado) # Ejemplo de Salting (manual): # A\u00f1adir un \"salt\" aleatorio a las claves num_salt_buckets = 10 salted_large_df = large_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (rand() * num_salt_buckets).cast(\"int\"))) salted_small_df = small_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (lit(0) + rand() * num_salt_buckets).cast(\"int\"))) # Si 'HotKey' tiene mucho skew, se replicar\u00eda 'HotKey' en small_df para cada sufijo de salt. # Luego, se unir\u00eda por (join_key, salted_key) # Uni\u00f3n con salting: # result_salted = salted_large_df.join(salted_small_df, on=\"salted_key\", how=\"inner\") # result_salted.show()","title":"Comprensi\u00f3n y Optimizaci\u00f3n de Shuffles"},{"location":"tema22/#tarea","text":"Crea un DataFrame de pedidos con las columnas order_id , customer_id , order_date (formato 'YYYY-MM-DD'), y amount (valor num\u00e9rico). Datos de ejemplo: [(\"ORD001\", \"C001\", \"2023-01-10\", 150.75), (\"ORD002\", \"C002\", \"2023-01-15\", 200.00), (\"ORD003\", \"C001\", \"2023-02-01\", 50.25), (\"ORD004\", \"C003\", \"2023-02-05\", 300.00)] Realiza las siguientes transformaciones: * A\u00f1ade una columna order_year que contenga solo el a\u00f1o de order_date . * A\u00f1ade una columna order_month que contenga el n\u00famero del mes de order_date . * A\u00f1ade una columna is_high_value que sea True si amount es mayor o igual a 100 , de lo contrario False . * Muestra el DataFrame resultante. Usando el DataFrame de pedidos del ejercicio 1, calcula el total_amount_spent y el num_orders por customer_id . Muestra los resultados. Crea un DataFrame de productos con columnas product_name y category . Datos de ejemplo: [(\"laptop Dell\", \"electronics\"), (\"teclado logitech\", \"electronics\"), (\"libro de cocina\", \"books\"), (\"Auriculares sony\", \"electronics\")] Realiza las siguientes transformaciones: * Normaliza la columna product_name a min\u00fasculas. * A\u00f1ade una columna brand que extraiga la primera palabra de product_name . * A\u00f1ade una columna product_type basada en la category : si es \"electronics\", \"Tech Gadget\"; si es \"books\", \"Reading Material\"; de lo contrario \"Other\". * Muestra el DataFrame resultante. Define una UDF de PySpark llamada classify_amount que tome un amount num\u00e9rico y devuelva una cadena: \"Peque\u00f1o\" si amount < 50 , \"Mediano\" si 50 <= amount < 200 , y \"Grande\" si amount >= 200 . Aplica esta UDF al DataFrame de pedidos del ejercicio 1 para crear una nueva columna amount_category . Muestra el DataFrame. Crea un segundo DataFrame de clientes con las columnas customer_id y customer_name . Datos de ejemplo: [(\"C001\", \"Ana Garcia\"), (\"C002\", \"Pedro Ruiz\"), (\"C003\", \"Laura Sanz\"), (\"C004\", \"Diego Marin\")] Realiza un INNER JOIN entre el DataFrame de pedidos (ejercicio 1) y el DataFrame de clientes para mostrar los order_id , customer_name y amount de cada pedido. Realiza un LEFT OUTER JOIN de clientes (izquierda) con pedidos (derecha). Muestra todos los clientes y sus pedidos (si los tienen). Filtra el resultado para mostrar solo los clientes que no han realizado ning\u00fan pedido (es decir, donde order_id es nulo). Crea un DataFrame de ventas_regionales con columnas region , product_category y sales_value . Datos de ejemplo: [(\"Norte\", \"Electronics\", 1000), (\"Norte\", \"Books\", 500), (\"Sur\", \"Electronics\", 1200), (\"Sur\", \"Books\", 600), (\"Centro\", \"Electronics\", 800)] Pivotea este DataFrame para mostrar sales_value por region (filas) y product_category (columnas). Crea un DataFrame con 1,000,000 de filas y dos columnas: id (entero secuencial) y random_value (n\u00famero aleatorio). Guarda este DataFrame en formato Parquet en un directorio temporal ( /tmp/large_data.parquet ). Lee el DataFrame de nuevo y averigua su n\u00famero de particiones. Reparticiona el DataFrame a 10 particiones y persist\u00e9lo en memoria ( MEMORY_AND_DISK ). Realiza una operaci\u00f3n de conteo y luego despersiste el DataFrame. Configura spark.sql.shuffle.partitions a un valor bajo (ej. 2) y luego a un valor m\u00e1s alto (ej. 20). Crea un DataFrame con una columna category que tenga 5 valores \u00fanicos y una columna value . Realiza una operaci\u00f3n groupBy por category y suma value . * Usa explain() para observar los planes de ejecuci\u00f3n y c\u00f3mo el n\u00famero de particiones de shuffle cambia. * (Opcional, para un entorno de cl\u00faster) Intenta medir el tiempo de ejecuci\u00f3n en ambos casos para ver el impacto. Crea un DataFrame dim_customers muy peque\u00f1o (ej. 10 filas, customer_id , customer_name ). Crea un DataFrame fact_transactions muy grande (ej. 1,000,000 filas, transaction_id , customer_id , amount ). Realiza un INNER JOIN entre fact_transactions y dim_customers en customer_id . * Usa .explain() para verificar si Spark ha utilizado autom\u00e1ticamente un BroadcastHashJoin . * Intenta forzar un BroadcastHashJoin si no se aplica autom\u00e1ticamente utilizando broadcast(dim_customers) en la uni\u00f3n, y verifica de nuevo el plan de ejecuci\u00f3n.","title":"Tarea"},{"location":"tema23/","text":"2. PySpark y SparkSQL Tema 2.3 Consultas y SQL en Spark Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender los fundamentos de SparkSQL como una interfaz declarativa para el procesamiento de datos distribuidos, ejecutar consultas SQL directamente sobre DataFrames y fuentes de datos, crear y gestionar vistas temporales, y aplicar sentencias SQL avanzadas para realizar an\u00e1lisis complejos y transformaciones eficientes de grandes vol\u00famenes de datos. Introducci\u00f3n : Spark SQL es un m\u00f3dulo de Apache Spark para trabajar con datos estructurados. Proporciona una interfaz unificada para interactuar con datos utilizando consultas SQL est\u00e1ndar, lo que lo convierte en una herramienta invaluable para analistas de datos, ingenieros y cient\u00edficos que ya est\u00e1n familiarizados con el lenguaje SQL. Permite a los usuarios consultar datos almacenados en DataFrames, as\u00ed como en diversas fuentes de datos como Parquet, ORC, JSON, CSV, bases de datos JDBC y Hive, aprovechando al mismo tiempo el motor de ejecuci\u00f3n optimizado de Spark para lograr un rendimiento excepcional en escala de Big Data. Desarrollo : Este tema explorar\u00e1 c\u00f3mo Spark SQL integra la potencia de SQL con la escalabilidad de Spark. Iniciaremos con los fundamentos de Spark SQL, comprendiendo c\u00f3mo los DataFrames pueden ser vistos y consultados como tablas relacionales. Luego, avanzaremos a la ejecuci\u00f3n de consultas SQL b\u00e1sicas y la creaci\u00f3n y gesti\u00f3n de vistas temporales, que son esenciales para estructurar flujos de trabajo SQL. Finalmente, nos sumergiremos en consultas SQL avanzadas, incluyendo uniones complejas, subconsultas, funciones de ventana y CTEs (Common Table Expressions), demostrando c\u00f3mo Spark SQL puede manejar escenarios de an\u00e1lisis y transformaci\u00f3n de datos altamente sofisticados. 2.3.1 Fundamentos de SparkSQL Spark SQL permite la ejecuci\u00f3n de consultas SQL sobre datos estructurados o semi-estructurados. Esencialmente, act\u00faa como un motor SQL distribuido, permitiendo a los usuarios interactuar con DataFrames como si fueran tablas de bases de datos tradicionales, combinando la familiaridad de SQL con el poder de procesamiento de Spark. La relaci\u00f3n entre DataFrames y Tablas/Vistas en SparkSQL La clave de SparkSQL reside en su capacidad para mapear DataFrames a estructuras relacionales como tablas o vistas. Esto permite que los datos en un DataFrame sean consultados usando sintaxis SQL est\u00e1ndar, lo que facilita la integraci\u00f3n para usuarios con experiencia en bases de datos relacionales. DataFrames como la base de SparkSQL: Todos los datos en SparkSQL se manejan como DataFrames. Cuando se ejecuta una consulta SQL, Spark la analiza, la optimiza y la ejecuta sobre los DataFrames subyacentes. El resultado de una consulta SQL es siempre un DataFrame. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLFundamentals\").getOrCreate() # Crear un DataFrame data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # El DataFrame es la representaci\u00f3n en memoria df.show() # +-------+---+----+ # | name|age|city| # +-------+---+----+ # | Alice| 25| NY| # | Bob| 30| LA| # |Charlie| 22| CHI| # +-------+---+----+ # Sin una vista temporal, no podemos consultarlo directamente con spark.sql try: spark.sql(\"SELECT * FROM my_table\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_table; spark.sql() para ejecutar consultas SQL: Esta es la funci\u00f3n principal para ejecutar consultas SQL directamente en el contexto de Spark. Las consultas se escriben como cadenas de texto. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLQuery\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Crear una vista temporal para poder consultar el DataFrame con SQL df.createOrReplaceTempView(\"people\") # Ejecutar una consulta SQL result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 25\") result_df.show() # Resultado: # +----+---+ # |name|age| # +----+---+ # | Bob| 30| # +----+---+ Optimizador Catalyst y Generaci\u00f3n de C\u00f3digo Tungsten: Spark SQL utiliza dos componentes clave para la optimizaci\u00f3n y ejecuci\u00f3n de consultas: Catalyst Optimizer: Un optimizador de consulta basado en reglas y costos que genera planes de ejecuci\u00f3n eficientes. Puede realizar optimizaciones como predicado pushdown, column pruning y reordenamiento de joins. Tungsten: Un motor de ejecuci\u00f3n que genera c\u00f3digo optimizado en tiempo de ejecuci\u00f3n para DataFrames, mejorando la eficiencia de la CPU y el uso de memoria a trav\u00e9s de t\u00e9cnicas como la eliminaci\u00f3n de punteros y la gesti\u00f3n expl\u00edcita de memoria. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CatalystTungsten\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\", 10), (\"Mouse\", 25, \"Electronics\", 50), (\"Book\", 15, \"Books\", 100)] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\", \"stock\"]) df.createOrReplaceTempView(\"products\") # Observar el plan de ejecuci\u00f3n l\u00f3gico y f\u00edsico (Catalyst y Tungsten en acci\u00f3n) # Explain muestra c\u00f3mo Spark traduce la consulta SQL a una serie de operaciones optimizadas spark.sql(\"SELECT product_name, price FROM products WHERE category = 'Electronics' AND price > 100\").explain(extended=True) # La salida mostrar\u00e1 el plan l\u00f3gico (Parsed, Analyzed, Optimized) y el plan f\u00edsico. # Notar \"PushedFilters\" y \"PushedProjections\" que son optimizaciones de Catalyst. # Los operadores f\u00edsicos son implementaciones optimizadas por Tungsten. 2.3.2 Consultas b\u00e1sicas con SparkSQL Una vez que un DataFrame se ha registrado como una vista temporal, se pueden realizar las operaciones de SQL m\u00e1s comunes sobre \u00e9l. Estas operaciones son equivalentes a las transformaciones de DataFrame API, pero expresadas en un lenguaje declarativo. SELECT y FROM La base de cualquier consulta SQL, permitiendo especificar qu\u00e9 columnas se quieren recuperar y de qu\u00e9 fuente de datos. Selecci\u00f3n de todas las columnas ( SELECT * ): Recupera todas las columnas de una vista o tabla. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"people\") spark.sql(\"SELECT * FROM people\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ Selecci\u00f3n de columnas espec\u00edficas ( SELECT column1, column2 ): Permite especificar las columnas que se desean recuperar, optimizando el rendimiento al no leer datos innecesarios. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.createOrReplaceTempView(\"users\") spark.sql(\"SELECT name, city FROM users\").show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # +-----+----+ Renombrar columnas con AS : Asigna un alias a una columna para hacer el resultado m\u00e1s legible o para evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"ProductA\", 100), (\"ProductB\", 200)] df = spark.createDataFrame(data, [\"item_name\", \"item_price\"]) df.createOrReplaceTempView(\"items\") spark.sql(\"SELECT item_name AS product, item_price AS price FROM items\").show() # Resultado: # +--------+-----+ # | product|price| # +--------+-----+ # |ProductA| 100| # |ProductB| 200| # +--------+-----+ WHERE (Filtrado) La cl\u00e1usula WHERE se utiliza para filtrar filas bas\u00e1ndose en una o m\u00e1s condiciones, al igual que en SQL tradicional. Condiciones de igualdad y desigualdad ( = , != , < , > , <= , >= ): Filtra filas donde una columna cumple una condici\u00f3n de comparaci\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhere\").getOrCreate() data = [(\"Juan\", 30), (\"Maria\", 25), (\"Pedro\", 35)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"employees\") spark.sql(\"SELECT name, age FROM employees WHERE age > 28\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # | Juan| 30| # |Pedro| 35| # +-----+---+ Operadores l\u00f3gicos ( AND , OR , NOT ): Combina m\u00faltiples condiciones de filtrado. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereLogic\").getOrCreate() data = [(\"Shirt\", \"Blue\", \"Small\"), (\"Pants\", \"Black\", \"Medium\"), (\"T-Shirt\", \"Red\", \"Large\")] df = spark.createDataFrame(data, [\"item\", \"color\", \"size\"]) df.createOrReplaceTempView(\"apparel\") spark.sql(\"SELECT item, color FROM apparel WHERE color = 'Blue' OR size = 'Large'\").show() # Resultado: # +-------+-----+ # | item|color| # +-------+-----+ # | Shirt| Blue| # |T-Shirt| Red| # +-------+-----+ LIKE , IN , BETWEEN , IS NULL / IS NOT NULL : Funciones de filtrado comunes para patrones, listas de valores, rangos y valores nulos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereSpecial\").getOrCreate() data = [(\"Apple\", 1.0), (\"Banana\", 0.5), (\"Cherry\", 2.0), (\"Date\", None)] df = spark.createDataFrame(data, [\"fruit\", \"price\"]) df.createOrReplaceTempView(\"fruits\") # LIKE spark.sql(\"SELECT fruit FROM fruits WHERE fruit LIKE 'B%'\").show() # Resultado: # +------+ # | fruit| # +------+ # |Banana| # +------+ # IN spark.sql(\"SELECT fruit FROM fruits WHERE fruit IN ('Apple', 'Cherry')\").show() # Resultado: # +------+ # | fruit| # +------+ # | Apple| # |Cherry| # +------+ # BETWEEN spark.sql(\"SELECT fruit, price FROM fruits WHERE price BETWEEN 0.8 AND 1.5\").show() # Resultado: # +-----+-----+ # |fruit|price| # +-----+-----+ # |Apple| 1.0| # +-----+-----+ # IS NULL / IS NOT NULL spark.sql(\"SELECT fruit, price FROM fruits WHERE price IS NULL\").show() # Resultado: # +----+-----+ # |fruit|price| # +----+-----+ # |Date| null| # +----+-----+ GROUP BY y HAVING (Agregaci\u00f3n) GROUP BY se utiliza para agrupar filas que tienen los mismos valores en una o m\u00e1s columnas, permitiendo aplicar funciones de agregaci\u00f3n. HAVING se usa para filtrar los resultados de las agregaciones. Funciones de agregaci\u00f3n ( COUNT , SUM , AVG , MIN , MAX ): Permiten resumir datos dentro de cada grupo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLAggregation\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") spark.sql(\"SELECT department, SUM(salary) AS total_salary, COUNT(*) AS num_employees FROM salaries GROUP BY department\").show() # Resultado: # +----------+------------+-------------+ # |department|total_salary|num_employees| # +----------+------------+-------------+ # | DeptA| 300| 2| # | DeptB| 250| 2| # | DeptC| 50| 1| # +----------+------------+-------------+ HAVING para filtrar resultados de agregaci\u00f3n: Aplica condiciones de filtrado despu\u00e9s de que las agregaciones se han calculado, a diferencia de WHERE que filtra antes. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLHaving\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") # Sumar salarios por departamento, pero solo para aquellos departamentos con un total de salarios > 200 spark.sql(\"SELECT department, SUM(salary) AS total_salary FROM salaries GROUP BY department HAVING SUM(salary) > 200\").show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | DeptA| 300| # +----------+------------+ 2.3.3 Creaci\u00f3n y uso de vistas temporales Las vistas temporales en SparkSQL son referencias a DataFrames que se registran en el cat\u00e1logo de Spark como si fueran tablas de bases de datos. Son \"temporales\" porque solo duran mientras la SparkSession est\u00e9 activa, o hasta que se eliminen expl\u00edcitamente. Son esenciales para permitir que las consultas SQL interact\u00faen con los DataFrames. Vistas Temporales Las vistas temporales son la forma principal de exponer un DataFrame para ser consultado mediante SQL. createTempView() vs. createOrReplaceTempView() : createTempView(view_name) : Crea una vista temporal con el nombre especificado. Si ya existe una vista con ese nombre, lanzar\u00e1 un error. createOrReplaceTempView(view_name) : Crea o reemplaza una vista temporal. Si una vista con el mismo nombre ya existe, la reemplazar\u00e1 sin lanzar un error. Esta es la m\u00e1s com\u00fanmente utilizada. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"TempViews\").getOrCreate() data1 = [(\"Laptop\", 1200)] df1 = spark.createDataFrame(data1, [\"item\", \"price\"]) data2 = [(\"Monitor\", 300)] df2 = spark.createDataFrame(data2, [\"item\", \"price\"]) # Usar createTempView (fallar\u00e1 si se ejecuta dos veces sin borrar) df1.createTempView(\"products_v1\") spark.sql(\"SELECT * FROM products_v1\").show() # Usar createOrReplaceTempView (permite sobrescribir) df2.createOrReplaceTempView(\"products_v1\") # Reemplaza la vista existente spark.sql(\"SELECT * FROM products_v1\").show() # Resultado: # +-------+-----+ # | item|price| # +-------+-----+ # |Monitor| 300| # +-------+-----+ Alcance de las vistas temporales (sesi\u00f3n): Las vistas temporales son locales a la SparkSession en la que se crearon. No son visibles para otras SparkSession s ni persisten despu\u00e9s de que la SparkSession finaliza. from pyspark.sql import SparkSession spark1 = SparkSession.builder.appName(\"Session1\").getOrCreate() spark2 = SparkSession.builder.appName(\"Session2\").getOrCreate() df_session1 = spark1.createDataFrame([(\"Data1\",)], [\"col\"]) df_session1.createOrReplaceTempView(\"my_data_session1\") # Consulta en la misma sesi\u00f3n (spark1) spark1.sql(\"SELECT * FROM my_data_session1\").show() # Intentar consultar desde otra sesi\u00f3n (spark2) - esto fallar\u00e1 try: spark2.sql(\"SELECT * FROM my_data_session1\").show() except Exception as e: print(f\"Error esperado en spark2: {e}\") # Resultado: Error esperado en spark2: Table or view not found: my_data_session1; Eliminaci\u00f3n de vistas temporales ( dropTempView() ): Aunque son temporales, es buena pr\u00e1ctica eliminarlas expl\u00edcitamente si ya no se necesitan para liberar recursos o evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropView\").getOrCreate() df = spark.createDataFrame([(\"test\",)], [\"col\"]) df.createOrReplaceTempView(\"my_test_view\") spark.sql(\"SELECT * FROM my_test_view\").show() spark.catalog.dropTempView(\"my_test_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM my_test_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_test_view; Vistas Globales Temporales A diferencia de las vistas temporales, las vistas globales temporales son visibles a trav\u00e9s de todas las SparkSession s dentro de la misma aplicaci\u00f3n Spark. Esto las hace \u00fatiles para compartir datos entre diferentes m\u00f3dulos o usuarios de una misma aplicaci\u00f3n. createGlobalTempView() vs. createOrReplaceGlobalTempView() : createGlobalTempView(view_name) : Crea una vista global temporal. Lanza un error si ya existe. createOrReplaceGlobalTempView(view_name) : Crea o reemplaza una vista global temporal. Las vistas globales temporales se acceden con el prefijo global_temp . view_name . from pyspark.sql import SparkSession # Session 1 spark1 = SparkSession.builder.appName(\"GlobalViewSession1\").getOrCreate() df_global = spark1.createDataFrame([(\"GlobalData\",)], [\"data_col\"]) df_global.createOrReplaceGlobalTempView(\"shared_data\") spark1.sql(\"SELECT * FROM global_temp.shared_data\").show() # Session 2 spark2 = SparkSession.builder.appName(\"GlobalViewSession2\").getOrCreate() # La vista global es accesible desde otra SparkSession spark2.sql(\"SELECT * FROM global_temp.shared_data\").show() # Resultado (desde spark2): # +----------+ # | data_col| # +----------+ # |GlobalData| # +----------+ Alcance de las vistas globales temporales (aplicaci\u00f3n Spark): Persisten mientras la aplicaci\u00f3n Spark est\u00e9 activa (es decir, el proceso SparkContext est\u00e9 en ejecuci\u00f3n). Se eliminan cuando la aplicaci\u00f3n finaliza. El ejemplo anterior demuestra el alcance a trav\u00e9s de sesiones, una vez que el script o la aplicaci\u00f3n Spark finalizan, la vista global temporal se destruye. Eliminaci\u00f3n de vistas globales temporales ( dropGlobalTempView() ): Se pueden eliminar expl\u00edcitamente si ya no se necesitan. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropGlobalView\").getOrCreate() df = spark.createDataFrame([(\"global test\",)], [\"col\"]) df.createOrReplaceGlobalTempView(\"my_global_view\") spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() spark.catalog.dropGlobalTempView(\"my_global_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_global_view; 2.3.4 Consultas avanzadas con SparkSQL SparkSQL no se limita a consultas b\u00e1sicas; soporta caracter\u00edsticas SQL avanzadas que son fundamentales para an\u00e1lisis complejos y transformaciones de datos, como uniones complejas, subconsultas, funciones de ventana y Common Table Expressions (CTEs). Uniones (JOINs) avanzadas M\u00e1s all\u00e1 del INNER JOIN , SparkSQL soporta una gama completa de tipos de uniones, incluyendo LEFT OUTER , RIGHT OUTER , FULL OUTER , LEFT SEMI y LEFT ANTI JOIN . Tipos de JOINs ( INNER , LEFT OUTER , RIGHT OUTER , FULL OUTER ): Comprender las diferencias entre los tipos de uniones es crucial para obtener los resultados deseados. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"AdvancedJoins\").getOrCreate() # Tabla de empleados employees_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 103)] employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"]) employees_df.createOrReplaceTempView(\"employees\") # Tabla de departamentos departments_data = [(101, \"Sales\"), (102, \"HR\"), (104, \"IT\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"]) departments_df.createOrReplaceTempView(\"departments\") # LEFT OUTER JOIN (todas las filas de la izquierda, coincidencias de la derecha o NULLs) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e LEFT OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Alice| Sales| # | Bob| HR| # | Charlie| null| # +---------+---------+ # FULL OUTER JOIN (todas las filas de ambas tablas, con NULLs donde no hay coincidencia) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e FULL OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Charlie| null| # | Alice| Sales| # | Bob| HR| # | null| IT| # +---------+---------+ LEFT SEMI JOIN y LEFT ANTI JOIN (Subconsultas optimizadas): LEFT SEMI JOIN : Retorna las filas del lado izquierdo (primera tabla) que tienen una coincidencia en el lado derecho (segunda tabla). No incluye columnas de la tabla derecha. Es similar a un INNER JOIN pero solo retorna columnas de la tabla izquierda y es m\u00e1s eficiente. LEFT ANTI JOIN : Retorna las filas del lado izquierdo que no tienen una coincidencia en el lado derecho. \u00datil para encontrar registros hu\u00e9rfanos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SemiAntiJoins\").getOrCreate() # Tabla de clientes customers_data = [(1, \"Ana\"), (2, \"Luis\"), (3, \"Marta\"), (4, \"Pedro\")] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\"]) customers_df.createOrReplaceTempView(\"customers\") # Tabla de pedidos orders_data = [(101, 1, 50), (102, 2, 75), (103, 1, 120)] # Marta y Pedro no tienen pedidos orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\"]) orders_df.createOrReplaceTempView(\"orders\") # LEFT SEMI JOIN (clientes que tienen al menos un pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT SEMI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Ana| # | Luis| # +-------------+ # LEFT ANTI JOIN (clientes que NO tienen ning\u00fan pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT ANTI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Marta| # | Pedro| # +-------------+ Subconsultas y CTEs (Common Table Expressions) Las subconsultas y CTEs permiten dividir consultas complejas en partes m\u00e1s manejables, mejorando la legibilidad y, en algunos casos, la optimizaci\u00f3n. Subconsultas en WHERE (IN, EXISTS): Permiten filtrar resultados bas\u00e1ndose en los valores de otra consulta. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Subqueries\").getOrCreate() # Tabla de productos products_data = [(1, \"Laptop\", 1200), (2, \"Mouse\", 25), (3, \"Keyboard\", 75), (4, \"Monitor\", 300)] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"]) products_df.createOrReplaceTempView(\"products\") # Tabla de stock stock_data = [(1, 10), (3, 5), (5, 20)] # Producto 2 y 4 no tienen stock stock_df = spark.createDataFrame(stock_data, [\"product_id\", \"quantity\"]) stock_df.createOrReplaceTempView(\"stock\") # Seleccionar productos que tienen stock (usando IN) spark.sql(\"\"\" SELECT product_name, price FROM products WHERE product_id IN (SELECT product_id FROM stock) \"\"\").show() # Resultado: # +------------+-----+ # |product_name|price| # +------------+-----+ # | Laptop| 1200| # | Keyboard| 75| # +------------+-----+ CTEs (Common Table Expressions) con WITH : Las CTEs definen un conjunto de resultados temporal y nombrado que se puede referenciar dentro de una \u00fanica sentencia SELECT , INSERT , UPDATE o DELETE . Mejoran la legibilidad y la modularidad de las consultas complejas. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CTEs\").getOrCreate() # Tabla de ventas sales_data = [(\"RegionA\", \"Q1\", 1000), (\"RegionA\", \"Q2\", 1200), (\"RegionB\", \"Q1\", 800), (\"RegionB\", \"Q2\", 900), (\"RegionA\", \"Q3\", 1500)] sales_df = spark.createDataFrame(sales_data, [\"region\", \"quarter\", \"revenue\"]) sales_df.createOrReplaceTempView(\"sales\") # Calcular el promedio de ingresos por regi\u00f3n usando una CTE spark.sql(\"\"\" WITH RegionalRevenue AS ( SELECT region, SUM(revenue) AS total_region_revenue FROM sales GROUP BY region ) SELECT r.region, r.total_region_revenue, (SELECT AVG(total_region_revenue) FROM RegionalRevenue) AS overall_avg_revenue FROM RegionalRevenue r \"\"\").show() # Resultado: # +-------+------------------+-------------------+ # | region|total_region_revenue|overall_avg_revenue| # +-------+------------------+-------------------+ # |RegionA| 3700| 2200.0| # |RegionB| 1700| 2200.0| # +-------+------------------+-------------------+ Funciones de Ventana (Window Functions): Permiten realizar c\u00e1lculos sobre un conjunto de filas relacionadas con la fila actual ( PARTITION BY , ORDER BY , ROWS BETWEEN / RANGE BETWEEN ). Son muy potentes para c\u00e1lculos anal\u00edticos como promedios m\u00f3viles, rankings, sumas acumuladas, etc. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate() # Tabla de transacciones transactions_data = [(\"StoreA\", \"2023-01-01\", 100), (\"StoreA\", \"2023-01-02\", 150), (\"StoreA\", \"2023-01-03\", 80), (\"StoreB\", \"2023-01-01\", 200), (\"StoreB\", \"2023-01-02\", 120)] transactions_df = spark.createDataFrame(transactions_data, [\"store_id\", \"sale_date\", \"amount\"]) transactions_df.createOrReplaceTempView(\"transactions\") # Calcular la suma acumulada de ventas por tienda, ordenada por fecha spark.sql(\"\"\" SELECT store_id, sale_date, amount, SUM(amount) OVER (PARTITION BY store_id ORDER BY sale_date) AS running_total FROM transactions ORDER BY store_id, sale_date \"\"\").show() # Resultado: # +--------+----------+------+-------------+ # |store_id| sale_date|amount|running_total| # +--------+----------+------+-------------+ # | StoreA|2023-01-01| 100| 100| # | StoreA|2023-01-02| 150| 250| # | StoreA|2023-01-03| 80| 330| # | StoreB|2023-01-01| 200| 200| # | StoreB|2023-01-02| 120| 320| # +--------+----------+------+-------------+ # Ranking de transacciones por tienda spark.sql(\"\"\" SELECT store_id, sale_date, amount, ROW_NUMBER() OVER (PARTITION BY store_id ORDER BY amount DESC) AS rank_by_amount FROM transactions ORDER BY store_id, rank_by_amount \"\"\").show() # Resultado: # +--------+----------+------+------------+ # |store_id| sale_date|amount|rank_by_amount| # +--------+----------+------+------------+ # | StoreA|2023-01-02| 150| 1| # | StoreA|2023-01-01| 100| 2| # | StoreA|2023-01-03| 80| 3| # | StoreB|2023-01-01| 200| 1| # | StoreB|2023-01-02| 120| 2| # +--------+----------+------+------------+ Tarea Ejercicios Pr\u00e1cticos relacionados con el tema 2.3 Crea un DataFrame de estudiantes con las columnas id , nombre , carrera y promedio_calificaciones . Datos de ejemplo: [(1, \"Juan\", \"Ingenier\u00eda\", 4.2), (2, \"Maria\", \"Medicina\", 3.8), (3, \"Pedro\", \"Ingenier\u00eda\", 4.5), (4, \"Ana\", \"Derecho\", 3.5)] . Registra este DataFrame como una vista temporal llamada estudiantes_temp . Luego, escribe una consulta SparkSQL que seleccione el nombre y la carrera de todos los estudiantes con un promedio_calificaciones superior a 4.0 . Usando la vista estudiantes_temp , escribe una consulta SparkSQL que seleccione el nombre , carrera y promedio_calificaciones de los estudiantes cuya carrera sea 'Ingenier\u00eda' Y cuyo nombre empiece con 'P'. Crea un DataFrame de ventas con columnas region , producto y cantidad . Datos de ejemplo: [(\"Norte\", \"A\", 10), (\"Norte\", \"B\", 15), (\"Sur\", \"A\", 20), (\"Norte\", \"A\", 5), (\"Sur\", \"B\", 12)] . Registra este DataFrame como ventas_temp . Escribe una consulta SparkSQL que calcule la SUM de cantidad por region y producto , pero solo para aquellos grupos donde la cantidad total sea mayor o igual a 25 . Crea un DataFrame de departamentos con id_departamento y nombre_departamento . Datos de ejemplo: [(1, \"Ventas\"), (2, \"Marketing\"), (3, \"Recursos Humanos\")] . Registra este DataFrame como departamentos_temp . Realiza un INNER JOIN entre estudiantes_temp (asume que carrera se puede unir con nombre_departamento ) y departamentos_temp para mostrar el nombre del estudiante y el nombre_departamento al que pertenecen. Nota: Para simplificar, puedes asumir que carrera de estudiantes_temp corresponde a nombre_departamento en departamentos_temp . Crea un DataFrame de productos_disponibles con producto_id y nombre_producto . Datos de ejemplo: [(101, \"Laptop\"), (102, \"Mouse\"), (103, \"Teclado\"), (104, \"Monitor\")] . Registra como productos_disponibles_temp . Crea otro DataFrame de productos_vendidos con producto_id y fecha_venta . Datos de ejemplo: [(101, \"2023-01-01\"), (103, \"2023-01-05\")] . Registra como productos_vendidos_temp . Escribe una consulta SparkSQL que encuentre los nombre_producto de los productos que nunca han sido vendidos. Utilizando las vistas productos_disponibles_temp y productos_vendidos_temp del ejercicio anterior, escribe una consulta SparkSQL usando una subconsulta con IN para seleccionar los nombre_producto de los productos que s\u00ed han sido vendidos. Crea un DataFrame de empleados_salarios con id_empleado , departamento y salario . Datos de ejemplo: [(1, \"IT\", 60000), (2, \"IT\", 70000), (3, \"HR\", 50000), (4, \"HR\", 55000), (5, \"IT\", 65000)] . Registra como empleados_salarios_temp . Usa una CTE para calcular el salario_promedio_departamental para cada departamento . Luego, en la consulta principal, selecciona id_empleado , departamento , salario y el salario_promedio_departamental de su departamento. Crea un DataFrame de puntajes_examen con clase , estudiante y puntaje . Datos de ejemplo: [(\"A\", \"Alice\", 90), (\"A\", \"Bob\", 85), (\"A\", \"Charlie\", 92), (\"B\", \"David\", 78), (\"B\", \"Eve\", 95)] . Registra como puntajes_examen_temp . Usa una funci\u00f3n de ventana para calcular el ranking de puntaje para cada estudiante dentro de cada clase , ordenando de mayor a menor puntaje. Muestra clase , estudiante , puntaje y ranking . Utilizando la vista ventas_temp del ejercicio 3 (a\u00f1ade una columna fecha_venta si no la tienes, ej. \"2023-01-01\" para todos por simplicidad, o mejor, crea nuevos datos con fechas distintas para cada venta en la misma regi\u00f3n): [(\"Norte\", \"A\", 10, \"2023-01-01\"), (\"Norte\", \"B\", 15, \"2023-01-02\"), (\"Norte\", \"A\", 5, \"2023-01-03\")] . Registra como ventas_fecha_temp . Calcula la cantidad_acumulada de ventas por region , ordenada por fecha_venta . Crea dos DataFrames: usuarios ( user_id , name ) y compras ( purchase_id , user_id , amount , purchase_date ). usuarios ejemplo: [(1, \"User A\"), (2, \"User B\")] compras ejemplo: [(101, 1, 50.0, \"2023-01-01\"), (102, 1, 75.0, \"2023-01-10\"), (103, 2, 120.0, \"2023-01-15\")] Registra ambos como vistas temporales ( users_temp , purchases_temp ). Usa una CTE para encontrar el total_comprado por cada user_id . Luego, en la consulta principal, une la CTE con users_temp para mostrar name y su total_comprado .","title":"Consultas y SQL en Spark"},{"location":"tema23/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema23/#tema-23-consultas-y-sql-en-spark","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender los fundamentos de SparkSQL como una interfaz declarativa para el procesamiento de datos distribuidos, ejecutar consultas SQL directamente sobre DataFrames y fuentes de datos, crear y gestionar vistas temporales, y aplicar sentencias SQL avanzadas para realizar an\u00e1lisis complejos y transformaciones eficientes de grandes vol\u00famenes de datos. Introducci\u00f3n : Spark SQL es un m\u00f3dulo de Apache Spark para trabajar con datos estructurados. Proporciona una interfaz unificada para interactuar con datos utilizando consultas SQL est\u00e1ndar, lo que lo convierte en una herramienta invaluable para analistas de datos, ingenieros y cient\u00edficos que ya est\u00e1n familiarizados con el lenguaje SQL. Permite a los usuarios consultar datos almacenados en DataFrames, as\u00ed como en diversas fuentes de datos como Parquet, ORC, JSON, CSV, bases de datos JDBC y Hive, aprovechando al mismo tiempo el motor de ejecuci\u00f3n optimizado de Spark para lograr un rendimiento excepcional en escala de Big Data. Desarrollo : Este tema explorar\u00e1 c\u00f3mo Spark SQL integra la potencia de SQL con la escalabilidad de Spark. Iniciaremos con los fundamentos de Spark SQL, comprendiendo c\u00f3mo los DataFrames pueden ser vistos y consultados como tablas relacionales. Luego, avanzaremos a la ejecuci\u00f3n de consultas SQL b\u00e1sicas y la creaci\u00f3n y gesti\u00f3n de vistas temporales, que son esenciales para estructurar flujos de trabajo SQL. Finalmente, nos sumergiremos en consultas SQL avanzadas, incluyendo uniones complejas, subconsultas, funciones de ventana y CTEs (Common Table Expressions), demostrando c\u00f3mo Spark SQL puede manejar escenarios de an\u00e1lisis y transformaci\u00f3n de datos altamente sofisticados.","title":"Tema 2.3 Consultas y SQL en Spark"},{"location":"tema23/#231-fundamentos-de-sparksql","text":"Spark SQL permite la ejecuci\u00f3n de consultas SQL sobre datos estructurados o semi-estructurados. Esencialmente, act\u00faa como un motor SQL distribuido, permitiendo a los usuarios interactuar con DataFrames como si fueran tablas de bases de datos tradicionales, combinando la familiaridad de SQL con el poder de procesamiento de Spark.","title":"2.3.1 Fundamentos de SparkSQL"},{"location":"tema23/#la-relacion-entre-dataframes-y-tablasvistas-en-sparksql","text":"La clave de SparkSQL reside en su capacidad para mapear DataFrames a estructuras relacionales como tablas o vistas. Esto permite que los datos en un DataFrame sean consultados usando sintaxis SQL est\u00e1ndar, lo que facilita la integraci\u00f3n para usuarios con experiencia en bases de datos relacionales. DataFrames como la base de SparkSQL: Todos los datos en SparkSQL se manejan como DataFrames. Cuando se ejecuta una consulta SQL, Spark la analiza, la optimiza y la ejecuta sobre los DataFrames subyacentes. El resultado de una consulta SQL es siempre un DataFrame. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLFundamentals\").getOrCreate() # Crear un DataFrame data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # El DataFrame es la representaci\u00f3n en memoria df.show() # +-------+---+----+ # | name|age|city| # +-------+---+----+ # | Alice| 25| NY| # | Bob| 30| LA| # |Charlie| 22| CHI| # +-------+---+----+ # Sin una vista temporal, no podemos consultarlo directamente con spark.sql try: spark.sql(\"SELECT * FROM my_table\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_table; spark.sql() para ejecutar consultas SQL: Esta es la funci\u00f3n principal para ejecutar consultas SQL directamente en el contexto de Spark. Las consultas se escriben como cadenas de texto. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLQuery\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Crear una vista temporal para poder consultar el DataFrame con SQL df.createOrReplaceTempView(\"people\") # Ejecutar una consulta SQL result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 25\") result_df.show() # Resultado: # +----+---+ # |name|age| # +----+---+ # | Bob| 30| # +----+---+ Optimizador Catalyst y Generaci\u00f3n de C\u00f3digo Tungsten: Spark SQL utiliza dos componentes clave para la optimizaci\u00f3n y ejecuci\u00f3n de consultas: Catalyst Optimizer: Un optimizador de consulta basado en reglas y costos que genera planes de ejecuci\u00f3n eficientes. Puede realizar optimizaciones como predicado pushdown, column pruning y reordenamiento de joins. Tungsten: Un motor de ejecuci\u00f3n que genera c\u00f3digo optimizado en tiempo de ejecuci\u00f3n para DataFrames, mejorando la eficiencia de la CPU y el uso de memoria a trav\u00e9s de t\u00e9cnicas como la eliminaci\u00f3n de punteros y la gesti\u00f3n expl\u00edcita de memoria. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CatalystTungsten\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\", 10), (\"Mouse\", 25, \"Electronics\", 50), (\"Book\", 15, \"Books\", 100)] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\", \"stock\"]) df.createOrReplaceTempView(\"products\") # Observar el plan de ejecuci\u00f3n l\u00f3gico y f\u00edsico (Catalyst y Tungsten en acci\u00f3n) # Explain muestra c\u00f3mo Spark traduce la consulta SQL a una serie de operaciones optimizadas spark.sql(\"SELECT product_name, price FROM products WHERE category = 'Electronics' AND price > 100\").explain(extended=True) # La salida mostrar\u00e1 el plan l\u00f3gico (Parsed, Analyzed, Optimized) y el plan f\u00edsico. # Notar \"PushedFilters\" y \"PushedProjections\" que son optimizaciones de Catalyst. # Los operadores f\u00edsicos son implementaciones optimizadas por Tungsten.","title":"La relaci\u00f3n entre DataFrames y Tablas/Vistas en SparkSQL"},{"location":"tema23/#232-consultas-basicas-con-sparksql","text":"Una vez que un DataFrame se ha registrado como una vista temporal, se pueden realizar las operaciones de SQL m\u00e1s comunes sobre \u00e9l. Estas operaciones son equivalentes a las transformaciones de DataFrame API, pero expresadas en un lenguaje declarativo.","title":"2.3.2 Consultas b\u00e1sicas con SparkSQL"},{"location":"tema23/#select-y-from","text":"La base de cualquier consulta SQL, permitiendo especificar qu\u00e9 columnas se quieren recuperar y de qu\u00e9 fuente de datos. Selecci\u00f3n de todas las columnas ( SELECT * ): Recupera todas las columnas de una vista o tabla. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"people\") spark.sql(\"SELECT * FROM people\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ Selecci\u00f3n de columnas espec\u00edficas ( SELECT column1, column2 ): Permite especificar las columnas que se desean recuperar, optimizando el rendimiento al no leer datos innecesarios. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.createOrReplaceTempView(\"users\") spark.sql(\"SELECT name, city FROM users\").show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # +-----+----+ Renombrar columnas con AS : Asigna un alias a una columna para hacer el resultado m\u00e1s legible o para evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"ProductA\", 100), (\"ProductB\", 200)] df = spark.createDataFrame(data, [\"item_name\", \"item_price\"]) df.createOrReplaceTempView(\"items\") spark.sql(\"SELECT item_name AS product, item_price AS price FROM items\").show() # Resultado: # +--------+-----+ # | product|price| # +--------+-----+ # |ProductA| 100| # |ProductB| 200| # +--------+-----+","title":"SELECT y FROM"},{"location":"tema23/#where-filtrado","text":"La cl\u00e1usula WHERE se utiliza para filtrar filas bas\u00e1ndose en una o m\u00e1s condiciones, al igual que en SQL tradicional. Condiciones de igualdad y desigualdad ( = , != , < , > , <= , >= ): Filtra filas donde una columna cumple una condici\u00f3n de comparaci\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhere\").getOrCreate() data = [(\"Juan\", 30), (\"Maria\", 25), (\"Pedro\", 35)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"employees\") spark.sql(\"SELECT name, age FROM employees WHERE age > 28\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # | Juan| 30| # |Pedro| 35| # +-----+---+ Operadores l\u00f3gicos ( AND , OR , NOT ): Combina m\u00faltiples condiciones de filtrado. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereLogic\").getOrCreate() data = [(\"Shirt\", \"Blue\", \"Small\"), (\"Pants\", \"Black\", \"Medium\"), (\"T-Shirt\", \"Red\", \"Large\")] df = spark.createDataFrame(data, [\"item\", \"color\", \"size\"]) df.createOrReplaceTempView(\"apparel\") spark.sql(\"SELECT item, color FROM apparel WHERE color = 'Blue' OR size = 'Large'\").show() # Resultado: # +-------+-----+ # | item|color| # +-------+-----+ # | Shirt| Blue| # |T-Shirt| Red| # +-------+-----+ LIKE , IN , BETWEEN , IS NULL / IS NOT NULL : Funciones de filtrado comunes para patrones, listas de valores, rangos y valores nulos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereSpecial\").getOrCreate() data = [(\"Apple\", 1.0), (\"Banana\", 0.5), (\"Cherry\", 2.0), (\"Date\", None)] df = spark.createDataFrame(data, [\"fruit\", \"price\"]) df.createOrReplaceTempView(\"fruits\") # LIKE spark.sql(\"SELECT fruit FROM fruits WHERE fruit LIKE 'B%'\").show() # Resultado: # +------+ # | fruit| # +------+ # |Banana| # +------+ # IN spark.sql(\"SELECT fruit FROM fruits WHERE fruit IN ('Apple', 'Cherry')\").show() # Resultado: # +------+ # | fruit| # +------+ # | Apple| # |Cherry| # +------+ # BETWEEN spark.sql(\"SELECT fruit, price FROM fruits WHERE price BETWEEN 0.8 AND 1.5\").show() # Resultado: # +-----+-----+ # |fruit|price| # +-----+-----+ # |Apple| 1.0| # +-----+-----+ # IS NULL / IS NOT NULL spark.sql(\"SELECT fruit, price FROM fruits WHERE price IS NULL\").show() # Resultado: # +----+-----+ # |fruit|price| # +----+-----+ # |Date| null| # +----+-----+","title":"WHERE (Filtrado)"},{"location":"tema23/#group-by-y-having-agregacion","text":"GROUP BY se utiliza para agrupar filas que tienen los mismos valores en una o m\u00e1s columnas, permitiendo aplicar funciones de agregaci\u00f3n. HAVING se usa para filtrar los resultados de las agregaciones. Funciones de agregaci\u00f3n ( COUNT , SUM , AVG , MIN , MAX ): Permiten resumir datos dentro de cada grupo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLAggregation\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") spark.sql(\"SELECT department, SUM(salary) AS total_salary, COUNT(*) AS num_employees FROM salaries GROUP BY department\").show() # Resultado: # +----------+------------+-------------+ # |department|total_salary|num_employees| # +----------+------------+-------------+ # | DeptA| 300| 2| # | DeptB| 250| 2| # | DeptC| 50| 1| # +----------+------------+-------------+ HAVING para filtrar resultados de agregaci\u00f3n: Aplica condiciones de filtrado despu\u00e9s de que las agregaciones se han calculado, a diferencia de WHERE que filtra antes. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLHaving\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") # Sumar salarios por departamento, pero solo para aquellos departamentos con un total de salarios > 200 spark.sql(\"SELECT department, SUM(salary) AS total_salary FROM salaries GROUP BY department HAVING SUM(salary) > 200\").show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | DeptA| 300| # +----------+------------+","title":"GROUP BY y HAVING (Agregaci\u00f3n)"},{"location":"tema23/#233-creacion-y-uso-de-vistas-temporales","text":"Las vistas temporales en SparkSQL son referencias a DataFrames que se registran en el cat\u00e1logo de Spark como si fueran tablas de bases de datos. Son \"temporales\" porque solo duran mientras la SparkSession est\u00e9 activa, o hasta que se eliminen expl\u00edcitamente. Son esenciales para permitir que las consultas SQL interact\u00faen con los DataFrames.","title":"2.3.3 Creaci\u00f3n y uso de vistas temporales"},{"location":"tema23/#vistas-temporales","text":"Las vistas temporales son la forma principal de exponer un DataFrame para ser consultado mediante SQL. createTempView() vs. createOrReplaceTempView() : createTempView(view_name) : Crea una vista temporal con el nombre especificado. Si ya existe una vista con ese nombre, lanzar\u00e1 un error. createOrReplaceTempView(view_name) : Crea o reemplaza una vista temporal. Si una vista con el mismo nombre ya existe, la reemplazar\u00e1 sin lanzar un error. Esta es la m\u00e1s com\u00fanmente utilizada. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"TempViews\").getOrCreate() data1 = [(\"Laptop\", 1200)] df1 = spark.createDataFrame(data1, [\"item\", \"price\"]) data2 = [(\"Monitor\", 300)] df2 = spark.createDataFrame(data2, [\"item\", \"price\"]) # Usar createTempView (fallar\u00e1 si se ejecuta dos veces sin borrar) df1.createTempView(\"products_v1\") spark.sql(\"SELECT * FROM products_v1\").show() # Usar createOrReplaceTempView (permite sobrescribir) df2.createOrReplaceTempView(\"products_v1\") # Reemplaza la vista existente spark.sql(\"SELECT * FROM products_v1\").show() # Resultado: # +-------+-----+ # | item|price| # +-------+-----+ # |Monitor| 300| # +-------+-----+ Alcance de las vistas temporales (sesi\u00f3n): Las vistas temporales son locales a la SparkSession en la que se crearon. No son visibles para otras SparkSession s ni persisten despu\u00e9s de que la SparkSession finaliza. from pyspark.sql import SparkSession spark1 = SparkSession.builder.appName(\"Session1\").getOrCreate() spark2 = SparkSession.builder.appName(\"Session2\").getOrCreate() df_session1 = spark1.createDataFrame([(\"Data1\",)], [\"col\"]) df_session1.createOrReplaceTempView(\"my_data_session1\") # Consulta en la misma sesi\u00f3n (spark1) spark1.sql(\"SELECT * FROM my_data_session1\").show() # Intentar consultar desde otra sesi\u00f3n (spark2) - esto fallar\u00e1 try: spark2.sql(\"SELECT * FROM my_data_session1\").show() except Exception as e: print(f\"Error esperado en spark2: {e}\") # Resultado: Error esperado en spark2: Table or view not found: my_data_session1; Eliminaci\u00f3n de vistas temporales ( dropTempView() ): Aunque son temporales, es buena pr\u00e1ctica eliminarlas expl\u00edcitamente si ya no se necesitan para liberar recursos o evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropView\").getOrCreate() df = spark.createDataFrame([(\"test\",)], [\"col\"]) df.createOrReplaceTempView(\"my_test_view\") spark.sql(\"SELECT * FROM my_test_view\").show() spark.catalog.dropTempView(\"my_test_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM my_test_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_test_view;","title":"Vistas Temporales"},{"location":"tema23/#vistas-globales-temporales","text":"A diferencia de las vistas temporales, las vistas globales temporales son visibles a trav\u00e9s de todas las SparkSession s dentro de la misma aplicaci\u00f3n Spark. Esto las hace \u00fatiles para compartir datos entre diferentes m\u00f3dulos o usuarios de una misma aplicaci\u00f3n. createGlobalTempView() vs. createOrReplaceGlobalTempView() : createGlobalTempView(view_name) : Crea una vista global temporal. Lanza un error si ya existe. createOrReplaceGlobalTempView(view_name) : Crea o reemplaza una vista global temporal. Las vistas globales temporales se acceden con el prefijo global_temp . view_name . from pyspark.sql import SparkSession # Session 1 spark1 = SparkSession.builder.appName(\"GlobalViewSession1\").getOrCreate() df_global = spark1.createDataFrame([(\"GlobalData\",)], [\"data_col\"]) df_global.createOrReplaceGlobalTempView(\"shared_data\") spark1.sql(\"SELECT * FROM global_temp.shared_data\").show() # Session 2 spark2 = SparkSession.builder.appName(\"GlobalViewSession2\").getOrCreate() # La vista global es accesible desde otra SparkSession spark2.sql(\"SELECT * FROM global_temp.shared_data\").show() # Resultado (desde spark2): # +----------+ # | data_col| # +----------+ # |GlobalData| # +----------+ Alcance de las vistas globales temporales (aplicaci\u00f3n Spark): Persisten mientras la aplicaci\u00f3n Spark est\u00e9 activa (es decir, el proceso SparkContext est\u00e9 en ejecuci\u00f3n). Se eliminan cuando la aplicaci\u00f3n finaliza. El ejemplo anterior demuestra el alcance a trav\u00e9s de sesiones, una vez que el script o la aplicaci\u00f3n Spark finalizan, la vista global temporal se destruye. Eliminaci\u00f3n de vistas globales temporales ( dropGlobalTempView() ): Se pueden eliminar expl\u00edcitamente si ya no se necesitan. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropGlobalView\").getOrCreate() df = spark.createDataFrame([(\"global test\",)], [\"col\"]) df.createOrReplaceGlobalTempView(\"my_global_view\") spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() spark.catalog.dropGlobalTempView(\"my_global_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_global_view;","title":"Vistas Globales Temporales"},{"location":"tema23/#234-consultas-avanzadas-con-sparksql","text":"SparkSQL no se limita a consultas b\u00e1sicas; soporta caracter\u00edsticas SQL avanzadas que son fundamentales para an\u00e1lisis complejos y transformaciones de datos, como uniones complejas, subconsultas, funciones de ventana y Common Table Expressions (CTEs).","title":"2.3.4 Consultas avanzadas con SparkSQL"},{"location":"tema23/#uniones-joins-avanzadas","text":"M\u00e1s all\u00e1 del INNER JOIN , SparkSQL soporta una gama completa de tipos de uniones, incluyendo LEFT OUTER , RIGHT OUTER , FULL OUTER , LEFT SEMI y LEFT ANTI JOIN . Tipos de JOINs ( INNER , LEFT OUTER , RIGHT OUTER , FULL OUTER ): Comprender las diferencias entre los tipos de uniones es crucial para obtener los resultados deseados. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"AdvancedJoins\").getOrCreate() # Tabla de empleados employees_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 103)] employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"]) employees_df.createOrReplaceTempView(\"employees\") # Tabla de departamentos departments_data = [(101, \"Sales\"), (102, \"HR\"), (104, \"IT\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"]) departments_df.createOrReplaceTempView(\"departments\") # LEFT OUTER JOIN (todas las filas de la izquierda, coincidencias de la derecha o NULLs) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e LEFT OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Alice| Sales| # | Bob| HR| # | Charlie| null| # +---------+---------+ # FULL OUTER JOIN (todas las filas de ambas tablas, con NULLs donde no hay coincidencia) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e FULL OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Charlie| null| # | Alice| Sales| # | Bob| HR| # | null| IT| # +---------+---------+ LEFT SEMI JOIN y LEFT ANTI JOIN (Subconsultas optimizadas): LEFT SEMI JOIN : Retorna las filas del lado izquierdo (primera tabla) que tienen una coincidencia en el lado derecho (segunda tabla). No incluye columnas de la tabla derecha. Es similar a un INNER JOIN pero solo retorna columnas de la tabla izquierda y es m\u00e1s eficiente. LEFT ANTI JOIN : Retorna las filas del lado izquierdo que no tienen una coincidencia en el lado derecho. \u00datil para encontrar registros hu\u00e9rfanos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SemiAntiJoins\").getOrCreate() # Tabla de clientes customers_data = [(1, \"Ana\"), (2, \"Luis\"), (3, \"Marta\"), (4, \"Pedro\")] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\"]) customers_df.createOrReplaceTempView(\"customers\") # Tabla de pedidos orders_data = [(101, 1, 50), (102, 2, 75), (103, 1, 120)] # Marta y Pedro no tienen pedidos orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\"]) orders_df.createOrReplaceTempView(\"orders\") # LEFT SEMI JOIN (clientes que tienen al menos un pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT SEMI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Ana| # | Luis| # +-------------+ # LEFT ANTI JOIN (clientes que NO tienen ning\u00fan pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT ANTI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Marta| # | Pedro| # +-------------+","title":"Uniones (JOINs) avanzadas"},{"location":"tema23/#subconsultas-y-ctes-common-table-expressions","text":"Las subconsultas y CTEs permiten dividir consultas complejas en partes m\u00e1s manejables, mejorando la legibilidad y, en algunos casos, la optimizaci\u00f3n. Subconsultas en WHERE (IN, EXISTS): Permiten filtrar resultados bas\u00e1ndose en los valores de otra consulta. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Subqueries\").getOrCreate() # Tabla de productos products_data = [(1, \"Laptop\", 1200), (2, \"Mouse\", 25), (3, \"Keyboard\", 75), (4, \"Monitor\", 300)] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"]) products_df.createOrReplaceTempView(\"products\") # Tabla de stock stock_data = [(1, 10), (3, 5), (5, 20)] # Producto 2 y 4 no tienen stock stock_df = spark.createDataFrame(stock_data, [\"product_id\", \"quantity\"]) stock_df.createOrReplaceTempView(\"stock\") # Seleccionar productos que tienen stock (usando IN) spark.sql(\"\"\" SELECT product_name, price FROM products WHERE product_id IN (SELECT product_id FROM stock) \"\"\").show() # Resultado: # +------------+-----+ # |product_name|price| # +------------+-----+ # | Laptop| 1200| # | Keyboard| 75| # +------------+-----+ CTEs (Common Table Expressions) con WITH : Las CTEs definen un conjunto de resultados temporal y nombrado que se puede referenciar dentro de una \u00fanica sentencia SELECT , INSERT , UPDATE o DELETE . Mejoran la legibilidad y la modularidad de las consultas complejas. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CTEs\").getOrCreate() # Tabla de ventas sales_data = [(\"RegionA\", \"Q1\", 1000), (\"RegionA\", \"Q2\", 1200), (\"RegionB\", \"Q1\", 800), (\"RegionB\", \"Q2\", 900), (\"RegionA\", \"Q3\", 1500)] sales_df = spark.createDataFrame(sales_data, [\"region\", \"quarter\", \"revenue\"]) sales_df.createOrReplaceTempView(\"sales\") # Calcular el promedio de ingresos por regi\u00f3n usando una CTE spark.sql(\"\"\" WITH RegionalRevenue AS ( SELECT region, SUM(revenue) AS total_region_revenue FROM sales GROUP BY region ) SELECT r.region, r.total_region_revenue, (SELECT AVG(total_region_revenue) FROM RegionalRevenue) AS overall_avg_revenue FROM RegionalRevenue r \"\"\").show() # Resultado: # +-------+------------------+-------------------+ # | region|total_region_revenue|overall_avg_revenue| # +-------+------------------+-------------------+ # |RegionA| 3700| 2200.0| # |RegionB| 1700| 2200.0| # +-------+------------------+-------------------+ Funciones de Ventana (Window Functions): Permiten realizar c\u00e1lculos sobre un conjunto de filas relacionadas con la fila actual ( PARTITION BY , ORDER BY , ROWS BETWEEN / RANGE BETWEEN ). Son muy potentes para c\u00e1lculos anal\u00edticos como promedios m\u00f3viles, rankings, sumas acumuladas, etc. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate() # Tabla de transacciones transactions_data = [(\"StoreA\", \"2023-01-01\", 100), (\"StoreA\", \"2023-01-02\", 150), (\"StoreA\", \"2023-01-03\", 80), (\"StoreB\", \"2023-01-01\", 200), (\"StoreB\", \"2023-01-02\", 120)] transactions_df = spark.createDataFrame(transactions_data, [\"store_id\", \"sale_date\", \"amount\"]) transactions_df.createOrReplaceTempView(\"transactions\") # Calcular la suma acumulada de ventas por tienda, ordenada por fecha spark.sql(\"\"\" SELECT store_id, sale_date, amount, SUM(amount) OVER (PARTITION BY store_id ORDER BY sale_date) AS running_total FROM transactions ORDER BY store_id, sale_date \"\"\").show() # Resultado: # +--------+----------+------+-------------+ # |store_id| sale_date|amount|running_total| # +--------+----------+------+-------------+ # | StoreA|2023-01-01| 100| 100| # | StoreA|2023-01-02| 150| 250| # | StoreA|2023-01-03| 80| 330| # | StoreB|2023-01-01| 200| 200| # | StoreB|2023-01-02| 120| 320| # +--------+----------+------+-------------+ # Ranking de transacciones por tienda spark.sql(\"\"\" SELECT store_id, sale_date, amount, ROW_NUMBER() OVER (PARTITION BY store_id ORDER BY amount DESC) AS rank_by_amount FROM transactions ORDER BY store_id, rank_by_amount \"\"\").show() # Resultado: # +--------+----------+------+------------+ # |store_id| sale_date|amount|rank_by_amount| # +--------+----------+------+------------+ # | StoreA|2023-01-02| 150| 1| # | StoreA|2023-01-01| 100| 2| # | StoreA|2023-01-03| 80| 3| # | StoreB|2023-01-01| 200| 1| # | StoreB|2023-01-02| 120| 2| # +--------+----------+------+------------+","title":"Subconsultas y CTEs (Common Table Expressions)"},{"location":"tema23/#tarea","text":"","title":"Tarea"},{"location":"tema23/#ejercicios-practicos-relacionados-con-el-tema-23","text":"Crea un DataFrame de estudiantes con las columnas id , nombre , carrera y promedio_calificaciones . Datos de ejemplo: [(1, \"Juan\", \"Ingenier\u00eda\", 4.2), (2, \"Maria\", \"Medicina\", 3.8), (3, \"Pedro\", \"Ingenier\u00eda\", 4.5), (4, \"Ana\", \"Derecho\", 3.5)] . Registra este DataFrame como una vista temporal llamada estudiantes_temp . Luego, escribe una consulta SparkSQL que seleccione el nombre y la carrera de todos los estudiantes con un promedio_calificaciones superior a 4.0 . Usando la vista estudiantes_temp , escribe una consulta SparkSQL que seleccione el nombre , carrera y promedio_calificaciones de los estudiantes cuya carrera sea 'Ingenier\u00eda' Y cuyo nombre empiece con 'P'. Crea un DataFrame de ventas con columnas region , producto y cantidad . Datos de ejemplo: [(\"Norte\", \"A\", 10), (\"Norte\", \"B\", 15), (\"Sur\", \"A\", 20), (\"Norte\", \"A\", 5), (\"Sur\", \"B\", 12)] . Registra este DataFrame como ventas_temp . Escribe una consulta SparkSQL que calcule la SUM de cantidad por region y producto , pero solo para aquellos grupos donde la cantidad total sea mayor o igual a 25 . Crea un DataFrame de departamentos con id_departamento y nombre_departamento . Datos de ejemplo: [(1, \"Ventas\"), (2, \"Marketing\"), (3, \"Recursos Humanos\")] . Registra este DataFrame como departamentos_temp . Realiza un INNER JOIN entre estudiantes_temp (asume que carrera se puede unir con nombre_departamento ) y departamentos_temp para mostrar el nombre del estudiante y el nombre_departamento al que pertenecen. Nota: Para simplificar, puedes asumir que carrera de estudiantes_temp corresponde a nombre_departamento en departamentos_temp . Crea un DataFrame de productos_disponibles con producto_id y nombre_producto . Datos de ejemplo: [(101, \"Laptop\"), (102, \"Mouse\"), (103, \"Teclado\"), (104, \"Monitor\")] . Registra como productos_disponibles_temp . Crea otro DataFrame de productos_vendidos con producto_id y fecha_venta . Datos de ejemplo: [(101, \"2023-01-01\"), (103, \"2023-01-05\")] . Registra como productos_vendidos_temp . Escribe una consulta SparkSQL que encuentre los nombre_producto de los productos que nunca han sido vendidos. Utilizando las vistas productos_disponibles_temp y productos_vendidos_temp del ejercicio anterior, escribe una consulta SparkSQL usando una subconsulta con IN para seleccionar los nombre_producto de los productos que s\u00ed han sido vendidos. Crea un DataFrame de empleados_salarios con id_empleado , departamento y salario . Datos de ejemplo: [(1, \"IT\", 60000), (2, \"IT\", 70000), (3, \"HR\", 50000), (4, \"HR\", 55000), (5, \"IT\", 65000)] . Registra como empleados_salarios_temp . Usa una CTE para calcular el salario_promedio_departamental para cada departamento . Luego, en la consulta principal, selecciona id_empleado , departamento , salario y el salario_promedio_departamental de su departamento. Crea un DataFrame de puntajes_examen con clase , estudiante y puntaje . Datos de ejemplo: [(\"A\", \"Alice\", 90), (\"A\", \"Bob\", 85), (\"A\", \"Charlie\", 92), (\"B\", \"David\", 78), (\"B\", \"Eve\", 95)] . Registra como puntajes_examen_temp . Usa una funci\u00f3n de ventana para calcular el ranking de puntaje para cada estudiante dentro de cada clase , ordenando de mayor a menor puntaje. Muestra clase , estudiante , puntaje y ranking . Utilizando la vista ventas_temp del ejercicio 3 (a\u00f1ade una columna fecha_venta si no la tienes, ej. \"2023-01-01\" para todos por simplicidad, o mejor, crea nuevos datos con fechas distintas para cada venta en la misma regi\u00f3n): [(\"Norte\", \"A\", 10, \"2023-01-01\"), (\"Norte\", \"B\", 15, \"2023-01-02\"), (\"Norte\", \"A\", 5, \"2023-01-03\")] . Registra como ventas_fecha_temp . Calcula la cantidad_acumulada de ventas por region , ordenada por fecha_venta . Crea dos DataFrames: usuarios ( user_id , name ) y compras ( purchase_id , user_id , amount , purchase_date ). usuarios ejemplo: [(1, \"User A\"), (2, \"User B\")] compras ejemplo: [(101, 1, 50.0, \"2023-01-01\"), (102, 1, 75.0, \"2023-01-10\"), (103, 2, 120.0, \"2023-01-15\")] Registra ambos como vistas temporales ( users_temp , purchases_temp ). Usa una CTE para encontrar el total_comprado por cada user_id . Luego, en la consulta principal, une la CTE con users_temp para mostrar name y su total_comprado .","title":"Ejercicios Pr\u00e1cticos relacionados con el tema 2.3"},{"location":"tema24/","text":"2. PySpark y SparkSQL Tema 2.4 Optimizaci\u00f3n y Rendimiento Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de identificar y aplicar t\u00e9cnicas avanzadas de optimizaci\u00f3n y gesti\u00f3n del rendimiento en aplicaciones Spark, comprendiendo el funcionamiento interno del motor y las estrategias para el manejo eficiente de datos distribuidos y operaciones complejas, con el fin de mejorar significativamente la eficiencia y escalabilidad de los pipelines de procesamiento de Big Data. Introducci\u00f3n : En el vasto universo del procesamiento de Big Data con Apache Spark, la capacidad de escribir c\u00f3digo funcional es solo la primera parte de la ecuaci\u00f3n. La verdadera maestr\u00eda reside en optimizar ese c\u00f3digo para que se ejecute de manera eficiente, consumiendo menos recursos y completando las tareas en el menor tiempo posible. Este tema se adentra en el coraz\u00f3n del rendimiento de Spark, explorando las herramientas y los principios subyacentes que permiten transformar una aplicaci\u00f3n de datos masivos en una soluci\u00f3n robusta y escalable. Desarrollo : La optimizaci\u00f3n en Spark es un proceso multifac\u00e9tico que abarca desde la gesti\u00f3n inteligente de la memoria y el disco hasta la comprensi\u00f3n profunda de c\u00f3mo Spark planifica y ejecuta las operaciones. Se explorar\u00e1n conceptos fundamentales como la persistencia de DataFrames, la arquitectura de optimizaci\u00f3n de Spark (Catalyst y Tungsten), y estrategias avanzadas para el manejo de joins y la reducci\u00f3n de operaciones costosas como los shuffles. Finalmente, se abordar\u00e1n consideraciones clave para escalar y mantener el desempe\u00f1o en entornos de producci\u00f3n, proporcionando al estudiante las herramientas para diagnosticar cuellos de botella y aplicar soluciones efectivas en sus proyectos de Big Data. 2.4.1 Persistencia y cach\u00e9 (persist() y cache()) La persistencia en Spark es una caracter\u00edstica fundamental para optimizar el rendimiento de las operaciones sobre DataFrames o RDDs que se reutilizan m\u00faltiples veces. Cuando una operaci\u00f3n se ejecuta sobre un DataFrame, Spark recalcula el DataFrame desde el origen cada vez que se invoca una acci\u00f3n. Esto puede ser ineficiente si el mismo DataFrame es utilizado en varias operaciones subsiguientes. Al aplicar persist() o cache() , Spark almacena el DataFrame o RDD resultante de una transformaci\u00f3n en memoria, en disco o una combinaci\u00f3n de ambos, evitando rec\u00e1lculos innecesarios y acelerando las operaciones posteriores. Niveles de almacenamiento en persist() Spark ofrece diferentes niveles de almacenamiento para persist() , lo que permite controlar d\u00f3nde y c\u00f3mo se guardan los datos para optimizar el equilibrio entre memoria, disco y replicaci\u00f3n. La elecci\u00f3n del nivel adecuado depende de la cantidad de memoria disponible, la necesidad de tolerancia a fallos y la frecuencia de acceso a los datos. MEMORY_ONLY : Es el nivel por defecto para cache() . Los RDDs/DataFrames se almacenan como objetos deserializados de Python (o Java/Scala si se usa Scala/Java) en la JVM. Si no cabe en memoria, algunas particiones se recalculan cuando son necesarias. Conjuntos de datos peque\u00f1os y medianos : Cuando se tiene un DataFrame que cabe completamente en la memoria de los ejecutores y se va a utilizar repetidamente en m\u00faltiples transformaciones. Por ejemplo, un cat\u00e1logo de productos que se une frecuentemente con transacciones. Operaciones iterativas : En algoritmos de machine learning (como K-Means o PageRank) donde un conjunto de datos se itera sobre muchas veces, manteniendo los datos en memoria reduce dr\u00e1sticamente el tiempo de ejecuci\u00f3n de cada iteraci\u00f3n. MEMORY_AND_DISK : Almacena las particiones en memoria. Si no hay suficiente memoria, las particiones que no caben se almacenan en disco. Las particiones en disco se leen y deserializan bajo demanda. DataFrames grandes con uso frecuente : Cuando se trabaja con un DataFrame que es demasiado grande para caber completamente en memoria, pero a\u00fan se necesita un acceso r\u00e1pido. Por ejemplo, un historial de eventos de usuario que se procesa a diario pero no cabe 100% en RAM. Reducir rec\u00e1lculos en fallos : Si se necesita persistir un DataFrame para su uso posterior y se busca una combinaci\u00f3n de rendimiento y resiliencia b\u00e1sica sin replicaci\u00f3n. MEMORY_ONLY_SER (Serialized) : Similar a MEMORY_ONLY , pero los objetos se almacenan en su forma serializada (bytes). Esto reduce el uso de memoria (hasta un 10x) a expensas de un mayor costo de CPU para deserializar los datos. Optimizaci\u00f3n de memoria en cl\u00fasteres limitados : Cuando la memoria es un recurso escaso y se prefiere sacrificar un poco de tiempo de CPU por un uso de memoria mucho m\u00e1s eficiente. \u00datil para DataFrames muy grandes que a\u00fan se quieren mantener mayormente en memoria. Uso de Kryo Serialization : Combinado con la serializaci\u00f3n Kryo personalizada, puede ser extremadamente eficiente en memoria y a\u00fan ofrecer buen rendimiento de acceso. MEMORY_AND_DISK_SER : Igual que MEMORY_ONLY_SER , pero las particiones que no caben en memoria se almacenan en disco. Grandes conjuntos de datos con memoria limitada : La opci\u00f3n m\u00e1s robusta para conjuntos de datos que exceden la memoria pero necesitan persistencia sin replicaci\u00f3n. Ofrece un buen equilibrio entre uso de memoria, rendimiento y fiabilidad. DISK_ONLY : Almacena las particiones solo en disco. Es el m\u00e1s lento de los niveles de persistencia ya que implica operaciones de E/S de disco. Debugging y auditor\u00eda : Cuando se quiere guardar el estado intermedio de un DataFrame para inspecci\u00f3n o para reiniciar un proceso sin tener que recalcular todo desde el principio, pero no se necesita el rendimiento de la memoria. Tolerancia a fallos en el estado intermedio : En casos donde la memoria es extremadamente limitada y se necesita garantizar que los resultados intermedios no se pierdan en caso de fallo de un ejecutor, aunque el acceso sea m\u00e1s lento. Diferencia entre persist() y cache() La funci\u00f3n cache() es simplemente un alias para persist() con el nivel de almacenamiento por defecto MEMORY_ONLY . Esto significa que df.cache() es equivalente a df.persist(StorageLevel.MEMORY_ONLY) . Generalmente, cache() se usa para la persistencia m\u00e1s com\u00fan y r\u00e1pida (en memoria), mientras que persist() se utiliza cuando se necesita un control m\u00e1s granular sobre c\u00f3mo se almacenan los datos. Es importante recordar que la persistencia es \"lazy\", es decir, los datos no se almacenan hasta que se ejecuta una acci\u00f3n sobre el DataFrame persistido por primera vez. Para despersistir un DataFrame, se utiliza unpersist() . 2.4.2 Optimizaci\u00f3n con Catalyst y Tungsten Apache Spark se basa en dos motores de optimizaci\u00f3n clave: Catalyst Optimizer y Project Tungsten . Juntos, estos componentes son responsables de la eficiencia y el alto rendimiento que Spark logra en el procesamiento de datos a gran escala, transformando las operaciones de DataFrames y SQL en planes de ejecuci\u00f3n optimizados y utilizando la memoria y la CPU de manera extremadamente eficiente. Catalyst Optimizer: El Cerebro de la Planificaci\u00f3n Catalyst Optimizer es el motor de optimizaci\u00f3n de consultas de Spark SQL (y DataFrames). Funciona en varias fases para traducir las transformaciones de alto nivel que el usuario escribe en un plan de ejecuci\u00f3n de bajo nivel y altamente optimizado. Su dise\u00f1o modular y extensible permite incorporar nuevas t\u00e9cnicas de optimizaci\u00f3n y fuentes de datos. Fase 1: An\u00e1lisis (Analysis) : Spark SQL analiza la consulta (DataFrame API o SQL) para resolver referencias, verificar la sintaxis y el esquema. Convierte el \u00e1rbol l\u00f3gico no resuelto (unresolved logical plan) en un \u00e1rbol l\u00f3gico resuelto (resolved logical plan). Es decir, mapea los nombres de columnas y tablas a sus respectivas fuentes de datos. Identificaci\u00f3n de errores de esquema : Si una columna referenciada no existe en el esquema de un DataFrame, Catalyst lo detectar\u00e1 en esta fase y lanzar\u00e1 una excepci\u00f3n. Resoluci\u00f3n de ambig\u00fcedades : Si una columna existe en m\u00faltiples tablas en un join, Catalyst requiere que se califique con el nombre de la tabla para resolver la ambig\u00fcedad. Fase 2: Optimizaci\u00f3n L\u00f3gica (Logical Optimization) : En esta fase, Catalyst aplica un conjunto de reglas de optimizaci\u00f3n sobre el plan l\u00f3gico resuelto para reducir la cantidad de datos a procesar o el n\u00famero de operaciones. Estas optimizaciones son independientes del tipo de motor de ejecuci\u00f3n. Predicado Pushdown (Predicate Pushdown) : Si se aplica un filtro ( .where() ) a un DataFrame que se lee de una fuente de datos (como Parquet), Catalyst empujar\u00e1 este filtro a la fuente de datos. Esto significa que la fuente de datos leer\u00e1 solo los registros que cumplan con la condici\u00f3n, reduciendo la cantidad de datos que se transfieren a Spark. Column Pruning : Si solo se seleccionan algunas columnas ( .select() ) de un DataFrame, Catalyst se asegura de que solo esas columnas se lean del origen de datos, en lugar de todo el conjunto de columnas. Combinaci\u00f3n de filtros : Si se tienen m\u00faltiples condiciones filter() o where() , Catalyst puede combinarlas en una sola expresi\u00f3n para una evaluaci\u00f3n m\u00e1s eficiente. Fase 3: Planificaci\u00f3n F\u00edsica (Physical Planning) : El plan l\u00f3gico optimizado se convierte en uno o m\u00e1s planes f\u00edsicos. Aqu\u00ed, Catalyst considera el entorno de ejecuci\u00f3n (tama\u00f1o del cl\u00faster, datos en cach\u00e9, etc.) y elige la mejor estrategia de ejecuci\u00f3n para cada operaci\u00f3n, generando c\u00f3digo ejecutable para el motor Tungsten. Elecci\u00f3n de estrategia de Join : Catalyst decide si usar un Broadcast Join , Shuffle Hash Join , Sort Merge Join , etc., bas\u00e1ndose en el tama\u00f1o de las tablas y la configuraci\u00f3n. Manejo de agregaciones : Decide si realizar agregaciones parciales (partial aggregations) en cada partici\u00f3n antes de combinarlas para reducir el shuffle. Fase 4: Generaci\u00f3n de C\u00f3digo (Code Generation) : La fase final donde se genera c\u00f3digo Java bytecode din\u00e1micamente en tiempo de ejecuci\u00f3n para ejecutar el plan f\u00edsico. Esto evita la sobrecarga de la interpretaci\u00f3n y permite que las operaciones se ejecuten a velocidades cercanas a las de c\u00f3digo nativo. Evaluaci\u00f3n de expresiones : Genera c\u00f3digo altamente optimizado para la evaluaci\u00f3n de expresiones complejas en lugar de usar llamadas a funciones gen\u00e9ricas, lo que reduce la sobrecarga de la JVM. Operaciones vectorizadas : Permite la ejecuci\u00f3n de operaciones por lotes (vectorizadas) en lugar de una fila a la vez, lo que es mucho m\u00e1s eficiente para operaciones como filtros y proyecciones. Tungsten: El Motor de Ejecuci\u00f3n de Bajo Nivel Project Tungsten es una iniciativa de optimizaci\u00f3n de bajo nivel en Spark que se enfoca en mejorar el uso de la memoria y la eficiencia de la CPU. Su objetivo principal es cerrar la brecha de rendimiento entre el c\u00f3digo Java/Scala y el c\u00f3digo nativo, utilizando t\u00e9cnicas como la gesti\u00f3n de memoria off-heap (fuera del heap de la JVM), la serializaci\u00f3n eficiente y la generaci\u00f3n de c\u00f3digo justo a tiempo (JIT). Gesti\u00f3n de Memoria Off-heap : Tungsten permite a Spark almacenar datos directamente en la memoria fuera del heap de la JVM, en formato binario y compactado. Esto reduce la sobrecarga de la recolecci\u00f3n de basura (Garbage Collection) de la JVM, que puede ser un cuello de botella significativo en cargas de trabajo de Big Data. Agregaciones y Joins con mucha memoria : Operaciones como groupBy o join que requieren mantener grandes tablas hash en memoria pueden beneficiarse enormemente al almacenar estas estructuras off-heap, evitando pausas prolongadas de GC. Ordenamiento (Sorting) : La clasificaci\u00f3n de grandes vol\u00famenes de datos puede ser m\u00e1s eficiente al manejar los datos directamente en memoria off-heap, reduciendo la presi\u00f3n sobre el heap de la JVM. Vectorizaci\u00f3n y Generaci\u00f3n de C\u00f3digo : Tungsten trabaja en conjunto con Catalyst para generar c\u00f3digo optimizado que procesa los datos de forma vectorial (por lotes) en lugar de fila por fila. Esto minimiza el costo de las llamadas a funciones y permite una mejor utilizaci\u00f3n del cach\u00e9 de la CPU. Procesamiento de columnas : Al leer datos en formato columnar (como Parquet), Tungsten puede procesar m\u00faltiples valores de una columna a la vez, aplicando operaciones de forma m\u00e1s eficiente. Operaciones de expresi\u00f3n : Para expresiones complejas que involucran m\u00faltiples funciones (ej. col1 + col2 * 5 - length(col3) ), Tungsten genera un \u00fanico bloque de c\u00f3digo que eval\u00faa toda la expresi\u00f3n de una vez. Serializaci\u00f3n Mejorada (Unsafe Row Format) : Tungsten introduce un formato de fila binario llamado \"Unsafe Row\", que es muy compacto y permite un acceso a datos basado en punteros, similar a c\u00f3mo se accede a los datos en C++. Esto elimina la necesidad de serializaci\u00f3n/deserializaci\u00f3n costosa entre pasos. Reducci\u00f3n de I/O en shuffles : Cuando los datos necesitan ser enviados a trav\u00e9s de la red durante un shuffle, el formato Unsafe Row minimiza el volumen de datos a transferir, reduciendo el cuello de botella de la red. ** Cach\u00e9 de datos eficiente *: Al almacenar datos en cach\u00e9, el formato Unsafe Row permite que los datos se almacenen de manera m\u00e1s compacta y se accedan directamente sin deserializaci\u00f3n completa, mejorando el rendimiento de las lecturas. 2.4.3 Broadcast joins y estrategias para evitar shuffles El \"shuffle\" es una operaci\u00f3n costosa en Spark que implica la reorganizaci\u00f3n de datos a trav\u00e9s de la red entre los ejecutores. Ocurre en operaciones como groupBy , join , orderBy , y repartition . Minimizar los shuffles es una de las estrategias m\u00e1s importantes para optimizar el rendimiento en Spark. Una t\u00e9cnica clave para evitar shuffles en joins es el uso de Broadcast Joins . Broadcast Join Un Broadcast Join es una estrategia de join en Spark donde una de las tablas (la m\u00e1s peque\u00f1a) se \"broadcast\" (transmite) a todos los nodos del cl\u00faster que participan en la operaci\u00f3n de join. Esto significa que cada ejecutor obtiene una copia completa de la tabla peque\u00f1a en su memoria local. Al tener la tabla peque\u00f1a localmente, cada ejecutor puede realizar el join con las particiones de la tabla grande sin necesidad de un shuffle, ya que no necesita intercambiar datos con otros ejecutores para encontrar las claves coincidentes. Spark detecta autom\u00e1ticamente si una tabla es lo suficientemente peque\u00f1a (o si se le indica expl\u00edcitamente con broadcast() ) para ser transmitida. La tabla peque\u00f1a se colecta al driver, se serializa y luego se env\u00eda a cada ejecutor. Los ejecutores pueden entonces realizar un Hash Join con las particiones de la tabla grande. Uni\u00f3n de una tabla de dimensiones peque\u00f1a con una tabla de hechos grande : Por ejemplo, unir una tabla de clientes (miles o cientos de miles de registros) con una tabla de transacciones (miles de millones de registros). Si la tabla de clientes es menor que el umbral de broadcast (por defecto 10 MB en Spark 3.x, configurable con spark.sql.autoBroadcastJoinThreshold ), Spark autom\u00e1ticamente realizar\u00e1 un Broadcast Join. Filtros complejos con Lookups : Cuando se tiene un conjunto de IDs de referencia (ej. una lista de c\u00f3digos de productos a excluir) que es peque\u00f1o y se necesita filtrar o enriquecer un DataFrame muy grande. Se puede crear un peque\u00f1o DataFrame con estos IDs y luego hacer un Broadcast Join. Estrategias para Evitar o Minimizar Shuffles M\u00e1s all\u00e1 de los Broadcast Joins, existen otras estrategias para reducir la necesidad de shuffles o mitigar su impacto en el rendimiento. Predicado Pushdown y Column Pruning : Estas optimizaciones (explicadas en la secci\u00f3n de Catalyst) reducen la cantidad de datos que se leen del origen y se procesan, lo que indirectamente reduce la cantidad de datos que potencialmente necesitar\u00edan ser shufflados. Al filtrar o seleccionar columnas tempranamente, se trabaja con un conjunto de datos m\u00e1s peque\u00f1o desde el principio. Filtrado por fecha antes del join : Si se va a unir una tabla de transacciones de varios a\u00f1os con una tabla de productos, y solo se necesitan transacciones del \u00faltimo mes, aplicar un filter(\"fecha >= '2025-01-01'\") antes del join reducir\u00e1 significativamente el volumen de datos que participan en el join y, por lo tanto, en cualquier shuffle subsiguiente. Seleccionar solo columnas necesarias : Si un DataFrame tiene 50 columnas pero solo se necesitan 5 para un an\u00e1lisis, realizar un .select('col1', 'col2', ...) al inicio reduce la cantidad de datos en memoria y en disco si hay shuffles. Co-ubicaci\u00f3n de Datos (Co-location) : Si los datos que se van a unir o agrupar est\u00e1n particionados de manera compatible en el almacenamiento subyacente (por ejemplo, en Hive o Parquet, utilizando la misma clave de partici\u00f3n que se usar\u00e1 para el join/group by), Spark puede aprovechar esto para realizar un Sort-Merge Join o Hash Join con menos o ning\u00fan shuffle. Esto requiere que las tablas se hayan escrito previamente con la misma estrategia de partici\u00f3n. Joins entre tablas particionadas por la misma clave : Si la tabla de pedidos y la tabla de \u00edtems_pedido est\u00e1n ambas particionadas por id_pedido , un join entre ellas por id_pedido ser\u00e1 mucho m\u00e1s eficiente ya que Spark puede simplemente unir las particiones coincidentes localmente en cada nodo. Agregaciones en datos pre-particionados : Si se agrupan datos por una columna que ya es la clave de partici\u00f3n de la tabla, Spark puede realizar agregaciones locales en cada partici\u00f3n antes de combinar resultados, reduciendo la cantidad de datos shufflados. Acumuladores y Broadcast Variables (para datos peque\u00f1os) : Aunque no evitan directamente un shuffle en DataFrames en la misma medida que un Broadcast Join, los acumuladores y las broadcast variables (a nivel de RDD, pero tambi\u00e9n \u00fatiles para datos peque\u00f1os en Spark) son herramientas para compartir datos peque\u00f1os de manera eficiente entre tareas. Las broadcast variables permiten enviar un valor de solo lectura a todos los nodos, \u00fatil para tablas de b\u00fasqueda o configuraciones. Listas de bloqueo o mapeos : Transmitir una lista peque\u00f1a de IDs prohibidos o un mapa de c\u00f3digos a descripciones a todos los ejecutores para filtrar o enriquecer datos sin realizar un join formal. Par\u00e1metros de configuraci\u00f3n din\u00e1micos : Si un algoritmo necesita un conjunto de par\u00e1metros que cambian din\u00e1micamente pero es peque\u00f1o, se puede transmitir usando una broadcast variable. Uso de repartition y coalesce con cuidado : Ambas funciones se utilizan para cambiar el n\u00famero de particiones de un DataFrame. repartition siempre implica un shuffle completo de los datos, mientras que coalesce intenta reducir el n\u00famero de particiones sin un shuffle completo si es posible (solo combina particiones existentes dentro del mismo nodo). Util\u00edzalas solo cuando sea estrictamente necesario (ej. para uniones eficientes con datos de gran tama\u00f1o, o para reducir el n\u00famero de archivos de salida). Reducir archivos de salida (small files problem) : Si un procesamiento genera miles de archivos peque\u00f1os (problema de \"small files\"), un df.coalesce(N).write.parquet(...) puede combinarlos en N archivos m\u00e1s grandes al final de la operaci\u00f3n, aunque coalesce tambi\u00e9n puede implicar un shuffle si se reduce el n\u00famero de particiones dr\u00e1sticamente. Preparaci\u00f3n para operaciones posteriores : En algunos escenarios, re-particionar un DataFrame por la clave de join antes del join puede ser beneficioso si se planean m\u00faltiples joins o agregaciones sobre la misma clave, aunque es una decisi\u00f3n que debe tomarse con base en un an\u00e1lisis de rendimiento. 2.4.4 Consideraciones de escalabilidad y desempe\u00f1o La escalabilidad y el desempe\u00f1o en Spark van m\u00e1s all\u00e1 de la optimizaci\u00f3n de c\u00f3digo individual; involucran la configuraci\u00f3n del cl\u00faster, la gesti\u00f3n de recursos y la elecci\u00f3n de las arquitecturas de datos adecuadas. Comprender estos aspectos es fundamental para dise\u00f1ar aplicaciones Spark robustas que puedan manejar vol\u00famenes de datos crecientes y mantener un rendimiento \u00f3ptimo en entornos de producci\u00f3n. Configuraci\u00f3n del Cl\u00faster y Asignaci\u00f3n de Recursos Una configuraci\u00f3n adecuada de Spark y la asignaci\u00f3n de recursos a los ejecutores son cr\u00edticas para el desempe\u00f1o. Un cl\u00faster mal configurado puede llevar a cuellos de botella incluso con el c\u00f3digo m\u00e1s optimizado. Tama\u00f1o de los Ejecutores ( spark.executor.cores , spark.executor.memory ) : Estos par\u00e1metros controlan cu\u00e1ntos n\u00facleos de CPU y cu\u00e1nta memoria se asignan a cada ejecutor. Un n\u00famero adecuado de n\u00facleos permite el paralelismo, mientras que suficiente memoria evita derrames a disco y optimiza el cach\u00e9. Un error com\u00fan es tener ejecutores muy grandes (pocos ejecutores con muchos n\u00facleos/memoria) o muy peque\u00f1os (muchos ejecutores con pocos recursos). Cl\u00faster con nodos de 64GB RAM, 16 n\u00facleos : Una configuraci\u00f3n com\u00fan podr\u00eda ser spark.executor.cores=5 y spark.executor.memory=20GB . Esto permite tener 2 ejecutores por nodo y deja memoria para el sistema operativo y el driver, maximizando la utilizaci\u00f3n de recursos sin sobrecargar. Tareas con mucha memoria (e.g., joins grandes sin broadcast) : Aumentar spark.executor.memory puede ser necesario para evitar derrames a disco durante operaciones intensivas en memoria. Memoria del Driver ( spark.driver.memory ) : La memoria asignada al nodo driver. El driver coordina las tareas, almacena metadatos y, en algunos casos, colecta resultados (como collect() ). Si se realizan operaciones que recolectan grandes cantidades de datos al driver, o si se manejan muchas broadcast variables, se necesita m\u00e1s memoria para el driver. Usar collect() en un DataFrame grande : Si se intenta df.collect() sobre un DataFrame con millones de filas, el driver podr\u00eda quedarse sin memoria. Ajustar spark.driver.memory o reestructurar el c\u00f3digo para evitar collect() en grandes vol\u00famenes. Broadcast de m\u00faltiples tablas peque\u00f1as : Si se transmiten muchas tablas peque\u00f1as, la memoria del driver podr\u00eda verse afectada. Configuraci\u00f3n de Shuffle ( spark.shuffle.service.enabled , spark.shuffle.file.buffer ) : Estos par\u00e1metros afectan c\u00f3mo Spark maneja los datos durante las operaciones de shuffle. Habilitar el servicio de shuffle externo ( spark.shuffle.service.enabled=true ) permite que los datos shufflados persistan incluso si un ejecutor falla, mejorando la fiabilidad. Ajustar el tama\u00f1o del buffer ( spark.shuffle.file.buffer ) puede optimizar las escrituras a disco durante el shuffle. Estabilidad en shuffles grandes : En entornos de producci\u00f3n con shuffles frecuentes y grandes, habilitar el shuffle service es crucial para la estabilidad y resiliencia. Rendimiento de I/O en shuffles : Para tareas con mucha escritura a disco durante shuffles, aumentar el buffer puede reducir la cantidad de peque\u00f1as escrituras y mejorar el rendimiento. Monitoreo y Diagn\u00f3stico Monitorear las aplicaciones Spark es esencial para identificar cuellos de botella y comprender el comportamiento del rendimiento. Spark UI es la herramienta principal para esto. Spark UI (Stages, Tasks, DAG Visualization) : La interfaz de usuario de Spark (accesible generalmente en http://<driver-ip>:4040 ) proporciona una visi\u00f3n detallada de las etapas (Stages), tareas (Tasks), y el Grafo Ac\u00edclico Dirigido (DAG) de la aplicaci\u00f3n. Permite identificar qu\u00e9 etapas son lentas, si hay desequilibrio de datos (skew), o si los ejecutores est\u00e1n infrautilizados/sobrecargados. Identificar Stage lento : Si una etapa particular (e.g., un join o groupBy ) toma mucho tiempo, se puede profundizar en esa etapa para ver si alguna tarea est\u00e1 tardando m\u00e1s de lo normal (indicando skew) o si hay problemas de I/O. An\u00e1lisis de Shuffles : La Spark UI muestra el tama\u00f1o de los datos shufflados y el tiempo que toma. Un shuffle excesivo es una se\u00f1al de que las estrategias de optimizaci\u00f3n (como Broadcast Join) podr\u00edan ser necesarias. Revisar el plan de ejecuci\u00f3n : En la pesta\u00f1a \"SQL\" de la Spark UI, se puede ver el plan de ejecuci\u00f3n generado por Catalyst, lo que ayuda a entender c\u00f3mo Spark est\u00e1 procesando la consulta y si se est\u00e1n aplicando las optimizaciones esperadas (e.g., Predicate Pushdown). M\u00e9tricas de Recursos (CPU, Memoria, Disk I/O) : Adem\u00e1s de Spark UI, es importante monitorear las m\u00e9tricas a nivel de cl\u00faster (CPU, uso de memoria, E/S de disco, red) para cada nodo ejecutor. Esto ayuda a identificar si el problema es de Spark en s\u00ed o si hay una limitaci\u00f3n de recursos a nivel de infraestructura. CPU subutilizada : Si la CPU de los ejecutores est\u00e1 consistentemente baja durante una etapa que se esperaba intensiva en CPU, podr\u00eda indicar un problema de paralelismo o un cuello de botella en I/O. Memoria agotada : Si los ejecutores est\u00e1n reportando errores OOM (Out Of Memory) o si hay mucho spill a disco, es una se\u00f1al de que los DataFrames son demasiado grandes para la memoria disponible o que la configuraci\u00f3n de persistencia es ineficiente. Consideraciones de Dise\u00f1o de Datos La forma en que se almacenan y estructuran los datos tiene un impacto directo en el rendimiento de Spark. Formato de Archivos (Parquet, ORC) : Optar por formatos de archivo columnares como Parquet u ORC es crucial. Estos formatos son auto-descriptivos, comprimidos y permiten optimizaciones como \"Predicate Pushdown\" y \"Column Pruning\" de forma nativa, lo que reduce dr\u00e1sticamente la cantidad de datos le\u00eddos del disco. Reemplazar CSV/JSON por Parquet : Migrar conjuntos de datos de formatos basados en filas (CSV, JSON) a Parquet para obtener mejoras significativas en el rendimiento de lectura y la eficiencia del almacenamiento. Beneficios del Predicate Pushdown : Si se tiene una tabla Parquet de logs y se filtra por timestamp > 'X' , Spark solo leer\u00e1 los bloques de datos relevantes, sin necesidad de escanear todo el archivo. Particionamiento de Datos : Organizar los datos en el sistema de archivos (HDFS, S3) en directorios basados en valores de columna (ej. data/year=2024/month=01/ ). Esto permite a Spark (y otras herramientas de Big Data) \"podar\" particiones (partition pruning), es decir, escanear solo los directorios relevantes para una consulta, evitando la lectura de datos innecesarios. Consulta de datos por fecha : Si los datos de ventas est\u00e1n particionados por a\u00f1o/mes/d\u00eda , una consulta para las ventas de un d\u00eda espec\u00edfico solo escanear\u00e1 el directorio correspondiente a ese d\u00eda, no toda la tabla. Optimizando Joins con partici\u00f3n por clave : Si se va a unir por una clave y ambas tablas est\u00e1n particionadas por esa misma clave, se puede evitar un shuffle completo (co-ubicaci\u00f3n, como se mencion\u00f3 anteriormente). Tama\u00f1o de los Archivos (Small Files Problem) : Tener un gran n\u00famero de archivos peque\u00f1os (ej. miles de archivos de unos pocos KB) en un sistema de archivos distribuido (como HDFS) puede degradar seriamente el rendimiento. Esto se debe a la sobrecarga de gestionar metadatos para cada archivo y la ineficiencia de la lectura de muchos archivos peque\u00f1os. Compactaci\u00f3n de datos hist\u00f3ricos : Si se ingieren datos en peque\u00f1os lotes, peri\u00f3dicamente se debe ejecutar un proceso de \"compactaci\u00f3n\" que combine estos peque\u00f1os archivos en unos pocos archivos m\u00e1s grandes (e.g., 128 MB a 1 GB) para optimizar las lecturas futuras. Ajustar el n\u00famero de particiones de salida : Al escribir resultados, se puede usar repartition() o coalesce() para controlar el n\u00famero de archivos de salida y evitar generar demasiados peque\u00f1os. Tarea Ejercicio de Persistencia con diferentes niveles : Crea un DataFrame con 10 millones de filas y varias columnas. Realiza una serie de transformaciones sobre \u00e9l. Luego, persiste el DataFrame utilizando MEMORY_ONLY , MEMORY_AND_DISK , y DISK_ONLY (en diferentes ejecuciones). Mide el tiempo de ejecuci\u00f3n de las operaciones posteriores a la primera acci\u00f3n para cada nivel de persistencia y explica las diferencias observadas en el rendimiento y el uso de memoria/disco (usando Spark UI). Identificaci\u00f3n de un Shuffle Costoso : Dise\u00f1a un escenario donde se genere un DataFrame grande y se realice una operaci\u00f3n que cause un shuffle significativo (ej. un groupBy por una columna de alta cardinalidad o un join entre dos DataFrames grandes sin una estrategia de optimizaci\u00f3n). Utiliza Spark UI para identificar la etapa del shuffle, el volumen de datos shufflados y el tiempo que consume. Implementaci\u00f3n de Broadcast Join : Toma el escenario del ejercicio anterior (o crea uno similar con un join entre una tabla grande y una peque\u00f1a). Implementa un Broadcast Join utilizando F.broadcast() y compara el tiempo de ejecuci\u00f3n y la ausencia/reducci\u00f3n del shuffle en Spark UI con respecto a un join normal. Predicado Pushdown y Column Pruning en acci\u00f3n : Lee un archivo Parquet con m\u00faltiples columnas y filtra por una columna indexada (si el formato lo permite) y selecciona solo un subconjunto de columnas. Observa el plan de ejecuci\u00f3n en Spark UI (pesta\u00f1a SQL -> Details) y explica c\u00f3mo Catalyst aplica el Predicado Pushdown y el Column Pruning. Simulaci\u00f3n de \"Small Files Problem\" y Soluci\u00f3n : Escribe un DataFrame grande en 1000 archivos peque\u00f1os. Luego, lee esos 1000 archivos y escribe el resultado en 10 archivos m\u00e1s grandes utilizando coalesce() o repartition() . Mide y compara los tiempos de lectura y escritura. Configuraci\u00f3n de Ejecutores : Experimenta con la configuraci\u00f3n de spark.executor.cores y spark.executor.memory en una aplicaci\u00f3n Spark. Ejecuta una carga de trabajo intensiva (ej. un join complejo o una agregaci\u00f3n grande) con diferentes configuraciones (ej. pocos ejecutores grandes vs. muchos ejecutores peque\u00f1os, manteniendo el mismo n\u00famero total de n\u00facleos y memoria para el cl\u00faster). Analiza el impacto en el rendimiento y la utilizaci\u00f3n de recursos en Spark UI. UDFs vs. Funciones Nativas para Optimizaci\u00f3n : Crea una funci\u00f3n para manipular una columna (ej. concatenar strings, transformar un valor num\u00e9rico). Primero, implementa la l\u00f3gica como una UDF. Luego, re-implementa la misma l\u00f3gica utilizando solo funciones nativas de Spark SQL (ej. concat_ws , when , lit ). Compara el rendimiento de ambas implementaciones y explica por qu\u00e9 una es m\u00e1s eficiente. An\u00e1lisis de Derrames a Disco (Spill) : Ejecuta una operaci\u00f3n Spark (ej. un groupBy con una clave de muy alta cardinalidad y un agregado complejo) con memoria de ejecutor limitada, forzando que Spark \"derrame\" datos al disco (spill). Monitorea el evento en Spark UI y explica qu\u00e9 significa el \"spill\" y c\u00f3mo afecta el rendimiento. Optimizaci\u00f3n de Sort : Genera un DataFrame con datos aleatorios y ord\u00e9nalo utilizando orderBy . Luego, compara el plan de ejecuci\u00f3n y el rendimiento si los datos ya estuvieran pre-ordenados o si se aplicara una partici\u00f3n adecuada antes del ordenamiento (esto puede ser un ejercicio conceptual o simulado). Lectura con Particionamiento (Partition Pruning) : Crea un DataFrame grande y escr\u00edbelo en Parquet particionando por una columna (ej. df.write.partitionBy(\"ciudad\").parquet(\"path/to/data\") ). Luego, lee los datos filtrando por esa columna particionada (ej. spark.read.parquet(\"path/to/data\").filter(\"ciudad = 'Bogota'\") ). Observa en Spark UI c\u00f3mo Spark solo escanea los directorios relevantes, demostrando el \"partition pruning\".","title":"Optimizaci\u00f3n y Rendimiento"},{"location":"tema24/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema24/#tema-24-optimizacion-y-rendimiento","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de identificar y aplicar t\u00e9cnicas avanzadas de optimizaci\u00f3n y gesti\u00f3n del rendimiento en aplicaciones Spark, comprendiendo el funcionamiento interno del motor y las estrategias para el manejo eficiente de datos distribuidos y operaciones complejas, con el fin de mejorar significativamente la eficiencia y escalabilidad de los pipelines de procesamiento de Big Data. Introducci\u00f3n : En el vasto universo del procesamiento de Big Data con Apache Spark, la capacidad de escribir c\u00f3digo funcional es solo la primera parte de la ecuaci\u00f3n. La verdadera maestr\u00eda reside en optimizar ese c\u00f3digo para que se ejecute de manera eficiente, consumiendo menos recursos y completando las tareas en el menor tiempo posible. Este tema se adentra en el coraz\u00f3n del rendimiento de Spark, explorando las herramientas y los principios subyacentes que permiten transformar una aplicaci\u00f3n de datos masivos en una soluci\u00f3n robusta y escalable. Desarrollo : La optimizaci\u00f3n en Spark es un proceso multifac\u00e9tico que abarca desde la gesti\u00f3n inteligente de la memoria y el disco hasta la comprensi\u00f3n profunda de c\u00f3mo Spark planifica y ejecuta las operaciones. Se explorar\u00e1n conceptos fundamentales como la persistencia de DataFrames, la arquitectura de optimizaci\u00f3n de Spark (Catalyst y Tungsten), y estrategias avanzadas para el manejo de joins y la reducci\u00f3n de operaciones costosas como los shuffles. Finalmente, se abordar\u00e1n consideraciones clave para escalar y mantener el desempe\u00f1o en entornos de producci\u00f3n, proporcionando al estudiante las herramientas para diagnosticar cuellos de botella y aplicar soluciones efectivas en sus proyectos de Big Data.","title":"Tema 2.4 Optimizaci\u00f3n y Rendimiento"},{"location":"tema24/#241-persistencia-y-cache-persist-y-cache","text":"La persistencia en Spark es una caracter\u00edstica fundamental para optimizar el rendimiento de las operaciones sobre DataFrames o RDDs que se reutilizan m\u00faltiples veces. Cuando una operaci\u00f3n se ejecuta sobre un DataFrame, Spark recalcula el DataFrame desde el origen cada vez que se invoca una acci\u00f3n. Esto puede ser ineficiente si el mismo DataFrame es utilizado en varias operaciones subsiguientes. Al aplicar persist() o cache() , Spark almacena el DataFrame o RDD resultante de una transformaci\u00f3n en memoria, en disco o una combinaci\u00f3n de ambos, evitando rec\u00e1lculos innecesarios y acelerando las operaciones posteriores.","title":"2.4.1 Persistencia y cach\u00e9 (persist() y cache())"},{"location":"tema24/#niveles-de-almacenamiento-en-persist","text":"Spark ofrece diferentes niveles de almacenamiento para persist() , lo que permite controlar d\u00f3nde y c\u00f3mo se guardan los datos para optimizar el equilibrio entre memoria, disco y replicaci\u00f3n. La elecci\u00f3n del nivel adecuado depende de la cantidad de memoria disponible, la necesidad de tolerancia a fallos y la frecuencia de acceso a los datos. MEMORY_ONLY : Es el nivel por defecto para cache() . Los RDDs/DataFrames se almacenan como objetos deserializados de Python (o Java/Scala si se usa Scala/Java) en la JVM. Si no cabe en memoria, algunas particiones se recalculan cuando son necesarias. Conjuntos de datos peque\u00f1os y medianos : Cuando se tiene un DataFrame que cabe completamente en la memoria de los ejecutores y se va a utilizar repetidamente en m\u00faltiples transformaciones. Por ejemplo, un cat\u00e1logo de productos que se une frecuentemente con transacciones. Operaciones iterativas : En algoritmos de machine learning (como K-Means o PageRank) donde un conjunto de datos se itera sobre muchas veces, manteniendo los datos en memoria reduce dr\u00e1sticamente el tiempo de ejecuci\u00f3n de cada iteraci\u00f3n. MEMORY_AND_DISK : Almacena las particiones en memoria. Si no hay suficiente memoria, las particiones que no caben se almacenan en disco. Las particiones en disco se leen y deserializan bajo demanda. DataFrames grandes con uso frecuente : Cuando se trabaja con un DataFrame que es demasiado grande para caber completamente en memoria, pero a\u00fan se necesita un acceso r\u00e1pido. Por ejemplo, un historial de eventos de usuario que se procesa a diario pero no cabe 100% en RAM. Reducir rec\u00e1lculos en fallos : Si se necesita persistir un DataFrame para su uso posterior y se busca una combinaci\u00f3n de rendimiento y resiliencia b\u00e1sica sin replicaci\u00f3n. MEMORY_ONLY_SER (Serialized) : Similar a MEMORY_ONLY , pero los objetos se almacenan en su forma serializada (bytes). Esto reduce el uso de memoria (hasta un 10x) a expensas de un mayor costo de CPU para deserializar los datos. Optimizaci\u00f3n de memoria en cl\u00fasteres limitados : Cuando la memoria es un recurso escaso y se prefiere sacrificar un poco de tiempo de CPU por un uso de memoria mucho m\u00e1s eficiente. \u00datil para DataFrames muy grandes que a\u00fan se quieren mantener mayormente en memoria. Uso de Kryo Serialization : Combinado con la serializaci\u00f3n Kryo personalizada, puede ser extremadamente eficiente en memoria y a\u00fan ofrecer buen rendimiento de acceso. MEMORY_AND_DISK_SER : Igual que MEMORY_ONLY_SER , pero las particiones que no caben en memoria se almacenan en disco. Grandes conjuntos de datos con memoria limitada : La opci\u00f3n m\u00e1s robusta para conjuntos de datos que exceden la memoria pero necesitan persistencia sin replicaci\u00f3n. Ofrece un buen equilibrio entre uso de memoria, rendimiento y fiabilidad. DISK_ONLY : Almacena las particiones solo en disco. Es el m\u00e1s lento de los niveles de persistencia ya que implica operaciones de E/S de disco. Debugging y auditor\u00eda : Cuando se quiere guardar el estado intermedio de un DataFrame para inspecci\u00f3n o para reiniciar un proceso sin tener que recalcular todo desde el principio, pero no se necesita el rendimiento de la memoria. Tolerancia a fallos en el estado intermedio : En casos donde la memoria es extremadamente limitada y se necesita garantizar que los resultados intermedios no se pierdan en caso de fallo de un ejecutor, aunque el acceso sea m\u00e1s lento.","title":"Niveles de almacenamiento en persist()"},{"location":"tema24/#diferencia-entre-persist-y-cache","text":"La funci\u00f3n cache() es simplemente un alias para persist() con el nivel de almacenamiento por defecto MEMORY_ONLY . Esto significa que df.cache() es equivalente a df.persist(StorageLevel.MEMORY_ONLY) . Generalmente, cache() se usa para la persistencia m\u00e1s com\u00fan y r\u00e1pida (en memoria), mientras que persist() se utiliza cuando se necesita un control m\u00e1s granular sobre c\u00f3mo se almacenan los datos. Es importante recordar que la persistencia es \"lazy\", es decir, los datos no se almacenan hasta que se ejecuta una acci\u00f3n sobre el DataFrame persistido por primera vez. Para despersistir un DataFrame, se utiliza unpersist() .","title":"Diferencia entre persist() y cache()"},{"location":"tema24/#242-optimizacion-con-catalyst-y-tungsten","text":"Apache Spark se basa en dos motores de optimizaci\u00f3n clave: Catalyst Optimizer y Project Tungsten . Juntos, estos componentes son responsables de la eficiencia y el alto rendimiento que Spark logra en el procesamiento de datos a gran escala, transformando las operaciones de DataFrames y SQL en planes de ejecuci\u00f3n optimizados y utilizando la memoria y la CPU de manera extremadamente eficiente.","title":"2.4.2 Optimizaci\u00f3n con Catalyst y Tungsten"},{"location":"tema24/#catalyst-optimizer-el-cerebro-de-la-planificacion","text":"Catalyst Optimizer es el motor de optimizaci\u00f3n de consultas de Spark SQL (y DataFrames). Funciona en varias fases para traducir las transformaciones de alto nivel que el usuario escribe en un plan de ejecuci\u00f3n de bajo nivel y altamente optimizado. Su dise\u00f1o modular y extensible permite incorporar nuevas t\u00e9cnicas de optimizaci\u00f3n y fuentes de datos. Fase 1: An\u00e1lisis (Analysis) : Spark SQL analiza la consulta (DataFrame API o SQL) para resolver referencias, verificar la sintaxis y el esquema. Convierte el \u00e1rbol l\u00f3gico no resuelto (unresolved logical plan) en un \u00e1rbol l\u00f3gico resuelto (resolved logical plan). Es decir, mapea los nombres de columnas y tablas a sus respectivas fuentes de datos. Identificaci\u00f3n de errores de esquema : Si una columna referenciada no existe en el esquema de un DataFrame, Catalyst lo detectar\u00e1 en esta fase y lanzar\u00e1 una excepci\u00f3n. Resoluci\u00f3n de ambig\u00fcedades : Si una columna existe en m\u00faltiples tablas en un join, Catalyst requiere que se califique con el nombre de la tabla para resolver la ambig\u00fcedad. Fase 2: Optimizaci\u00f3n L\u00f3gica (Logical Optimization) : En esta fase, Catalyst aplica un conjunto de reglas de optimizaci\u00f3n sobre el plan l\u00f3gico resuelto para reducir la cantidad de datos a procesar o el n\u00famero de operaciones. Estas optimizaciones son independientes del tipo de motor de ejecuci\u00f3n. Predicado Pushdown (Predicate Pushdown) : Si se aplica un filtro ( .where() ) a un DataFrame que se lee de una fuente de datos (como Parquet), Catalyst empujar\u00e1 este filtro a la fuente de datos. Esto significa que la fuente de datos leer\u00e1 solo los registros que cumplan con la condici\u00f3n, reduciendo la cantidad de datos que se transfieren a Spark. Column Pruning : Si solo se seleccionan algunas columnas ( .select() ) de un DataFrame, Catalyst se asegura de que solo esas columnas se lean del origen de datos, en lugar de todo el conjunto de columnas. Combinaci\u00f3n de filtros : Si se tienen m\u00faltiples condiciones filter() o where() , Catalyst puede combinarlas en una sola expresi\u00f3n para una evaluaci\u00f3n m\u00e1s eficiente. Fase 3: Planificaci\u00f3n F\u00edsica (Physical Planning) : El plan l\u00f3gico optimizado se convierte en uno o m\u00e1s planes f\u00edsicos. Aqu\u00ed, Catalyst considera el entorno de ejecuci\u00f3n (tama\u00f1o del cl\u00faster, datos en cach\u00e9, etc.) y elige la mejor estrategia de ejecuci\u00f3n para cada operaci\u00f3n, generando c\u00f3digo ejecutable para el motor Tungsten. Elecci\u00f3n de estrategia de Join : Catalyst decide si usar un Broadcast Join , Shuffle Hash Join , Sort Merge Join , etc., bas\u00e1ndose en el tama\u00f1o de las tablas y la configuraci\u00f3n. Manejo de agregaciones : Decide si realizar agregaciones parciales (partial aggregations) en cada partici\u00f3n antes de combinarlas para reducir el shuffle. Fase 4: Generaci\u00f3n de C\u00f3digo (Code Generation) : La fase final donde se genera c\u00f3digo Java bytecode din\u00e1micamente en tiempo de ejecuci\u00f3n para ejecutar el plan f\u00edsico. Esto evita la sobrecarga de la interpretaci\u00f3n y permite que las operaciones se ejecuten a velocidades cercanas a las de c\u00f3digo nativo. Evaluaci\u00f3n de expresiones : Genera c\u00f3digo altamente optimizado para la evaluaci\u00f3n de expresiones complejas en lugar de usar llamadas a funciones gen\u00e9ricas, lo que reduce la sobrecarga de la JVM. Operaciones vectorizadas : Permite la ejecuci\u00f3n de operaciones por lotes (vectorizadas) en lugar de una fila a la vez, lo que es mucho m\u00e1s eficiente para operaciones como filtros y proyecciones.","title":"Catalyst Optimizer: El Cerebro de la Planificaci\u00f3n"},{"location":"tema24/#tungsten-el-motor-de-ejecucion-de-bajo-nivel","text":"Project Tungsten es una iniciativa de optimizaci\u00f3n de bajo nivel en Spark que se enfoca en mejorar el uso de la memoria y la eficiencia de la CPU. Su objetivo principal es cerrar la brecha de rendimiento entre el c\u00f3digo Java/Scala y el c\u00f3digo nativo, utilizando t\u00e9cnicas como la gesti\u00f3n de memoria off-heap (fuera del heap de la JVM), la serializaci\u00f3n eficiente y la generaci\u00f3n de c\u00f3digo justo a tiempo (JIT). Gesti\u00f3n de Memoria Off-heap : Tungsten permite a Spark almacenar datos directamente en la memoria fuera del heap de la JVM, en formato binario y compactado. Esto reduce la sobrecarga de la recolecci\u00f3n de basura (Garbage Collection) de la JVM, que puede ser un cuello de botella significativo en cargas de trabajo de Big Data. Agregaciones y Joins con mucha memoria : Operaciones como groupBy o join que requieren mantener grandes tablas hash en memoria pueden beneficiarse enormemente al almacenar estas estructuras off-heap, evitando pausas prolongadas de GC. Ordenamiento (Sorting) : La clasificaci\u00f3n de grandes vol\u00famenes de datos puede ser m\u00e1s eficiente al manejar los datos directamente en memoria off-heap, reduciendo la presi\u00f3n sobre el heap de la JVM. Vectorizaci\u00f3n y Generaci\u00f3n de C\u00f3digo : Tungsten trabaja en conjunto con Catalyst para generar c\u00f3digo optimizado que procesa los datos de forma vectorial (por lotes) en lugar de fila por fila. Esto minimiza el costo de las llamadas a funciones y permite una mejor utilizaci\u00f3n del cach\u00e9 de la CPU. Procesamiento de columnas : Al leer datos en formato columnar (como Parquet), Tungsten puede procesar m\u00faltiples valores de una columna a la vez, aplicando operaciones de forma m\u00e1s eficiente. Operaciones de expresi\u00f3n : Para expresiones complejas que involucran m\u00faltiples funciones (ej. col1 + col2 * 5 - length(col3) ), Tungsten genera un \u00fanico bloque de c\u00f3digo que eval\u00faa toda la expresi\u00f3n de una vez. Serializaci\u00f3n Mejorada (Unsafe Row Format) : Tungsten introduce un formato de fila binario llamado \"Unsafe Row\", que es muy compacto y permite un acceso a datos basado en punteros, similar a c\u00f3mo se accede a los datos en C++. Esto elimina la necesidad de serializaci\u00f3n/deserializaci\u00f3n costosa entre pasos. Reducci\u00f3n de I/O en shuffles : Cuando los datos necesitan ser enviados a trav\u00e9s de la red durante un shuffle, el formato Unsafe Row minimiza el volumen de datos a transferir, reduciendo el cuello de botella de la red. ** Cach\u00e9 de datos eficiente *: Al almacenar datos en cach\u00e9, el formato Unsafe Row permite que los datos se almacenen de manera m\u00e1s compacta y se accedan directamente sin deserializaci\u00f3n completa, mejorando el rendimiento de las lecturas.","title":"Tungsten: El Motor de Ejecuci\u00f3n de Bajo Nivel"},{"location":"tema24/#243-broadcast-joins-y-estrategias-para-evitar-shuffles","text":"El \"shuffle\" es una operaci\u00f3n costosa en Spark que implica la reorganizaci\u00f3n de datos a trav\u00e9s de la red entre los ejecutores. Ocurre en operaciones como groupBy , join , orderBy , y repartition . Minimizar los shuffles es una de las estrategias m\u00e1s importantes para optimizar el rendimiento en Spark. Una t\u00e9cnica clave para evitar shuffles en joins es el uso de Broadcast Joins .","title":"2.4.3 Broadcast joins y estrategias para evitar shuffles"},{"location":"tema24/#broadcast-join","text":"Un Broadcast Join es una estrategia de join en Spark donde una de las tablas (la m\u00e1s peque\u00f1a) se \"broadcast\" (transmite) a todos los nodos del cl\u00faster que participan en la operaci\u00f3n de join. Esto significa que cada ejecutor obtiene una copia completa de la tabla peque\u00f1a en su memoria local. Al tener la tabla peque\u00f1a localmente, cada ejecutor puede realizar el join con las particiones de la tabla grande sin necesidad de un shuffle, ya que no necesita intercambiar datos con otros ejecutores para encontrar las claves coincidentes. Spark detecta autom\u00e1ticamente si una tabla es lo suficientemente peque\u00f1a (o si se le indica expl\u00edcitamente con broadcast() ) para ser transmitida. La tabla peque\u00f1a se colecta al driver, se serializa y luego se env\u00eda a cada ejecutor. Los ejecutores pueden entonces realizar un Hash Join con las particiones de la tabla grande. Uni\u00f3n de una tabla de dimensiones peque\u00f1a con una tabla de hechos grande : Por ejemplo, unir una tabla de clientes (miles o cientos de miles de registros) con una tabla de transacciones (miles de millones de registros). Si la tabla de clientes es menor que el umbral de broadcast (por defecto 10 MB en Spark 3.x, configurable con spark.sql.autoBroadcastJoinThreshold ), Spark autom\u00e1ticamente realizar\u00e1 un Broadcast Join. Filtros complejos con Lookups : Cuando se tiene un conjunto de IDs de referencia (ej. una lista de c\u00f3digos de productos a excluir) que es peque\u00f1o y se necesita filtrar o enriquecer un DataFrame muy grande. Se puede crear un peque\u00f1o DataFrame con estos IDs y luego hacer un Broadcast Join.","title":"Broadcast Join"},{"location":"tema24/#estrategias-para-evitar-o-minimizar-shuffles","text":"M\u00e1s all\u00e1 de los Broadcast Joins, existen otras estrategias para reducir la necesidad de shuffles o mitigar su impacto en el rendimiento. Predicado Pushdown y Column Pruning : Estas optimizaciones (explicadas en la secci\u00f3n de Catalyst) reducen la cantidad de datos que se leen del origen y se procesan, lo que indirectamente reduce la cantidad de datos que potencialmente necesitar\u00edan ser shufflados. Al filtrar o seleccionar columnas tempranamente, se trabaja con un conjunto de datos m\u00e1s peque\u00f1o desde el principio. Filtrado por fecha antes del join : Si se va a unir una tabla de transacciones de varios a\u00f1os con una tabla de productos, y solo se necesitan transacciones del \u00faltimo mes, aplicar un filter(\"fecha >= '2025-01-01'\") antes del join reducir\u00e1 significativamente el volumen de datos que participan en el join y, por lo tanto, en cualquier shuffle subsiguiente. Seleccionar solo columnas necesarias : Si un DataFrame tiene 50 columnas pero solo se necesitan 5 para un an\u00e1lisis, realizar un .select('col1', 'col2', ...) al inicio reduce la cantidad de datos en memoria y en disco si hay shuffles. Co-ubicaci\u00f3n de Datos (Co-location) : Si los datos que se van a unir o agrupar est\u00e1n particionados de manera compatible en el almacenamiento subyacente (por ejemplo, en Hive o Parquet, utilizando la misma clave de partici\u00f3n que se usar\u00e1 para el join/group by), Spark puede aprovechar esto para realizar un Sort-Merge Join o Hash Join con menos o ning\u00fan shuffle. Esto requiere que las tablas se hayan escrito previamente con la misma estrategia de partici\u00f3n. Joins entre tablas particionadas por la misma clave : Si la tabla de pedidos y la tabla de \u00edtems_pedido est\u00e1n ambas particionadas por id_pedido , un join entre ellas por id_pedido ser\u00e1 mucho m\u00e1s eficiente ya que Spark puede simplemente unir las particiones coincidentes localmente en cada nodo. Agregaciones en datos pre-particionados : Si se agrupan datos por una columna que ya es la clave de partici\u00f3n de la tabla, Spark puede realizar agregaciones locales en cada partici\u00f3n antes de combinar resultados, reduciendo la cantidad de datos shufflados. Acumuladores y Broadcast Variables (para datos peque\u00f1os) : Aunque no evitan directamente un shuffle en DataFrames en la misma medida que un Broadcast Join, los acumuladores y las broadcast variables (a nivel de RDD, pero tambi\u00e9n \u00fatiles para datos peque\u00f1os en Spark) son herramientas para compartir datos peque\u00f1os de manera eficiente entre tareas. Las broadcast variables permiten enviar un valor de solo lectura a todos los nodos, \u00fatil para tablas de b\u00fasqueda o configuraciones. Listas de bloqueo o mapeos : Transmitir una lista peque\u00f1a de IDs prohibidos o un mapa de c\u00f3digos a descripciones a todos los ejecutores para filtrar o enriquecer datos sin realizar un join formal. Par\u00e1metros de configuraci\u00f3n din\u00e1micos : Si un algoritmo necesita un conjunto de par\u00e1metros que cambian din\u00e1micamente pero es peque\u00f1o, se puede transmitir usando una broadcast variable. Uso de repartition y coalesce con cuidado : Ambas funciones se utilizan para cambiar el n\u00famero de particiones de un DataFrame. repartition siempre implica un shuffle completo de los datos, mientras que coalesce intenta reducir el n\u00famero de particiones sin un shuffle completo si es posible (solo combina particiones existentes dentro del mismo nodo). Util\u00edzalas solo cuando sea estrictamente necesario (ej. para uniones eficientes con datos de gran tama\u00f1o, o para reducir el n\u00famero de archivos de salida). Reducir archivos de salida (small files problem) : Si un procesamiento genera miles de archivos peque\u00f1os (problema de \"small files\"), un df.coalesce(N).write.parquet(...) puede combinarlos en N archivos m\u00e1s grandes al final de la operaci\u00f3n, aunque coalesce tambi\u00e9n puede implicar un shuffle si se reduce el n\u00famero de particiones dr\u00e1sticamente. Preparaci\u00f3n para operaciones posteriores : En algunos escenarios, re-particionar un DataFrame por la clave de join antes del join puede ser beneficioso si se planean m\u00faltiples joins o agregaciones sobre la misma clave, aunque es una decisi\u00f3n que debe tomarse con base en un an\u00e1lisis de rendimiento.","title":"Estrategias para Evitar o Minimizar Shuffles"},{"location":"tema24/#244-consideraciones-de-escalabilidad-y-desempeno","text":"La escalabilidad y el desempe\u00f1o en Spark van m\u00e1s all\u00e1 de la optimizaci\u00f3n de c\u00f3digo individual; involucran la configuraci\u00f3n del cl\u00faster, la gesti\u00f3n de recursos y la elecci\u00f3n de las arquitecturas de datos adecuadas. Comprender estos aspectos es fundamental para dise\u00f1ar aplicaciones Spark robustas que puedan manejar vol\u00famenes de datos crecientes y mantener un rendimiento \u00f3ptimo en entornos de producci\u00f3n.","title":"2.4.4 Consideraciones de escalabilidad y desempe\u00f1o"},{"location":"tema24/#configuracion-del-cluster-y-asignacion-de-recursos","text":"Una configuraci\u00f3n adecuada de Spark y la asignaci\u00f3n de recursos a los ejecutores son cr\u00edticas para el desempe\u00f1o. Un cl\u00faster mal configurado puede llevar a cuellos de botella incluso con el c\u00f3digo m\u00e1s optimizado. Tama\u00f1o de los Ejecutores ( spark.executor.cores , spark.executor.memory ) : Estos par\u00e1metros controlan cu\u00e1ntos n\u00facleos de CPU y cu\u00e1nta memoria se asignan a cada ejecutor. Un n\u00famero adecuado de n\u00facleos permite el paralelismo, mientras que suficiente memoria evita derrames a disco y optimiza el cach\u00e9. Un error com\u00fan es tener ejecutores muy grandes (pocos ejecutores con muchos n\u00facleos/memoria) o muy peque\u00f1os (muchos ejecutores con pocos recursos). Cl\u00faster con nodos de 64GB RAM, 16 n\u00facleos : Una configuraci\u00f3n com\u00fan podr\u00eda ser spark.executor.cores=5 y spark.executor.memory=20GB . Esto permite tener 2 ejecutores por nodo y deja memoria para el sistema operativo y el driver, maximizando la utilizaci\u00f3n de recursos sin sobrecargar. Tareas con mucha memoria (e.g., joins grandes sin broadcast) : Aumentar spark.executor.memory puede ser necesario para evitar derrames a disco durante operaciones intensivas en memoria. Memoria del Driver ( spark.driver.memory ) : La memoria asignada al nodo driver. El driver coordina las tareas, almacena metadatos y, en algunos casos, colecta resultados (como collect() ). Si se realizan operaciones que recolectan grandes cantidades de datos al driver, o si se manejan muchas broadcast variables, se necesita m\u00e1s memoria para el driver. Usar collect() en un DataFrame grande : Si se intenta df.collect() sobre un DataFrame con millones de filas, el driver podr\u00eda quedarse sin memoria. Ajustar spark.driver.memory o reestructurar el c\u00f3digo para evitar collect() en grandes vol\u00famenes. Broadcast de m\u00faltiples tablas peque\u00f1as : Si se transmiten muchas tablas peque\u00f1as, la memoria del driver podr\u00eda verse afectada. Configuraci\u00f3n de Shuffle ( spark.shuffle.service.enabled , spark.shuffle.file.buffer ) : Estos par\u00e1metros afectan c\u00f3mo Spark maneja los datos durante las operaciones de shuffle. Habilitar el servicio de shuffle externo ( spark.shuffle.service.enabled=true ) permite que los datos shufflados persistan incluso si un ejecutor falla, mejorando la fiabilidad. Ajustar el tama\u00f1o del buffer ( spark.shuffle.file.buffer ) puede optimizar las escrituras a disco durante el shuffle. Estabilidad en shuffles grandes : En entornos de producci\u00f3n con shuffles frecuentes y grandes, habilitar el shuffle service es crucial para la estabilidad y resiliencia. Rendimiento de I/O en shuffles : Para tareas con mucha escritura a disco durante shuffles, aumentar el buffer puede reducir la cantidad de peque\u00f1as escrituras y mejorar el rendimiento.","title":"Configuraci\u00f3n del Cl\u00faster y Asignaci\u00f3n de Recursos"},{"location":"tema24/#monitoreo-y-diagnostico","text":"Monitorear las aplicaciones Spark es esencial para identificar cuellos de botella y comprender el comportamiento del rendimiento. Spark UI es la herramienta principal para esto. Spark UI (Stages, Tasks, DAG Visualization) : La interfaz de usuario de Spark (accesible generalmente en http://<driver-ip>:4040 ) proporciona una visi\u00f3n detallada de las etapas (Stages), tareas (Tasks), y el Grafo Ac\u00edclico Dirigido (DAG) de la aplicaci\u00f3n. Permite identificar qu\u00e9 etapas son lentas, si hay desequilibrio de datos (skew), o si los ejecutores est\u00e1n infrautilizados/sobrecargados. Identificar Stage lento : Si una etapa particular (e.g., un join o groupBy ) toma mucho tiempo, se puede profundizar en esa etapa para ver si alguna tarea est\u00e1 tardando m\u00e1s de lo normal (indicando skew) o si hay problemas de I/O. An\u00e1lisis de Shuffles : La Spark UI muestra el tama\u00f1o de los datos shufflados y el tiempo que toma. Un shuffle excesivo es una se\u00f1al de que las estrategias de optimizaci\u00f3n (como Broadcast Join) podr\u00edan ser necesarias. Revisar el plan de ejecuci\u00f3n : En la pesta\u00f1a \"SQL\" de la Spark UI, se puede ver el plan de ejecuci\u00f3n generado por Catalyst, lo que ayuda a entender c\u00f3mo Spark est\u00e1 procesando la consulta y si se est\u00e1n aplicando las optimizaciones esperadas (e.g., Predicate Pushdown). M\u00e9tricas de Recursos (CPU, Memoria, Disk I/O) : Adem\u00e1s de Spark UI, es importante monitorear las m\u00e9tricas a nivel de cl\u00faster (CPU, uso de memoria, E/S de disco, red) para cada nodo ejecutor. Esto ayuda a identificar si el problema es de Spark en s\u00ed o si hay una limitaci\u00f3n de recursos a nivel de infraestructura. CPU subutilizada : Si la CPU de los ejecutores est\u00e1 consistentemente baja durante una etapa que se esperaba intensiva en CPU, podr\u00eda indicar un problema de paralelismo o un cuello de botella en I/O. Memoria agotada : Si los ejecutores est\u00e1n reportando errores OOM (Out Of Memory) o si hay mucho spill a disco, es una se\u00f1al de que los DataFrames son demasiado grandes para la memoria disponible o que la configuraci\u00f3n de persistencia es ineficiente.","title":"Monitoreo y Diagn\u00f3stico"},{"location":"tema24/#consideraciones-de-diseno-de-datos","text":"La forma en que se almacenan y estructuran los datos tiene un impacto directo en el rendimiento de Spark. Formato de Archivos (Parquet, ORC) : Optar por formatos de archivo columnares como Parquet u ORC es crucial. Estos formatos son auto-descriptivos, comprimidos y permiten optimizaciones como \"Predicate Pushdown\" y \"Column Pruning\" de forma nativa, lo que reduce dr\u00e1sticamente la cantidad de datos le\u00eddos del disco. Reemplazar CSV/JSON por Parquet : Migrar conjuntos de datos de formatos basados en filas (CSV, JSON) a Parquet para obtener mejoras significativas en el rendimiento de lectura y la eficiencia del almacenamiento. Beneficios del Predicate Pushdown : Si se tiene una tabla Parquet de logs y se filtra por timestamp > 'X' , Spark solo leer\u00e1 los bloques de datos relevantes, sin necesidad de escanear todo el archivo. Particionamiento de Datos : Organizar los datos en el sistema de archivos (HDFS, S3) en directorios basados en valores de columna (ej. data/year=2024/month=01/ ). Esto permite a Spark (y otras herramientas de Big Data) \"podar\" particiones (partition pruning), es decir, escanear solo los directorios relevantes para una consulta, evitando la lectura de datos innecesarios. Consulta de datos por fecha : Si los datos de ventas est\u00e1n particionados por a\u00f1o/mes/d\u00eda , una consulta para las ventas de un d\u00eda espec\u00edfico solo escanear\u00e1 el directorio correspondiente a ese d\u00eda, no toda la tabla. Optimizando Joins con partici\u00f3n por clave : Si se va a unir por una clave y ambas tablas est\u00e1n particionadas por esa misma clave, se puede evitar un shuffle completo (co-ubicaci\u00f3n, como se mencion\u00f3 anteriormente). Tama\u00f1o de los Archivos (Small Files Problem) : Tener un gran n\u00famero de archivos peque\u00f1os (ej. miles de archivos de unos pocos KB) en un sistema de archivos distribuido (como HDFS) puede degradar seriamente el rendimiento. Esto se debe a la sobrecarga de gestionar metadatos para cada archivo y la ineficiencia de la lectura de muchos archivos peque\u00f1os. Compactaci\u00f3n de datos hist\u00f3ricos : Si se ingieren datos en peque\u00f1os lotes, peri\u00f3dicamente se debe ejecutar un proceso de \"compactaci\u00f3n\" que combine estos peque\u00f1os archivos en unos pocos archivos m\u00e1s grandes (e.g., 128 MB a 1 GB) para optimizar las lecturas futuras. Ajustar el n\u00famero de particiones de salida : Al escribir resultados, se puede usar repartition() o coalesce() para controlar el n\u00famero de archivos de salida y evitar generar demasiados peque\u00f1os.","title":"Consideraciones de Dise\u00f1o de Datos"},{"location":"tema24/#tarea","text":"Ejercicio de Persistencia con diferentes niveles : Crea un DataFrame con 10 millones de filas y varias columnas. Realiza una serie de transformaciones sobre \u00e9l. Luego, persiste el DataFrame utilizando MEMORY_ONLY , MEMORY_AND_DISK , y DISK_ONLY (en diferentes ejecuciones). Mide el tiempo de ejecuci\u00f3n de las operaciones posteriores a la primera acci\u00f3n para cada nivel de persistencia y explica las diferencias observadas en el rendimiento y el uso de memoria/disco (usando Spark UI). Identificaci\u00f3n de un Shuffle Costoso : Dise\u00f1a un escenario donde se genere un DataFrame grande y se realice una operaci\u00f3n que cause un shuffle significativo (ej. un groupBy por una columna de alta cardinalidad o un join entre dos DataFrames grandes sin una estrategia de optimizaci\u00f3n). Utiliza Spark UI para identificar la etapa del shuffle, el volumen de datos shufflados y el tiempo que consume. Implementaci\u00f3n de Broadcast Join : Toma el escenario del ejercicio anterior (o crea uno similar con un join entre una tabla grande y una peque\u00f1a). Implementa un Broadcast Join utilizando F.broadcast() y compara el tiempo de ejecuci\u00f3n y la ausencia/reducci\u00f3n del shuffle en Spark UI con respecto a un join normal. Predicado Pushdown y Column Pruning en acci\u00f3n : Lee un archivo Parquet con m\u00faltiples columnas y filtra por una columna indexada (si el formato lo permite) y selecciona solo un subconjunto de columnas. Observa el plan de ejecuci\u00f3n en Spark UI (pesta\u00f1a SQL -> Details) y explica c\u00f3mo Catalyst aplica el Predicado Pushdown y el Column Pruning. Simulaci\u00f3n de \"Small Files Problem\" y Soluci\u00f3n : Escribe un DataFrame grande en 1000 archivos peque\u00f1os. Luego, lee esos 1000 archivos y escribe el resultado en 10 archivos m\u00e1s grandes utilizando coalesce() o repartition() . Mide y compara los tiempos de lectura y escritura. Configuraci\u00f3n de Ejecutores : Experimenta con la configuraci\u00f3n de spark.executor.cores y spark.executor.memory en una aplicaci\u00f3n Spark. Ejecuta una carga de trabajo intensiva (ej. un join complejo o una agregaci\u00f3n grande) con diferentes configuraciones (ej. pocos ejecutores grandes vs. muchos ejecutores peque\u00f1os, manteniendo el mismo n\u00famero total de n\u00facleos y memoria para el cl\u00faster). Analiza el impacto en el rendimiento y la utilizaci\u00f3n de recursos en Spark UI. UDFs vs. Funciones Nativas para Optimizaci\u00f3n : Crea una funci\u00f3n para manipular una columna (ej. concatenar strings, transformar un valor num\u00e9rico). Primero, implementa la l\u00f3gica como una UDF. Luego, re-implementa la misma l\u00f3gica utilizando solo funciones nativas de Spark SQL (ej. concat_ws , when , lit ). Compara el rendimiento de ambas implementaciones y explica por qu\u00e9 una es m\u00e1s eficiente. An\u00e1lisis de Derrames a Disco (Spill) : Ejecuta una operaci\u00f3n Spark (ej. un groupBy con una clave de muy alta cardinalidad y un agregado complejo) con memoria de ejecutor limitada, forzando que Spark \"derrame\" datos al disco (spill). Monitorea el evento en Spark UI y explica qu\u00e9 significa el \"spill\" y c\u00f3mo afecta el rendimiento. Optimizaci\u00f3n de Sort : Genera un DataFrame con datos aleatorios y ord\u00e9nalo utilizando orderBy . Luego, compara el plan de ejecuci\u00f3n y el rendimiento si los datos ya estuvieran pre-ordenados o si se aplicara una partici\u00f3n adecuada antes del ordenamiento (esto puede ser un ejercicio conceptual o simulado). Lectura con Particionamiento (Partition Pruning) : Crea un DataFrame grande y escr\u00edbelo en Parquet particionando por una columna (ej. df.write.partitionBy(\"ciudad\").parquet(\"path/to/data\") ). Luego, lee los datos filtrando por esa columna particionada (ej. spark.read.parquet(\"path/to/data\").filter(\"ciudad = 'Bogota'\") ). Observa en Spark UI c\u00f3mo Spark solo escanea los directorios relevantes, demostrando el \"partition pruning\".","title":"Tarea"},{"location":"tema31/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.1. Dise\u00f1o y Orquestaci\u00f3n de Pipelines ETL Objetivo : Comprender la estructura l\u00f3gica y la gesti\u00f3n de la ejecuci\u00f3n de flujos de datos mediante el dise\u00f1o y la orquestaci\u00f3n de pipelines ETL eficientes. El estudiante ser\u00e1 capaz de aplicar principios fundamentales del proceso ETL, diferenciar entre ETL y ELT, dise\u00f1ar pipelines desacoplados y escalables, e introducirse en herramientas de orquestaci\u00f3n como Apache Airflow para la coordinaci\u00f3n de tareas distribuidas en el tiempo. Introducci\u00f3n : Los pipelines ETL son la columna vertebral de cualquier soluci\u00f3n de integraci\u00f3n de datos en el ecosistema Big Data. Desde la recopilaci\u00f3n de datos de m\u00faltiples fuentes hasta su transformaci\u00f3n y carga en sistemas de almacenamiento anal\u00edticos, su dise\u00f1o debe ser eficiente, escalable y mantenible. Adem\u00e1s, la orquestaci\u00f3n adecuada permite automatizar estos procesos, garantizando su ejecuci\u00f3n en el orden correcto y en el momento adecuado. Desarrollo : Este tema explora los fundamentos de los procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga), sus variantes como ELT, y las decisiones arquitect\u00f3nicas que conllevan. Se introduce el concepto de dise\u00f1o modular para facilitar la reutilizaci\u00f3n y el mantenimiento de componentes. Finalmente, se estudian herramientas como Apache Airflow, que permiten orquestar tareas complejas y distribuidas a trav\u00e9s de la definici\u00f3n de flujos de trabajo (DAGs) y la gesti\u00f3n de dependencias y programaci\u00f3n. \u00a1Claro que s\u00ed! Aqu\u00ed tienes una versi\u00f3n m\u00e1s completa y clara del sub-tema 3.1.1, con ejemplos detallados y adicionales para cada etapa del proceso ETL: 3.1.1 Principios y etapas del proceso ETL El proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) es una metodolog\u00eda fundamental en el mundo del Big Data para la integraci\u00f3n de datos. Su objetivo principal es mover datos de diversas fuentes a un sistema de destino, como un data warehouse o un data lake, asegurando su calidad, consistencia y disponibilidad para an\u00e1lisis y toma de decisiones. Este proceso se divide en tres etapas principales: Extracci\u00f3n de datos La extracci\u00f3n es la primera fase del proceso ETL y consiste en la recolecci\u00f3n de datos desde sus fuentes originales para llevarlos a un \u00e1rea de staging o procesamiento inicial. Esta \u00e1rea temporal es crucial, ya que permite trabajar con los datos sin afectar el rendimiento de las fuentes de origen. Las fuentes pueden ser muy variadas y complejas, desde bases de datos relacionales hasta sensores en tiempo real. Extracci\u00f3n desde bases de datos relacionales (usando JDBC) : Escenario : Necesitas extraer los datos de ventas diarias de una base de datos de producci\u00f3n (por ejemplo, PostgreSQL o MySQL) para un informe de ventas. Proceso : Se utiliza una conexi\u00f3n JDBC (Java Database Connectivity) para establecer comunicaci\u00f3n con la base de datos. Puedes ejecutar consultas SQL complejas para seleccionar solo los datos relevantes, aplicar filtros por fecha o estado, y unirlos con otras tablas si es necesario. Por ejemplo, extraer solo las transacciones del \u00faltimo d\u00eda con estado \"completado\" y que superen un cierto monto. Ejemplo : Extraer datos de clientes de una base de datos Oracle, seleccionando aquellos que han realizado compras en los \u00faltimos 6 meses y excluyendo los clientes inactivos. Se podr\u00eda usar una consulta como SELECT * FROM clientes WHERE ultima_compra >= CURRENT_DATE - INTERVAL '6 months' AND estado = 'activo'; . Recolecci\u00f3n de datos de APIs REST con autenticaci\u00f3n OAuth2 : Escenario : Quieres obtener datos de interacci\u00f3n de usuarios de una plataforma de redes sociales o un sistema de CRM externo que expone sus datos a trav\u00e9s de una API REST. Proceso : Para acceder a estas APIs, a menudo se requiere autenticaci\u00f3n OAuth2 . Esto implica un flujo donde tu aplicaci\u00f3n solicita un token de acceso al servidor de autorizaci\u00f3n despu\u00e9s de que el usuario (o la aplicaci\u00f3n misma) ha otorgado los permisos necesarios. Con este token, puedes realizar solicitudes HTTP (GET, POST) a los endpoints de la API para recuperar los datos. Es com\u00fan manejar la paginaci\u00f3n y los l\u00edmites de tasa (rate limiting) de la API para asegurar una extracci\u00f3n completa y eficiente. Ejemplo : Extraer datos de \u00f3rdenes de una plataforma de comercio electr\u00f3nico como Shopify o Stripe. Se configura la aplicaci\u00f3n para obtener tokens de acceso OAuth2 y luego se hacen llamadas a endpoints como /admin/api/2023-10/orders.json para obtener los detalles de las \u00f3rdenes, filtrando por fecha o estado de pago. Consumo de archivos planos (CSV, JSON) desde buckets en la nube : Escenario : Una empresa recibe archivos CSV con datos de transacciones de sus socios comerciales o archivos JSON con logs de aplicaciones que se almacenan en servicios de almacenamiento en la nube como Amazon S3, Google Cloud Storage o Azure Blob Storage. Proceso : Se utilizan los SDKs (Software Development Kits) o APIs de estos servicios en la nube para listar y descargar los archivos. Es fundamental tener en cuenta la estructura de los directorios (por ejemplo, s3://bucket-name/raw_data/2025/06/03/transacciones.csv ) para una extracci\u00f3n organizada. Para archivos grandes, se pueden usar t\u00e9cnicas de descarga por partes o streaming para evitar problemas de memoria. Ejemplo : Consumir archivos de logs de servidores web en formato JSON que se suben diariamente a un bucket de Azure Blob Storage. Se podr\u00eda programar un script para que a las 2 AM descargue todos los archivos JSON del d\u00eda anterior que se encuentren en un prefijo espec\u00edfico, como logs/webserver/2025/06/03/ . Transformaci\u00f3n de datos Una vez extra\u00eddos los datos, la etapa de transformaci\u00f3n es donde el valor real se agrega. Aqu\u00ed, los datos son limpiados, combinados, enriquecidos y modificados para ajustarse a los requisitos del negocio y a la estructura del sistema de destino. Esta fase es cr\u00edtica para garantizar la calidad y la usabilidad de los datos para an\u00e1lisis. Es com\u00fan que esta etapa sea la m\u00e1s intensiva en recursos computacionales. Conversi\u00f3n de formatos de fecha y limpieza de valores nulos en Spark : Escenario : Los datos extra\u00eddos contienen fechas en diferentes formatos (por ejemplo, \"MM/DD/YYYY\", \"DD-MM-YY\", \"YYYYMMDD\") y tienen muchos valores nulos en campos importantes como el ID de cliente o el monto de la transacci\u00f3n. Proceso con Spark : Utilizando Apache Spark, puedes leer los datos (por ejemplo, desde un DataFrame). Para la conversi\u00f3n de fechas, se usan funciones como to_date() y date_format() para estandarizar a un formato \u00fanico (por ejemplo, \"YYYY-MM-DD\"). Para los valores nulos, Spark ofrece funciones como fillna() para reemplazar nulos con un valor por defecto (0 para n\u00fameros, \"Desconocido\" para cadenas) o dropna() para eliminar filas que contengan nulos en columnas espec\u00edficas. Ejemplo : Un conjunto de datos de productos tiene una columna precio con algunos valores nulos y una columna fecha_ingreso con formatos inconsistentes. En Spark, puedes rellenar los nulos de precio con el promedio de los precios existentes y estandarizar fecha_ingreso a 'YYYY-MM-DD' usando algo como: python df = df.withColumn(\"fecha_ingreso\", to_date(col(\"fecha_ingreso\"), \"MM/dd/yyyy\")) \\ .fillna({\"precio\": df.agg({\"precio\": \"avg\"}).collect()[0][0]}) Enriquecimiento de registros con datos geogr\u00e1ficos desde una tabla de referencia externa : Escenario : Tienes un conjunto de datos de ventas con el ID de la tienda, pero necesitas analizar las ventas por regi\u00f3n geogr\u00e1fica (ciudad, estado, pa\u00eds). Dispones de una tabla maestra de tiendas con sus coordenadas geogr\u00e1ficas y ubicaciones. Proceso : Se realiza una operaci\u00f3n de join (uni\u00f3n) entre el conjunto de datos de ventas y la tabla de referencia de tiendas utilizando el ID de la tienda como clave. Una vez unidos, puedes agregar columnas como ciudad , estado , pa\u00eds a cada registro de venta, permitiendo an\u00e1lisis geogr\u00e1ficos detallados. Si la tabla de referencia es muy grande, se pueden usar t\u00e9cnicas de broadcast join en Spark para optimizar el rendimiento. Ejemplo : Enriquecer un registro de llamadas de clientes con informaci\u00f3n del plan de servicio del cliente. Si la llamada solo contiene el ID_cliente , puedes unirla con una tabla maestra de clientes_servicios que contenga ID_cliente y tipo_plan para agregar esa informaci\u00f3n a cada registro de llamada. Validaci\u00f3n de estructuras de datos usando esquemas definidos (p. ej., Avro/Parquet) : Escenario : Se reciben datos de diferentes sistemas que deben cumplir con una estructura y tipo de datos predefinidos para asegurar la consistencia en el data warehouse. Proceso : Se utilizan esquemas predefinidos (como Avro o Parquet, o incluso JSON Schema) para validar que los datos entrantes cumplan con las especificaciones. Esto implica verificar que todas las columnas esperadas est\u00e9n presentes, que los tipos de datos sean correctos (por ejemplo, que una columna de edad sea un entero y no una cadena), y que no haya datos inesperados. Si los datos no cumplen el esquema, pueden ser rechazados, puestos en cuarentena para revisi\u00f3n o corregidos autom\u00e1ticamente si es posible. Spark, al trabajar con Parquet o Avro, infiere o aplica esquemas, permitiendo la validaci\u00f3n autom\u00e1tica. Ejemplo : Validar que los datos de un feed de productos contengan siempre las columnas id_producto (entero), nombre_producto (cadena), precio (decimal) y disponible (booleano). Si un archivo de entrada tiene precio como cadena o le falta la columna disponible , el proceso de validaci\u00f3n lo marcar\u00e1 como un error. Carga de datos La etapa final del proceso ETL es la carga , donde los datos transformados se mueven al sistema de destino final. Este sistema puede ser un data warehouse (para an\u00e1lisis estructurados y reportes), un data lake (para almacenar datos brutos o semi-estructurados para futuros usos), o una base de datos anal\u00edtica (optimizada para consultas complejas). La eficiencia y la robustez de esta etapa son cruciales para la disponibilidad de los datos. Carga de datos en Amazon Redshift mediante COPY desde S3 : Escenario : Tienes grandes vol\u00famenes de datos transformados en archivos Parquet o CSV en un bucket de Amazon S3, y necesitas cargarlos eficientemente en tu data warehouse columnar, Amazon Redshift. Proceso : Redshift ofrece el comando COPY que es altamente optimizado para cargar datos masivos directamente desde S3. Este comando puede inferir el esquema o utilizar uno definido, manejar la compresi\u00f3n de archivos, y distribuir los datos de manera \u00f3ptima entre los nodos de Redshift para un rendimiento de consulta superior. Es mucho m\u00e1s r\u00e1pido que insertar fila por fila. Ejemplo : Cargar los datos de logs de aplicaciones procesados en S3 a una tabla app_logs en Redshift. El comando Redshift ser\u00eda algo como: sql COPY app_logs FROM 's3://your-bucket/processed_logs/2025/06/03/' CREDENTIALS 'aws_access_key_id=YOUR_ACCESS_KEY_ID;aws_secret_access_key=YOUR_SECRET_ACCESS_KEY' FORMAT AS PARQUET; Inserci\u00f3n por lotes en Snowflake con control de errores : Escenario : Necesitas cargar datos de transacciones financieras en Snowflake, y es cr\u00edtico que cualquier fila con errores (por ejemplo, datos faltantes o incorrectos) sea identificada y aislada sin detener el proceso de carga. Proceso : Snowflake soporta la carga por lotes (batch inserts) y ofrece robustas capacidades de manejo de errores . Puedes usar el comando COPY INTO <table> con opciones como ON_ERROR = CONTINUE o ON_ERROR = ABORT_STATEMENT y VALIDATION_MODE = RETURN_ERRORS para especificar c\u00f3mo manejar los errores. Esto permite cargar las filas v\u00e1lidas y registrar las filas con errores en una tabla separada o un archivo para su posterior revisi\u00f3n y correcci\u00f3n. Ejemplo : Cargar datos de clientes desde archivos CSV a una tabla clientes en Snowflake. Si una fila CSV tiene una fecha de nacimiento inv\u00e1lida, Snowflake puede saltar esa fila y continuar con el resto, registrando el error: sql COPY INTO clientes FROM @my_s3_stage/clientes.csv FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1) ON_ERROR = 'SKIP_FILE'; -- O 'CONTINUE' para registrar errores por fila Escritura de particiones optimizadas en HDFS para procesamiento posterior : Escenario : Est\u00e1s procesando grandes vol\u00famenes de datos en un cl\u00faster de Hadoop (por ejemplo, con Spark), y los datos transformados ser\u00e1n utilizados por otros procesos anal\u00edticos (como Hive o Impala) que se benefician enormemente de la partici\u00f3n de datos. Proceso : Al escribir los datos en HDFS (Hadoop Distributed File System) , se pueden particionar los datos por una o m\u00e1s columnas (por ejemplo, a\u00f1o , mes , d\u00eda , regi\u00f3n ). Esto crea una estructura de directorios que permite que las consultas posteriores lean solo los datos relevantes, mejorando dr\u00e1sticamente el rendimiento. Adem\u00e1s, se pueden guardar los datos en formatos optimizados para lectura como Parquet o ORC, que son auto-descriptivos y soportan compresi\u00f3n columnar. Ejemplo : Guardar datos de eventos de usuario procesados en Spark en HDFS, particionando por fecha y tipo_evento . Esto resultar\u00eda en una estructura de directorios como /user/events/fecha=2025-06-03/tipo_evento=clic/ o /user/events/fecha=2025-06-03/tipo_evento=vista_pagina/ . Cuando se consulta, por ejemplo, SELECT * FROM events WHERE fecha = '2025-06-03' AND tipo_evento = 'clic' , el motor de consulta solo leer\u00e1 el directorio espec\u00edfico, no todo el conjunto de datos. En Spark, esto se hace con: python df_processed.write.partitionBy(\"fecha\", \"tipo_evento\").parquet(\"/user/events/\") 3.1.2 Diferencias entre ETL y ELT ETL (Extract, Transform, Load) y ELT (Extract, Load, Transform) son dos enfoques fundamentales para la integraci\u00f3n de datos, dise\u00f1ados para mover y preparar informaci\u00f3n de diversas fuentes para su an\u00e1lisis y uso. Aunque ambos buscan el mismo objetivo final de disponibilizar datos transformados, se distinguen crucialmente en la secuencia de sus operaciones y en los entornos tecnol\u00f3gicos m\u00e1s adecuados para cada uno. Caracter\u00edsticas del patr\u00f3n ETL En el modelo ETL tradicional, la transformaci\u00f3n de los datos se realiza antes de la carga en el sistema de destino. Esto t\u00edpicamente ocurre en un servidor o motor de procesamiento intermedio, distinto del sistema de origen y del sistema de destino final. \u00datil en entornos donde la transformaci\u00f3n debe ocurrir antes de tocar el sistema de destino : Este patr\u00f3n es ideal cuando los datos necesitan ser limpiados, validados, enriquecidos o agregados de manera significativa antes de ser almacenados. Esto asegura que solo datos de alta calidad y estructurados de una forma espec\u00edfica lleguen al sistema de destino, lo cual es crucial para bases de datos transaccionales, sistemas de reportes operativos o aplicaciones que dependen de una estructura de datos r\u00edgida y predefinida. Ejemplo : Una empresa de telecomunicaciones que procesa registros de llamadas ( CDRs - Call Detail Records ). Antes de cargar estos datos en un sistema de facturaci\u00f3n o un data warehouse para an\u00e1lisis de uso, los datos de los CDRs (que pueden ser crudos, contener errores o duplicados) se extraen. Luego, se transforman: se eliminan duplicados, se estandarizan formatos de n\u00fameros telef\u00f3nicos, se calculan duraciones de llamadas, se asocian a clientes espec\u00edficos y se validan contra cat\u00e1logos de tarifas. Solo despu\u00e9s de estas rigurosas transformaciones, los datos limpios y estructurados se cargan en el sistema de destino, asegurando la precisi\u00f3n de la facturaci\u00f3n y los reportes. Com\u00fan en soluciones on-premise con control sobre la l\u00f3gica de negocio previa : Hist\u00f3ricamente, el ETL ha sido el pilar de los entornos de data warehousing on-premise , donde las organizaciones tienen un control completo sobre la infraestructura y los servidores de procesamiento intermedios. Permite implementar reglas de negocio complejas y detalladas antes de que los datos ingresen al data warehouse , garantizando la consistencia y la integridad. Ejemplo : Una instituci\u00f3n bancaria que gestiona datos de transacciones financieras. Dada la sensibilidad y la necesidad de cumplimiento normativo, los datos de transacciones de m\u00faltiples sistemas (cajeros autom\u00e1ticos, banca en l\u00ednea, sucursales) se extraen. Se aplican transformaciones en servidores on-premise para anonimizar informaci\u00f3n sensible, validar la coherencia entre cuentas, agregar transacciones diarias por cliente y aplicar reglas de negocio para detectar fraudes o patrones sospechosos. Esta preparaci\u00f3n se realiza en entornos controlados y seguros antes de cargar los datos en un data mart departamental o un data warehouse central para an\u00e1lisis financiero y regulatorio. Caracter\u00edsticas del patr\u00f3n ELT En el modelo ELT, los datos se cargan en el sistema de destino tal como est\u00e1n (o con transformaciones m\u00ednimas) y la transformaci\u00f3n ocurre despu\u00e9s , directamente en el sistema de destino. Este enfoque capitaliza la capacidad de c\u00f3mputo y almacenamiento escalable de las plataformas de datos modernas. Escenarios en los que el almacenamiento y procesamiento escalable est\u00e1 disponible (BigQuery, Snowflake, Amazon Redshift, Azure Synapse Analytics) : El ELT florece en el ecosistema de Big Data y la computaci\u00f3n en la nube. Los data warehouses en la nube, data lakes y motores de procesamiento distribuido ofrecen una escalabilidad masiva tanto para el almacenamiento como para el c\u00f3mputo, lo que permite cargar grandes vol\u00famenes de datos crudos r\u00e1pidamente y luego procesarlos internamente. Ejemplo : Una empresa de e-commerce recopila clics de usuarios, eventos de navegaci\u00f3n, datos de carritos abandonados y registros de logs de servidores web. Estos datos, a menudo semi-estructurados o no estructurados y en vol\u00famenes masivos, se ingieren directamente en un data lake (por ejemplo, Amazon S3) o un data warehouse en la nube (como Google BigQuery) con m\u00ednimas transformaciones iniciales. Una vez cargados, los analistas de datos o los ingenieros de datos utilizan las capacidades de procesamiento del propio data warehouse (SQL, UDFs, etc.) o motores de procesamiento sobre el data lake (Spark, Presto) para limpiar, unir, agregar y transformar los datos seg\u00fan sea necesario para an\u00e1lisis de comportamiento del usuario, personalizaci\u00f3n o machine learning . \u00datil para agilizar la carga y reducir el tiempo de espera en ingesti\u00f3n de datos crudos : Al posponer las transformaciones, el ELT permite una ingesti\u00f3n de datos mucho m\u00e1s r\u00e1pida. Los datos se cargan \"crudos\" o \"lo m\u00e1s crudos posible\" en el sistema de destino, minimizando los cuellos de botella en la fase de carga. Esto es especialmente valioso para datos en tiempo real o casi real, donde la velocidad de ingesti\u00f3n es cr\u00edtica. Ejemplo : Una plataforma de redes sociales que ingiere constantemente millones de publicaciones, comentarios, \"me gusta\" y metadatos de usuarios. Ser\u00eda ineficiente y lento aplicar transformaciones complejas a cada pieza de datos antes de cargarla. En cambio, todos estos datos se ingieren r\u00e1pidamente en un data lake escalable. Posteriormente, los equipos de an\u00e1lisis utilizan herramientas y frameworks que operan directamente sobre el data lake para transformar estos datos crudos en conjuntos de datos agregados (por ejemplo, conteo de \"me gusta\" por publicaci\u00f3n, an\u00e1lisis de sentimiento de comentarios) para reportes, feeds personalizados o modelos de recomendaci\u00f3n. Cu\u00e1ndo usar cada patr\u00f3n Usar ETL cuando la calidad y la estructura de los datos debe garantizarse antes de la carga : Este patr\u00f3n es preferible cuando el sistema de destino es sensible a la calidad de los datos, tiene esquemas r\u00edgidos, o se requiere que los datos est\u00e9n completamente limpios y conformes a reglas de negocio antes de su almacenamiento. Ejemplo : Un sistema de Enterprise Resource Planning ( ERP ) o un sistema de gesti\u00f3n de clientes ( CRM ) que requiere datos maestros (clientes, productos, proveedores) con una estructura y validaci\u00f3n estrictas. Si se integran datos de m\u00faltiples fuentes (sistemas legados, hojas de c\u00e1lculo, sistemas de terceros) para poblar estos sistemas, el ETL se asegura de que la calidad y el formato de los datos sean impecables antes de la carga, evitando inconsistencias o errores que podr\u00edan afectar las operaciones diarias. Usar ELT cuando el destino tiene suficiente capacidad de c\u00f3mputo y permite transformaciones complejas dentro del motor : Este enfoque es ideal para entornos de Big Data y la nube, donde la escalabilidad de almacenamiento y procesamiento del destino es una ventaja. Permite a los analistas y cient\u00edficos de datos trabajar directamente con los datos crudos y transformarlos ad-hoc seg\u00fan sus necesidades, sin la necesidad de un paso intermedio de transformaci\u00f3n previo. Ejemplo : Una empresa que busca realizar an\u00e1lisis predictivos y machine learning sobre vastos conjuntos de datos de comportamiento del cliente, datos de sensores IoT o registros de transacciones financieras a gran escala. Cargar todos los datos crudos en un data lake o data warehouse en la nube y luego utilizar herramientas como Apache Spark, SQL en data warehouses o incluso lenguajes de programaci\u00f3n como Python y R directamente sobre la plataforma para realizar transformaciones complejas, ingenier\u00edas de caracter\u00edsticas y construir modelos. Esto permite una mayor flexibilidad y experimentaci\u00f3n con los datos. 3.1.3 Dise\u00f1o modular y desacoplado de pipelines El dise\u00f1o modular es una estrategia fundamental en la construcci\u00f3n de pipelines de datos eficientes y resilientes . Consiste en descomponer un pipeline complejo en unidades m\u00e1s peque\u00f1as, aut\u00f3nomas y bien definidas, conocidas como m\u00f3dulos o componentes . Cada uno de estos m\u00f3dulos tiene una responsabilidad \u00fanica y bien delimitada , lo que los hace independientes entre s\u00ed. Este enfoque no solo simplifica enormemente el mantenimiento y la depuraci\u00f3n de los pipelines, sino que tambi\u00e9n mejora significativamente su escalabilidad, flexibilidad y la capacidad de integrar sistemas de monitoreo y versionado de manera efectiva. Al dividir el problema en partes manejables, se facilita la colaboraci\u00f3n entre equipos y se reduce el riesgo de errores en sistemas de gran escala. Principios del dise\u00f1o modular Separaci\u00f3n de responsabilidades : Este principio es la piedra angular del dise\u00f1o modular. Cada m\u00f3dulo debe tener una \u00fanica raz\u00f3n para cambiar, es decir, debe encargarse de una funci\u00f3n espec\u00edfica y bien definida . Por ejemplo, un m\u00f3dulo podr\u00eda ser responsable exclusivamente de la extracci\u00f3n de datos, otro de las transformaciones de limpieza y enriquecimiento, y un tercero de la carga en un destino final. Esto evita que los cambios en una parte del sistema afecten a otras \u00e1reas no relacionadas. Reutilizaci\u00f3n : Los m\u00f3dulos dise\u00f1ados bajo este principio son gen\u00e9ricos y parametrizables , lo que permite su utilizaci\u00f3n en m\u00faltiples pipelines con diferentes configuraciones o fuentes de datos. En lugar de escribir c\u00f3digo desde cero para cada nueva tarea, se pueden ensamblar pipelines a partir de una biblioteca de m\u00f3dulos existentes, lo que acelera el desarrollo y reduce la duplicaci\u00f3n de c\u00f3digo . Testabilidad : La independencia de los m\u00f3dulos facilita la prueba unitaria y la integraci\u00f3n continua . Cada componente puede ser verificado de manera aislada para asegurar que cumple con su funci\u00f3n esperada, simplificando la detecci\u00f3n y correcci\u00f3n de errores antes de que se propaguen a todo el pipeline. Esto lleva a una mayor confiabilidad del sistema y ciclos de desarrollo m\u00e1s r\u00e1pidos. Ejemplo en un pipeline de datos M\u00f3dulo de Extracci\u00f3n de Datos (Extract Layer) : Este m\u00f3dulo se especializa en la lectura de datos desde diversas fuentes y su preparaci\u00f3n para las etapas posteriores. Un m\u00f3dulo de extracci\u00f3n gen\u00e9rico que puede leer datos de diferentes bases de datos relacionales (MySQL, PostgreSQL) usando JDBC, as\u00ed como de servicios de almacenamiento en la nube como Amazon S3 o Google Cloud Storage. Este m\u00f3dulo se encargar\u00eda de normalizar la lectura, manejar las credenciales de conexi\u00f3n de forma segura y escribir los datos crudos en un formato intermedio eficiente como Parquet o Avro en un Data Lake (por ejemplo, HDFS o un bucket S3). Podr\u00eda configurarse mediante par\u00e1metros como la URL de la base de datos, la tabla a leer, las credenciales, la ruta de S3 o el nombre del bucket. M\u00f3dulo de Transformaci\u00f3n de Calidad y Enriquecimiento (Transform Layer) : Se enfoca en la aplicaci\u00f3n de reglas de negocio y limpieza de datos. Un m\u00f3dulo de transformaci\u00f3n que recibe un DataFrame de Spark. Este m\u00f3dulo podr\u00eda incluir una serie de funciones parametrizables para: Validaci\u00f3n de tipos de datos : Asegurando que las columnas contengan los tipos esperados (ej. precio es num\u00e9rico). Eliminaci\u00f3n o imputaci\u00f3n de valores nulos : Rellenando valores faltantes con promedios, medianas o valores por defecto, o eliminando registros incompletos. Estandarizaci\u00f3n de formatos : Por ejemplo, convertir todas las fechas a un formato ISO 8601 o estandarizar c\u00f3digos postales. Enriquecimiento de datos : Uniendo el conjunto de datos actual con otras fuentes (ej. tablas de referencia de clientes o cat\u00e1logos de productos) para a\u00f1adir informaci\u00f3n relevante. Este m\u00f3dulo recibir\u00eda las reglas de transformaci\u00f3n y las fuentes de datos adicionales como configuraciones, permitiendo su reutilizaci\u00f3n en diferentes contextos. M\u00f3dulo de Carga de Datos (Load Layer) : Responsable de persistir los datos transformados en los destinos finales. Un m\u00f3dulo de carga que puede conectarse a diversos destinos de almacenamiento o bases de datos anal\u00edticas. Podr\u00eda configurarse para: Cargar datos en un data warehouse columnar como Snowflake o Amazon Redshift, optimizando la escritura para el rendimiento de consultas. Escribir datos en un Data Lakehouse (como Delta Lake en Databricks) para permitir futuras consultas con SQL o herramientas de BI. Publicar datos en un servicio de mensajer\u00eda como Apache Kafka para consumo en tiempo real por otras aplicaciones o microservicios. Este m\u00f3dulo ser\u00eda configurable a trav\u00e9s de par\u00e1metros que definan el destino (ej. tipo de base de datos, nombre del cluster, tema de Kafka), el modo de escritura (ej. append , overwrite , upsert ) y las credenciales de autenticaci\u00f3n. Desacoplamiento de componentes El desacoplamiento es un principio de dise\u00f1o que busca minimizar las dependencias directas entre los m\u00f3dulos de un pipeline. Su objetivo principal es asegurar que los cambios o fallas en un componente no tengan un impacto en cascada sobre los dem\u00e1s, lo que aumenta la tolerancia a fallos, la flexibilidad y la capacidad de evoluci\u00f3n del sistema . Cuando los componentes est\u00e1n fuertemente acoplados, un cambio en uno de ellos podr\u00eda requerir modificaciones en otros, lo que complica el mantenimiento y ralentiza el desarrollo. El desacoplamiento promueve la independencia operativa y tecnol\u00f3gica. Uso de colas de mensajes para desacoplar operaciones as\u00edncronas : Las colas de mensajes act\u00faan como intermediarios entre los productores y consumidores de datos. Por ejemplo, Apache Kafka es una plataforma de streaming distribuida que permite que un m\u00f3dulo de extracci\u00f3n escriba eventos o registros en un t\u00f3pico de Kafka , mientras que un m\u00f3dulo de transformaci\u00f3n lee de ese t\u00f3pico de forma as\u00edncrona e independiente . Si el m\u00f3dulo de transformaci\u00f3n experimenta una falla temporal, el m\u00f3dulo de extracci\u00f3n puede seguir produciendo datos sin interrupci\u00f3n, ya que los mensajes se acumulan en la cola. Esto mejora la resiliencia del pipeline y permite que los m\u00f3dulos operen a diferentes velocidades. Serializaci\u00f3n y persistencia de datos intermedios : En lugar de pasar datos directamente entre m\u00f3dulos en memoria, se puede serializar y almacenar los resultados intermedios de un m\u00f3dulo en un sistema de almacenamiento distribuido como HDFS (Hadoop Distributed File System) , Amazon S3 o Azure Data Lake Storage . Esto permite que cada m\u00f3dulo \"lea\" los datos que necesita del almacenamiento y \"escriba\" sus resultados de vuelta, rompiendo la dependencia directa. Por ejemplo, el m\u00f3dulo de extracci\u00f3n podr\u00eda escribir datos brutos en S3, y luego el m\u00f3dulo de transformaci\u00f3n leer\u00eda desde S3, procesar\u00eda los datos y escribir\u00eda los resultados transformados en otra ubicaci\u00f3n en S3. Esto no solo desacopla, sino que tambi\u00e9n proporciona puntos de recuperaci\u00f3n en caso de fallas y facilita la auditor\u00eda de las etapas del pipeline. Configuraci\u00f3n din\u00e1mica y externa de pipelines : En lugar de codificar las dependencias y la l\u00f3gica del pipeline directamente en el c\u00f3digo, se utilizan archivos de configuraci\u00f3n externos (como YAML o JSON) para definir la secuencia de ejecuci\u00f3n de los m\u00f3dulos, las fuentes de datos, los destinos y los par\u00e1metros espec\u00edficos. Esto permite modificar el comportamiento de un pipeline sin necesidad de recompilar o redeployar el c\u00f3digo . Por ejemplo, un archivo YAML podr\u00eda especificar que el m\u00f3dulo de extracci\u00f3n ModuloExtraccionDB debe leer de una base de datos db_produccion y escribir en s3://raw-data/ , y que posteriormente el ModuloTransformacionCalidad debe leer de s3://raw-data/ y escribir en s3://cleaned-data/ . Este enfoque promueve la flexibilidad y la agilidad en la gesti\u00f3n de pipelines. 3.1.4 Introducci\u00f3n a Apache Airflow Apache Airflow es una potente plataforma de c\u00f3digo abierto dise\u00f1ada para la programaci\u00f3n, orquestaci\u00f3n y monitoreo de flujos de trabajo (workflows) . En esencia, permite definir, programar y supervisar secuencias complejas de tareas de manera program\u00e1tica, utilizando un enfoque de DAGs (Directed Acyclic Graphs) . Su capacidad para manejar dependencias, reintentos y la ejecuci\u00f3n distribuida lo convierte en una herramienta fundamental para la automatizaci\u00f3n de procesos de datos, ETL y otras operaciones de TI. DAGs (Directed Acyclic Graphs) Los DAGs son el coraz\u00f3n de Airflow. Representan un flujo de tareas con dependencias claramente definidas y, como su nombre lo indica, no pueden contener ciclos. Cada nodo en un DAG es una tarea, y las aristas representan las dependencias entre ellas, indicando qu\u00e9 tareas deben completarse antes de que otras puedan comenzar. Definici\u00f3n de un DAG en Python para ejecutar una serie de tareas de limpieza y carga diaria : Un DAG en Airflow se define completamente en c\u00f3digo Python. Esto permite una gran flexibilidad y control de versiones. Por ejemplo, un DAG para un proceso ETL diario podr\u00eda incluir tareas como la extracci\u00f3n de datos de una base de datos, la limpieza y transformaci\u00f3n de esos datos, y finalmente la carga en un data warehouse. from airflow import DAG from airflow.operators.bash import BashOperator from airflow.utils.dates import days_ago with DAG( dag_id='etl_diario', start_date=days_ago(1), schedule_interval='@daily', catchup=False, tags=['etl', 'datos'] ) as dag: # Tarea 1: Extracci\u00f3n de datos extraer_datos = BashOperator( task_id='extraer_datos', bash_command='python /app/scripts/extraer_datos.py' ) # Tarea 2: Limpieza de datos limpiar_datos = BashOperator( task_id='limpiar_datos', bash_command='python /app/scripts/limpiar_datos.py' ) # Tarea 3: Carga de datos cargar_datos = BashOperator( task_id='cargar_datos', bash_command='python /app/scripts/cargar_datos.py' ) extraer_datos >> limpiar_datos >> cargar_datos Representaci\u00f3n visual del flujo de tareas en la interfaz de Airflow : Una de las caracter\u00edsticas m\u00e1s destacadas de Airflow es su interfaz de usuario web intuitiva. Esta interfaz permite visualizar el DAG de forma gr\u00e1fica, mostrando las tareas, sus dependencias y el estado de cada ejecuci\u00f3n. Esto facilita el monitoreo y la depuraci\u00f3n de los flujos de trabajo. Control de versiones y reutilizaci\u00f3n de DAGs en entornos dev/test/prod : Dado que los DAGs son c\u00f3digo, se benefician de las pr\u00e1cticas de control de versiones (Git, por ejemplo). Esto permite un desarrollo colaborativo, seguimiento de cambios y la promoci\u00f3n de DAGs a trav\u00e9s de diferentes entornos (desarrollo, pruebas, producci\u00f3n) de manera controlada y reproducible, asegurando la consistencia del flujo de trabajo en todas las fases. Operadores, sensores y tareas En Airflow, las tareas son las unidades b\u00e1sicas de trabajo que un DAG ejecuta. Estas tareas se definen utilizando operadores y sensores . Operadores : Son clases predefinidas que encapsulan la l\u00f3gica para ejecutar una acci\u00f3n espec\u00edfica. Airflow ofrece una amplia variedad de operadores para interactuar con diferentes sistemas y tecnolog\u00edas. BashOperator : Permite ejecutar comandos de shell. Es \u00fatil para scripts sencillos, mover archivos o cualquier comando que pueda ejecutarse en la terminal. from airflow.operators.bash import BashOperator # ... dentro de un DAG ... ejecutar_script_shell = BashOperator( task_id='ejecutar_script_shell', bash_command='sh /opt/airflow/scripts/mi_script.sh arg1 arg2' ) PythonOperator : Permite ejecutar cualquier funci\u00f3n de Python. Esto es extremadamente flexible para l\u00f3gica de negocio personalizada, transformaciones de datos complejas o integraci\u00f3n con bibliotecas de Python. from airflow.operators.python import PythonOperator import pandas as pd def transformar_datos_py(): # Ejemplo: Cargar, transformar y guardar datos con Pandas df = pd.read_csv('/tmp/raw_data.csv') df['columna_nueva'] = df['columna_existente'] * 2 df.to_csv('/tmp/transformed_data.csv', index=False) print(\"Datos transformados exitosamente con Python.\") # ... dentro de un DAG ... transformacion_python = PythonOperator( task_id='transformacion_python', python_callable=transformar_datos_py ) SparkSubmitOperator : Facilita la ejecuci\u00f3n de jobs de Apache Spark en un cl\u00faster. Es ideal para procesamientos de Big Data distribuidos. from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator # ... dentro de un DAG ... ejecutar_spark_job = SparkSubmitOperator( task_id='ejecutar_spark_job', application='/opt/airflow/dags/spark_jobs/procesar_ventas.py', conn_id='spark_default', # Conexi\u00f3n Spark configurada en Airflow total_executor_cores='2', executor_memory='2g', num_executors='10', application_args=['--input', '/path/to/input', '--output', '/path/to/output'] ) Sensores : Son un tipo especial de operador que espera a que se cumpla una condici\u00f3n externa antes de que una tarea pueda continuar. Esto es crucial para flujos de trabajo que dependen de la disponibilidad de recursos o eventos externos. Implementaci\u00f3n de un sensor que espera archivos nuevos en S3 : Un S3KeySensor puede ser utilizado para pausar un flujo de trabajo hasta que un archivo espec\u00edfico (o un patr\u00f3n de archivos) aparezca en un bucket de S3. from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor # ... dentro de un DAG ... esperar_archivo_s3 = S3KeySensor( task_id='esperar_archivo_s3', bucket_name='mi-bucket-s3', bucket_key='entrada_diaria/{{ ds_nodash }}.csv', # Espera un archivo con la fecha de ejecuci\u00f3n aws_conn_id='aws_default', poke_interval=60, # Chequea cada 60 segundos timeout=60 * 60 * 24 # Timeout de 24 horas ) Combinaci\u00f3n de tareas condicionales con BranchPythonOperator : El BranchPythonOperator permite ejecutar una funci\u00f3n Python que retorna el task_id de la siguiente tarea a ejecutar, creando as\u00ed flujos de trabajo condicionales. Esto es \u00fatil para ramificar la ejecuci\u00f3n bas\u00e1ndose en la l\u00f3gica de negocio o en resultados de tareas previas. from airflow.operators.python import BranchPythonOperator from airflow.operators.dummy import DummyOperator # Utilizado para tareas que no hacen nada from random import random def decidir_ramal(): if random() > 0.5: return 'procesar_datos_a' else: return 'procesar_datos_b' # ... dentro de un DAG ... iniciar_proceso = DummyOperator(task_id='iniciar_proceso') decidir_camino = BranchPythonOperator( task_id='decidir_camino', python_callable=decidir_ramal ) procesar_datos_a = BashOperator( task_id='procesar_datos_a', bash_command='echo \"Procesando datos con la l\u00f3gica A\"' ) procesar_datos_b = BashOperator( task_id='procesar_datos_b', bash_command='echo \"Procesando datos con la l\u00f3gica B\"' ) finalizar_proceso = DummyOperator(task_id='finalizar_proceso', trigger_rule='none_failed_min_one_success') iniciar_proceso >> decidir_camino decidir_camino >> [procesar_datos_a, procesar_datos_b] [procesar_datos_a, procesar_datos_b] >> finalizar_proceso Gesti\u00f3n de dependencias y programaci\u00f3n Una gesti\u00f3n eficiente de las dependencias y la programaci\u00f3n es crucial para la estabilidad y el rendimiento de los flujos de trabajo ETL. Airflow proporciona mecanismos robustos para esto. Programar un DAG para que se ejecute cada hora y recupere los datos m\u00e1s recientes : Airflow utiliza el par\u00e1metro schedule_interval para definir la frecuencia de ejecuci\u00f3n de un DAG. Este puede ser una expresi\u00f3n cron, un timedelta , o una cadena predefinida como @hourly , @daily , etc. Para recuperar los datos m\u00e1s recientes, las tareas dentro del DAG pueden hacer uso de las variables de contexto de Airflow, como {{ ds }} (fecha de ejecuci\u00f3n) o {{ data_interval_start }} y {{ data_interval_end }} (rango de tiempo de los datos a procesar). from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='ingesta_horaria', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(hours=1), # Ejecuci\u00f3n cada hora catchup=False, tags=['ingesta'] ) as dag: ingestar_datos = BashOperator( task_id='ingestar_datos', # Usando macros para el rango de tiempo de ejecuci\u00f3n bash_command='python /app/scripts/ingestar_datos.py --start_time {{ data_interval_start }} --end_time {{ data_interval_end }}' ) Gesti\u00f3n de tareas paralelas con depends_on_past=False para mejorar el rendimiento : Por defecto, las tareas de un DAG pueden tener una dependencia impl\u00edcita de las ejecuciones previas exitosas. Sin embargo, en muchos escenarios, especialmente con grandes vol\u00famenes de datos, es deseable que las tareas se ejecuten de forma paralela sin esperar la finalizaci\u00f3n de ejecuciones anteriores del mismo DAG. El par\u00e1metro depends_on_past=False a nivel de DAG o tarea permite que una tarea se ejecute incluso si su instancia anterior (de una ejecuci\u00f3n previa del DAG) fall\u00f3 o a\u00fan no ha terminado, mejorando el rendimiento al permitir el paralelismo en el tiempo. Implementaci\u00f3n de backfill para recuperar ejecuciones pasadas no procesadas : Cuando un DAG se define por primera vez o ha estado inactivo y se necesita procesar datos de un per\u00edodo anterior, la funci\u00f3n backfill de Airflow es invaluable. Permite ejecutar el DAG para un rango de fechas espec\u00edfico en el pasado, como si el DAG hubiera estado activo y programado durante ese tiempo. Esto es fundamental para cargar datos hist\u00f3ricos o para recuperarse de interrupciones prolongadas en el procesamiento. Se puede ejecutar mediante la l\u00ednea de comandos de Airflow: airflow dags backfill -s 2024-01-01 -e 2024-01-31 mi_dag_id Esto ejecutar\u00eda mi_dag_id para cada schedule_interval entre el 1 y el 31 de enero de 2024. 3.1.5 Orquestaci\u00f3n de tareas distribuidas La orquestaci\u00f3n de tareas distribuidas es el proceso de gestionar y coordinar la ejecuci\u00f3n de m\u00faltiples procesos de datos que operan en sistemas distribuidos. Esto implica no solo controlar cu\u00e1ndo y c\u00f3mo se ejecutan estas tareas, sino tambi\u00e9n asegurar que se sincronicen correctamente y que el sistema sea capaz de recuperarse de fallos inesperados. En el contexto de Big Data y ETL, esto es fundamental para manejar vol\u00famenes masivos de informaci\u00f3n de manera eficiente y confiable. Coordinaci\u00f3n temporal de tareas La coordinaci\u00f3n temporal de tareas se refiere a la capacidad de definir y hacer cumplir el orden y el momento de ejecuci\u00f3n de las tareas. Las tareas en un flujo de trabajo de datos a menudo tienen interdependencias, lo que significa que una tarea solo puede comenzar una vez que otra ha finalizado o ha producido ciertos resultados. Los sistemas de orquestaci\u00f3n permiten establecer horarios fijos (por ejemplo, ejecutar un ETL todas las noches a medianoche), condiciones de ejecuci\u00f3n (como iniciar una transformaci\u00f3n solo si se ha recibido un archivo espec\u00edfico) o secuencias dependientes (ejecutar el pre-procesamiento de datos antes de la carga en un data warehouse ). Esta coordinaci\u00f3n asegura que los datos se procesen en el orden correcto y que se cumplan los acuerdos de nivel de servicio (SLA). Ejemplos : Pipeline diario que inicia a las 3am y espera la finalizaci\u00f3n de cargas previas Este pipeline implementa un ETL nocturno que procesa datos transaccionales del d\u00eda anterior. Utiliza schedule_interval='0 3 * * *' para ejecutarse diariamente a las 3:00 AM. La orquestaci\u00f3n incluye sensores que verifican la disponibilidad de archivos fuente, tareas de extracci\u00f3n desde m\u00faltiples sistemas (CRM, ERP, logs web), transformaciones de limpieza y normalizaci\u00f3n de datos, y finalmente la carga hacia el data warehouse. Las dependencias aseguran que cada etapa complete exitosamente antes de proceder, con mecanismos de retry autom\u00e1tico y alertas por email en caso de fallos. El pipeline tambi\u00e9n incluye checkpoints de validaci\u00f3n que verifican la integridad de los datos antes de cada transformaci\u00f3n mayor. Pipeline semanal que consolida los datos de los \u00faltimos siete d\u00edas Este proceso se ejecuta cada lunes a las 6:00 AM usando schedule_interval='0 6 * * 1' para generar reportes ejecutivos y m\u00e9tricas de negocio. La orquestaci\u00f3n coordina la agregaci\u00f3n de datos desde m\u00faltiples fuentes diarias ya procesadas, calculando KPIs como retention de usuarios, revenue por segmento, y m\u00e9tricas de calidad de servicio. Incluye tareas paralelas que procesan diferentes dimensiones de an\u00e1lisis (geogr\u00e1fico, demogr\u00e1fico, temporal), seguidas por una tarea de consolidaci\u00f3n final que genera dashboards actualizados y env\u00eda reportes automatizados a stakeholders. El pipeline implementa branching logic para manejar semanas con d\u00edas festivos o datos faltantes. Tarea condicional que solo se ejecute si la calidad de los datos supera cierto umbral Esta implementaci\u00f3n utiliza BranchPythonOperator para evaluar m\u00e9tricas de calidad como completitud, consistencia y validez de los datos. La funci\u00f3n de decisi\u00f3n consulta reglas de negocio predefinidas (por ejemplo, <5% de valores nulos, fechas dentro de rangos esperados, concordancia entre sistemas). Si los datos pasan las validaciones, se ejecuta el flujo principal de procesamiento; si no, se activa un flujo alternativo que registra los problemas, notifica al equipo de datos, y potencialmente ejecuta rutinas de limpieza autom\u00e1tica. La orquestaci\u00f3n incluye tareas downstream que solo se ejecutan tras confirmaci\u00f3n manual o autom\u00e1tica de que los datos corregidos cumplen los est\u00e1ndares de calidad. Pipeline de ML con reentrenamiento condicional basado en drift de datos Este pipeline monitorea continuamente la performance de modelos de machine learning en producci\u00f3n. Se ejecuta cada 4 horas usando sensores que eval\u00faan m\u00e9tricas como accuracy, precision y recall contra umbrales predefinidos. Cuando detecta degradaci\u00f3n del modelo (data drift o concept drift), automaticamente activa un flujo de reentrenamiento que incluye feature engineering actualizado, validaci\u00f3n cruzada, y despliegue A/B testing del nuevo modelo. La orquestaci\u00f3n coordina tareas paralelas para entrenar m\u00faltiples algoritmos, seleccionar el mejor performante, y actualizar gradualmente el modelo en producci\u00f3n solo despu\u00e9s de validaciones exhaustivas en ambiente staging. Pipeline de procesamiento de eventos en tiempo real con ventanas deslizantes Este flujo procesa streams de datos de IoT o clickstream usando ventanas temporales de 15 minutos que se solapan cada 5 minutos. La orquestaci\u00f3n maneja la complejidad de procesar datos que llegan con diferentes latencias, implementando buffers y mecanismos de ordenamiento temporal. Las tareas incluyen detecci\u00f3n de anomal\u00edas en tiempo real, c\u00e1lculo de m\u00e9tricas agregadas (promedios m\u00f3viles, percentiles), y triggers autom\u00e1ticos para alertas cuando se detectan patrones cr\u00edticos. El pipeline coordina la persistencia de resultados en sistemas OLTP para consultas r\u00e1pidas y OLAP para an\u00e1lisis hist\u00f3rico, manejando backpressure y failover autom\u00e1tico entre clusters de procesamiento. Pipeline de compliance y auditor\u00eda con retenci\u00f3n de datos personalizada Este sistema implementa pol\u00edticas de governanza de datos automatizadas que se ejecutan mensualmente para cumplir regulaciones como GDPR o HIPAA. La orquestaci\u00f3n coordina tareas que identifican datos sensibles usando classificaci\u00f3n autom\u00e1tica, aplican t\u00e9cnicas de anonimizaci\u00f3n o pseudonimizaci\u00f3n seg\u00fan pol\u00edticas definidas, y gestionan la retenci\u00f3n/eliminaci\u00f3n de datos basada en reglas legales espec\u00edficas por jurisdicci\u00f3n. Incluye generaci\u00f3n autom\u00e1tica de reportes de compliance, logs de auditor\u00eda inmutables, y workflows de aprobaci\u00f3n que requieren intervenci\u00f3n humana para operaciones cr\u00edticas como eliminaci\u00f3n masiva de datos personales o cambios en pol\u00edticas de retenci\u00f3n. Distribuci\u00f3n de cargas y escalabilidad La distribuci\u00f3n de cargas y escalabilidad es una de las mayores ventajas de la orquestaci\u00f3n en entornos distribuidos. Al dise\u00f1ar los flujos de trabajo para que se ejecuten en paralelo o en nodos separados, es posible procesar grandes vol\u00famenes de datos de manera mucho m\u00e1s r\u00e1pida y eficiente. Esto permite que el sistema escale horizontalmente , lo que significa que se pueden a\u00f1adir m\u00e1s recursos (nodos o m\u00e1quinas) para manejar un aumento en la demanda de procesamiento sin afectar el rendimiento. Los orquestadores facilitan la asignaci\u00f3n din\u00e1mica de recursos y la gesti\u00f3n de la concurrencia. Ejemplos : Ejecuci\u00f3n paralela de tareas de carga para m\u00faltiples regiones geogr\u00e1ficas Imagina una empresa global que recopila datos de ventas de sus operaciones en Norteam\u00e9rica, Europa y Asia. Un sistema de orquestaci\u00f3n podr\u00eda lanzar simult\u00e1neamente tres tareas de carga de datos, una para cada regi\u00f3n, cada una procesando los datos de su respectiva fuente en paralelo. Esto reduce significativamente el tiempo total necesario para cargar todos los datos, en comparaci\u00f3n con un enfoque secuencial. Distribuci\u00f3n de la transformaci\u00f3n por particiones de tiempo usando Spark Considera un pipeline ETL que procesa grandes vol\u00famenes de datos transaccionales diarios. Usando Apache Spark, un orquestador podr\u00eda dividir la transformaci\u00f3n de los datos en particiones de tiempo . Por ejemplo, los datos de la primera mitad del d\u00eda se procesan en un conjunto de workers de Spark, mientras que los datos de la segunda mitad se procesan simult\u00e1neamente en otro conjunto. Esto maximiza la utilizaci\u00f3n de recursos y acelera la finalizaci\u00f3n de la transformaci\u00f3n. Escalado autom\u00e1tico de workers de Airflow seg\u00fan la demanda de procesamiento Si un flujo de trabajo de datos tiene picos de demanda (por ejemplo, a fin de mes cuando se generan informes), un orquestador como Apache Airflow, integrado con plataformas de nube (AWS, GCP, Azure), puede configurarse para escalar autom\u00e1ticamente el n\u00famero de workers disponibles. Durante los per\u00edodos de alta carga, se aprovisionan m\u00e1s workers para manejar las tareas concurrentes, y una vez que la demanda disminuye, los workers adicionales se desaprovisionan para optimizar costos. Tarea Dise\u00f1a un pipeline ETL en pseudoc\u00f3digo que extraiga datos desde una base de datos PostgreSQL, los transforme en Spark y los cargue en un bucket S3 particionado por fecha. Compara las ventajas y desventajas de ETL y ELT en el contexto de una empresa que utiliza Snowflake como data warehouse. Define un DAG de Airflow simple que incluya tres tareas secuenciales: lectura de archivo, transformaci\u00f3n y carga en base de datos. Identifica tres puntos donde aplicar\u00edas modularidad en un pipeline ETL y justifica su beneficio. Escribe una estrategia de orquestaci\u00f3n para un pipeline que debe ejecutarse cada 15 minutos y detectar datos faltantes si alguna ejecuci\u00f3n falla.","title":"Dise\u00f1o y Orquestaci\u00f3n de Pipelines ETL"},{"location":"tema31/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema31/#tema-31-diseno-y-orquestacion-de-pipelines-etl","text":"Objetivo : Comprender la estructura l\u00f3gica y la gesti\u00f3n de la ejecuci\u00f3n de flujos de datos mediante el dise\u00f1o y la orquestaci\u00f3n de pipelines ETL eficientes. El estudiante ser\u00e1 capaz de aplicar principios fundamentales del proceso ETL, diferenciar entre ETL y ELT, dise\u00f1ar pipelines desacoplados y escalables, e introducirse en herramientas de orquestaci\u00f3n como Apache Airflow para la coordinaci\u00f3n de tareas distribuidas en el tiempo. Introducci\u00f3n : Los pipelines ETL son la columna vertebral de cualquier soluci\u00f3n de integraci\u00f3n de datos en el ecosistema Big Data. Desde la recopilaci\u00f3n de datos de m\u00faltiples fuentes hasta su transformaci\u00f3n y carga en sistemas de almacenamiento anal\u00edticos, su dise\u00f1o debe ser eficiente, escalable y mantenible. Adem\u00e1s, la orquestaci\u00f3n adecuada permite automatizar estos procesos, garantizando su ejecuci\u00f3n en el orden correcto y en el momento adecuado. Desarrollo : Este tema explora los fundamentos de los procesos ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga), sus variantes como ELT, y las decisiones arquitect\u00f3nicas que conllevan. Se introduce el concepto de dise\u00f1o modular para facilitar la reutilizaci\u00f3n y el mantenimiento de componentes. Finalmente, se estudian herramientas como Apache Airflow, que permiten orquestar tareas complejas y distribuidas a trav\u00e9s de la definici\u00f3n de flujos de trabajo (DAGs) y la gesti\u00f3n de dependencias y programaci\u00f3n. \u00a1Claro que s\u00ed! Aqu\u00ed tienes una versi\u00f3n m\u00e1s completa y clara del sub-tema 3.1.1, con ejemplos detallados y adicionales para cada etapa del proceso ETL:","title":"Tema 3.1. Dise\u00f1o y Orquestaci\u00f3n de Pipelines ETL"},{"location":"tema31/#311-principios-y-etapas-del-proceso-etl","text":"El proceso ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) es una metodolog\u00eda fundamental en el mundo del Big Data para la integraci\u00f3n de datos. Su objetivo principal es mover datos de diversas fuentes a un sistema de destino, como un data warehouse o un data lake, asegurando su calidad, consistencia y disponibilidad para an\u00e1lisis y toma de decisiones. Este proceso se divide en tres etapas principales:","title":"3.1.1 Principios y etapas del proceso ETL"},{"location":"tema31/#extraccion-de-datos","text":"La extracci\u00f3n es la primera fase del proceso ETL y consiste en la recolecci\u00f3n de datos desde sus fuentes originales para llevarlos a un \u00e1rea de staging o procesamiento inicial. Esta \u00e1rea temporal es crucial, ya que permite trabajar con los datos sin afectar el rendimiento de las fuentes de origen. Las fuentes pueden ser muy variadas y complejas, desde bases de datos relacionales hasta sensores en tiempo real. Extracci\u00f3n desde bases de datos relacionales (usando JDBC) : Escenario : Necesitas extraer los datos de ventas diarias de una base de datos de producci\u00f3n (por ejemplo, PostgreSQL o MySQL) para un informe de ventas. Proceso : Se utiliza una conexi\u00f3n JDBC (Java Database Connectivity) para establecer comunicaci\u00f3n con la base de datos. Puedes ejecutar consultas SQL complejas para seleccionar solo los datos relevantes, aplicar filtros por fecha o estado, y unirlos con otras tablas si es necesario. Por ejemplo, extraer solo las transacciones del \u00faltimo d\u00eda con estado \"completado\" y que superen un cierto monto. Ejemplo : Extraer datos de clientes de una base de datos Oracle, seleccionando aquellos que han realizado compras en los \u00faltimos 6 meses y excluyendo los clientes inactivos. Se podr\u00eda usar una consulta como SELECT * FROM clientes WHERE ultima_compra >= CURRENT_DATE - INTERVAL '6 months' AND estado = 'activo'; . Recolecci\u00f3n de datos de APIs REST con autenticaci\u00f3n OAuth2 : Escenario : Quieres obtener datos de interacci\u00f3n de usuarios de una plataforma de redes sociales o un sistema de CRM externo que expone sus datos a trav\u00e9s de una API REST. Proceso : Para acceder a estas APIs, a menudo se requiere autenticaci\u00f3n OAuth2 . Esto implica un flujo donde tu aplicaci\u00f3n solicita un token de acceso al servidor de autorizaci\u00f3n despu\u00e9s de que el usuario (o la aplicaci\u00f3n misma) ha otorgado los permisos necesarios. Con este token, puedes realizar solicitudes HTTP (GET, POST) a los endpoints de la API para recuperar los datos. Es com\u00fan manejar la paginaci\u00f3n y los l\u00edmites de tasa (rate limiting) de la API para asegurar una extracci\u00f3n completa y eficiente. Ejemplo : Extraer datos de \u00f3rdenes de una plataforma de comercio electr\u00f3nico como Shopify o Stripe. Se configura la aplicaci\u00f3n para obtener tokens de acceso OAuth2 y luego se hacen llamadas a endpoints como /admin/api/2023-10/orders.json para obtener los detalles de las \u00f3rdenes, filtrando por fecha o estado de pago. Consumo de archivos planos (CSV, JSON) desde buckets en la nube : Escenario : Una empresa recibe archivos CSV con datos de transacciones de sus socios comerciales o archivos JSON con logs de aplicaciones que se almacenan en servicios de almacenamiento en la nube como Amazon S3, Google Cloud Storage o Azure Blob Storage. Proceso : Se utilizan los SDKs (Software Development Kits) o APIs de estos servicios en la nube para listar y descargar los archivos. Es fundamental tener en cuenta la estructura de los directorios (por ejemplo, s3://bucket-name/raw_data/2025/06/03/transacciones.csv ) para una extracci\u00f3n organizada. Para archivos grandes, se pueden usar t\u00e9cnicas de descarga por partes o streaming para evitar problemas de memoria. Ejemplo : Consumir archivos de logs de servidores web en formato JSON que se suben diariamente a un bucket de Azure Blob Storage. Se podr\u00eda programar un script para que a las 2 AM descargue todos los archivos JSON del d\u00eda anterior que se encuentren en un prefijo espec\u00edfico, como logs/webserver/2025/06/03/ .","title":"Extracci\u00f3n de datos"},{"location":"tema31/#transformacion-de-datos","text":"Una vez extra\u00eddos los datos, la etapa de transformaci\u00f3n es donde el valor real se agrega. Aqu\u00ed, los datos son limpiados, combinados, enriquecidos y modificados para ajustarse a los requisitos del negocio y a la estructura del sistema de destino. Esta fase es cr\u00edtica para garantizar la calidad y la usabilidad de los datos para an\u00e1lisis. Es com\u00fan que esta etapa sea la m\u00e1s intensiva en recursos computacionales. Conversi\u00f3n de formatos de fecha y limpieza de valores nulos en Spark : Escenario : Los datos extra\u00eddos contienen fechas en diferentes formatos (por ejemplo, \"MM/DD/YYYY\", \"DD-MM-YY\", \"YYYYMMDD\") y tienen muchos valores nulos en campos importantes como el ID de cliente o el monto de la transacci\u00f3n. Proceso con Spark : Utilizando Apache Spark, puedes leer los datos (por ejemplo, desde un DataFrame). Para la conversi\u00f3n de fechas, se usan funciones como to_date() y date_format() para estandarizar a un formato \u00fanico (por ejemplo, \"YYYY-MM-DD\"). Para los valores nulos, Spark ofrece funciones como fillna() para reemplazar nulos con un valor por defecto (0 para n\u00fameros, \"Desconocido\" para cadenas) o dropna() para eliminar filas que contengan nulos en columnas espec\u00edficas. Ejemplo : Un conjunto de datos de productos tiene una columna precio con algunos valores nulos y una columna fecha_ingreso con formatos inconsistentes. En Spark, puedes rellenar los nulos de precio con el promedio de los precios existentes y estandarizar fecha_ingreso a 'YYYY-MM-DD' usando algo como: python df = df.withColumn(\"fecha_ingreso\", to_date(col(\"fecha_ingreso\"), \"MM/dd/yyyy\")) \\ .fillna({\"precio\": df.agg({\"precio\": \"avg\"}).collect()[0][0]}) Enriquecimiento de registros con datos geogr\u00e1ficos desde una tabla de referencia externa : Escenario : Tienes un conjunto de datos de ventas con el ID de la tienda, pero necesitas analizar las ventas por regi\u00f3n geogr\u00e1fica (ciudad, estado, pa\u00eds). Dispones de una tabla maestra de tiendas con sus coordenadas geogr\u00e1ficas y ubicaciones. Proceso : Se realiza una operaci\u00f3n de join (uni\u00f3n) entre el conjunto de datos de ventas y la tabla de referencia de tiendas utilizando el ID de la tienda como clave. Una vez unidos, puedes agregar columnas como ciudad , estado , pa\u00eds a cada registro de venta, permitiendo an\u00e1lisis geogr\u00e1ficos detallados. Si la tabla de referencia es muy grande, se pueden usar t\u00e9cnicas de broadcast join en Spark para optimizar el rendimiento. Ejemplo : Enriquecer un registro de llamadas de clientes con informaci\u00f3n del plan de servicio del cliente. Si la llamada solo contiene el ID_cliente , puedes unirla con una tabla maestra de clientes_servicios que contenga ID_cliente y tipo_plan para agregar esa informaci\u00f3n a cada registro de llamada. Validaci\u00f3n de estructuras de datos usando esquemas definidos (p. ej., Avro/Parquet) : Escenario : Se reciben datos de diferentes sistemas que deben cumplir con una estructura y tipo de datos predefinidos para asegurar la consistencia en el data warehouse. Proceso : Se utilizan esquemas predefinidos (como Avro o Parquet, o incluso JSON Schema) para validar que los datos entrantes cumplan con las especificaciones. Esto implica verificar que todas las columnas esperadas est\u00e9n presentes, que los tipos de datos sean correctos (por ejemplo, que una columna de edad sea un entero y no una cadena), y que no haya datos inesperados. Si los datos no cumplen el esquema, pueden ser rechazados, puestos en cuarentena para revisi\u00f3n o corregidos autom\u00e1ticamente si es posible. Spark, al trabajar con Parquet o Avro, infiere o aplica esquemas, permitiendo la validaci\u00f3n autom\u00e1tica. Ejemplo : Validar que los datos de un feed de productos contengan siempre las columnas id_producto (entero), nombre_producto (cadena), precio (decimal) y disponible (booleano). Si un archivo de entrada tiene precio como cadena o le falta la columna disponible , el proceso de validaci\u00f3n lo marcar\u00e1 como un error.","title":"Transformaci\u00f3n de datos"},{"location":"tema31/#carga-de-datos","text":"La etapa final del proceso ETL es la carga , donde los datos transformados se mueven al sistema de destino final. Este sistema puede ser un data warehouse (para an\u00e1lisis estructurados y reportes), un data lake (para almacenar datos brutos o semi-estructurados para futuros usos), o una base de datos anal\u00edtica (optimizada para consultas complejas). La eficiencia y la robustez de esta etapa son cruciales para la disponibilidad de los datos. Carga de datos en Amazon Redshift mediante COPY desde S3 : Escenario : Tienes grandes vol\u00famenes de datos transformados en archivos Parquet o CSV en un bucket de Amazon S3, y necesitas cargarlos eficientemente en tu data warehouse columnar, Amazon Redshift. Proceso : Redshift ofrece el comando COPY que es altamente optimizado para cargar datos masivos directamente desde S3. Este comando puede inferir el esquema o utilizar uno definido, manejar la compresi\u00f3n de archivos, y distribuir los datos de manera \u00f3ptima entre los nodos de Redshift para un rendimiento de consulta superior. Es mucho m\u00e1s r\u00e1pido que insertar fila por fila. Ejemplo : Cargar los datos de logs de aplicaciones procesados en S3 a una tabla app_logs en Redshift. El comando Redshift ser\u00eda algo como: sql COPY app_logs FROM 's3://your-bucket/processed_logs/2025/06/03/' CREDENTIALS 'aws_access_key_id=YOUR_ACCESS_KEY_ID;aws_secret_access_key=YOUR_SECRET_ACCESS_KEY' FORMAT AS PARQUET; Inserci\u00f3n por lotes en Snowflake con control de errores : Escenario : Necesitas cargar datos de transacciones financieras en Snowflake, y es cr\u00edtico que cualquier fila con errores (por ejemplo, datos faltantes o incorrectos) sea identificada y aislada sin detener el proceso de carga. Proceso : Snowflake soporta la carga por lotes (batch inserts) y ofrece robustas capacidades de manejo de errores . Puedes usar el comando COPY INTO <table> con opciones como ON_ERROR = CONTINUE o ON_ERROR = ABORT_STATEMENT y VALIDATION_MODE = RETURN_ERRORS para especificar c\u00f3mo manejar los errores. Esto permite cargar las filas v\u00e1lidas y registrar las filas con errores en una tabla separada o un archivo para su posterior revisi\u00f3n y correcci\u00f3n. Ejemplo : Cargar datos de clientes desde archivos CSV a una tabla clientes en Snowflake. Si una fila CSV tiene una fecha de nacimiento inv\u00e1lida, Snowflake puede saltar esa fila y continuar con el resto, registrando el error: sql COPY INTO clientes FROM @my_s3_stage/clientes.csv FILE_FORMAT = (TYPE = CSV FIELD_DELIMITER = ',' SKIP_HEADER = 1) ON_ERROR = 'SKIP_FILE'; -- O 'CONTINUE' para registrar errores por fila Escritura de particiones optimizadas en HDFS para procesamiento posterior : Escenario : Est\u00e1s procesando grandes vol\u00famenes de datos en un cl\u00faster de Hadoop (por ejemplo, con Spark), y los datos transformados ser\u00e1n utilizados por otros procesos anal\u00edticos (como Hive o Impala) que se benefician enormemente de la partici\u00f3n de datos. Proceso : Al escribir los datos en HDFS (Hadoop Distributed File System) , se pueden particionar los datos por una o m\u00e1s columnas (por ejemplo, a\u00f1o , mes , d\u00eda , regi\u00f3n ). Esto crea una estructura de directorios que permite que las consultas posteriores lean solo los datos relevantes, mejorando dr\u00e1sticamente el rendimiento. Adem\u00e1s, se pueden guardar los datos en formatos optimizados para lectura como Parquet o ORC, que son auto-descriptivos y soportan compresi\u00f3n columnar. Ejemplo : Guardar datos de eventos de usuario procesados en Spark en HDFS, particionando por fecha y tipo_evento . Esto resultar\u00eda en una estructura de directorios como /user/events/fecha=2025-06-03/tipo_evento=clic/ o /user/events/fecha=2025-06-03/tipo_evento=vista_pagina/ . Cuando se consulta, por ejemplo, SELECT * FROM events WHERE fecha = '2025-06-03' AND tipo_evento = 'clic' , el motor de consulta solo leer\u00e1 el directorio espec\u00edfico, no todo el conjunto de datos. En Spark, esto se hace con: python df_processed.write.partitionBy(\"fecha\", \"tipo_evento\").parquet(\"/user/events/\")","title":"Carga de datos"},{"location":"tema31/#312-diferencias-entre-etl-y-elt","text":"ETL (Extract, Transform, Load) y ELT (Extract, Load, Transform) son dos enfoques fundamentales para la integraci\u00f3n de datos, dise\u00f1ados para mover y preparar informaci\u00f3n de diversas fuentes para su an\u00e1lisis y uso. Aunque ambos buscan el mismo objetivo final de disponibilizar datos transformados, se distinguen crucialmente en la secuencia de sus operaciones y en los entornos tecnol\u00f3gicos m\u00e1s adecuados para cada uno.","title":"3.1.2 Diferencias entre ETL y ELT"},{"location":"tema31/#caracteristicas-del-patron-etl","text":"En el modelo ETL tradicional, la transformaci\u00f3n de los datos se realiza antes de la carga en el sistema de destino. Esto t\u00edpicamente ocurre en un servidor o motor de procesamiento intermedio, distinto del sistema de origen y del sistema de destino final. \u00datil en entornos donde la transformaci\u00f3n debe ocurrir antes de tocar el sistema de destino : Este patr\u00f3n es ideal cuando los datos necesitan ser limpiados, validados, enriquecidos o agregados de manera significativa antes de ser almacenados. Esto asegura que solo datos de alta calidad y estructurados de una forma espec\u00edfica lleguen al sistema de destino, lo cual es crucial para bases de datos transaccionales, sistemas de reportes operativos o aplicaciones que dependen de una estructura de datos r\u00edgida y predefinida. Ejemplo : Una empresa de telecomunicaciones que procesa registros de llamadas ( CDRs - Call Detail Records ). Antes de cargar estos datos en un sistema de facturaci\u00f3n o un data warehouse para an\u00e1lisis de uso, los datos de los CDRs (que pueden ser crudos, contener errores o duplicados) se extraen. Luego, se transforman: se eliminan duplicados, se estandarizan formatos de n\u00fameros telef\u00f3nicos, se calculan duraciones de llamadas, se asocian a clientes espec\u00edficos y se validan contra cat\u00e1logos de tarifas. Solo despu\u00e9s de estas rigurosas transformaciones, los datos limpios y estructurados se cargan en el sistema de destino, asegurando la precisi\u00f3n de la facturaci\u00f3n y los reportes. Com\u00fan en soluciones on-premise con control sobre la l\u00f3gica de negocio previa : Hist\u00f3ricamente, el ETL ha sido el pilar de los entornos de data warehousing on-premise , donde las organizaciones tienen un control completo sobre la infraestructura y los servidores de procesamiento intermedios. Permite implementar reglas de negocio complejas y detalladas antes de que los datos ingresen al data warehouse , garantizando la consistencia y la integridad. Ejemplo : Una instituci\u00f3n bancaria que gestiona datos de transacciones financieras. Dada la sensibilidad y la necesidad de cumplimiento normativo, los datos de transacciones de m\u00faltiples sistemas (cajeros autom\u00e1ticos, banca en l\u00ednea, sucursales) se extraen. Se aplican transformaciones en servidores on-premise para anonimizar informaci\u00f3n sensible, validar la coherencia entre cuentas, agregar transacciones diarias por cliente y aplicar reglas de negocio para detectar fraudes o patrones sospechosos. Esta preparaci\u00f3n se realiza en entornos controlados y seguros antes de cargar los datos en un data mart departamental o un data warehouse central para an\u00e1lisis financiero y regulatorio.","title":"Caracter\u00edsticas del patr\u00f3n ETL"},{"location":"tema31/#caracteristicas-del-patron-elt","text":"En el modelo ELT, los datos se cargan en el sistema de destino tal como est\u00e1n (o con transformaciones m\u00ednimas) y la transformaci\u00f3n ocurre despu\u00e9s , directamente en el sistema de destino. Este enfoque capitaliza la capacidad de c\u00f3mputo y almacenamiento escalable de las plataformas de datos modernas. Escenarios en los que el almacenamiento y procesamiento escalable est\u00e1 disponible (BigQuery, Snowflake, Amazon Redshift, Azure Synapse Analytics) : El ELT florece en el ecosistema de Big Data y la computaci\u00f3n en la nube. Los data warehouses en la nube, data lakes y motores de procesamiento distribuido ofrecen una escalabilidad masiva tanto para el almacenamiento como para el c\u00f3mputo, lo que permite cargar grandes vol\u00famenes de datos crudos r\u00e1pidamente y luego procesarlos internamente. Ejemplo : Una empresa de e-commerce recopila clics de usuarios, eventos de navegaci\u00f3n, datos de carritos abandonados y registros de logs de servidores web. Estos datos, a menudo semi-estructurados o no estructurados y en vol\u00famenes masivos, se ingieren directamente en un data lake (por ejemplo, Amazon S3) o un data warehouse en la nube (como Google BigQuery) con m\u00ednimas transformaciones iniciales. Una vez cargados, los analistas de datos o los ingenieros de datos utilizan las capacidades de procesamiento del propio data warehouse (SQL, UDFs, etc.) o motores de procesamiento sobre el data lake (Spark, Presto) para limpiar, unir, agregar y transformar los datos seg\u00fan sea necesario para an\u00e1lisis de comportamiento del usuario, personalizaci\u00f3n o machine learning . \u00datil para agilizar la carga y reducir el tiempo de espera en ingesti\u00f3n de datos crudos : Al posponer las transformaciones, el ELT permite una ingesti\u00f3n de datos mucho m\u00e1s r\u00e1pida. Los datos se cargan \"crudos\" o \"lo m\u00e1s crudos posible\" en el sistema de destino, minimizando los cuellos de botella en la fase de carga. Esto es especialmente valioso para datos en tiempo real o casi real, donde la velocidad de ingesti\u00f3n es cr\u00edtica. Ejemplo : Una plataforma de redes sociales que ingiere constantemente millones de publicaciones, comentarios, \"me gusta\" y metadatos de usuarios. Ser\u00eda ineficiente y lento aplicar transformaciones complejas a cada pieza de datos antes de cargarla. En cambio, todos estos datos se ingieren r\u00e1pidamente en un data lake escalable. Posteriormente, los equipos de an\u00e1lisis utilizan herramientas y frameworks que operan directamente sobre el data lake para transformar estos datos crudos en conjuntos de datos agregados (por ejemplo, conteo de \"me gusta\" por publicaci\u00f3n, an\u00e1lisis de sentimiento de comentarios) para reportes, feeds personalizados o modelos de recomendaci\u00f3n.","title":"Caracter\u00edsticas del patr\u00f3n ELT"},{"location":"tema31/#cuando-usar-cada-patron","text":"Usar ETL cuando la calidad y la estructura de los datos debe garantizarse antes de la carga : Este patr\u00f3n es preferible cuando el sistema de destino es sensible a la calidad de los datos, tiene esquemas r\u00edgidos, o se requiere que los datos est\u00e9n completamente limpios y conformes a reglas de negocio antes de su almacenamiento. Ejemplo : Un sistema de Enterprise Resource Planning ( ERP ) o un sistema de gesti\u00f3n de clientes ( CRM ) que requiere datos maestros (clientes, productos, proveedores) con una estructura y validaci\u00f3n estrictas. Si se integran datos de m\u00faltiples fuentes (sistemas legados, hojas de c\u00e1lculo, sistemas de terceros) para poblar estos sistemas, el ETL se asegura de que la calidad y el formato de los datos sean impecables antes de la carga, evitando inconsistencias o errores que podr\u00edan afectar las operaciones diarias. Usar ELT cuando el destino tiene suficiente capacidad de c\u00f3mputo y permite transformaciones complejas dentro del motor : Este enfoque es ideal para entornos de Big Data y la nube, donde la escalabilidad de almacenamiento y procesamiento del destino es una ventaja. Permite a los analistas y cient\u00edficos de datos trabajar directamente con los datos crudos y transformarlos ad-hoc seg\u00fan sus necesidades, sin la necesidad de un paso intermedio de transformaci\u00f3n previo. Ejemplo : Una empresa que busca realizar an\u00e1lisis predictivos y machine learning sobre vastos conjuntos de datos de comportamiento del cliente, datos de sensores IoT o registros de transacciones financieras a gran escala. Cargar todos los datos crudos en un data lake o data warehouse en la nube y luego utilizar herramientas como Apache Spark, SQL en data warehouses o incluso lenguajes de programaci\u00f3n como Python y R directamente sobre la plataforma para realizar transformaciones complejas, ingenier\u00edas de caracter\u00edsticas y construir modelos. Esto permite una mayor flexibilidad y experimentaci\u00f3n con los datos.","title":"Cu\u00e1ndo usar cada patr\u00f3n"},{"location":"tema31/#313-diseno-modular-y-desacoplado-de-pipelines","text":"El dise\u00f1o modular es una estrategia fundamental en la construcci\u00f3n de pipelines de datos eficientes y resilientes . Consiste en descomponer un pipeline complejo en unidades m\u00e1s peque\u00f1as, aut\u00f3nomas y bien definidas, conocidas como m\u00f3dulos o componentes . Cada uno de estos m\u00f3dulos tiene una responsabilidad \u00fanica y bien delimitada , lo que los hace independientes entre s\u00ed. Este enfoque no solo simplifica enormemente el mantenimiento y la depuraci\u00f3n de los pipelines, sino que tambi\u00e9n mejora significativamente su escalabilidad, flexibilidad y la capacidad de integrar sistemas de monitoreo y versionado de manera efectiva. Al dividir el problema en partes manejables, se facilita la colaboraci\u00f3n entre equipos y se reduce el riesgo de errores en sistemas de gran escala.","title":"3.1.3 Dise\u00f1o modular y desacoplado de pipelines"},{"location":"tema31/#principios-del-diseno-modular","text":"Separaci\u00f3n de responsabilidades : Este principio es la piedra angular del dise\u00f1o modular. Cada m\u00f3dulo debe tener una \u00fanica raz\u00f3n para cambiar, es decir, debe encargarse de una funci\u00f3n espec\u00edfica y bien definida . Por ejemplo, un m\u00f3dulo podr\u00eda ser responsable exclusivamente de la extracci\u00f3n de datos, otro de las transformaciones de limpieza y enriquecimiento, y un tercero de la carga en un destino final. Esto evita que los cambios en una parte del sistema afecten a otras \u00e1reas no relacionadas. Reutilizaci\u00f3n : Los m\u00f3dulos dise\u00f1ados bajo este principio son gen\u00e9ricos y parametrizables , lo que permite su utilizaci\u00f3n en m\u00faltiples pipelines con diferentes configuraciones o fuentes de datos. En lugar de escribir c\u00f3digo desde cero para cada nueva tarea, se pueden ensamblar pipelines a partir de una biblioteca de m\u00f3dulos existentes, lo que acelera el desarrollo y reduce la duplicaci\u00f3n de c\u00f3digo . Testabilidad : La independencia de los m\u00f3dulos facilita la prueba unitaria y la integraci\u00f3n continua . Cada componente puede ser verificado de manera aislada para asegurar que cumple con su funci\u00f3n esperada, simplificando la detecci\u00f3n y correcci\u00f3n de errores antes de que se propaguen a todo el pipeline. Esto lleva a una mayor confiabilidad del sistema y ciclos de desarrollo m\u00e1s r\u00e1pidos. Ejemplo en un pipeline de datos M\u00f3dulo de Extracci\u00f3n de Datos (Extract Layer) : Este m\u00f3dulo se especializa en la lectura de datos desde diversas fuentes y su preparaci\u00f3n para las etapas posteriores. Un m\u00f3dulo de extracci\u00f3n gen\u00e9rico que puede leer datos de diferentes bases de datos relacionales (MySQL, PostgreSQL) usando JDBC, as\u00ed como de servicios de almacenamiento en la nube como Amazon S3 o Google Cloud Storage. Este m\u00f3dulo se encargar\u00eda de normalizar la lectura, manejar las credenciales de conexi\u00f3n de forma segura y escribir los datos crudos en un formato intermedio eficiente como Parquet o Avro en un Data Lake (por ejemplo, HDFS o un bucket S3). Podr\u00eda configurarse mediante par\u00e1metros como la URL de la base de datos, la tabla a leer, las credenciales, la ruta de S3 o el nombre del bucket. M\u00f3dulo de Transformaci\u00f3n de Calidad y Enriquecimiento (Transform Layer) : Se enfoca en la aplicaci\u00f3n de reglas de negocio y limpieza de datos. Un m\u00f3dulo de transformaci\u00f3n que recibe un DataFrame de Spark. Este m\u00f3dulo podr\u00eda incluir una serie de funciones parametrizables para: Validaci\u00f3n de tipos de datos : Asegurando que las columnas contengan los tipos esperados (ej. precio es num\u00e9rico). Eliminaci\u00f3n o imputaci\u00f3n de valores nulos : Rellenando valores faltantes con promedios, medianas o valores por defecto, o eliminando registros incompletos. Estandarizaci\u00f3n de formatos : Por ejemplo, convertir todas las fechas a un formato ISO 8601 o estandarizar c\u00f3digos postales. Enriquecimiento de datos : Uniendo el conjunto de datos actual con otras fuentes (ej. tablas de referencia de clientes o cat\u00e1logos de productos) para a\u00f1adir informaci\u00f3n relevante. Este m\u00f3dulo recibir\u00eda las reglas de transformaci\u00f3n y las fuentes de datos adicionales como configuraciones, permitiendo su reutilizaci\u00f3n en diferentes contextos. M\u00f3dulo de Carga de Datos (Load Layer) : Responsable de persistir los datos transformados en los destinos finales. Un m\u00f3dulo de carga que puede conectarse a diversos destinos de almacenamiento o bases de datos anal\u00edticas. Podr\u00eda configurarse para: Cargar datos en un data warehouse columnar como Snowflake o Amazon Redshift, optimizando la escritura para el rendimiento de consultas. Escribir datos en un Data Lakehouse (como Delta Lake en Databricks) para permitir futuras consultas con SQL o herramientas de BI. Publicar datos en un servicio de mensajer\u00eda como Apache Kafka para consumo en tiempo real por otras aplicaciones o microservicios. Este m\u00f3dulo ser\u00eda configurable a trav\u00e9s de par\u00e1metros que definan el destino (ej. tipo de base de datos, nombre del cluster, tema de Kafka), el modo de escritura (ej. append , overwrite , upsert ) y las credenciales de autenticaci\u00f3n.","title":"Principios del dise\u00f1o modular"},{"location":"tema31/#desacoplamiento-de-componentes","text":"El desacoplamiento es un principio de dise\u00f1o que busca minimizar las dependencias directas entre los m\u00f3dulos de un pipeline. Su objetivo principal es asegurar que los cambios o fallas en un componente no tengan un impacto en cascada sobre los dem\u00e1s, lo que aumenta la tolerancia a fallos, la flexibilidad y la capacidad de evoluci\u00f3n del sistema . Cuando los componentes est\u00e1n fuertemente acoplados, un cambio en uno de ellos podr\u00eda requerir modificaciones en otros, lo que complica el mantenimiento y ralentiza el desarrollo. El desacoplamiento promueve la independencia operativa y tecnol\u00f3gica. Uso de colas de mensajes para desacoplar operaciones as\u00edncronas : Las colas de mensajes act\u00faan como intermediarios entre los productores y consumidores de datos. Por ejemplo, Apache Kafka es una plataforma de streaming distribuida que permite que un m\u00f3dulo de extracci\u00f3n escriba eventos o registros en un t\u00f3pico de Kafka , mientras que un m\u00f3dulo de transformaci\u00f3n lee de ese t\u00f3pico de forma as\u00edncrona e independiente . Si el m\u00f3dulo de transformaci\u00f3n experimenta una falla temporal, el m\u00f3dulo de extracci\u00f3n puede seguir produciendo datos sin interrupci\u00f3n, ya que los mensajes se acumulan en la cola. Esto mejora la resiliencia del pipeline y permite que los m\u00f3dulos operen a diferentes velocidades. Serializaci\u00f3n y persistencia de datos intermedios : En lugar de pasar datos directamente entre m\u00f3dulos en memoria, se puede serializar y almacenar los resultados intermedios de un m\u00f3dulo en un sistema de almacenamiento distribuido como HDFS (Hadoop Distributed File System) , Amazon S3 o Azure Data Lake Storage . Esto permite que cada m\u00f3dulo \"lea\" los datos que necesita del almacenamiento y \"escriba\" sus resultados de vuelta, rompiendo la dependencia directa. Por ejemplo, el m\u00f3dulo de extracci\u00f3n podr\u00eda escribir datos brutos en S3, y luego el m\u00f3dulo de transformaci\u00f3n leer\u00eda desde S3, procesar\u00eda los datos y escribir\u00eda los resultados transformados en otra ubicaci\u00f3n en S3. Esto no solo desacopla, sino que tambi\u00e9n proporciona puntos de recuperaci\u00f3n en caso de fallas y facilita la auditor\u00eda de las etapas del pipeline. Configuraci\u00f3n din\u00e1mica y externa de pipelines : En lugar de codificar las dependencias y la l\u00f3gica del pipeline directamente en el c\u00f3digo, se utilizan archivos de configuraci\u00f3n externos (como YAML o JSON) para definir la secuencia de ejecuci\u00f3n de los m\u00f3dulos, las fuentes de datos, los destinos y los par\u00e1metros espec\u00edficos. Esto permite modificar el comportamiento de un pipeline sin necesidad de recompilar o redeployar el c\u00f3digo . Por ejemplo, un archivo YAML podr\u00eda especificar que el m\u00f3dulo de extracci\u00f3n ModuloExtraccionDB debe leer de una base de datos db_produccion y escribir en s3://raw-data/ , y que posteriormente el ModuloTransformacionCalidad debe leer de s3://raw-data/ y escribir en s3://cleaned-data/ . Este enfoque promueve la flexibilidad y la agilidad en la gesti\u00f3n de pipelines.","title":"Desacoplamiento de componentes"},{"location":"tema31/#314-introduccion-a-apache-airflow","text":"Apache Airflow es una potente plataforma de c\u00f3digo abierto dise\u00f1ada para la programaci\u00f3n, orquestaci\u00f3n y monitoreo de flujos de trabajo (workflows) . En esencia, permite definir, programar y supervisar secuencias complejas de tareas de manera program\u00e1tica, utilizando un enfoque de DAGs (Directed Acyclic Graphs) . Su capacidad para manejar dependencias, reintentos y la ejecuci\u00f3n distribuida lo convierte en una herramienta fundamental para la automatizaci\u00f3n de procesos de datos, ETL y otras operaciones de TI.","title":"3.1.4 Introducci\u00f3n a Apache Airflow"},{"location":"tema31/#dags-directed-acyclic-graphs","text":"Los DAGs son el coraz\u00f3n de Airflow. Representan un flujo de tareas con dependencias claramente definidas y, como su nombre lo indica, no pueden contener ciclos. Cada nodo en un DAG es una tarea, y las aristas representan las dependencias entre ellas, indicando qu\u00e9 tareas deben completarse antes de que otras puedan comenzar. Definici\u00f3n de un DAG en Python para ejecutar una serie de tareas de limpieza y carga diaria : Un DAG en Airflow se define completamente en c\u00f3digo Python. Esto permite una gran flexibilidad y control de versiones. Por ejemplo, un DAG para un proceso ETL diario podr\u00eda incluir tareas como la extracci\u00f3n de datos de una base de datos, la limpieza y transformaci\u00f3n de esos datos, y finalmente la carga en un data warehouse. from airflow import DAG from airflow.operators.bash import BashOperator from airflow.utils.dates import days_ago with DAG( dag_id='etl_diario', start_date=days_ago(1), schedule_interval='@daily', catchup=False, tags=['etl', 'datos'] ) as dag: # Tarea 1: Extracci\u00f3n de datos extraer_datos = BashOperator( task_id='extraer_datos', bash_command='python /app/scripts/extraer_datos.py' ) # Tarea 2: Limpieza de datos limpiar_datos = BashOperator( task_id='limpiar_datos', bash_command='python /app/scripts/limpiar_datos.py' ) # Tarea 3: Carga de datos cargar_datos = BashOperator( task_id='cargar_datos', bash_command='python /app/scripts/cargar_datos.py' ) extraer_datos >> limpiar_datos >> cargar_datos Representaci\u00f3n visual del flujo de tareas en la interfaz de Airflow : Una de las caracter\u00edsticas m\u00e1s destacadas de Airflow es su interfaz de usuario web intuitiva. Esta interfaz permite visualizar el DAG de forma gr\u00e1fica, mostrando las tareas, sus dependencias y el estado de cada ejecuci\u00f3n. Esto facilita el monitoreo y la depuraci\u00f3n de los flujos de trabajo. Control de versiones y reutilizaci\u00f3n de DAGs en entornos dev/test/prod : Dado que los DAGs son c\u00f3digo, se benefician de las pr\u00e1cticas de control de versiones (Git, por ejemplo). Esto permite un desarrollo colaborativo, seguimiento de cambios y la promoci\u00f3n de DAGs a trav\u00e9s de diferentes entornos (desarrollo, pruebas, producci\u00f3n) de manera controlada y reproducible, asegurando la consistencia del flujo de trabajo en todas las fases.","title":"DAGs (Directed Acyclic Graphs)"},{"location":"tema31/#operadores-sensores-y-tareas","text":"En Airflow, las tareas son las unidades b\u00e1sicas de trabajo que un DAG ejecuta. Estas tareas se definen utilizando operadores y sensores . Operadores : Son clases predefinidas que encapsulan la l\u00f3gica para ejecutar una acci\u00f3n espec\u00edfica. Airflow ofrece una amplia variedad de operadores para interactuar con diferentes sistemas y tecnolog\u00edas. BashOperator : Permite ejecutar comandos de shell. Es \u00fatil para scripts sencillos, mover archivos o cualquier comando que pueda ejecutarse en la terminal. from airflow.operators.bash import BashOperator # ... dentro de un DAG ... ejecutar_script_shell = BashOperator( task_id='ejecutar_script_shell', bash_command='sh /opt/airflow/scripts/mi_script.sh arg1 arg2' ) PythonOperator : Permite ejecutar cualquier funci\u00f3n de Python. Esto es extremadamente flexible para l\u00f3gica de negocio personalizada, transformaciones de datos complejas o integraci\u00f3n con bibliotecas de Python. from airflow.operators.python import PythonOperator import pandas as pd def transformar_datos_py(): # Ejemplo: Cargar, transformar y guardar datos con Pandas df = pd.read_csv('/tmp/raw_data.csv') df['columna_nueva'] = df['columna_existente'] * 2 df.to_csv('/tmp/transformed_data.csv', index=False) print(\"Datos transformados exitosamente con Python.\") # ... dentro de un DAG ... transformacion_python = PythonOperator( task_id='transformacion_python', python_callable=transformar_datos_py ) SparkSubmitOperator : Facilita la ejecuci\u00f3n de jobs de Apache Spark en un cl\u00faster. Es ideal para procesamientos de Big Data distribuidos. from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator # ... dentro de un DAG ... ejecutar_spark_job = SparkSubmitOperator( task_id='ejecutar_spark_job', application='/opt/airflow/dags/spark_jobs/procesar_ventas.py', conn_id='spark_default', # Conexi\u00f3n Spark configurada en Airflow total_executor_cores='2', executor_memory='2g', num_executors='10', application_args=['--input', '/path/to/input', '--output', '/path/to/output'] ) Sensores : Son un tipo especial de operador que espera a que se cumpla una condici\u00f3n externa antes de que una tarea pueda continuar. Esto es crucial para flujos de trabajo que dependen de la disponibilidad de recursos o eventos externos. Implementaci\u00f3n de un sensor que espera archivos nuevos en S3 : Un S3KeySensor puede ser utilizado para pausar un flujo de trabajo hasta que un archivo espec\u00edfico (o un patr\u00f3n de archivos) aparezca en un bucket de S3. from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor # ... dentro de un DAG ... esperar_archivo_s3 = S3KeySensor( task_id='esperar_archivo_s3', bucket_name='mi-bucket-s3', bucket_key='entrada_diaria/{{ ds_nodash }}.csv', # Espera un archivo con la fecha de ejecuci\u00f3n aws_conn_id='aws_default', poke_interval=60, # Chequea cada 60 segundos timeout=60 * 60 * 24 # Timeout de 24 horas ) Combinaci\u00f3n de tareas condicionales con BranchPythonOperator : El BranchPythonOperator permite ejecutar una funci\u00f3n Python que retorna el task_id de la siguiente tarea a ejecutar, creando as\u00ed flujos de trabajo condicionales. Esto es \u00fatil para ramificar la ejecuci\u00f3n bas\u00e1ndose en la l\u00f3gica de negocio o en resultados de tareas previas. from airflow.operators.python import BranchPythonOperator from airflow.operators.dummy import DummyOperator # Utilizado para tareas que no hacen nada from random import random def decidir_ramal(): if random() > 0.5: return 'procesar_datos_a' else: return 'procesar_datos_b' # ... dentro de un DAG ... iniciar_proceso = DummyOperator(task_id='iniciar_proceso') decidir_camino = BranchPythonOperator( task_id='decidir_camino', python_callable=decidir_ramal ) procesar_datos_a = BashOperator( task_id='procesar_datos_a', bash_command='echo \"Procesando datos con la l\u00f3gica A\"' ) procesar_datos_b = BashOperator( task_id='procesar_datos_b', bash_command='echo \"Procesando datos con la l\u00f3gica B\"' ) finalizar_proceso = DummyOperator(task_id='finalizar_proceso', trigger_rule='none_failed_min_one_success') iniciar_proceso >> decidir_camino decidir_camino >> [procesar_datos_a, procesar_datos_b] [procesar_datos_a, procesar_datos_b] >> finalizar_proceso","title":"Operadores, sensores y tareas"},{"location":"tema31/#gestion-de-dependencias-y-programacion","text":"Una gesti\u00f3n eficiente de las dependencias y la programaci\u00f3n es crucial para la estabilidad y el rendimiento de los flujos de trabajo ETL. Airflow proporciona mecanismos robustos para esto. Programar un DAG para que se ejecute cada hora y recupere los datos m\u00e1s recientes : Airflow utiliza el par\u00e1metro schedule_interval para definir la frecuencia de ejecuci\u00f3n de un DAG. Este puede ser una expresi\u00f3n cron, un timedelta , o una cadena predefinida como @hourly , @daily , etc. Para recuperar los datos m\u00e1s recientes, las tareas dentro del DAG pueden hacer uso de las variables de contexto de Airflow, como {{ ds }} (fecha de ejecuci\u00f3n) o {{ data_interval_start }} y {{ data_interval_end }} (rango de tiempo de los datos a procesar). from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='ingesta_horaria', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(hours=1), # Ejecuci\u00f3n cada hora catchup=False, tags=['ingesta'] ) as dag: ingestar_datos = BashOperator( task_id='ingestar_datos', # Usando macros para el rango de tiempo de ejecuci\u00f3n bash_command='python /app/scripts/ingestar_datos.py --start_time {{ data_interval_start }} --end_time {{ data_interval_end }}' ) Gesti\u00f3n de tareas paralelas con depends_on_past=False para mejorar el rendimiento : Por defecto, las tareas de un DAG pueden tener una dependencia impl\u00edcita de las ejecuciones previas exitosas. Sin embargo, en muchos escenarios, especialmente con grandes vol\u00famenes de datos, es deseable que las tareas se ejecuten de forma paralela sin esperar la finalizaci\u00f3n de ejecuciones anteriores del mismo DAG. El par\u00e1metro depends_on_past=False a nivel de DAG o tarea permite que una tarea se ejecute incluso si su instancia anterior (de una ejecuci\u00f3n previa del DAG) fall\u00f3 o a\u00fan no ha terminado, mejorando el rendimiento al permitir el paralelismo en el tiempo. Implementaci\u00f3n de backfill para recuperar ejecuciones pasadas no procesadas : Cuando un DAG se define por primera vez o ha estado inactivo y se necesita procesar datos de un per\u00edodo anterior, la funci\u00f3n backfill de Airflow es invaluable. Permite ejecutar el DAG para un rango de fechas espec\u00edfico en el pasado, como si el DAG hubiera estado activo y programado durante ese tiempo. Esto es fundamental para cargar datos hist\u00f3ricos o para recuperarse de interrupciones prolongadas en el procesamiento. Se puede ejecutar mediante la l\u00ednea de comandos de Airflow: airflow dags backfill -s 2024-01-01 -e 2024-01-31 mi_dag_id Esto ejecutar\u00eda mi_dag_id para cada schedule_interval entre el 1 y el 31 de enero de 2024.","title":"Gesti\u00f3n de dependencias y programaci\u00f3n"},{"location":"tema31/#315-orquestacion-de-tareas-distribuidas","text":"La orquestaci\u00f3n de tareas distribuidas es el proceso de gestionar y coordinar la ejecuci\u00f3n de m\u00faltiples procesos de datos que operan en sistemas distribuidos. Esto implica no solo controlar cu\u00e1ndo y c\u00f3mo se ejecutan estas tareas, sino tambi\u00e9n asegurar que se sincronicen correctamente y que el sistema sea capaz de recuperarse de fallos inesperados. En el contexto de Big Data y ETL, esto es fundamental para manejar vol\u00famenes masivos de informaci\u00f3n de manera eficiente y confiable.","title":"3.1.5 Orquestaci\u00f3n de tareas distribuidas"},{"location":"tema31/#coordinacion-temporal-de-tareas","text":"La coordinaci\u00f3n temporal de tareas se refiere a la capacidad de definir y hacer cumplir el orden y el momento de ejecuci\u00f3n de las tareas. Las tareas en un flujo de trabajo de datos a menudo tienen interdependencias, lo que significa que una tarea solo puede comenzar una vez que otra ha finalizado o ha producido ciertos resultados. Los sistemas de orquestaci\u00f3n permiten establecer horarios fijos (por ejemplo, ejecutar un ETL todas las noches a medianoche), condiciones de ejecuci\u00f3n (como iniciar una transformaci\u00f3n solo si se ha recibido un archivo espec\u00edfico) o secuencias dependientes (ejecutar el pre-procesamiento de datos antes de la carga en un data warehouse ). Esta coordinaci\u00f3n asegura que los datos se procesen en el orden correcto y que se cumplan los acuerdos de nivel de servicio (SLA). Ejemplos : Pipeline diario que inicia a las 3am y espera la finalizaci\u00f3n de cargas previas Este pipeline implementa un ETL nocturno que procesa datos transaccionales del d\u00eda anterior. Utiliza schedule_interval='0 3 * * *' para ejecutarse diariamente a las 3:00 AM. La orquestaci\u00f3n incluye sensores que verifican la disponibilidad de archivos fuente, tareas de extracci\u00f3n desde m\u00faltiples sistemas (CRM, ERP, logs web), transformaciones de limpieza y normalizaci\u00f3n de datos, y finalmente la carga hacia el data warehouse. Las dependencias aseguran que cada etapa complete exitosamente antes de proceder, con mecanismos de retry autom\u00e1tico y alertas por email en caso de fallos. El pipeline tambi\u00e9n incluye checkpoints de validaci\u00f3n que verifican la integridad de los datos antes de cada transformaci\u00f3n mayor. Pipeline semanal que consolida los datos de los \u00faltimos siete d\u00edas Este proceso se ejecuta cada lunes a las 6:00 AM usando schedule_interval='0 6 * * 1' para generar reportes ejecutivos y m\u00e9tricas de negocio. La orquestaci\u00f3n coordina la agregaci\u00f3n de datos desde m\u00faltiples fuentes diarias ya procesadas, calculando KPIs como retention de usuarios, revenue por segmento, y m\u00e9tricas de calidad de servicio. Incluye tareas paralelas que procesan diferentes dimensiones de an\u00e1lisis (geogr\u00e1fico, demogr\u00e1fico, temporal), seguidas por una tarea de consolidaci\u00f3n final que genera dashboards actualizados y env\u00eda reportes automatizados a stakeholders. El pipeline implementa branching logic para manejar semanas con d\u00edas festivos o datos faltantes. Tarea condicional que solo se ejecute si la calidad de los datos supera cierto umbral Esta implementaci\u00f3n utiliza BranchPythonOperator para evaluar m\u00e9tricas de calidad como completitud, consistencia y validez de los datos. La funci\u00f3n de decisi\u00f3n consulta reglas de negocio predefinidas (por ejemplo, <5% de valores nulos, fechas dentro de rangos esperados, concordancia entre sistemas). Si los datos pasan las validaciones, se ejecuta el flujo principal de procesamiento; si no, se activa un flujo alternativo que registra los problemas, notifica al equipo de datos, y potencialmente ejecuta rutinas de limpieza autom\u00e1tica. La orquestaci\u00f3n incluye tareas downstream que solo se ejecutan tras confirmaci\u00f3n manual o autom\u00e1tica de que los datos corregidos cumplen los est\u00e1ndares de calidad. Pipeline de ML con reentrenamiento condicional basado en drift de datos Este pipeline monitorea continuamente la performance de modelos de machine learning en producci\u00f3n. Se ejecuta cada 4 horas usando sensores que eval\u00faan m\u00e9tricas como accuracy, precision y recall contra umbrales predefinidos. Cuando detecta degradaci\u00f3n del modelo (data drift o concept drift), automaticamente activa un flujo de reentrenamiento que incluye feature engineering actualizado, validaci\u00f3n cruzada, y despliegue A/B testing del nuevo modelo. La orquestaci\u00f3n coordina tareas paralelas para entrenar m\u00faltiples algoritmos, seleccionar el mejor performante, y actualizar gradualmente el modelo en producci\u00f3n solo despu\u00e9s de validaciones exhaustivas en ambiente staging. Pipeline de procesamiento de eventos en tiempo real con ventanas deslizantes Este flujo procesa streams de datos de IoT o clickstream usando ventanas temporales de 15 minutos que se solapan cada 5 minutos. La orquestaci\u00f3n maneja la complejidad de procesar datos que llegan con diferentes latencias, implementando buffers y mecanismos de ordenamiento temporal. Las tareas incluyen detecci\u00f3n de anomal\u00edas en tiempo real, c\u00e1lculo de m\u00e9tricas agregadas (promedios m\u00f3viles, percentiles), y triggers autom\u00e1ticos para alertas cuando se detectan patrones cr\u00edticos. El pipeline coordina la persistencia de resultados en sistemas OLTP para consultas r\u00e1pidas y OLAP para an\u00e1lisis hist\u00f3rico, manejando backpressure y failover autom\u00e1tico entre clusters de procesamiento. Pipeline de compliance y auditor\u00eda con retenci\u00f3n de datos personalizada Este sistema implementa pol\u00edticas de governanza de datos automatizadas que se ejecutan mensualmente para cumplir regulaciones como GDPR o HIPAA. La orquestaci\u00f3n coordina tareas que identifican datos sensibles usando classificaci\u00f3n autom\u00e1tica, aplican t\u00e9cnicas de anonimizaci\u00f3n o pseudonimizaci\u00f3n seg\u00fan pol\u00edticas definidas, y gestionan la retenci\u00f3n/eliminaci\u00f3n de datos basada en reglas legales espec\u00edficas por jurisdicci\u00f3n. Incluye generaci\u00f3n autom\u00e1tica de reportes de compliance, logs de auditor\u00eda inmutables, y workflows de aprobaci\u00f3n que requieren intervenci\u00f3n humana para operaciones cr\u00edticas como eliminaci\u00f3n masiva de datos personales o cambios en pol\u00edticas de retenci\u00f3n.","title":"Coordinaci\u00f3n temporal de tareas"},{"location":"tema31/#distribucion-de-cargas-y-escalabilidad","text":"La distribuci\u00f3n de cargas y escalabilidad es una de las mayores ventajas de la orquestaci\u00f3n en entornos distribuidos. Al dise\u00f1ar los flujos de trabajo para que se ejecuten en paralelo o en nodos separados, es posible procesar grandes vol\u00famenes de datos de manera mucho m\u00e1s r\u00e1pida y eficiente. Esto permite que el sistema escale horizontalmente , lo que significa que se pueden a\u00f1adir m\u00e1s recursos (nodos o m\u00e1quinas) para manejar un aumento en la demanda de procesamiento sin afectar el rendimiento. Los orquestadores facilitan la asignaci\u00f3n din\u00e1mica de recursos y la gesti\u00f3n de la concurrencia. Ejemplos : Ejecuci\u00f3n paralela de tareas de carga para m\u00faltiples regiones geogr\u00e1ficas Imagina una empresa global que recopila datos de ventas de sus operaciones en Norteam\u00e9rica, Europa y Asia. Un sistema de orquestaci\u00f3n podr\u00eda lanzar simult\u00e1neamente tres tareas de carga de datos, una para cada regi\u00f3n, cada una procesando los datos de su respectiva fuente en paralelo. Esto reduce significativamente el tiempo total necesario para cargar todos los datos, en comparaci\u00f3n con un enfoque secuencial. Distribuci\u00f3n de la transformaci\u00f3n por particiones de tiempo usando Spark Considera un pipeline ETL que procesa grandes vol\u00famenes de datos transaccionales diarios. Usando Apache Spark, un orquestador podr\u00eda dividir la transformaci\u00f3n de los datos en particiones de tiempo . Por ejemplo, los datos de la primera mitad del d\u00eda se procesan en un conjunto de workers de Spark, mientras que los datos de la segunda mitad se procesan simult\u00e1neamente en otro conjunto. Esto maximiza la utilizaci\u00f3n de recursos y acelera la finalizaci\u00f3n de la transformaci\u00f3n. Escalado autom\u00e1tico de workers de Airflow seg\u00fan la demanda de procesamiento Si un flujo de trabajo de datos tiene picos de demanda (por ejemplo, a fin de mes cuando se generan informes), un orquestador como Apache Airflow, integrado con plataformas de nube (AWS, GCP, Azure), puede configurarse para escalar autom\u00e1ticamente el n\u00famero de workers disponibles. Durante los per\u00edodos de alta carga, se aprovisionan m\u00e1s workers para manejar las tareas concurrentes, y una vez que la demanda disminuye, los workers adicionales se desaprovisionan para optimizar costos.","title":"Distribuci\u00f3n de cargas y escalabilidad"},{"location":"tema31/#tarea","text":"Dise\u00f1a un pipeline ETL en pseudoc\u00f3digo que extraiga datos desde una base de datos PostgreSQL, los transforme en Spark y los cargue en un bucket S3 particionado por fecha. Compara las ventajas y desventajas de ETL y ELT en el contexto de una empresa que utiliza Snowflake como data warehouse. Define un DAG de Airflow simple que incluya tres tareas secuenciales: lectura de archivo, transformaci\u00f3n y carga en base de datos. Identifica tres puntos donde aplicar\u00edas modularidad en un pipeline ETL y justifica su beneficio. Escribe una estrategia de orquestaci\u00f3n para un pipeline que debe ejecutarse cada 15 minutos y detectar datos faltantes si alguna ejecuci\u00f3n falla.","title":"Tarea"},{"location":"tema32/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.2. Conexi\u00f3n a M\u00faltiples Fuentes de Datos Objetivo : Comprender c\u00f3mo integrar m\u00faltiples tipos de fuentes de datos \u2014estructuradas, semiestructuradas y no estructuradas\u2014 en un pipeline ETL, utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye el uso de conectores apropiados, estrategias de autenticaci\u00f3n segura, y t\u00e9cnicas de lectura eficientes. Introducci\u00f3n : En entornos Big Data, los datos se originan desde diversas fuentes con distintos formatos, velocidades y estructuras. La capacidad de extraer datos de m\u00faltiples or\u00edgenes y consolidarlos de manera eficiente en un pipeline ETL es clave para cualquier sistema anal\u00edtico moderno. Este tema explora las fuentes de datos m\u00e1s comunes y c\u00f3mo integrarlas usando tecnolog\u00edas como Spark y Airflow. Desarrollo : La integraci\u00f3n de m\u00faltiples fuentes en un pipeline ETL implica tanto desaf\u00edos t\u00e9cnicos como de gobernanza. Cada tipo de fuente \u2014bases de datos, archivos, APIs\u2014 requiere t\u00e9cnicas y conectores espec\u00edficos. Adem\u00e1s, aspectos como la autenticaci\u00f3n segura, la lectura incremental y el manejo de configuraciones son fundamentales para construir flujos de datos robustos y escalables. Spark y Airflow ofrecen mecanismos potentes para esta integraci\u00f3n, facilitando el desarrollo de pipelines mantenibles y eficientes. 3.2.1 Tipos de fuentes comunes La diversidad de fuentes de datos implica conocer sus caracter\u00edsticas y particularidades para una integraci\u00f3n adecuada en el pipeline. Cada tipo de fuente presenta desaf\u00edos \u00fanicos en t\u00e9rminos de conectividad, formato de datos, escalabilidad y optimizaci\u00f3n de rendimiento. Bases de datos relacionales (PostgreSQL, MySQL) Las bases de datos relacionales constituyen el backbone de muchas organizaciones, almacenando datos estructurados con esquemas bien definidos y relaciones entre tablas. Estas fuentes requieren conexi\u00f3n v\u00eda JDBC (Java Database Connectivity) y ofrecen la ventaja de poder ejecutar consultas SQL complejas directamente en la fuente, reduciendo la transferencia de datos innecesarios. Para una integraci\u00f3n efectiva, es crucial considerar aspectos como la partici\u00f3n de datos, el control de paralelismo para evitar sobrecargar la base de datos origen, y la gesti\u00f3n de transacciones. Spark permite configurar par\u00e1metros como numPartitions , lowerBound , upperBound y partitionColumn para optimizar la lectura distribuida. Usar JDBC en Spark para leer de PostgreSQL y transformar los resultados : from pyspark.sql import SparkSession # Configuraci\u00f3n de Spark con driver JDBC spark = SparkSession.builder \\ .appName(\"PostgreSQL_ETL\") \\ .config(\"spark.jars\", \"/path/to/postgresql-42.6.0.jar\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n jdbc_url = \"jdbc:postgresql://localhost:5432/production_db\" connection_properties = { \"user\": \"spark_user\", \"password\": \"secure_password\", \"driver\": \"org.postgresql.Driver\", \"numPartitions\": \"4\", \"partitionColumn\": \"id\", \"lowerBound\": \"1\", \"upperBound\": \"1000000\" } # Lectura optimizada con particionamiento df = spark.read \\ .jdbc(url=jdbc_url, table=\"(SELECT * FROM sales WHERE created_date >= '2024-01-01') as sales_subset\", properties=connection_properties) # Transformaciones sobre los datos df_transformed = df \\ .filter(df.amount > 100) \\ .groupBy(\"customer_id\", \"product_category\") \\ .agg( sum(\"amount\").alias(\"total_sales\"), count(\"order_id\").alias(\"order_count\"), avg(\"amount\").alias(\"avg_order_value\") ) # Escritura a formato optimizado df_transformed.write \\ .mode(\"overwrite\") \\ .parquet(\"s3://data-lake/processed/sales_summary/\") Ejecutar consultas filtradas desde Airflow para limitar la carga de datos : from airflow import DAG from airflow.providers.postgres.operators.postgres import PostgresOperator from airflow.providers.spark.operators.spark_submit import SparkSubmitOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from datetime import datetime, timedelta default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'postgresql_incremental_etl', default_args=default_args, description='ETL incremental desde PostgreSQL', schedule_interval='@daily', catchup=False ) # Operador para extraer datos incrementales extract_incremental_data = PostgresOperator( task_id='extract_incremental_data', postgres_conn_id='postgres_production', sql=\"\"\" CREATE TEMP TABLE temp_incremental_data AS SELECT customer_id, order_date, product_id, quantity, unit_price, total_amount FROM orders WHERE order_date >= '{{ ds }}' AND order_date < '{{ next_ds }}' AND status = 'completed'; COPY temp_incremental_data TO '/tmp/incremental_data_{{ ds }}.csv' WITH CSV HEADER; \"\"\", dag=dag ) # Funci\u00f3n personalizada para transferir datos def transfer_to_spark(**context): postgres_hook = PostgresHook(postgres_conn_id='postgres_production') # Consulta optimizada con l\u00edmites sql_query = f\"\"\" SELECT * FROM orders WHERE order_date = '{context['ds']}' AND total_amount > 0 ORDER BY order_id LIMIT 100000 \"\"\" # Exportar a formato intermedio df = postgres_hook.get_pandas_df(sql_query) df.to_parquet(f\"/tmp/orders_{context['ds']}.parquet\") return f\"/tmp/orders_{context['ds']}.parquet\" transfer_task = PythonOperator( task_id='transfer_to_staging', python_callable=transfer_to_spark, dag=dag ) extract_incremental_data >> transfer_task Almacenes NoSQL (MongoDB, Cassandra) Los almacenes NoSQL ofrecen flexibilidad de esquema y est\u00e1n dise\u00f1ados para manejar grandes vol\u00famenes de datos semiestructurados con alta disponibilidad y escalabilidad horizontal. MongoDB es ideal para documentos JSON complejos, mientras que Cassandra sobresale en casos de uso que requieren escrituras masivas y alta disponibilidad. La integraci\u00f3n con estos sistemas requiere conectores espec\u00edficos y consideraciones especiales para el particionamiento y la distribuci\u00f3n de carga. Es importante entender los patrones de acceso a datos y las limitaciones de cada sistema para optimizar las consultas. Ingestar documentos de MongoDB con Spark para an\u00e1lisis de logs : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n de Spark con conector MongoDB spark = SparkSession.builder \\ .appName(\"MongoDB_Log_Analysis\") \\ .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/logs.application_logs\") \\ .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/analytics.processed_logs\") \\ .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\ .getOrCreate() # Lectura de documentos con filtros a nivel de MongoDB df_logs = spark.read \\ .format(\"mongo\") \\ .option(\"collection\", \"application_logs\") \\ .option(\"pipeline\", '[{\"$match\": {\"timestamp\": {\"$gte\": {\"$date\": \"2024-01-01T00:00:00Z\"}}}}]') \\ .load() # An\u00e1lisis de logs con transformaciones complejas df_analyzed = df_logs \\ .withColumn(\"hour\", hour(col(\"timestamp\"))) \\ .withColumn(\"log_level\", upper(col(\"level\"))) \\ .withColumn(\"response_time_ms\", col(\"response_time\").cast(\"integer\")) \\ .filter(col(\"response_time_ms\").isNotNull()) \\ .groupBy(\"hour\", \"log_level\", \"service_name\") \\ .agg( count(\"*\").alias(\"log_count\"), avg(\"response_time_ms\").alias(\"avg_response_time\"), max(\"response_time_ms\").alias(\"max_response_time\"), countDistinct(\"user_id\").alias(\"unique_users\") ) # Detecci\u00f3n de anomal\u00edas df_anomalies = df_analyzed \\ .filter( (col(\"avg_response_time\") > 5000) | (col(\"log_level\") == \"ERROR\") | (col(\"log_count\") > 10000) ) # Escritura de resultados procesados df_analyzed.write \\ .format(\"mongo\") \\ .option(\"collection\", \"hourly_metrics\") \\ .mode(\"overwrite\") \\ .save() df_anomalies.write \\ .format(\"mongo\") \\ .option(\"collection\", \"detected_anomalies\") \\ .mode(\"append\") \\ .save() Usar Cassandra para almacenar datos de sensores IoT y analizarlos por lotes : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n para Cassandra spark = SparkSession.builder \\ .appName(\"IoT_Sensor_Analytics\") \\ .config(\"spark.cassandra.connection.host\", \"cassandra-cluster.example.com\") \\ .config(\"spark.cassandra.connection.port\", \"9042\") \\ .config(\"spark.cassandra.auth.username\", \"spark_user\") \\ .config(\"spark.cassandra.auth.password\", \"secure_password\") \\ .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\ .getOrCreate() # Lectura de datos de sensores con filtros optimizados df_sensor_data = spark.read \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"sensor_readings\", keyspace=\"iot_data\") \\ .option(\"spark.cassandra.input.split.size_in_mb\", \"512\") \\ .load() \\ .filter(col(\"timestamp\") >= lit(\"2024-01-01\")) \\ .filter(col(\"sensor_type\").isin([\"temperature\", \"humidity\", \"pressure\"])) # An\u00e1lisis de tendencias y agregaciones df_hourly_aggregates = df_sensor_data \\ .withColumn(\"hour_bucket\", date_trunc(\"hour\", col(\"timestamp\"))) \\ .groupBy(\"device_id\", \"sensor_type\", \"location\", \"hour_bucket\") \\ .agg( avg(\"value\").alias(\"avg_value\"), min(\"value\").alias(\"min_value\"), max(\"value\").alias(\"max_value\"), stddev(\"value\").alias(\"stddev_value\"), count(\"*\").alias(\"reading_count\") ) # Detecci\u00f3n de valores an\u00f3malos usando ventanas from pyspark.sql.window import Window window_spec = Window.partitionBy(\"device_id\", \"sensor_type\").orderBy(\"hour_bucket\") df_with_trends = df_hourly_aggregates \\ .withColumn(\"prev_avg\", lag(\"avg_value\").over(window_spec)) \\ .withColumn(\"value_change\", col(\"avg_value\") - col(\"prev_avg\")) \\ .withColumn(\"is_anomaly\", when(abs(col(\"value_change\")) > 3 * col(\"stddev_value\"), True) .otherwise(False)) # Escritura optimizada a Cassandra con TTL df_hourly_aggregates.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"hourly_sensor_metrics\", keyspace=\"iot_analytics\") \\ .option(\"spark.cassandra.output.ttl\", \"2592000\") \\ .mode(\"append\") \\ .save() # Almacenar anomal\u00edas para alertas df_anomalies = df_with_trends.filter(col(\"is_anomaly\") == True) df_anomalies.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"sensor_anomalies\", keyspace=\"iot_alerts\") \\ .mode(\"append\") \\ .save() Archivos planos (CSV, JSON, Parquet) Los archivos planos representan una forma com\u00fan de almacenar y intercambiar datos, especialmente en arquitecturas de data lake. Cada formato tiene sus ventajas: CSV para simplicidad e interoperabilidad, JSON para datos semiestructurados, y Parquet para optimizaci\u00f3n de almacenamiento y consultas anal\u00edticas. Spark proporciona APIs nativas optimizadas para estos formatos, con capacidades avanzadas como lectura schema-on-read, particionamiento autom\u00e1tico, y optimizaciones de predicados pushdown en el caso de Parquet. Leer archivos CSV con Spark y aplicar transformaciones : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder \\ .appName(\"CSV_Processing_Pipeline\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Definici\u00f3n de schema para optimizar la lectura sales_schema = StructType([ StructField(\"transaction_id\", StringType(), False), StructField(\"customer_id\", IntegerType(), True), StructField(\"product_id\", StringType(), True), StructField(\"quantity\", IntegerType(), True), StructField(\"unit_price\", DecimalType(10,2), True), StructField(\"discount\", DecimalType(5,2), True), StructField(\"transaction_date\", DateType(), True), StructField(\"store_location\", StringType(), True) ]) # Lectura optimizada de m\u00faltiples archivos CSV df_sales = spark.read \\ .option(\"header\", \"true\") \\ .option(\"inferSchema\", \"false\") \\ .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\ .option(\"multiline\", \"true\") \\ .option(\"escape\", '\"') \\ .schema(sales_schema) \\ .csv(\"s3://data-lake/raw/sales/2024/*/sales_*.csv\") # Limpieza y validaci\u00f3n de datos df_cleaned = df_sales \\ .filter(col(\"quantity\") > 0) \\ .filter(col(\"unit_price\") > 0) \\ .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\") * (1 - col(\"discount\")/100)) \\ .withColumn(\"year_month\", date_format(col(\"transaction_date\"), \"yyyy-MM\")) \\ .withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\"))) \\ .filter(col(\"total_amount\").isNotNull()) # Agregaciones para an\u00e1lisis de ventas df_sales_summary = df_cleaned \\ .groupBy(\"year_month\", \"store_location\", \"day_of_week\") \\ .agg( sum(\"total_amount\").alias(\"total_revenue\"), sum(\"quantity\").alias(\"total_units_sold\"), countDistinct(\"customer_id\").alias(\"unique_customers\"), avg(\"total_amount\").alias(\"avg_transaction_value\"), count(\"transaction_id\").alias(\"transaction_count\") ) \\ .withColumn(\"revenue_per_customer\", col(\"total_revenue\") / col(\"unique_customers\")) # Escritura particionada para optimizar consultas futuras df_sales_summary.write \\ .partitionBy(\"year_month\") \\ .mode(\"overwrite\") \\ .option(\"compression\", \"snappy\") \\ .parquet(\"s3://data-lake/processed/sales_summary/\") # Cache para reutilizaci\u00f3n en m\u00faltiples an\u00e1lisis df_cleaned.cache() # An\u00e1lisis adicional: productos top por ubicaci\u00f3n df_top_products = df_cleaned \\ .groupBy(\"store_location\", \"product_id\") \\ .agg(sum(\"quantity\").alias(\"total_sold\")) \\ .withColumn(\"rank\", row_number().over( Window.partitionBy(\"store_location\") .orderBy(desc(\"total_sold\")) )) \\ .filter(col(\"rank\") <= 10) df_top_products.write \\ .mode(\"overwrite\") \\ .json(\"s3://data-lake/analytics/top_products_by_location/\") Ingerir JSON de logs web para an\u00e1lisis de tr\u00e1fico : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder \\ .appName(\"Web_Logs_Traffic_Analysis\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .getOrCreate() # Lectura de logs JSON con schema flexible df_logs = spark.read \\ .option(\"multiline\", \"true\") \\ .option(\"mode\", \"PERMISSIVE\") \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .json(\"s3://logs-bucket/web-logs/2024/*/*/*.json\") # Extracci\u00f3n y procesamiento de campos anidados df_processed = df_logs \\ .withColumn(\"timestamp\", to_timestamp(col(\"@timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \\ .withColumn(\"ip_address\", col(\"clientip\")) \\ .withColumn(\"user_agent\", col(\"agent\")) \\ .withColumn(\"request_method\", regexp_extract(col(\"request\"), r'^(\\w+)', 1)) \\ .withColumn(\"request_url\", regexp_extract(col(\"request\"), r'^\\w+\\s+([^\\s]+)', 1)) \\ .withColumn(\"response_code\", col(\"response\").cast(\"integer\")) \\ .withColumn(\"response_size\", col(\"bytes\").cast(\"long\")) \\ .withColumn(\"referrer\", col(\"referrer\")) \\ .withColumn(\"hour\", hour(col(\"timestamp\"))) \\ .withColumn(\"date\", to_date(col(\"timestamp\"))) \\ .filter(col(\"_corrupt_record\").isNull()) \\ .drop(\"_corrupt_record\") # An\u00e1lisis de tr\u00e1fico web df_traffic_analysis = df_processed \\ .groupBy(\"date\", \"hour\", \"request_method\") \\ .agg( count(\"*\").alias(\"request_count\"), countDistinct(\"ip_address\").alias(\"unique_visitors\"), avg(\"response_size\").alias(\"avg_response_size\"), sum(\"response_size\").alias(\"total_bytes_served\"), sum(when(col(\"response_code\") >= 400, 1).otherwise(0)).alias(\"error_count\"), sum(when(col(\"response_code\") == 200, 1).otherwise(0)).alias(\"success_count\") ) \\ .withColumn(\"error_rate\", col(\"error_count\") / col(\"request_count\") * 100) \\ .withColumn(\"success_rate\", col(\"success_count\") / col(\"request_count\") * 100) # An\u00e1lisis de p\u00e1ginas m\u00e1s visitadas df_popular_pages = df_processed \\ .filter(col(\"request_method\") == \"GET\") \\ .filter(col(\"response_code\") == 200) \\ .groupBy(\"date\", \"request_url\") \\ .agg( count(\"*\").alias(\"page_views\"), countDistinct(\"ip_address\").alias(\"unique_visitors\") ) \\ .withColumn(\"rank\", row_number().over( Window.partitionBy(\"date\") .orderBy(desc(\"page_views\")) )) # Detecci\u00f3n de posibles ataques o comportamientos an\u00f3malos df_security_analysis = df_processed \\ .groupBy(\"ip_address\", \"date\") \\ .agg( count(\"*\").alias(\"requests_per_day\"), countDistinct(\"request_url\").alias(\"unique_pages_accessed\"), sum(when(col(\"response_code\") >= 400, 1).otherwise(0)).alias(\"failed_requests\") ) \\ .withColumn(\"suspicious_activity\", when( (col(\"requests_per_day\") > 10000) | (col(\"failed_requests\") > 100) | (col(\"unique_pages_accessed\") < 2), True ).otherwise(False)) # Escritura de resultados en diferentes formatos df_traffic_analysis.write \\ .partitionBy(\"date\") \\ .mode(\"overwrite\") \\ .parquet(\"s3://analytics-bucket/web-traffic-analysis/\") df_popular_pages.filter(col(\"rank\") <= 50).write \\ .partitionBy(\"date\") \\ .mode(\"overwrite\") \\ .json(\"s3://analytics-bucket/popular-pages/\") df_security_analysis.filter(col(\"suspicious_activity\") == True).write \\ .mode(\"append\") \\ .option(\"compression\", \"gzip\") \\ .csv(\"s3://security-bucket/suspicious-activity/\", header=True) APIs y servicios web Las APIs y servicios web permiten acceder a datos en tiempo real o expuestos por terceros, proporcionando informaci\u00f3n din\u00e1mica y actualizada. La integraci\u00f3n requiere manejo de autenticaci\u00f3n, rate limiting, paginaci\u00f3n, y gesti\u00f3n de errores. Airflow es ideal para orquestar estas integraciones mediante HttpHook, mientras que Spark puede procesar los datos obtenidos. Obtener tasas de cambio de divisas desde una API REST : from airflow import DAG from airflow.providers.http.operators.http import SimpleHttpOperator from airflow.providers.http.hooks.http import HttpHook from airflow.operators.python import PythonOperator from airflow.providers.postgres.operators.postgres import PostgresOperator from datetime import datetime, timedelta import json import pandas as pd # Configuraci\u00f3n del DAG default_args = { 'owner': 'finance-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'currency_exchange_etl', default_args=default_args, description='ETL para tasas de cambio de divisas', schedule_interval=timedelta(hours=1), catchup=False ) # Funci\u00f3n para extraer datos de API con manejo de errores def extract_exchange_rates(**context): http_hook = HttpHook(http_conn_id='exchange_api', method='GET') # Configuraci\u00f3n de la petici\u00f3n endpoint = 'v1/latest' headers = { 'Authorization': 'Bearer {{ var.value.exchange_api_key }}', 'Content-Type': 'application/json' } params = { 'base': 'USD', 'symbols': 'EUR,GBP,JPY,CAD,AUD,CHF,CNY,MXN,BRL' } try: # Realizar petici\u00f3n HTTP response = http_hook.run( endpoint=endpoint, headers=headers, data=params ) exchange_data = json.loads(response.content) # Validar respuesta if 'rates' not in exchange_data: raise ValueError(\"API response missing 'rates' field\") # Transformar datos para almacenamiento rates_list = [] base_currency = exchange_data.get('base', 'USD') timestamp = exchange_data.get('timestamp', context['ts']) for currency, rate in exchange_data['rates'].items(): rates_list.append({ 'base_currency': base_currency, 'target_currency': currency, 'exchange_rate': float(rate), 'timestamp': timestamp, 'source': 'exchangerates-api', 'extraction_date': context['ds'] }) # Guardar en archivo temporal para siguiente tarea df = pd.DataFrame(rates_list) temp_file = f\"/tmp/exchange_rates_{context['ds']}.json\" df.to_json(temp_file, orient='records', date_format='iso') return temp_file except Exception as e: print(f\"Error extracting exchange rates: {str(e)}\") raise # Funci\u00f3n para procesar y validar datos def process_exchange_data(**context): temp_file = context['task_instance'].xcom_pull(task_ids='extract_rates') if not temp_file: raise ValueError(\"No data file received from extraction task\") # Cargar y procesar datos df = pd.read_json(temp_file) # Validaciones de calidad de datos df = df.dropna(subset=['exchange_rate']) df = df[df['exchange_rate'] > 0] # Eliminar tasas negativas o cero # Detecci\u00f3n de valores an\u00f3malos (cambios > 10% desde \u00faltima lectura) postgres_hook = PostgresHook(postgres_conn_id='postgres_warehouse') for currency in df['target_currency'].unique(): last_rate_query = f\"\"\" SELECT exchange_rate FROM exchange_rates WHERE target_currency = '{currency}' AND base_currency = 'USD' ORDER BY timestamp DESC LIMIT 1 \"\"\" last_rate = postgres_hook.get_first(last_rate_query) if last_rate: current_rate = df[df['target_currency'] == currency]['exchange_rate'].iloc[0] change_pct = abs((current_rate - last_rate[0]) / last_rate[0] * 100) if change_pct > 10: print(f\"ALERT: {currency} rate changed by {change_pct:.2f}%\") # Guardar datos procesados processed_file = f\"/tmp/processed_rates_{context['ds']}.csv\" df.to_csv(processed_file, index=False) return processed_file # Tareas del DAG extract_rates = PythonOperator( task_id='extract_rates', python_callable=extract_exchange_rates, dag=dag ) process_rates = PythonOperator( task_id='process_rates', python_callable=process_exchange_data, dag=dag ) # Cargar datos a PostgreSQL load_to_db = PostgresOperator( task_id='load_to_database', postgres_conn_id='postgres_warehouse', sql=\"\"\" INSERT INTO exchange_rates (base_currency, target_currency, exchange_rate, timestamp, source, extraction_date) SELECT base_currency, target_currency, exchange_rate, TO_TIMESTAMP(timestamp), source, TO_DATE(extraction_date, 'YYYY-MM-DD') FROM temp_exchange_staging ON CONFLICT (base_currency, target_currency, DATE(timestamp)) DO UPDATE SET exchange_rate = EXCLUDED.exchange_rate, timestamp = EXCLUDED.timestamp; \"\"\", dag=dag ) # Definir dependencias extract_rates >> process_rates >> load_to_db Ingerir datos meteorol\u00f3gicos en tiempo real para an\u00e1lisis predictivo : from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.providers.spark.operators.spark_submit import SparkSubmitOperator from datetime import datetime, timedelta import requests import json import boto3 default_args = { 'owner': 'weather-analytics', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } dag = DAG( 'weather_data_pipeline', default_args=default_args, description='Pipeline de datos meteorol\u00f3gicos en tiempo real', schedule_interval=timedelta(minutes=15), catchup=False ) def fetch_weather_data(**context): \"\"\"Extrae datos meteorol\u00f3gicos de m\u00faltiples APIs\"\"\" # Lista de ciudades para monitorear cities = [ {'name': 'New York', 'lat': 40.7128, 'lon': -74.0060}, {'name': 'London', 'lat': 51.5074, 'lon': -0.1278}, {'name': 'Tokyo', 'lat': 35.6762, 'lon': 139.6503}, {'name': 'Sydney', 'lat': -33.8688, 'lon': 151.2093} ] weather_data = [] api_key = \"{{ var.value.openweather_api_key }}\" for city in cities: try: # Datos meteorol\u00f3gicos actuales current_url = f\"https://api.openweathermap.org/data/2.5/weather\" current_params = { 'lat': city['lat'], 'lon': city['lon'], 'appid': api_key, 'units': 'metric' } current_response = requests.get(current_url, params=current_params) current_response.raise_for_status() current_data = current_response.json() # Pron\u00f3stico extendido forecast_url = f\"https://api.openweathermap.org/data/2.5/forecast\" forecast_response = requests.get(forecast_url, params=current_params) forecast_response.raise_for_status() forecast_data = forecast_response.json() # Estructurar datos actuales current_record = { 'city_name': city['name'], 'latitude': city['lat'], 'longitude': city['lon'], 'timestamp': context['ts'], 'temperature': current_data['main']['temp'], 'feels_like': current_data['main']['feels_like'], 'humidity': current_data['main']['humidity'], 'pressure': current_data['main']['pressure'], 'wind_speed': current_data.get('wind', {}).get('speed', 0), 'wind_direction': current_data.get('wind', {}).get('deg', 0), 'cloud_coverage': current_data['clouds']['all'], 'weather_condition': current_data['weather'][0]['main'], 'weather_description': current_data['weather'][0]['description'], 'visibility': current_data.get('visibility', 0), 'data_type': 'current' } weather_data.append(current_record) # Procesar datos de pron\u00f3stico for forecast_item in forecast_data['list'][:8]: # Pr\u00f3ximas 24 horas forecast_record = { 'city_name': city['name'], 'latitude': city['lat'], 'longitude': city['lon'], 'timestamp': forecast_item['dt_txt'], 'temperature': forecast_item['main']['temp'], 'feels_like': forecast_item['main']['feels_like'], 'humidity': forecast_item['main']['humidity'], 'pressure': forecast_item['main']['pressure'], 'wind_speed': forecast_item.get('wind', {}).get('speed', 0), 'wind_direction': forecast_item.get('wind', {}).get('deg', 0), 'cloud_coverage': forecast_item['clouds']['all'], 'weather_condition': forecast_item['weather'][0]['main'], 'weather_description': forecast_item['weather'][0]['description'], 'visibility': forecast_item.get('visibility', 0), 'data_type': 'forecast', 'precipitation_probability': forecast_item.get('pop', 0) * 100 } weather_data.append(forecast_record) except requests.exceptions.RequestException as e: print(f\"Error fetching data for {city['name']}: {str(e)}\") continue except KeyError as e: print(f\"Missing data field for {city['name']}: {str(e)}\") continue # Guardar datos en S3 para procesamiento con Spark s3_client = boto3.client('s3') file_key = f\"weather-data/raw/{context['ds']}/{context['ts']}/weather_data.json\" s3_client.put_object( Bucket='weather-data-lake', Key=file_key, Body=json.dumps(weather_data, indent=2), ContentType='application/json' ) return f\"s3://weather-data-lake/{file_key}\" # Funci\u00f3n para procesar datos con Spark def create_spark_weather_analysis(): \"\"\"Script de Spark para an\u00e1lisis predictivo de datos meteorol\u00f3gicos\"\"\" spark_script = \"\"\" from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import RandomForestRegressor from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml import Pipeline # Configuraci\u00f3n de Spark spark = SparkSession.builder \\\\ .appName(\"Weather_Predictive_Analytics\") \\\\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\\\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\\ .getOrCreate() # Schema para datos meteorol\u00f3gicos weather_schema = StructType([ StructField(\"city_name\", StringType(), True), StructField(\"latitude\", DoubleType(), True), StructField(\"longitude\", DoubleType(), True), StructField(\"timestamp\", StringType(), True), StructField(\"temperature\", DoubleType(), True), StructField(\"feels_like\", DoubleType(), True), StructField(\"humidity\", IntegerType(), True), StructField(\"pressure\", DoubleType(), True), StructField(\"wind_speed\", DoubleType(), True), StructField(\"wind_direction\", DoubleType(), True), StructField(\"cloud_coverage\", IntegerType(), True), StructField(\"weather_condition\", StringType(), True), StructField(\"weather_description\", StringType(), True), StructField(\"visibility\", DoubleType(), True), StructField(\"data_type\", StringType(), True), StructField(\"precipitation_probability\", DoubleType(), True) ]) # Lectura de datos actuales y hist\u00f3ricos df_current = spark.read \\\\ .schema(weather_schema) \\\\ .json(\"s3://weather-data-lake/weather-data/raw/*/*/weather_data.json\") # Procesamiento temporal y feature engineering df_processed = df_current \\\\ .withColumn(\"timestamp_parsed\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\\ .withColumn(\"hour\", hour(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"day_of_week\", dayofweek(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"month\", month(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"season\", when(col(\"month\").isin([12, 1, 2]), \"winter\") .when(col(\"month\").isin([3, 4, 5]), \"spring\") .when(col(\"month\").isin([6, 7, 8]), \"summer\") .otherwise(\"autumn\")) \\\\ .withColumn(\"temp_humidity_ratio\", col(\"temperature\") / col(\"humidity\")) \\\\ .withColumn(\"pressure_normalized\", (col(\"pressure\") - 1013.25) / 50) \\\\ .withColumn(\"wind_pressure_interaction\", col(\"wind_speed\") * col(\"pressure_normalized\")) # Crear caracter\u00edsticas de ventana temporal para tendencias window_spec = Window.partitionBy(\"city_name\").orderBy(\"timestamp_parsed\") df_with_trends = df_processed \\\\ .withColumn(\"temp_lag_1h\", lag(\"temperature\", 1).over(window_spec)) \\\\ .withColumn(\"temp_lag_3h\", lag(\"temperature\", 3).over(window_spec)) \\\\ .withColumn(\"pressure_lag_1h\", lag(\"pressure\", 1).over(window_spec)) \\\\ .withColumn(\"temp_change_1h\", col(\"temperature\") - col(\"temp_lag_1h\")) \\\\ .withColumn(\"temp_change_3h\", col(\"temperature\") - col(\"temp_lag_3h\")) \\\\ .withColumn(\"pressure_change_1h\", col(\"pressure\") - col(\"pressure_lag_1h\")) # Filtrar datos v\u00e1lidos para el modelo df_model_ready = df_with_trends \\\\ .filter(col(\"data_type\") == \"current\") \\\\ .filter(col(\"temp_lag_1h\").isNotNull()) \\\\ .filter(col(\"temperature\").between(-50, 60)) \\\\ .filter(col(\"humidity\").between(0, 100)) # Preparar features para modelo predictivo feature_cols = [ \"latitude\", \"longitude\", \"hour\", \"day_of_week\", \"month\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_direction\", \"cloud_coverage\", \"visibility\", \"temp_lag_1h\", \"temp_lag_3h\", \"pressure_lag_1h\", \"temp_change_1h\", \"pressure_change_1h\", \"temp_humidity_ratio\", \"pressure_normalized\", \"wind_pressure_interaction\" ] assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\") rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"temperature\", numTrees=100) # Pipeline de ML pipeline = Pipeline(stages=[assembler, rf]) # Divisi\u00f3n de datos para entrenamiento y validaci\u00f3n train_data, test_data = df_model_ready.randomSplit([0.8, 0.2], seed=42) # Entrenar modelo model = pipeline.fit(train_data) # Predicciones y evaluaci\u00f3n predictions = model.transform(test_data) evaluator = RegressionEvaluator(labelCol=\"temperature\", predictionCol=\"prediction\", metricName=\"rmse\") rmse = evaluator.evaluate(predictions) print(f\"Root Mean Square Error: {rmse}\") # An\u00e1lisis de patrones clim\u00e1ticos df_climate_patterns = df_processed \\\\ .groupBy(\"city_name\", \"season\", \"hour\") \\\\ .agg( avg(\"temperature\").alias(\"avg_temperature\"), avg(\"humidity\").alias(\"avg_humidity\"), avg(\"pressure\").alias(\"avg_pressure\"), avg(\"wind_speed\").alias(\"avg_wind_speed\"), max(\"temperature\").alias(\"max_temperature\"), min(\"temperature\").alias(\"min_temperature\"), stddev(\"temperature\").alias(\"temp_volatility\"), count(\"*\").alias(\"observation_count\") ) # Detecci\u00f3n de eventos clim\u00e1ticos extremos df_extreme_events = df_processed \\\\ .withColumn(\"is_extreme_temp\", when((col(\"temperature\") > 35) | (col(\"temperature\") < -10), True) .otherwise(False)) \\\\ .withColumn(\"is_high_wind\", when(col(\"wind_speed\") > 15, True).otherwise(False)) \\\\ .withColumn(\"is_low_pressure\", when(col(\"pressure\") < 1000, True).otherwise(False)) \\\\ .filter((col(\"is_extreme_temp\") == True) | (col(\"is_high_wind\") == True) | (col(\"is_low_pressure\") == True)) # Predicciones futuras para las pr\u00f3ximas 6 horas df_for_prediction = df_processed \\\\ .filter(col(\"data_type\") == \"current\") \\\\ .orderBy(desc(\"timestamp_parsed\")) \\\\ .limit(4) # Una observaci\u00f3n por ciudad future_predictions = model.transform(df_for_prediction.select(*feature_cols + [\"city_name\", \"timestamp_parsed\"])) # Guardar resultados df_climate_patterns.write \\\\ .partitionBy(\"city_name\", \"season\") \\\\ .mode(\"overwrite\") \\\\ .parquet(\"s3://weather-data-lake/analytics/climate_patterns/\") df_extreme_events.write \\\\ .partitionBy(\"city_name\") \\\\ .mode(\"append\") \\\\ .parquet(\"s3://weather-data-lake/analytics/extreme_events/\") future_predictions.select(\"city_name\", \"timestamp_parsed\", \"prediction\") \\\\ .write \\\\ .mode(\"overwrite\") \\\\ .json(\"s3://weather-data-lake/predictions/temperature_forecast/\") # M\u00e9tricas del modelo para monitoreo model_metrics = spark.createDataFrame([ (\"temperature_prediction_rmse\", rmse, \"{{ ds }}\") ], [\"metric_name\", \"metric_value\", \"date\"]) model_metrics.write \\\\ .mode(\"append\") \\\\ .parquet(\"s3://weather-data-lake/model_metrics/\") spark.stop() \"\"\" # Guardar script en S3 s3_client = boto3.client('s3') s3_client.put_object( Bucket='weather-data-lake', Key='scripts/weather_analysis.py', Body=spark_script, ContentType='text/plain' ) return 's3://weather-data-lake/scripts/weather_analysis.py' # Funci\u00f3n para generar alertas basadas en predicciones def generate_weather_alerts(**context): \"\"\"Genera alertas basadas en condiciones meteorol\u00f3gicas extremas\"\"\" s3_client = boto3.client('s3') try: # Leer predicciones m\u00e1s recientes response = s3_client.get_object( Bucket='weather-data-lake', Key='predictions/temperature_forecast/part-00000-*.json' ) predictions_data = json.loads(response['Body'].read()) alerts = [] for prediction in predictions_data: city = prediction['city_name'] predicted_temp = prediction['prediction'] # Generar alertas por temperatura extrema if predicted_temp > 40: alerts.append({ 'city': city, 'alert_type': 'EXTREME_HEAT', 'severity': 'HIGH', 'predicted_temperature': predicted_temp, 'message': f'Temperatura extrema prevista en {city}: {predicted_temp:.1f}\u00b0C' }) elif predicted_temp < -15: alerts.append({ 'city': city, 'alert_type': 'EXTREME_COLD', 'severity': 'HIGH', 'predicted_temperature': predicted_temp, 'message': f'Temperatura extrema fr\u00eda prevista en {city}: {predicted_temp:.1f}\u00b0C' }) # Enviar alertas si existen if alerts: # Aqu\u00ed se podr\u00eda integrar con servicios de notificaci\u00f3n # como SNS, Slack, email, etc. print(f\"WEATHER ALERTS GENERATED: {len(alerts)} alerts\") for alert in alerts: print(f\"ALERT: {alert['message']}\") return alerts except Exception as e: print(f\"Error generating weather alerts: {str(e)}\") return [] # Definici\u00f3n de tareas del DAG fetch_weather = PythonOperator( task_id='fetch_weather_data', python_callable=fetch_weather_data, dag=dag ) create_spark_script = PythonOperator( task_id='create_spark_script', python_callable=create_spark_weather_analysis, dag=dag ) # Tarea de Spark para an\u00e1lisis predictivo weather_analysis = SparkSubmitOperator( task_id='weather_predictive_analysis', application='s3://weather-data-lake/scripts/weather_analysis.py', conn_id='spark_cluster', conf={ 'spark.executor.memory': '4g', 'spark.executor.cores': '2', 'spark.executor.instances': '4', 'spark.sql.adaptive.enabled': 'true', 'spark.sql.adaptive.coalescePartitions.enabled': 'true' }, dag=dag ) generate_alerts = PythonOperator( task_id='generate_weather_alerts', python_callable=generate_weather_alerts, dag=dag ) # Definir dependencias del pipeline fetch_weather >> create_spark_script >> weather_analysis >> generate_alerts Este pipeline demuestra: Extracci\u00f3n de APIs m\u00faltiples : Obtiene datos actuales y pron\u00f3sticos de OpenWeatherMap Procesamiento distribuido : Utiliza Spark para an\u00e1lisis a gran escala Machine Learning : Implementa modelos predictivos con MLlib Feature Engineering : Crea caracter\u00edsticas temporales y combinadas Detecci\u00f3n de anomal\u00edas : Identifica eventos clim\u00e1ticos extremos Almacenamiento optimizado : Usa particionamiento por ciudad y estaci\u00f3n Sistema de alertas : Genera notificaciones basadas en predicciones Monitoreo de modelos : Rastrea m\u00e9tricas de rendimiento del ML La arquitectura permite escalabilidad horizontal, procesamiento en tiempo real, y an\u00e1lisis predictivo avanzado para aplicaciones meteorol\u00f3gicas cr\u00edticas. 3.2.2 Conectores Spark: JDBC, FileSource, Delta, etc. Apache Spark proporciona una amplia gama de conectores nativos y de terceros que permiten la integraci\u00f3n fluida con diversas fuentes de datos, desde bases de datos relacionales tradicionales hasta sistemas de almacenamiento distribuido modernos. Estos conectores abstraen la complejidad de acceso a datos y proporcionan APIs unificadas para lectura, escritura y procesamiento de datos a gran escala. Conectores JDBC Los conectores JDBC de Spark permiten establecer conexiones directas con bases de datos relacionales utilizando drivers est\u00e1ndar JDBC. Estos conectores soportan paralelizaci\u00f3n autom\u00e1tica de consultas mediante particionado de datos, optimizaci\u00f3n de predicados (predicate pushdown) y gesti\u00f3n eficiente de conexiones para maximizar el rendimiento en entornos distribuidos. Aplicaciones : Migraci\u00f3n de datos desde sistemas legacy Integraci\u00f3n con data warehouses tradicionales Sincronizaci\u00f3n incremental de datos transaccionales Ejecuci\u00f3n de consultas anal\u00edticas distribuidas Conectar Spark a una base de datos MySQL para lectura de tablas normalizadas : from pyspark.sql import SparkSession from pyspark.sql.functions import * # Configuraci\u00f3n de Spark con driver MySQL spark = SparkSession.builder \\ .appName(\"MySQL_JDBC_Connector\") \\ .config(\"spark.jars\", \"/path/to/mysql-connector-java-8.0.33.jar\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n JDBC jdbc_url = \"jdbc:mysql://localhost:3306/ecommerce_db\" connection_properties = { \"user\": \"spark_user\", \"password\": \"secure_password\", \"driver\": \"com.mysql.cj.jdbc.Driver\", \"fetchsize\": \"10000\", # Optimizaci\u00f3n de fetch \"numPartitions\": \"8\", # Paralelizaci\u00f3n \"partitionColumn\": \"customer_id\", \"lowerBound\": \"1\", \"upperBound\": \"1000000\" } # Lectura de tabla completa customers_df = spark.read \\ .jdbc(url=jdbc_url, table=\"customers\", properties=connection_properties) # Lectura con particionado personalizado orders_df = spark.read \\ .jdbc(url=jdbc_url, table=\"orders\", column=\"order_date\", lowerBound=\"2023-01-01\", upperBound=\"2024-12-31\", numPartitions=12, # Una partici\u00f3n por mes properties=connection_properties) # Mostrar esquema y datos customers_df.printSchema() customers_df.show(20, truncate=False) Ejecutar una consulta SQL desde Spark para filtrar solo los datos necesarios : # Consulta SQL personalizada con predicado pushdown custom_query = \"\"\" (SELECT c.customer_id, c.customer_name, c.registration_date, o.order_id, o.order_date, o.total_amount, p.product_name, p.category FROM customers c INNER JOIN orders o ON c.customer_id = o.customer_id INNER JOIN order_items oi ON o.order_id = oi.order_id INNER JOIN products p ON oi.product_id = p.product_id WHERE o.order_date >= '2024-01-01' AND p.category IN ('Electronics', 'Books') AND o.total_amount > 100) AS filtered_data \"\"\" # Ejecuci\u00f3n de consulta optimizada filtered_df = spark.read \\ .jdbc(url=jdbc_url, table=custom_query, properties=connection_properties) # Procesamiento adicional en Spark result_df = filtered_df \\ .groupBy(\"customer_id\", \"customer_name\", \"category\") \\ .agg( count(\"order_id\").alias(\"total_orders\"), sum(\"total_amount\").alias(\"total_spent\"), max(\"order_date\").alias(\"last_purchase_date\") ) \\ .filter(col(\"total_orders\") >= 3) \\ .orderBy(desc(\"total_spent\")) result_df.show() # Escritura de resultados de vuelta a MySQL result_df.write \\ .mode(\"overwrite\") \\ .jdbc(url=jdbc_url, table=\"customer_analytics\", properties=connection_properties) FileSource y formatos soportados Spark FileSource es un conector unificado que proporciona acceso eficiente a sistemas de archivos distribuidos como HDFS, Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, y sistemas de archivos locales. Soporta m\u00faltiples formatos de archivo optimizados para big data, incluyendo formatos columnares (Parquet, ORC), formatos de texto (CSV, JSON, XML) y formatos binarios personalizados. Caracter\u00edsticas : Particionado autom\u00e1tico y manual Compresi\u00f3n transparente (gzip, snappy, lz4, brotli) Schema evolution y schema inference Predicate pushdown para formatos columnares Vectorizaci\u00f3n para mejor rendimiento Leer archivos Parquet de S3 con particiones por fecha : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n optimizada para S3 spark = SparkSession.builder \\ .appName(\"S3_Parquet_Reader\") \\ .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Ruta base con estructura particionada s3_base_path = \"s3a://data-lake-bucket/sales_data/year=*/month=*/day=*/\" # Lectura con filtros de partici\u00f3n (partition pruning) sales_df = spark.read \\ .option(\"basePath\", \"s3a://data-lake-bucket/sales_data/\") \\ .parquet(s3_base_path) \\ .filter( (col(\"year\") == 2024) & (col(\"month\").isin([10, 11, 12])) & (col(\"day\") >= 1) ) # Lectura con esquema espec\u00edfico para mejor rendimiento schema = StructType([ StructField(\"transaction_id\", StringType(), True), StructField(\"customer_id\", LongType(), True), StructField(\"product_id\", StringType(), True), StructField(\"quantity\", IntegerType(), True), StructField(\"unit_price\", DecimalType(10,2), True), StructField(\"timestamp\", TimestampType(), True), StructField(\"year\", IntegerType(), True), StructField(\"month\", IntegerType(), True), StructField(\"day\", IntegerType(), True) ]) optimized_sales_df = spark.read \\ .schema(schema) \\ .parquet(\"s3a://data-lake-bucket/sales_data/year=2024/month=12/\") # An\u00e1lisis de particiones y estad\u00edsticas print(f\"N\u00famero de particiones: {optimized_sales_df.rdd.getNumPartitions()}\") print(f\"Total de registros: {optimized_sales_df.count()}\") # Agregaciones optimizadas daily_summary = optimized_sales_df \\ .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\ .groupBy(\"year\", \"month\", \"day\") \\ .agg( count(\"transaction_id\").alias(\"total_transactions\"), sum(\"total_amount\").alias(\"daily_revenue\"), countDistinct(\"customer_id\").alias(\"unique_customers\"), avg(\"total_amount\").alias(\"avg_transaction_value\") ) daily_summary.show() Procesar archivos CSV diarios desde un directorio HDFS : import os from datetime import datetime, timedelta from pyspark.sql.functions import * from pyspark.sql.types import * # Esquema expl\u00edcito para archivos CSV csv_schema = StructType([ StructField(\"log_timestamp\", TimestampType(), True), StructField(\"user_id\", StringType(), True), StructField(\"session_id\", StringType(), True), StructField(\"page_url\", StringType(), True), StructField(\"user_agent\", StringType(), True), StructField(\"ip_address\", StringType(), True), StructField(\"response_code\", IntegerType(), True), StructField(\"response_size\", LongType(), True) ]) # Funci\u00f3n para procesar archivos por rango de fechas def process_daily_logs(start_date, end_date, hdfs_base_path): current_date = start_date all_files = [] while current_date <= end_date: date_str = current_date.strftime(\"%Y-%m-%d\") daily_path = f\"{hdfs_base_path}/date={date_str}/*.csv\" all_files.append(daily_path) current_date += timedelta(days=1) return all_files # Configuraci\u00f3n para procesamiento de archivos grandes spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"268435456\") # 256MB spark.conf.set(\"spark.sql.files.openCostInBytes\", \"8388608\") # 8MB # Lectura de m\u00faltiples archivos CSV diarios hdfs_path = \"hdfs://namenode:9000/logs/web_access\" start_date = datetime(2024, 12, 1) end_date = datetime(2024, 12, 7) file_paths = process_daily_logs(start_date, end_date, hdfs_path) # Lectura optimizada con m\u00faltiples opciones web_logs_df = spark.read \\ .schema(csv_schema) \\ .option(\"header\", \"true\") \\ .option(\"multiline\", \"false\") \\ .option(\"escape\", '\"') \\ .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\ .option(\"mode\", \"PERMISSIVE\") \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .csv(file_paths) # Limpieza y enriquecimiento de datos cleaned_logs_df = web_logs_df \\ .filter(col(\"_corrupt_record\").isNull()) \\ .withColumn(\"date\", to_date(col(\"log_timestamp\"))) \\ .withColumn(\"hour\", hour(col(\"log_timestamp\"))) \\ .withColumn(\"is_bot\", when(col(\"user_agent\").rlike(\"(?i)bot|crawler|spider\"), True).otherwise(False)) \\ .withColumn(\"status_category\", when(col(\"response_code\").between(200, 299), \"Success\") .when(col(\"response_code\").between(400, 499), \"Client_Error\") .when(col(\"response_code\").between(500, 599), \"Server_Error\") .otherwise(\"Other\")) # An\u00e1lisis de patrones de tr\u00e1fico traffic_analysis = cleaned_logs_df \\ .filter(~col(\"is_bot\")) \\ .groupBy(\"date\", \"hour\", \"status_category\") \\ .agg( count(\"*\").alias(\"request_count\"), countDistinct(\"user_id\").alias(\"unique_users\"), countDistinct(\"ip_address\").alias(\"unique_ips\"), sum(\"response_size\").alias(\"total_bytes\"), avg(\"response_size\").alias(\"avg_response_size\") ) # Escritura de resultados procesados en formato Parquet particionado traffic_analysis.write \\ .mode(\"overwrite\") \\ .partitionBy(\"date\") \\ .parquet(\"hdfs://namenode:9000/analytics/web_traffic_summary\") traffic_analysis.show(50) Conector Delta Lake Delta Lake es una capa de almacenamiento open-source que proporciona transacciones ACID, manejo de metadatos escalable y unificaci\u00f3n de streaming y batch processing sobre data lakes. El conector Delta para Spark permite operaciones avanzadas como merge, time travel, schema evolution y optimizaciones autom\u00e1ticas, siendo ideal para arquitecturas de datos modernas que requieren consistencia y versionado. Capacidades : Transacciones ACID completas Versionado autom\u00e1tico con time travel Schema enforcement y evolution Optimizaci\u00f3n autom\u00e1tica (Auto Optimize, Z-Ordering) Change Data Feed (CDF) para captura de cambios Vacuum para limpieza de archivos obsoletos Ingerir datos en formato Delta y aplicar MERGE INTO para deduplicaci\u00f3n : from delta import * from pyspark.sql import SparkSession from pyspark.sql.functions import * # Configuraci\u00f3n de Spark con Delta Lake builder = SparkSession.builder \\ .appName(\"Delta_Lake_Upsert\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\ .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") spark = configure_spark_with_delta_pip(builder).getOrCreate() # Ruta de la tabla Delta delta_table_path = \"/delta/customer_profiles\" # Datos iniciales (simulando carga inicial) initial_data = [ (1, \"John Doe\", \"john.doe@email.com\", \"2024-01-15\", \"Premium\", 5000.0), (2, \"Jane Smith\", \"jane.smith@email.com\", \"2024-01-20\", \"Standard\", 2500.0), (3, \"Bob Johnson\", \"bob.johnson@email.com\", \"2024-02-01\", \"Premium\", 7500.0) ] columns = [\"customer_id\", \"name\", \"email\", \"registration_date\", \"tier\", \"lifetime_value\"] initial_df = spark.createDataFrame(initial_data, columns) # Creaci\u00f3n de tabla Delta inicial initial_df.write \\ .format(\"delta\") \\ .mode(\"overwrite\") \\ .option(\"overwriteSchema\", \"true\") \\ .save(delta_table_path) # Creaci\u00f3n de DeltaTable para operaciones avanzadas delta_table = DeltaTable.forPath(spark, delta_table_path) # Datos nuevos y actualizados (simulando ingesta incremental) new_data = [ (2, \"Jane Smith-Wilson\", \"jane.wilson@email.com\", \"2024-01-20\", \"Premium\", 3500.0), # Actualizaci\u00f3n (4, \"Alice Brown\", \"alice.brown@email.com\", \"2024-03-10\", \"Standard\", 1800.0), # Nuevo (5, \"Charlie Davis\", \"charlie.davis@email.com\", \"2024-03-15\", \"Premium\", 9200.0), # Nuevo (1, \"John Doe\", \"john.doe@newemail.com\", \"2024-01-15\", \"Premium\", 5500.0) # Actualizaci\u00f3n email ] updates_df = spark.createDataFrame(new_data, columns) \\ .withColumn(\"last_updated\", current_timestamp()) # Operaci\u00f3n MERGE con l\u00f3gica de deduplicaci\u00f3n delta_table.alias(\"target\") \\ .merge( updates_df.alias(\"source\"), \"target.customer_id = source.customer_id\" ) \\ .whenMatchedUpdate(set={ \"name\": \"source.name\", \"email\": \"source.email\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .whenNotMatchedInsert(values={ \"customer_id\": \"source.customer_id\", \"name\": \"source.name\", \"email\": \"source.email\", \"registration_date\": \"source.registration_date\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .execute() # Verificaci\u00f3n de resultados print(\"Estado actual de la tabla:\") spark.read.format(\"delta\").load(delta_table_path).show() # MERGE avanzado con condiciones complejas complex_updates = [ (1, \"John Doe Sr.\", \"john.doe.sr@email.com\", \"2024-01-15\", \"Platinum\", 12000.0), (6, \"Diana Prince\", \"diana.prince@email.com\", \"2024-04-01\", \"Premium\", 8500.0), (2, \"Jane Smith-Wilson\", \"jane.wilson@email.com\", \"2024-01-20\", \"Standard\", 2800.0) # Downgrade ] complex_df = spark.createDataFrame(complex_updates, columns) \\ .withColumn(\"last_updated\", current_timestamp()) # MERGE con condiciones de negocio delta_table.alias(\"target\") \\ .merge( complex_df.alias(\"source\"), \"target.customer_id = source.customer_id\" ) \\ .whenMatchedUpdate( condition=\"source.lifetime_value > target.lifetime_value OR source.tier != target.tier\", set={ \"name\": \"source.name\", \"email\": \"source.email\", \"tier\": \"source.tier\", \"lifetime_value\": \"greatest(target.lifetime_value, source.lifetime_value)\", \"last_updated\": \"source.last_updated\" } ) \\ .whenNotMatchedInsert(values={ \"customer_id\": \"source.customer_id\", \"name\": \"source.name\", \"email\": \"source.email\", \"registration_date\": \"source.registration_date\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .execute() print(\"Estado despu\u00e9s del MERGE condicional:\") spark.read.format(\"delta\").load(delta_table_path).show() Mantener versiones de datasets para auditor\u00eda : # Consulta del historial de versiones print(\"Historial de versiones de la tabla:\") delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False) # Time Travel - consultar versi\u00f3n espec\u00edfica version_1_df = spark.read \\ .format(\"delta\") \\ .option(\"versionAsOf\", 1) \\ .load(delta_table_path) print(\"Estado en versi\u00f3n 1:\") version_1_df.show() # Time Travel por timestamp from datetime import datetime, timedelta yesterday = datetime.now() - timedelta(days=1) timestamp_query = spark.read \\ .format(\"delta\") \\ .option(\"timestampAsOf\", yesterday.strftime(\"%Y-%m-%d %H:%M:%S\")) \\ .load(delta_table_path) # Auditor\u00eda de cambios entre versiones current_version = spark.read.format(\"delta\").load(delta_table_path) previous_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path) # Detectar registros modificados changes_audit = current_version.alias(\"current\") \\ .join(previous_version.alias(\"previous\"), col(\"current.customer_id\") == col(\"previous.customer_id\"), \"full_outer\") \\ .select( coalesce(col(\"current.customer_id\"), col(\"previous.customer_id\")).alias(\"customer_id\"), when(col(\"previous.customer_id\").isNull(), \"INSERTED\") .when(col(\"current.customer_id\").isNull(), \"DELETED\") .when(col(\"current.name\") != col(\"previous.name\") | col(\"current.email\") != col(\"previous.email\") | col(\"current.tier\") != col(\"previous.tier\") | col(\"current.lifetime_value\") != col(\"previous.lifetime_value\"), \"UPDATED\") .otherwise(\"UNCHANGED\").alias(\"change_type\"), col(\"current.name\").alias(\"current_name\"), col(\"previous.name\").alias(\"previous_name\"), col(\"current.email\").alias(\"current_email\"), col(\"previous.email\").alias(\"previous_email\") ) \\ .filter(col(\"change_type\") != \"UNCHANGED\") print(\"Auditor\u00eda de cambios:\") changes_audit.show(truncate=False) # Configuraci\u00f3n de retenci\u00f3n y optimizaci\u00f3n # Vacuum para eliminar archivos antiguos (cuidado en producci\u00f3n) delta_table.vacuum(retentionHours=168) # 7 d\u00edas # Optimize con Z-Ordering para mejor rendimiento de consultas spark.sql(f\"OPTIMIZE delta.`{delta_table_path}` ZORDER BY (customer_id, tier)\") # Habilitar Change Data Feed para captura de cambios spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\") # Consultar cambios usando CDF changes_df = spark.read \\ .format(\"delta\") \\ .option(\"readChangeFeed\", \"true\") \\ .option(\"startingVersion\", 0) \\ .load(delta_table_path) print(\"Change Data Feed:\") changes_df.select(\"customer_id\", \"name\", \"_change_type\", \"_commit_version\", \"_commit_timestamp\").show() # Metadatos y estad\u00edsticas de la tabla print(\"Detalles de la tabla Delta:\") spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\").show(truncate=False) # Estad\u00edsticas de archivos print(\"Estad\u00edsticas de archivos:\") print(f\"N\u00famero de archivos: {len(delta_table.detail().collect()[0]['numFiles'])}\") print(f\"Tama\u00f1o total: {delta_table.detail().collect()[0]['sizeInBytes']} bytes\") 3.2.3 Airflow: uso de Hooks y Connections Apache Airflow proporciona mecanismos reutilizables y seguros para conectarse de forma modular a distintas fuentes de datos y servicios externos. Esta arquitectura permite mantener las credenciales separadas del c\u00f3digo y facilita la reutilizaci\u00f3n de conexiones a trav\u00e9s de m\u00faltiples DAGs y tareas. Hooks en Airflow Los Hooks son interfaces de bajo nivel que proporcionan una capa de abstracci\u00f3n para acceder a sistemas externos como bases de datos, APIs REST, servicios de almacenamiento en la nube (S3, GCS), y otros servicios. Cada operador en Airflow utiliza internamente un Hook espec\u00edfico para realizar sus operaciones, lo que permite encapsular la l\u00f3gica de conexi\u00f3n y las operaciones espec\u00edficas del sistema. Ventajas Reutilizaci\u00f3n : Un mismo Hook puede ser usado por m\u00faltiples operadores y tareas Abstracci\u00f3n : Ocultan la complejidad de las conexiones y protocolos espec\u00edficos Consistencia : Proporcionan una interfaz uniforme para sistemas similares Mantenibilidad : Centralizan la l\u00f3gica de conexi\u00f3n en un solo lugar Usar PostgresHook para ejecutar una consulta y pasar los resultados a otra tarea : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from airflow.operators.email import EmailOperator import pandas as pd default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5) } def extract_sales_data(**context): \"\"\"Extrae datos de ventas desde PostgreSQL y los pasa a la siguiente tarea\"\"\" # Inicializar el Hook de PostgreSQL usando la conexi\u00f3n configurada postgres_hook = PostgresHook(postgres_conn_id='postgres_sales_db') # Ejecutar consulta SQL sql_query = \"\"\" SELECT DATE(sale_date) as fecha, SUM(amount) as total_ventas, COUNT(*) as num_transacciones, AVG(amount) as promedio_venta FROM sales WHERE sale_date >= CURRENT_DATE - INTERVAL '7 days' GROUP BY DATE(sale_date) ORDER BY fecha DESC; \"\"\" # Obtener resultados como DataFrame de pandas df = postgres_hook.get_pandas_df(sql_query) # Convertir a diccionario para pasar entre tareas sales_data = df.to_dict('records') # Guardar en XCom para la siguiente tarea context['task_instance'].xcom_push(key='weekly_sales', value=sales_data) print(f\"Extra\u00eddos {len(sales_data)} registros de ventas\") return sales_data def process_sales_report(**context): \"\"\"Procesa los datos de ventas y genera un reporte\"\"\" # Obtener datos de la tarea anterior sales_data = context['task_instance'].xcom_pull( task_ids='extract_sales_data', key='weekly_sales' ) if not sales_data: raise ValueError(\"No se recibieron datos de ventas\") # Procesar datos total_sales = sum([record['total_ventas'] for record in sales_data]) avg_daily_sales = total_sales / len(sales_data) # Generar reporte report = f\"\"\" Reporte Semanal de Ventas: - Total de ventas: ${total_sales:,.2f} - Promedio diario: ${avg_daily_sales:,.2f} - D\u00edas analizados: {len(sales_data)} \"\"\" print(report) context['task_instance'].xcom_push(key='sales_report', value=report) return report # Definir el DAG dag = DAG( 'sales_reporting_dag', default_args=default_args, description='Pipeline de reporte de ventas usando PostgresHook', schedule_interval='@daily', catchup=False ) # Definir tareas extract_task = PythonOperator( task_id='extract_sales_data', python_callable=extract_sales_data, dag=dag ) process_task = PythonOperator( task_id='process_sales_report', python_callable=process_sales_report, dag=dag ) # Definir dependencias extract_task >> process_task Usar HttpHook para llamar a una API y procesar su JSON : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.operators.postgres import PostgresOperator import json default_args = { 'owner': 'api-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=2) } def fetch_weather_data(**context): \"\"\"Obtiene datos meteorol\u00f3gicos desde una API externa\"\"\" # Inicializar HttpHook con la conexi\u00f3n configurada http_hook = HttpHook( method='GET', http_conn_id='weather_api_conn' # Conexi\u00f3n configurada en Airflow UI ) # Par\u00e1metros para la API endpoint = '/current' headers = { 'Accept': 'application/json', 'User-Agent': 'Airflow-Weather-Pipeline/1.0' } # Obtener ciudad desde variables de Airflow from airflow.models import Variable city = Variable.get(\"weather_city\", default_var=\"Cali\") data = { 'q': city, 'units': 'metric', 'lang': 'es' } try: # Realizar petici\u00f3n HTTP response = http_hook.run( endpoint=endpoint, headers=headers, data=data ) # Procesar respuesta JSON weather_data = json.loads(response.content) # Extraer informaci\u00f3n relevante processed_data = { 'timestamp': datetime.now().isoformat(), 'ciudad': weather_data.get('name'), 'temperatura': weather_data['main']['temp'], 'sensacion_termica': weather_data['main']['feels_like'], 'humedad': weather_data['main']['humidity'], 'presion': weather_data['main']['pressure'], 'descripcion': weather_data['weather'][0]['description'], 'visibilidad': weather_data.get('visibility', 0) / 1000, # km 'viento_velocidad': weather_data.get('wind', {}).get('speed', 0) } print(f\"Datos meteorol\u00f3gicos obtenidos para {processed_data['ciudad']}\") print(f\"Temperatura: {processed_data['temperatura']}\u00b0C\") # Guardar en XCom context['task_instance'].xcom_push( key='weather_data', value=processed_data ) return processed_data except Exception as e: print(f\"Error al obtener datos meteorol\u00f3gicos: {str(e)}\") raise def validate_and_transform_weather(**context): \"\"\"Valida y transforma los datos meteorol\u00f3gicos\"\"\" # Obtener datos de la tarea anterior weather_data = context['task_instance'].xcom_pull( task_ids='fetch_weather_data', key='weather_data' ) if not weather_data: raise ValueError(\"No se recibieron datos meteorol\u00f3gicos\") # Validaciones if weather_data['temperatura'] < -50 or weather_data['temperatura'] > 60: raise ValueError(f\"Temperatura fuera de rango: {weather_data['temperatura']}\u00b0C\") if weather_data['humedad'] < 0 or weather_data['humedad'] > 100: raise ValueError(f\"Humedad fuera de rango: {weather_data['humedad']}%\") # Transformaciones adicionales weather_data['categoria_temperatura'] = ( 'Muy Fr\u00edo' if weather_data['temperatura'] < 10 else 'Fr\u00edo' if weather_data['temperatura'] < 18 else 'Templado' if weather_data['temperatura'] < 25 else 'C\u00e1lido' if weather_data['temperatura'] < 30 else 'Muy C\u00e1lido' ) weather_data['indice_confort'] = ( weather_data['temperatura'] - (weather_data['humedad'] / 100) * 2 - weather_data['viento_velocidad'] * 0.5 ) print(f\"Datos validados y transformados exitosamente\") print(f\"Categor\u00eda de temperatura: {weather_data['categoria_temperatura']}\") print(f\"\u00cdndice de confort: {weather_data['indice_confort']:.1f}\") return weather_data def store_weather_data(**context): \"\"\"Almacena los datos procesados en la base de datos\"\"\" weather_data = context['task_instance'].xcom_pull( task_ids='validate_and_transform_weather' ) # Aqu\u00ed normalmente insertar\u00edas en la base de datos # usando otro Hook como PostgresHook print(\"Datos meteorol\u00f3gicos almacenados exitosamente\") print(json.dumps(weather_data, indent=2, ensure_ascii=False)) # Definir el DAG dag = DAG( 'weather_api_pipeline', default_args=default_args, description='Pipeline para consumir API meteorol\u00f3gica usando HttpHook', schedule_interval=timedelta(hours=3), catchup=False, tags=['api', 'weather', 'http'] ) # Definir tareas fetch_task = PythonOperator( task_id='fetch_weather_data', python_callable=fetch_weather_data, dag=dag ) validate_task = PythonOperator( task_id='validate_and_transform_weather', python_callable=validate_and_transform_weather, dag=dag ) store_task = PythonOperator( task_id='store_weather_data', python_callable=store_weather_data, dag=dag ) # Definir dependencias fetch_task >> validate_task >> store_task Connections y Variables Las Connections y Variables de Airflow proporcionan un mecanismo seguro y centralizado para gestionar credenciales, configuraciones y par\u00e1metros sin exponerlos directamente en el c\u00f3digo del DAG. Esto mejora la seguridad, facilita el mantenimiento y permite diferentes configuraciones entre entornos (desarrollo, testing, producci\u00f3n). Connections almacenan informaci\u00f3n de conexi\u00f3n completa (host, puerto, usuario, contrase\u00f1a, esquemas) mientras que Variables almacenan valores individuales que pueden ser utilizados a trav\u00e9s de m\u00faltiples DAGs. Definir una conexi\u00f3n S3 en Airflow UI y usarla desde un S3Hook : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.amazon.aws.hooks.s3 import S3Hook from airflow.models import Variable import pandas as pd import io default_args = { 'owner': 'data-engineering', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } def upload_data_to_s3(**context): \"\"\"Sube datos procesados a Amazon S3 usando conexi\u00f3n configurada\"\"\" # Inicializar S3Hook usando la conexi\u00f3n 'aws_s3_conn' definida en Airflow UI s3_hook = S3Hook(aws_conn_id='aws_s3_conn') # Obtener configuraci\u00f3n desde Variables de Airflow bucket_name = Variable.get(\"s3_data_bucket\", default_var=\"mi-bucket-datos\") s3_prefix = Variable.get(\"s3_data_prefix\", default_var=\"processed-data\") # Generar datos de ejemplo (normalmente vendr\u00edan de una tarea anterior) sample_data = { 'fecha': pd.date_range('2024-01-01', periods=100, freq='D'), 'ventas': pd.Series(range(100)) * 150.5, 'region': ['Norte', 'Sur', 'Este', 'Oeste'] * 25, 'producto': ['A', 'B', 'C'] * 33 + ['A'] } df = pd.DataFrame(sample_data) # Convertir DataFrame a CSV en memoria csv_buffer = io.StringIO() df.to_csv(csv_buffer, index=False, encoding='utf-8') csv_content = csv_buffer.getvalue() # Generar nombre de archivo con timestamp timestamp = context['execution_date'].strftime('%Y%m%d_%H%M%S') s3_key = f\"{s3_prefix}/ventas_diarias_{timestamp}.csv\" try: # Subir archivo a S3 s3_hook.load_string( string_data=csv_content, key=s3_key, bucket_name=bucket_name, replace=True, content_type='text/csv' ) print(f\"Archivo subido exitosamente a S3:\") print(f\"Bucket: {bucket_name}\") print(f\"Key: {s3_key}\") print(f\"Registros: {len(df)}\") # Verificar que el archivo existe if s3_hook.check_for_key(key=s3_key, bucket_name=bucket_name): print(\"\u2713 Verificaci\u00f3n exitosa: El archivo existe en S3\") else: raise Exception(\"Error: El archivo no se encontr\u00f3 en S3 despu\u00e9s de la subida\") # Guardar informaci\u00f3n del archivo en XCom file_info = { 'bucket': bucket_name, 's3_key': s3_key, 'size_bytes': len(csv_content.encode('utf-8')), 'records_count': len(df), 'upload_timestamp': timestamp } context['task_instance'].xcom_push(key='s3_file_info', value=file_info) return file_info except Exception as e: print(f\"Error al subir archivo a S3: {str(e)}\") raise def process_s3_files(**context): \"\"\"Lee y procesa archivos desde S3\"\"\" s3_hook = S3Hook(aws_conn_id='aws_s3_conn') bucket_name = Variable.get(\"s3_data_bucket\") s3_prefix = Variable.get(\"s3_data_prefix\") # Listar archivos en el bucket con el prefijo especificado file_keys = s3_hook.list_keys( bucket_name=bucket_name, prefix=s3_prefix ) print(f\"Archivos encontrados en S3: {len(file_keys)}\") # Procesar el archivo m\u00e1s reciente if file_keys: latest_file = sorted(file_keys)[-1] # Leer archivo desde S3 file_content = s3_hook.read_key( key=latest_file, bucket_name=bucket_name ) # Convertir a DataFrame df = pd.read_csv(io.StringIO(file_content)) # Realizar an\u00e1lisis b\u00e1sico analysis = { 'total_records': len(df), 'total_sales': df['ventas'].sum(), 'avg_sales': df['ventas'].mean(), 'unique_regions': df['region'].nunique(), 'date_range': f\"{df['fecha'].min()} to {df['fecha'].max()}\" } print(\"An\u00e1lisis del archivo procesado:\") for key, value in analysis.items(): print(f\" {key}: {value}\") return analysis else: print(\"No se encontraron archivos para procesar\") return {} # Definir el DAG dag = DAG( 's3_data_pipeline', default_args=default_args, description='Pipeline de datos usando S3Hook y conexiones configuradas', schedule_interval='@daily', catchup=False, tags=['s3', 'aws', 'data-pipeline'] ) # Definir tareas upload_task = PythonOperator( task_id='upload_data_to_s3', python_callable=upload_data_to_s3, dag=dag ) process_task = PythonOperator( task_id='process_s3_files', python_callable=process_s3_files, dag=dag ) # Definir dependencias upload_task >> process_task Almacenar claves API en variables cifradas para ser usadas desde el DAG : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.models import Variable from airflow.providers.postgres.hooks.postgres import PostgresHook import requests import json import hashlib default_args = { 'owner': 'api-integration-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=5) } def fetch_secure_api_data(**context): \"\"\"Consume API externa usando claves cifradas almacenadas en Variables\"\"\" # Obtener claves API cifradas desde Variables de Airflow # Estas variables se configuran en Airflow UI marcadas como \"encrypt\" api_key = Variable.get(\"external_api_key\") # Variable cifrada api_secret = Variable.get(\"external_api_secret\") # Variable cifrada # Obtener configuraciones no sensibles api_base_url = Variable.get(\"external_api_base_url\", default_var=\"https://api.example.com\") max_records = int(Variable.get(\"api_max_records\", default_var=\"1000\")) # Generar token de autenticaci\u00f3n (ejemplo con HMAC) timestamp = str(int(datetime.now().timestamp())) message = f\"{api_key}{timestamp}\" signature = hashlib.hmac( api_secret.encode('utf-8'), message.encode('utf-8'), hashlib.sha256 ).hexdigest() # Configurar headers de autenticaci\u00f3n headers = { 'Authorization': f'Bearer {api_key}', 'X-API-Timestamp': timestamp, 'X-API-Signature': signature, 'Content-Type': 'application/json', 'User-Agent': 'Airflow-Data-Pipeline/2.0' } # Par\u00e1metros de la consulta params = { 'limit': max_records, 'format': 'json', 'date_from': context['execution_date'].strftime('%Y-%m-%d'), 'include_metadata': True } try: # Realizar petici\u00f3n usando requests directamente para mayor control response = requests.get( f\"{api_base_url}/v1/data\", headers=headers, params=params, timeout=30 ) response.raise_for_status() # Lanza excepci\u00f3n si status != 2xx data = response.json() # Validar respuesta if 'error' in data: raise Exception(f\"Error en API: {data['error']}\") records = data.get('records', []) metadata = data.get('metadata', {}) print(f\"Datos obtenidos exitosamente:\") print(f\" Registros: {len(records)}\") print(f\" P\u00e1gina: {metadata.get('page', 1)}\") print(f\" Total disponible: {metadata.get('total_count', 'N/A')}\") # Procesar y limpiar datos processed_records = [] for record in records: processed_record = { 'id': record.get('id'), 'timestamp': record.get('created_at'), 'value': float(record.get('value', 0)), 'category': record.get('category', 'unknown'), 'status': record.get('status', 'active'), 'metadata': json.dumps(record.get('additional_data', {})) } processed_records.append(processed_record) # Guardar en XCom para tareas posteriores context['task_instance'].xcom_push( key='api_data', value=processed_records ) context['task_instance'].xcom_push( key='api_metadata', value=metadata ) return { 'records_count': len(processed_records), 'api_response_time': response.elapsed.total_seconds(), 'status_code': response.status_code } except requests.exceptions.RequestException as e: print(f\"Error de conexi\u00f3n con la API: {str(e)}\") raise except json.JSONDecodeError as e: print(f\"Error al decodificar respuesta JSON: {str(e)}\") raise except Exception as e: print(f\"Error general al consumir API: {str(e)}\") raise def validate_and_enrich_data(**context): \"\"\"Valida y enriquece los datos obtenidos de la API\"\"\" # Obtener datos de la tarea anterior api_data = context['task_instance'].xcom_pull( task_ids='fetch_secure_api_data', key='api_data' ) if not api_data: raise ValueError(\"No se recibieron datos de la API\") # Obtener configuraciones de validaci\u00f3n desde Variables min_value = float(Variable.get(\"validation_min_value\", default_var=\"0\")) max_value = float(Variable.get(\"validation_max_value\", default_var=\"10000\")) valid_categories = Variable.get(\"valid_categories\", default_var=\"A,B,C\").split(',') validated_records = [] validation_errors = [] for i, record in enumerate(api_data): errors = [] # Validaciones if record['value'] < min_value or record['value'] > max_value: errors.append(f\"Valor fuera de rango: {record['value']}\") if record['category'] not in valid_categories: errors.append(f\"Categor\u00eda inv\u00e1lida: {record['category']}\") if not record['id']: errors.append(\"ID faltante\") # Si hay errores, registrar y omitir registro if errors: validation_errors.append({ 'record_index': i, 'record_id': record.get('id', 'N/A'), 'errors': errors }) continue # Enriquecer datos v\u00e1lidos enriched_record = record.copy() enriched_record['validation_timestamp'] = datetime.now().isoformat() enriched_record['value_category'] = ( 'low' if record['value'] < 100 else 'medium' if record['value'] < 1000 else 'high' ) enriched_record['processing_batch'] = context['run_id'] validated_records.append(enriched_record) print(f\"Validaci\u00f3n completada:\") print(f\" Registros v\u00e1lidos: {len(validated_records)}\") print(f\" Registros con errores: {len(validation_errors)}\") if validation_errors: print(\"Errores de validaci\u00f3n encontrados:\") for error in validation_errors[:5]: # Mostrar solo los primeros 5 print(f\" Record {error['record_index']}: {', '.join(error['errors'])}\") # Guardar resultados context['task_instance'].xcom_push( key='validated_data', value=validated_records ) context['task_instance'].xcom_push( key='validation_errors', value=validation_errors ) return { 'valid_records': len(validated_records), 'invalid_records': len(validation_errors), 'validation_success_rate': len(validated_records) / len(api_data) * 100 } def store_processed_data(**context): \"\"\"Almacena los datos procesados en la base de datos usando conexi\u00f3n configurada\"\"\" # Obtener datos validados validated_data = context['task_instance'].xcom_pull( task_ids='validate_and_enrich_data', key='validated_data' ) if not validated_data: print(\"No hay datos v\u00e1lidos para almacenar\") return # Usar PostgresHook con conexi\u00f3n configurada postgres_hook = PostgresHook(postgres_conn_id='postgres_data_warehouse') # Obtener configuraci\u00f3n de tabla desde Variables target_table = Variable.get(\"target_table_name\", default_var=\"api_data\") try: # Preparar datos para inserci\u00f3n masiva insert_sql = f\"\"\" INSERT INTO {target_table} (external_id, timestamp, value, category, status, metadata, validation_timestamp, value_category, processing_batch) VALUES %s ON CONFLICT (external_id) DO UPDATE SET value = EXCLUDED.value, category = EXCLUDED.category, status = EXCLUDED.status, metadata = EXCLUDED.metadata, validation_timestamp = EXCLUDED.validation_timestamp, value_category = EXCLUDED.value_category, processing_batch = EXCLUDED.processing_batch; \"\"\" # Preparar tuplas de datos data_tuples = [] for record in validated_data: data_tuple = ( record['id'], record['timestamp'], record['value'], record['category'], record['status'], record['metadata'], record['validation_timestamp'], record['value_category'], record['processing_batch'] ) data_tuples.append(data_tuple) # Ejecutar inserci\u00f3n masiva usando extras de psycopg2 from psycopg2.extras import execute_values with postgres_hook.get_conn() as conn: with conn.cursor() as cursor: execute_values(cursor, insert_sql, data_tuples) conn.commit() print(f\"Almacenados {len(data_tuples)} registros en {target_table}\") # Registrar estad\u00edsticas stats_sql = f\"\"\" SELECT COUNT(*) as total_records, AVG(value) as avg_value, MIN(timestamp) as min_timestamp, MAX(timestamp) as max_timestamp FROM {target_table} WHERE processing_batch = %s; \"\"\" stats = postgres_hook.get_first(stats_sql, parameters=[context['run_id']]) print(\"Estad\u00edsticas del lote procesado:\") print(f\" Total de registros: {stats[0]}\") print(f\" Valor promedio: {stats[1]:.2f}\") print(f\" Rango temporal: {stats[2]} - {stats[3]}\") return { 'records_stored': len(data_tuples), 'table_name': target_table, 'batch_id': context['run_id'] } except Exception as e: print(f\"Error al almacenar datos: {str(e)}\") raise # Definir el DAG dag = DAG( 'secure_api_integration', default_args=default_args, description='Pipeline seguro para integraci\u00f3n de API externa con variables cifradas', schedule_interval=timedelta(hours=6), catchup=False, tags=['api', 'security', 'encryption', 'variables'] ) # Definir tareas fetch_task = PythonOperator( task_id='fetch_secure_api_data', python_callable=fetch_secure_api_data, dag=dag ) validate_task = PythonOperator( task_id='validate_and_enrich_data', python_callable=validate_and_enrich_data, dag=dag ) store_task = PythonOperator( task_id='store_processed_data', python_callable=store_processed_data, dag=dag ) # Definir dependencias fetch_task >> validate_task >> store_task Configuraci\u00f3n de Connections y Variables en Airflow UI : Para que estos ejemplos funcionen correctamente, es necesario configurar las siguientes conexiones y variables en la interfaz de Airflow: Connections (Admin \u2192 Connections) : postgres_sales_db : Conexi\u00f3n PostgreSQL para base de datos de ventas weather_api_conn : Conexi\u00f3n HTTP para API meteorol\u00f3gica aws_s3_conn : Conexi\u00f3n AWS para acceso a S3 postgres_data_warehouse : Conexi\u00f3n PostgreSQL para almac\u00e9n de datos Variables (Admin \u2192 Variables) : weather_city : Ciudad para datos meteorol\u00f3gicos s3_data_bucket : Nombre del bucket S3 s3_data_prefix : Prefijo para archivos en S3 external_api_key : Clave API (marcada como cifrada) external_api_secret : Secreto API (marcada como cifrada) validation_min_value : Valor m\u00ednimo para validaci\u00f3n validation_max_value : Valor m\u00e1ximo para validaci\u00f3n valid_categories : Categor\u00edas v\u00e1lidas separadas por coma target_table_name : Nombre de la tabla destino Mejores Pr\u00e1cticas para Hooks y Connections Gesti\u00f3n de Conexiones : Separaci\u00f3n por entorno : Utiliza diferentes conexiones para desarrollo, testing y producci\u00f3n Nomenclatura consistente : Usa un patr\u00f3n claro como {env}_{service}_{purpose}_conn Principio de menor privilegio : Configura conexiones con los permisos m\u00ednimos necesarios Rotaci\u00f3n de credenciales : Implementa un proceso regular de rotaci\u00f3n de passwords y tokens Monitoreo : Registra y monitorea el uso de conexiones para detectar anomal\u00edas Gesti\u00f3n de Variables : Cifrado de datos sensibles : Siempre marca como \"encrypt\" las variables que contienen informaci\u00f3n sensible Versionado : Mant\u00e9n un registro de cambios en variables cr\u00edticas Valores por defecto : Proporciona valores por defecto sensatos para variables opcionales Documentaci\u00f3n : Documenta el prop\u00f3sito y formato esperado de cada variable Validaci\u00f3n : Implementa validaci\u00f3n de formato y rango para variables cr\u00edticas Optimizaci\u00f3n de Rendimiento : Reutilizaci\u00f3n de conexiones : Los Hooks autom\u00e1ticamente reutilizan conexiones dentro de una tarea Pool de conexiones : Configura pools de conexiones para sistemas de alta concurrencia Timeouts apropiados : Establece timeouts adecuados para evitar tareas colgadas Batch operations : Prefiere operaciones por lotes sobre m\u00faltiples operaciones individuales Limpieza de recursos : Aseg\u00farate de que las conexiones se cierren adecuadamente Manejo de Errores y Reintentos : Excepciones espec\u00edficas : Captura y maneja excepciones espec\u00edficas seg\u00fan el tipo de error Logging detallado : Registra informaci\u00f3n suficiente para diagnosticar problemas Reintentos inteligentes : Configura diferentes estrategias de reintento seg\u00fan el tipo de falla Alertas apropiadas : Configura alertas para fallos cr\u00edticos vs. fallos recuperables Fallback mechanisms : Implementa mecanismos de respaldo cuando sea posible Seguridad : Validaci\u00f3n de entrada : Siempre valida y sanitiza datos de entrada Auditor\u00eda : Mant\u00e9n logs de auditor\u00eda para accesos a sistemas sensibles Cifrado en tr\u00e1nsito : Usa siempre conexiones cifradas (HTTPS, SSL/TLS) Segregaci\u00f3n de redes : Utiliza redes privadas para comunicaci\u00f3n entre servicios Secretos externos : Considera usar servicios externos de gesti\u00f3n de secretos (AWS Secrets Manager, HashiCorp Vault) Ejemplo Avanzado: Pipeline Completo con M\u00faltiples Hooks Este ejemplo avanzado demuestra c\u00f3mo integrar m\u00faltiples Hooks y Connections en un pipeline completo de datos, incluyendo manejo de errores, notificaciones, y mejores pr\u00e1cticas de producci\u00f3n. from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.operators.bash import BashOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from airflow.providers.amazon.aws.hooks.s3 import S3Hook from airflow.providers.http.hooks.http import HttpHook from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook from airflow.models import Variable from airflow.utils.task_group import TaskGroup import pandas as pd import json import logging # Configurar logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) default_args = { 'owner': 'data-platform-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': False, # Usaremos Slack en su lugar 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5), 'on_failure_callback': lambda context: send_failure_notification(context) } def send_failure_notification(context): \"\"\"Env\u00eda notificaci\u00f3n de fallo a Slack\"\"\" try: slack_hook = SlackWebhookHook( http_conn_id='slack_webhook_conn', webhook_token=Variable.get('slack_webhook_token') ) message = f\"\"\" *Fallo en Pipeline de Datos* *DAG:* {context['dag'].dag_id} *Tarea:* {context['task'].task_id} *Fecha de ejecuci\u00f3n:* {context['execution_date']} *Error:* {context.get('exception', 'Error desconocido')} Por favor revisar los logs para m\u00e1s detalles. \"\"\" slack_hook.send_text(message) except Exception as e: logger.error(f\"Error enviando notificaci\u00f3n a Slack: {str(e)}\") def extract_from_multiple_sources(**context): \"\"\"Extrae datos de m\u00faltiples fuentes usando diferentes Hooks\"\"\" results = {} # 1. Extraer desde PostgreSQL try: postgres_hook = PostgresHook(postgres_conn_id='postgres_source_db') sql_query = \"\"\" SELECT id, created_date, amount, customer_id, product_category FROM transactions WHERE DATE(created_date) = CURRENT_DATE - INTERVAL '1 day' \"\"\" transactions_df = postgres_hook.get_pandas_df(sql_query) results['transactions'] = { 'count': len(transactions_df), 'data': transactions_df.to_dict('records') } logger.info(f\"Extra\u00eddas {len(transactions_df)} transacciones de PostgreSQL\") except Exception as e: logger.error(f\"Error extrayendo de PostgreSQL: {str(e)}\") results['transactions'] = {'count': 0, 'data': [], 'error': str(e)} # 2. Extraer desde API externa try: http_hook = HttpHook( method='GET', http_conn_id='external_api_conn' ) # Obtener datos de referencia (cat\u00e1logo de productos) response = http_hook.run( endpoint='/api/v1/products', headers={ 'Authorization': f\"Bearer {Variable.get('api_token')}\", 'Content-Type': 'application/json' } ) api_data = json.loads(response.content) results['products'] = { 'count': len(api_data.get('products', [])), 'data': api_data.get('products', []) } logger.info(f\"Extra\u00eddos {len(api_data.get('products', []))} productos de API\") except Exception as e: logger.error(f\"Error extrayendo de API: {str(e)}\") results['products'] = {'count': 0, 'data': [], 'error': str(e)} # 3. Extraer archivo desde S3 try: s3_hook = S3Hook(aws_conn_id='aws_s3_conn') bucket_name = Variable.get('source_data_bucket') # Buscar archivo m\u00e1s reciente yesterday = (context['execution_date'] - timedelta(days=1)).strftime('%Y%m%d') s3_key = f\"customer-data/customers_{yesterday}.csv\" if s3_hook.check_for_key(key=s3_key, bucket_name=bucket_name): file_content = s3_hook.read_key(key=s3_key, bucket_name=bucket_name) customers_df = pd.read_csv(pd.io.common.StringIO(file_content)) results['customers'] = { 'count': len(customers_df), 'data': customers_df.to_dict('records') } logger.info(f\"Extra\u00eddos {len(customers_df)} clientes de S3\") else: logger.warning(f\"Archivo {s3_key} no encontrado en S3\") results['customers'] = {'count': 0, 'data': [], 'error': 'Archivo no encontrado'} except Exception as e: logger.error(f\"Error extrayendo de S3: {str(e)}\") results['customers'] = {'count': 0, 'data': [], 'error': str(e)} # Guardar resultados en XCom context['task_instance'].xcom_push(key='extraction_results', value=results) return results def transform_and_join_data(**context): \"\"\"Transforma y combina datos de m\u00faltiples fuentes\"\"\" # Obtener datos extra\u00eddos extraction_results = context['task_instance'].xcom_pull( task_ids='extract_from_multiple_sources', key='extraction_results' ) if not extraction_results: raise ValueError(\"No se recibieron datos de extracci\u00f3n\") # Convertir a DataFrames transactions_df = pd.DataFrame(extraction_results['transactions']['data']) products_df = pd.DataFrame(extraction_results['products']['data']) customers_df = pd.DataFrame(extraction_results['customers']['data']) logger.info(f\"Procesando {len(transactions_df)} transacciones\") if transactions_df.empty: logger.warning(\"No hay transacciones para procesar\") return {'processed_records': 0} # Realizar joins if not products_df.empty: # Join con productos para obtener informaci\u00f3n adicional transactions_df = transactions_df.merge( products_df[['id', 'name', 'price', 'margin']], left_on='product_category', right_on='id', how='left', suffixes=('', '_product') ) logger.info(\"Join con productos completado\") if not customers_df.empty: # Join con clientes para segmentaci\u00f3n transactions_df = transactions_df.merge( customers_df[['id', 'segment', 'region', 'registration_date']], left_on='customer_id', right_on='id', how='left', suffixes=('', '_customer') ) logger.info(\"Join con clientes completado\") # Transformaciones adicionales transactions_df['profit'] = transactions_df['amount'] * transactions_df.get('margin', 0.2) transactions_df['transaction_date'] = pd.to_datetime(transactions_df['created_date']) transactions_df['month_year'] = transactions_df['transaction_date'].dt.to_period('M') # Calcular m\u00e9tricas agregadas summary_stats = { 'total_transactions': len(transactions_df), 'total_revenue': transactions_df['amount'].sum(), 'total_profit': transactions_df['profit'].sum(), 'avg_transaction_amount': transactions_df['amount'].mean(), 'unique_customers': transactions_df['customer_id'].nunique() if 'customer_id' in transactions_df.columns else 0, 'processing_timestamp': datetime.now().isoformat() } logger.info(f\"Transformaci\u00f3n completada: {summary_stats}\") # Guardar datos transformados processed_data = transactions_df.to_dict('records') context['task_instance'].xcom_push(key='processed_data', value=processed_data) context['task_instance'].xcom_push(key='summary_stats', value=summary_stats) return summary_stats def load_to_data_warehouse(**context): \"\"\"Carga datos transformados al data warehouse\"\"\" # Obtener datos procesados processed_data = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='processed_data' ) summary_stats = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='summary_stats' ) if not processed_data: logger.warning(\"No hay datos procesados para cargar\") return # Conectar al data warehouse dw_hook = PostgresHook(postgres_conn_id='postgres_data_warehouse') try: # Crear tabla temporal para la carga temp_table = f\"temp_transactions_{context['run_id'].replace('-', '_')}\" create_temp_table_sql = f\"\"\" CREATE TEMP TABLE {temp_table} ( transaction_id BIGINT, transaction_date TIMESTAMP, amount DECIMAL(10,2), profit DECIMAL(10,2), customer_id BIGINT, customer_segment VARCHAR(50), customer_region VARCHAR(50), product_name VARCHAR(200), product_category VARCHAR(100), month_year VARCHAR(10), processing_timestamp TIMESTAMP ); \"\"\" dw_hook.run(create_temp_table_sql) logger.info(f\"Tabla temporal {temp_table} creada\") # Insertar datos en tabla temporal insert_count = 0 batch_size = 1000 for i in range(0, len(processed_data), batch_size): batch = processed_data[i:i + batch_size] # Preparar valores para inserci\u00f3n values = [] for record in batch: values.append(( record.get('id'), record.get('created_date'), record.get('amount', 0), record.get('profit', 0), record.get('customer_id'), record.get('segment'), record.get('region'), record.get('name'), record.get('product_category'), record.get('month_year'), summary_stats['processing_timestamp'] )) # Inserci\u00f3n por lotes insert_sql = f\"\"\" INSERT INTO {temp_table} VALUES %s \"\"\" dw_hook.run(insert_sql, parameters=[values]) insert_count += len(batch) logger.info(f\"Insertados {insert_count}/{len(processed_data)} registros\") # Mover datos de tabla temporal a tabla final merge_sql = f\"\"\" INSERT INTO fact_transactions SELECT * FROM {temp_table} ON CONFLICT (transaction_id) DO UPDATE SET amount = EXCLUDED.amount, profit = EXCLUDED.profit, processing_timestamp = EXCLUDED.processing_timestamp; \"\"\" dw_hook.run(merge_sql) # Actualizar tabla de m\u00e9tricas diarias metrics_sql = \"\"\" INSERT INTO daily_metrics (date, total_transactions, total_revenue, total_profit, avg_transaction_amount) VALUES (CURRENT_DATE, %s, %s, %s, %s) ON CONFLICT (date) DO UPDATE SET total_transactions = EXCLUDED.total_transactions, total_revenue = EXCLUDED.total_revenue, total_profit = EXCLUDED.total_profit, avg_transaction_amount = EXCLUDED.avg_transaction_amount; \"\"\" dw_hook.run(metrics_sql, parameters=[ summary_stats['total_transactions'], summary_stats['total_revenue'], summary_stats['total_profit'], summary_stats['avg_transaction_amount'] ]) logger.info(f\"Carga completada: {insert_count} registros procesados\") return { 'loaded_records': insert_count, 'load_timestamp': datetime.now().isoformat() } except Exception as e: logger.error(f\"Error en carga al data warehouse: {str(e)}\") raise def send_success_notification(**context): \"\"\"Env\u00eda notificaci\u00f3n de \u00e9xito con resumen del pipeline\"\"\" summary_stats = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='summary_stats' ) try: slack_hook = SlackWebhookHook( http_conn_id='slack_webhook_conn', webhook_token=Variable.get('slack_webhook_token') ) message = f\"\"\" \u2705 *Pipeline de Datos Completado Exitosamente* *DAG:* {context['dag'].dag_id} *Fecha de ejecuci\u00f3n:* {context['execution_date']} *Resumen:* \u2022 Transacciones procesadas: {summary_stats.get('total_transactions', 0):,} \u2022 Ingresos totales: ${summary_stats.get('total_revenue', 0):,.2f} \u2022 Ganancia total: ${summary_stats.get('total_profit', 0):,.2f} \u2022 Promedio por transacci\u00f3n: ${summary_stats.get('avg_transaction_amount', 0):,.2f} \u2022 Clientes \u00fanicos: {summary_stats.get('unique_customers', 0):,} El pipeline se ejecut\u00f3 sin errores. \u2728 \"\"\" slack_hook.send_text(message) logger.info(\"Notificaci\u00f3n de \u00e9xito enviada a Slack\") except Exception as e: logger.error(f\"Error enviando notificaci\u00f3n de \u00e9xito: {str(e)}\") # Definir el DAG dag = DAG( 'comprehensive_data_pipeline', default_args=default_args, description='Pipeline completo de datos usando m\u00faltiples Hooks y Connections', schedule_interval='@daily', catchup=False, max_active_runs=1, tags=['etl', 'multi-source', 'production', 'comprehensive'] ) # Definir tareas principales extract_task = PythonOperator( task_id='extract_from_multiple_sources', python_callable=extract_from_multiple_sources, dag=dag ) transform_task = PythonOperator( task_id='transform_and_join_data', python_callable=transform_and_join_data, dag=dag ) load_task = PythonOperator( task_id='load_to_data_warehouse', python_callable=load_to_data_warehouse, dag=dag ) # Tarea de validaci\u00f3n de calidad de datos quality_check = BashOperator( task_id='data_quality_check', bash_command=\"\"\" python /opt/airflow/scripts/data_quality_check.py \\ --date {{ ds }} \\ --threshold 0.95 \"\"\", dag=dag ) notify_success = PythonOperator( task_id='send_success_notification', python_callable=send_success_notification, dag=dag, trigger_rule='all_success' ) # Definir dependencias extract_task >> transform_task >> load_task >> quality_check >> notify_success 3.2.4 Lectura incremental vs. completa La estrategia de carga de datos es fundamental para optimizar el rendimiento, minimizar el uso de recursos y garantizar la consistencia de los datos. La elecci\u00f3n entre lectura completa e incremental depende de factores como el volumen de datos, la frecuencia de cambios, los recursos disponibles y los requisitos de latencia del negocio. Lectura completa (full load) La lectura completa implica extraer y procesar todo el conjunto de datos desde la fuente en cada ejecuci\u00f3n, independientemente de si los datos han cambiado o no. Esta estrategia es apropiada cuando el volumen de datos es manejable, no existe un mecanismo confiable para identificar cambios, o cuando se requiere una reconstrucci\u00f3n completa del dataset por motivos de integridad o correcci\u00f3n de errores. Ventajas Desventajas Simplicidad en la implementaci\u00f3n y l\u00f3gica de procesamiento Alto consumo de recursos computacionales y de red Garantiza consistencia completa de los datos Mayor tiempo de procesamiento No requiere mecanismos de control de cambios en la fuente Impacto en sistemas fuente por carga completa repetitiva Ideal para datasets peque\u00f1os a medianos Cargar todo un cat\u00e1logo de productos cada noche \u00datil cuando el cat\u00e1logo es relativamente peque\u00f1o y se requiere una vista completa y consistente para an\u00e1lisis o sincronizaci\u00f3n con otros sistemas. # Ejemplo con PySpark - Carga completa de cat\u00e1logo de productos from pyspark.sql import SparkSession from pyspark.sql.functions import current_timestamp def full_load_product_catalog(): spark = SparkSession.builder \\ .appName(\"ProductCatalogFullLoad\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n a base de datos fuente jdbc_url = \"jdbc:postgresql://prod-db:5432/ecommerce\" connection_properties = { \"user\": \"etl_user\", \"password\": \"secure_password\", \"driver\": \"org.postgresql.Driver\" } # Lectura completa del cat\u00e1logo products_df = spark.read \\ .jdbc(url=jdbc_url, table=\"products\", properties=connection_properties) # Agregar timestamp de procesamiento products_with_timestamp = products_df.withColumn( \"load_timestamp\", current_timestamp() ) # Escribir a data lake (sobrescribiendo completamente) products_with_timestamp.write \\ .mode(\"overwrite\") \\ .option(\"path\", \"s3a://datalake/bronze/products/\") \\ .saveAsTable(\"bronze.products\") print(f\"Carga completa finalizada. Registros procesados: {products_df.count()}\") spark.stop() # Programaci\u00f3n en Airflow from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime, timedelta default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'product_catalog_full_load', default_args=default_args, description='Carga completa diaria del cat\u00e1logo de productos', schedule_interval='0 2 * * *', # Diario a las 2 AM catchup=False ) full_load_task = PythonOperator( task_id='load_complete_catalog', python_callable=full_load_product_catalog, dag=dag ) Reprocesar un hist\u00f3rico completo de transacciones por correcci\u00f3n Necesario cuando se detectan errores en el procesamiento previo o cuando se implementan nuevas reglas de negocio que requieren recalcular todo el hist\u00f3rico. # Ejemplo con PySpark - Reprocesamiento completo de transacciones from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * def reprocess_complete_transaction_history(): spark = SparkSession.builder \\ .appName(\"TransactionHistoryReprocess\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Lectura completa del hist\u00f3rico de transacciones transactions_df = spark.read \\ .option(\"multiline\", \"true\") \\ .option(\"inferSchema\", \"true\") \\ .parquet(\"s3a://datalake/raw/transactions/\") # Aplicar nuevas reglas de negocio y correcciones corrected_transactions = transactions_df \\ .withColumn(\"amount_usd\", when(col(\"currency\") == \"EUR\", col(\"amount\") * 1.1) .when(col(\"currency\") == \"GBP\", col(\"amount\") * 1.25) .otherwise(col(\"amount\"))) \\ .withColumn(\"transaction_category\", when(col(\"merchant_category\").isin([\"grocery\", \"supermarket\"]), \"essential\") .when(col(\"merchant_category\").isin([\"entertainment\", \"dining\"]), \"lifestyle\") .otherwise(\"other\")) \\ .withColumn(\"reprocessed_at\", current_timestamp()) \\ .withColumn(\"processing_version\", lit(\"v2.1\")) # Escribir datos corregidos (particionado por a\u00f1o-mes para mejor performance) corrected_transactions \\ .withColumn(\"year_month\", date_format(col(\"transaction_date\"), \"yyyy-MM\")) \\ .write \\ .mode(\"overwrite\") \\ .partitionBy(\"year_month\") \\ .option(\"path\", \"s3a://datalake/silver/transactions_corrected/\") \\ .saveAsTable(\"silver.transactions_corrected\") # Generar m\u00e9tricas de reprocesamiento total_records = corrected_transactions.count() date_range = corrected_transactions.agg( min(\"transaction_date\").alias(\"min_date\"), max(\"transaction_date\").alias(\"max_date\") ).collect()[0] print(f\"Reprocesamiento completo finalizado:\") print(f\"- Registros procesados: {total_records}\") print(f\"- Rango de fechas: {date_range['min_date']} a {date_range['max_date']}\") spark.stop() # DAG de Airflow para reprocesamiento manual reprocess_dag = DAG( 'transaction_history_reprocess', default_args=default_args, description='Reprocesamiento completo del hist\u00f3rico de transacciones', schedule_interval=None, # Ejecuci\u00f3n manual solamente catchup=False ) reprocess_task = PythonOperator( task_id='reprocess_transaction_history', python_callable=reprocess_complete_transaction_history, dag=reprocess_dag ) Lectura incremental La lectura incremental procesa \u00fanicamente los datos nuevos o modificados desde la \u00faltima ejecuci\u00f3n, utilizando mecanismos de control como timestamps, IDs auto-incrementales, flags de estado o columnas de versionado. Esta estrategia es esencial para sistemas de alto volumen donde procesar todos los datos en cada ejecuci\u00f3n ser\u00eda ineficiente o impracticable. Ventajas Desventajas Significativa reducci\u00f3n en tiempo de procesamiento y uso de recursos Mayor complejidad en la implementaci\u00f3n Menor impacto en sistemas fuente Requiere mecanismos confiables de control de cambios Permite procesamiento en tiempo real o near-real-time Necesidad de manejar datos duplicados o fuera de orden Escalabilidad mejorada para grandes vol\u00famenes de datos Potencial p\u00e9rdida de datos si falla el mecanismo de control Cargar \u00f3rdenes nuevas con base en un campo created_at Estrategia com\u00fan para sistemas transaccionales donde se procesan solo las \u00f3rdenes creadas desde la \u00faltima ejecuci\u00f3n. # Ejemplo con PySpark - Carga incremental de \u00f3rdenes from pyspark.sql import SparkSession from pyspark.sql.functions import * from datetime import datetime, timedelta import boto3 def incremental_orders_load(): spark = SparkSession.builder \\ .appName(\"OrdersIncrementalLoad\") \\ .getOrCreate() # Obtener \u00faltimo timestamp procesado desde metastore o archivo de control def get_last_processed_timestamp(): try: last_run_df = spark.read.table(\"control.incremental_loads\") \\ .filter(col(\"table_name\") == \"orders\") \\ .orderBy(col(\"last_processed_timestamp\").desc()) \\ .limit(1) if last_run_df.count() > 0: return last_run_df.collect()[0][\"last_processed_timestamp\"] else: # Si es la primera ejecuci\u00f3n, usar fecha de inicio del proyecto return datetime(2024, 1, 1) except: return datetime(2024, 1, 1) last_timestamp = get_last_processed_timestamp() current_timestamp = datetime.now() print(f\"Procesando \u00f3rdenes desde: {last_timestamp}\") # Configuraci\u00f3n de conexi\u00f3n jdbc_url = \"jdbc:mysql://orders-db:3306/ecommerce\" connection_properties = { \"user\": \"etl_user\", \"password\": \"secure_password\", \"driver\": \"com.mysql.cj.jdbc.Driver\" } # Query incremental con filtro por timestamp incremental_query = f\"\"\" (SELECT order_id, customer_id, order_date, total_amount, status, created_at, updated_at FROM orders WHERE created_at > '{last_timestamp}' AND created_at <= '{current_timestamp}' ORDER BY created_at) as incremental_orders \"\"\" # Lectura incremental new_orders_df = spark.read \\ .jdbc(url=jdbc_url, table=incremental_query, properties=connection_properties) if new_orders_df.count() > 0: # Procesamiento y enriquecimiento de datos processed_orders = new_orders_df \\ .withColumn(\"load_timestamp\", lit(current_timestamp)) \\ .withColumn(\"order_year_month\", date_format(col(\"order_date\"), \"yyyy-MM\")) # Escribir nuevos datos (append mode) processed_orders.write \\ .mode(\"append\") \\ .partitionBy(\"order_year_month\") \\ .option(\"path\", \"s3a://datalake/bronze/orders/\") \\ .saveAsTable(\"bronze.orders\") # Actualizar tabla de control control_record = spark.createDataFrame([{ \"table_name\": \"orders\", \"last_processed_timestamp\": current_timestamp, \"records_processed\": new_orders_df.count(), \"processing_timestamp\": datetime.now() }]) control_record.write \\ .mode(\"append\") \\ .saveAsTable(\"control.incremental_loads\") print(f\"Carga incremental completada. Nuevos registros: {new_orders_df.count()}\") else: print(\"No hay nuevos registros para procesar\") spark.stop() # DAG de Airflow para carga incremental incremental_dag = DAG( 'orders_incremental_load', default_args=default_args, description='Carga incremental de \u00f3rdenes cada 15 minutos', schedule_interval='*/15 * * * *', # Cada 15 minutos catchup=True # Permite recuperar ejecuciones perdidas ) incremental_task = PythonOperator( task_id='load_incremental_orders', python_callable=incremental_orders_load, dag=incremental_dag ) Usar watermark en Spark para carga de eventos recientes desde Kafka T\u00e9cnica avanzada para procesamiento de streaming que maneja eventos tard\u00edos y garantiza procesamiento exactly-once. # Ejemplo con Spark Structured Streaming - Watermark desde Kafka from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * def kafka_streaming_with_watermark(): spark = SparkSession.builder \\ .appName(\"KafkaStreamingWatermark\") \\ .config(\"spark.sql.streaming.checkpointLocation\", \"s3a://checkpoints/kafka-events/\") \\ .getOrCreate() # Schema para eventos de Kafka event_schema = StructType([ StructField(\"event_id\", StringType(), True), StructField(\"user_id\", LongType(), True), StructField(\"event_type\", StringType(), True), StructField(\"event_timestamp\", TimestampType(), True), StructField(\"properties\", MapType(StringType(), StringType()), True) ]) # Lectura desde Kafka con configuraci\u00f3n de consumer kafka_stream = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"kafka-cluster:9092\") \\ .option(\"subscribe\", \"user-events\") \\ .option(\"startingOffsets\", \"latest\") \\ .option(\"maxOffsetsPerTrigger\", 10000) \\ .option(\"kafka.group.id\", \"spark-streaming-consumer\") \\ .load() # Parsear JSON y extraer timestamp parsed_events = kafka_stream \\ .select( from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"event\"), col(\"timestamp\").alias(\"kafka_timestamp\"), col(\"offset\"), col(\"partition\") ) \\ .select(\"event.*\", \"kafka_timestamp\", \"offset\", \"partition\") # Aplicar watermark para manejar eventos tard\u00edos # Permite eventos hasta 5 minutos tarde watermarked_events = parsed_events \\ .withWatermark(\"event_timestamp\", \"5 minutes\") # Agregaciones por ventana de tiempo con watermark windowed_aggregations = watermarked_events \\ .groupBy( window(col(\"event_timestamp\"), \"1 minute\", \"30 seconds\"), # Ventana deslizante col(\"event_type\") ) \\ .agg( count(\"*\").alias(\"event_count\"), countDistinct(\"user_id\").alias(\"unique_users\"), collect_set(\"user_id\").alias(\"user_list\") ) \\ .withColumn(\"window_start\", col(\"window.start\")) \\ .withColumn(\"window_end\", col(\"window.end\")) \\ .drop(\"window\") # Configurar sink para escritura incremental query = windowed_aggregations.writeStream \\ .outputMode(\"append\") \\ .format(\"delta\") \\ .option(\"path\", \"s3a://datalake/silver/event_aggregations/\") \\ .option(\"checkpointLocation\", \"s3a://checkpoints/event-aggregations/\") \\ .partitionBy(\"event_type\") \\ .trigger(processingTime='30 seconds') \\ .start() # Tambi\u00e9n escribir eventos raw para auditor\u00eda raw_events_query = parsed_events.writeStream \\ .outputMode(\"append\") \\ .format(\"delta\") \\ .option(\"path\", \"s3a://datalake/bronze/raw_events/\") \\ .option(\"checkpointLocation\", \"s3a://checkpoints/raw-events/\") \\ .partitionBy(date_format(col(\"event_timestamp\"), \"yyyy-MM-dd\")) \\ .trigger(processingTime='10 seconds') \\ .start() # Monitoreo del stream print(\"Streaming iniciado. Estad\u00edsticas del stream:\") print(f\"- Checkpoint location: {query.lastProgress}\") return query, raw_events_query # Integraci\u00f3n con Airflow para monitoreo y control from airflow.operators.bash import BashOperator from airflow.sensors.filesystem import FileSensor streaming_dag = DAG( 'kafka_streaming_monitor', default_args=default_args, description='Monitoreo y control de streaming desde Kafka', schedule_interval='*/5 * * * *', # Verificaci\u00f3n cada 5 minutos catchup=False ) # Sensor para verificar que el checkpoint existe checkpoint_sensor = FileSensor( task_id='check_streaming_checkpoint', filepath='s3a://checkpoints/kafka-events/_metadata', fs_conn_id='aws_default', poke_interval=60, timeout=300, dag=streaming_dag ) # Tarea para verificar salud del streaming job health_check = BashOperator( task_id='streaming_health_check', bash_command=''' # Verificar que los archivos de checkpoint se est\u00e1n actualizando latest_checkpoint=$(aws s3 ls s3://checkpoints/kafka-events/commits/ --recursive | tail -1 | awk '{print $1\" \"$2}') current_time=$(date -u +\"%Y-%m-%d %H:%M:%S\") # Calcular diferencia en minutos checkpoint_age=$(( ($(date -d \"$current_time\" +%s) - $(date -d \"$latest_checkpoint\" +%s)) / 60 )) if [ $checkpoint_age -gt 10 ]; then echo \"WARNING: Streaming job checkpoint is $checkpoint_age minutes old\" exit 1 else echo \"Streaming job is healthy. Checkpoint age: $checkpoint_age minutes\" fi ''', dag=streaming_dag ) checkpoint_sensor >> health_check Esta implementaci\u00f3n completa muestra c\u00f3mo ambas estrategias pueden ser efectivamente utilizadas seg\u00fan los requisitos espec\u00edficos del caso de uso, considerando factores como volumen de datos, latencia requerida, y recursos disponibles. 3.2.5 Gesti\u00f3n de autenticaci\u00f3n y configuraci\u00f3n segura Proteger accesos y configurar credenciales de forma segura es una pr\u00e1ctica obligatoria en entornos productivos. La gesti\u00f3n inadecuada de secretos y credenciales representa uno de los principales vectores de ataque en sistemas de datos modernos, por lo que implementar estrategias robustas de autenticaci\u00f3n es fundamental para mantener la integridad y confidencialidad de los pipelines de datos. Uso de variables y secretos La externalizaci\u00f3n de credenciales mediante sistemas especializados de gesti\u00f3n de secretos permite eliminar completamente las credenciales hardcodeadas del c\u00f3digo fuente. Estas herramientas proporcionan cifrado en reposo y en tr\u00e1nsito, control de acceso granular, rotaci\u00f3n autom\u00e1tica de credenciales y auditor\u00eda completa de accesos. Airflow ofrece m\u00faltiples backends de secretos que se integran nativamente con servicios como AWS Secrets Manager, Azure Key Vault, Google Secret Manager y HashiCorp Vault. Guardar contrase\u00f1as de bases de datos en Airflow usando Variable.get(\"db_password\") . from airflow import DAG from airflow.operators.python import PythonOperator from airflow.models import Variable from airflow.providers.postgres.hooks.postgres import PostgresHook from datetime import datetime, timedelta import logging def connect_to_database(**context): \"\"\" Funci\u00f3n que establece conexi\u00f3n segura a PostgreSQL usando variables de Airflow \"\"\" try: # Obtener credenciales de forma segura desde Variables de Airflow db_host = Variable.get(\"postgres_host\") db_port = Variable.get(\"postgres_port\", default_var=\"5432\") db_name = Variable.get(\"postgres_database\") db_user = Variable.get(\"postgres_username\") db_password = Variable.get(\"postgres_password\") # Marcada como sensitive en UI # Crear conexi\u00f3n usando PostgresHook postgres_hook = PostgresHook( postgres_conn_id='postgres_default', schema=db_name ) # Alternativa: conexi\u00f3n manual con credenciales desde variables connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\" # Ejecutar consulta de prueba records = postgres_hook.get_records(\"SELECT version();\") logging.info(f\"Conexi\u00f3n exitosa. Versi\u00f3n de PostgreSQL: {records[0][0]}\") return {\"status\": \"success\", \"connection\": \"established\"} except Exception as e: logging.error(f\"Error conectando a la base de datos: {str(e)}\") raise # Configuraci\u00f3n segura usando Airflow Secrets Backend default_args = { 'owner': 'data_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 3, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'secure_database_connection', default_args=default_args, description='Ejemplo de conexi\u00f3n segura a base de datos', schedule_interval='@daily', catchup=False, tags=['security', 'database'] ) # Task que utiliza credenciales seguras secure_db_task = PythonOperator( task_id='connect_secure_database', python_callable=connect_to_database, dag=dag ) Usar Azure Key Vault para proteger secretos de conexi\u00f3n a Blob Storage. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.microsoft.azure.hooks.key_vault import AzureKeyVaultHook from azure.storage.blob import BlobServiceClient from azure.identity import DefaultAzureCredential from datetime import datetime, timedelta import logging def process_blob_data_securely(**context): \"\"\" Funci\u00f3n que accede a Azure Blob Storage usando secretos desde Key Vault \"\"\" try: # Configurar Azure Key Vault Hook key_vault_hook = AzureKeyVaultHook( key_vault_name=\"your-key-vault-name\", key_vault_conn_id=\"azure_key_vault_default\" ) # Obtener secretos del Key Vault storage_account_name = key_vault_hook.get_secret(\"storage-account-name\") storage_account_key = key_vault_hook.get_secret(\"storage-account-key\") container_name = key_vault_hook.get_secret(\"blob-container-name\") # Crear cliente de Blob Storage con credenciales seguras connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Listar blobs en el contenedor container_client = blob_service_client.get_container_client(container_name) blob_list = list(container_client.list_blobs()) logging.info(f\"Encontrados {len(blob_list)} blobs en el contenedor {container_name}\") # Procesar cada blob (ejemplo: leer archivos CSV) processed_files = [] for blob in blob_list[:5]: # Procesar solo los primeros 5 archivos if blob.name.endswith('.csv'): blob_client = blob_service_client.get_blob_client( container=container_name, blob=blob.name ) # Descargar contenido del blob blob_data = blob_client.download_blob() content = blob_data.readall().decode('utf-8') logging.info(f\"Procesado archivo: {blob.name}, tama\u00f1o: {len(content)} caracteres\") processed_files.append({ 'filename': blob.name, 'size': blob.size, 'last_modified': blob.last_modified }) return { \"status\": \"success\", \"processed_files\": processed_files, \"total_blobs\": len(blob_list) } except Exception as e: logging.error(f\"Error accediendo a Azure Blob Storage: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'data_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } dag = DAG( 'azure_blob_secure_access', default_args=default_args, description='Acceso seguro a Azure Blob Storage con Key Vault', schedule_interval='@hourly', catchup=False, tags=['azure', 'security', 'blob-storage'] ) # Task principal secure_blob_task = PythonOperator( task_id='process_blob_data_secure', python_callable=process_blob_data_securely, dag=dag ) Autenticaci\u00f3n en servicios y APIs La autenticaci\u00f3n en servicios externos requiere implementar protocolos est\u00e1ndar como OAuth2, JWT, o sistemas de roles basados en la nube (IAM). Estos mecanismos proporcionan acceso granular y temporal, permitiendo la rotaci\u00f3n autom\u00e1tica de credenciales y el principio de menor privilegio. La integraci\u00f3n con proveedores de identidad corporativos facilita la gesti\u00f3n centralizada de accesos y el cumplimiento de pol\u00edticas de seguridad organizacionales. Conectarse a un API de terceros mediante OAuth2. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.models import Variable from requests_oauthlib import OAuth2Session import requests import json from datetime import datetime, timedelta import logging def authenticate_and_call_api(**context): \"\"\" Funci\u00f3n que implementa flujo OAuth2 para acceder a API externa \"\"\" try: # Obtener credenciales OAuth2 desde Variables de Airflow client_id = Variable.get(\"oauth2_client_id\") client_secret = Variable.get(\"oauth2_client_secret\") token_url = Variable.get(\"oauth2_token_url\") api_base_url = Variable.get(\"api_base_url\") # Configurar datos para solicitud de token token_data = { 'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret, 'scope': 'read write' # Scopes requeridos } # Solicitar token de acceso token_response = requests.post( token_url, data=token_data, headers={'Content-Type': 'application/x-www-form-urlencoded'} ) if token_response.status_code != 200: raise Exception(f\"Error obteniendo token: {token_response.text}\") token_info = token_response.json() access_token = token_info['access_token'] logging.info(\"Token OAuth2 obtenido exitosamente\") # Usar token para llamadas a la API headers = { 'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json' } # Ejemplo: obtener datos de usuarios users_response = requests.get( f\"{api_base_url}/users\", headers=headers ) if users_response.status_code == 200: users_data = users_response.json() logging.info(f\"Obtenidos {len(users_data)} usuarios de la API\") # Ejemplo: crear un nuevo recurso new_resource = { 'name': 'Data Pipeline Resource', 'type': 'automated', 'created_by': 'airflow' } create_response = requests.post( f\"{api_base_url}/resources\", headers=headers, json=new_resource ) if create_response.status_code == 201: created_resource = create_response.json() logging.info(f\"Recurso creado con ID: {created_resource.get('id')}\") return { \"status\": \"success\", \"users_count\": len(users_data), \"resource_created\": create_response.status_code == 201 } else: raise Exception(f\"Error en API call: {users_response.text}\") except Exception as e: logging.error(f\"Error en autenticaci\u00f3n OAuth2: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'api_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=2) } dag = DAG( 'oauth2_api_integration', default_args=default_args, description='Integraci\u00f3n con API externa usando OAuth2', schedule_interval='@daily', catchup=False, tags=['api', 'oauth2', 'integration'] ) oauth2_task = PythonOperator( task_id='authenticate_and_call_external_api', python_callable=authenticate_and_call_api, dag=dag ) Usar roles IAM en AWS para acceso a buckets S3 sin credenciales expl\u00edcitas. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.amazon.aws.hooks.s3 import S3Hook import boto3 from botocore.exceptions import ClientError, NoCredentialsError import pandas as pd from datetime import datetime, timedelta import logging import io def process_s3_data_with_iam_role(**context): \"\"\" Funci\u00f3n que accede a S3 usando roles IAM sin credenciales expl\u00edcitas \"\"\" try: # Usar S3Hook de Airflow (utiliza roles IAM autom\u00e1ticamente) s3_hook = S3Hook(aws_conn_id='aws_default') # Configuraci\u00f3n de bucket y prefijos bucket_name = 'your-data-bucket' input_prefix = 'raw-data/' output_prefix = 'processed-data/' # Listar objetos en el bucket objects = s3_hook.list_keys( bucket_name=bucket_name, prefix=input_prefix ) logging.info(f\"Encontrados {len(objects)} objetos en {bucket_name}/{input_prefix}\") # Procesar archivos CSV processed_files = [] for obj_key in objects: if obj_key.endswith('.csv'): try: # Leer archivo CSV desde S3 csv_content = s3_hook.read_key( key=obj_key, bucket_name=bucket_name ) # Procesar con pandas df = pd.read_csv(io.StringIO(csv_content)) # Realizar transformaciones (ejemplo) df_processed = df.copy() df_processed['processed_date'] = datetime.now().strftime('%Y-%m-%d') df_processed['record_count'] = len(df) # Guardar archivo procesado de vuelta a S3 output_key = obj_key.replace(input_prefix, output_prefix).replace('.csv', '_processed.csv') csv_buffer = io.StringIO() df_processed.to_csv(csv_buffer, index=False) s3_hook.load_string( string_data=csv_buffer.getvalue(), key=output_key, bucket_name=bucket_name, replace=True ) logging.info(f\"Procesado: {obj_key} -> {output_key}\") processed_files.append({ 'input_file': obj_key, 'output_file': output_key, 'records_processed': len(df), 'file_size_bytes': len(csv_content) }) except Exception as file_error: logging.error(f\"Error procesando {obj_key}: {str(file_error)}\") continue # Crear reporte de procesamiento report_data = { 'processing_date': datetime.now().isoformat(), 'bucket': bucket_name, 'total_files_processed': len(processed_files), 'files_detail': processed_files } # Guardar reporte en S3 report_key = f\"reports/processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\" s3_hook.load_string( string_data=json.dumps(report_data, indent=2), key=report_key, bucket_name=bucket_name, replace=True ) return { 'status': 'success', 'files_processed': len(processed_files), 'report_location': f's3://{bucket_name}/{report_key}' } except NoCredentialsError: logging.error(\"No se encontraron credenciales AWS. Verificar configuraci\u00f3n de roles IAM.\") raise except ClientError as e: logging.error(f\"Error de cliente AWS: {e.response['Error']['Message']}\") raise except Exception as e: logging.error(f\"Error general procesando datos S3: {str(e)}\") raise def verify_iam_permissions(**context): \"\"\" Funci\u00f3n auxiliar para verificar permisos IAM \"\"\" try: # Crear cliente STS para verificar identidad sts_client = boto3.client('sts') identity = sts_client.get_caller_identity() logging.info(f\"Ejecutando como: {identity.get('Arn')}\") logging.info(f\"Account ID: {identity.get('Account')}\") # Verificar acceso b\u00e1sico a S3 s3_client = boto3.client('s3') response = s3_client.list_buckets() logging.info(f\"Acceso a S3 verificado. Buckets accesibles: {len(response['Buckets'])}\") return { 'iam_role': identity.get('Arn'), 'account_id': identity.get('Account'), 's3_access': True } except Exception as e: logging.error(f\"Error verificando permisos IAM: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'data_engineering', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 's3_iam_role_processing', default_args=default_args, description='Procesamiento de datos S3 usando roles IAM', schedule_interval='@daily', catchup=False, tags=['aws', 's3', 'iam', 'data-processing'] ) # Tasks del pipeline verify_permissions_task = PythonOperator( task_id='verify_iam_permissions', python_callable=verify_iam_permissions, dag=dag ) process_s3_data_task = PythonOperator( task_id='process_s3_data_with_iam', python_callable=process_s3_data_with_iam_role, dag=dag ) # Definir dependencias verify_permissions_task >> process_s3_data_task Tarea Desarrolla los siguientes ejercicios pr\u00e1cticos relacionados con el tema 3.2 : Crea un pipeline en Apache Spark que lea datos de una base de datos PostgreSQL usando JDBC y los almacene como archivos Parquet. Implementa un DAG en Airflow que consuma una API REST con HttpHook y almacene los resultados en un archivo JSON en S3. Compara el rendimiento entre una lectura completa y una lectura incremental sobre una tabla con campo updated_at en MySQL. Configura una conexi\u00f3n segura en Airflow utilizando variables y crea un DAG que lea de MongoDB. Dise\u00f1a un pipeline en Spark que combine datos de una API y archivos CSV, y cargue el resultado en un Delta Lake particionado por fecha.","title":"Conexi\u00f3n a M\u00faltiples Fuentes de Datos"},{"location":"tema32/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema32/#tema-32-conexion-a-multiples-fuentes-de-datos","text":"Objetivo : Comprender c\u00f3mo integrar m\u00faltiples tipos de fuentes de datos \u2014estructuradas, semiestructuradas y no estructuradas\u2014 en un pipeline ETL, utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye el uso de conectores apropiados, estrategias de autenticaci\u00f3n segura, y t\u00e9cnicas de lectura eficientes. Introducci\u00f3n : En entornos Big Data, los datos se originan desde diversas fuentes con distintos formatos, velocidades y estructuras. La capacidad de extraer datos de m\u00faltiples or\u00edgenes y consolidarlos de manera eficiente en un pipeline ETL es clave para cualquier sistema anal\u00edtico moderno. Este tema explora las fuentes de datos m\u00e1s comunes y c\u00f3mo integrarlas usando tecnolog\u00edas como Spark y Airflow. Desarrollo : La integraci\u00f3n de m\u00faltiples fuentes en un pipeline ETL implica tanto desaf\u00edos t\u00e9cnicos como de gobernanza. Cada tipo de fuente \u2014bases de datos, archivos, APIs\u2014 requiere t\u00e9cnicas y conectores espec\u00edficos. Adem\u00e1s, aspectos como la autenticaci\u00f3n segura, la lectura incremental y el manejo de configuraciones son fundamentales para construir flujos de datos robustos y escalables. Spark y Airflow ofrecen mecanismos potentes para esta integraci\u00f3n, facilitando el desarrollo de pipelines mantenibles y eficientes.","title":"Tema 3.2. Conexi\u00f3n a M\u00faltiples Fuentes de Datos"},{"location":"tema32/#321-tipos-de-fuentes-comunes","text":"La diversidad de fuentes de datos implica conocer sus caracter\u00edsticas y particularidades para una integraci\u00f3n adecuada en el pipeline. Cada tipo de fuente presenta desaf\u00edos \u00fanicos en t\u00e9rminos de conectividad, formato de datos, escalabilidad y optimizaci\u00f3n de rendimiento.","title":"3.2.1 Tipos de fuentes comunes"},{"location":"tema32/#bases-de-datos-relacionales-postgresql-mysql","text":"Las bases de datos relacionales constituyen el backbone de muchas organizaciones, almacenando datos estructurados con esquemas bien definidos y relaciones entre tablas. Estas fuentes requieren conexi\u00f3n v\u00eda JDBC (Java Database Connectivity) y ofrecen la ventaja de poder ejecutar consultas SQL complejas directamente en la fuente, reduciendo la transferencia de datos innecesarios. Para una integraci\u00f3n efectiva, es crucial considerar aspectos como la partici\u00f3n de datos, el control de paralelismo para evitar sobrecargar la base de datos origen, y la gesti\u00f3n de transacciones. Spark permite configurar par\u00e1metros como numPartitions , lowerBound , upperBound y partitionColumn para optimizar la lectura distribuida. Usar JDBC en Spark para leer de PostgreSQL y transformar los resultados : from pyspark.sql import SparkSession # Configuraci\u00f3n de Spark con driver JDBC spark = SparkSession.builder \\ .appName(\"PostgreSQL_ETL\") \\ .config(\"spark.jars\", \"/path/to/postgresql-42.6.0.jar\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n jdbc_url = \"jdbc:postgresql://localhost:5432/production_db\" connection_properties = { \"user\": \"spark_user\", \"password\": \"secure_password\", \"driver\": \"org.postgresql.Driver\", \"numPartitions\": \"4\", \"partitionColumn\": \"id\", \"lowerBound\": \"1\", \"upperBound\": \"1000000\" } # Lectura optimizada con particionamiento df = spark.read \\ .jdbc(url=jdbc_url, table=\"(SELECT * FROM sales WHERE created_date >= '2024-01-01') as sales_subset\", properties=connection_properties) # Transformaciones sobre los datos df_transformed = df \\ .filter(df.amount > 100) \\ .groupBy(\"customer_id\", \"product_category\") \\ .agg( sum(\"amount\").alias(\"total_sales\"), count(\"order_id\").alias(\"order_count\"), avg(\"amount\").alias(\"avg_order_value\") ) # Escritura a formato optimizado df_transformed.write \\ .mode(\"overwrite\") \\ .parquet(\"s3://data-lake/processed/sales_summary/\") Ejecutar consultas filtradas desde Airflow para limitar la carga de datos : from airflow import DAG from airflow.providers.postgres.operators.postgres import PostgresOperator from airflow.providers.spark.operators.spark_submit import SparkSubmitOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from datetime import datetime, timedelta default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'postgresql_incremental_etl', default_args=default_args, description='ETL incremental desde PostgreSQL', schedule_interval='@daily', catchup=False ) # Operador para extraer datos incrementales extract_incremental_data = PostgresOperator( task_id='extract_incremental_data', postgres_conn_id='postgres_production', sql=\"\"\" CREATE TEMP TABLE temp_incremental_data AS SELECT customer_id, order_date, product_id, quantity, unit_price, total_amount FROM orders WHERE order_date >= '{{ ds }}' AND order_date < '{{ next_ds }}' AND status = 'completed'; COPY temp_incremental_data TO '/tmp/incremental_data_{{ ds }}.csv' WITH CSV HEADER; \"\"\", dag=dag ) # Funci\u00f3n personalizada para transferir datos def transfer_to_spark(**context): postgres_hook = PostgresHook(postgres_conn_id='postgres_production') # Consulta optimizada con l\u00edmites sql_query = f\"\"\" SELECT * FROM orders WHERE order_date = '{context['ds']}' AND total_amount > 0 ORDER BY order_id LIMIT 100000 \"\"\" # Exportar a formato intermedio df = postgres_hook.get_pandas_df(sql_query) df.to_parquet(f\"/tmp/orders_{context['ds']}.parquet\") return f\"/tmp/orders_{context['ds']}.parquet\" transfer_task = PythonOperator( task_id='transfer_to_staging', python_callable=transfer_to_spark, dag=dag ) extract_incremental_data >> transfer_task","title":"Bases de datos relacionales (PostgreSQL, MySQL)"},{"location":"tema32/#almacenes-nosql-mongodb-cassandra","text":"Los almacenes NoSQL ofrecen flexibilidad de esquema y est\u00e1n dise\u00f1ados para manejar grandes vol\u00famenes de datos semiestructurados con alta disponibilidad y escalabilidad horizontal. MongoDB es ideal para documentos JSON complejos, mientras que Cassandra sobresale en casos de uso que requieren escrituras masivas y alta disponibilidad. La integraci\u00f3n con estos sistemas requiere conectores espec\u00edficos y consideraciones especiales para el particionamiento y la distribuci\u00f3n de carga. Es importante entender los patrones de acceso a datos y las limitaciones de cada sistema para optimizar las consultas. Ingestar documentos de MongoDB con Spark para an\u00e1lisis de logs : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n de Spark con conector MongoDB spark = SparkSession.builder \\ .appName(\"MongoDB_Log_Analysis\") \\ .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/logs.application_logs\") \\ .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/analytics.processed_logs\") \\ .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\ .getOrCreate() # Lectura de documentos con filtros a nivel de MongoDB df_logs = spark.read \\ .format(\"mongo\") \\ .option(\"collection\", \"application_logs\") \\ .option(\"pipeline\", '[{\"$match\": {\"timestamp\": {\"$gte\": {\"$date\": \"2024-01-01T00:00:00Z\"}}}}]') \\ .load() # An\u00e1lisis de logs con transformaciones complejas df_analyzed = df_logs \\ .withColumn(\"hour\", hour(col(\"timestamp\"))) \\ .withColumn(\"log_level\", upper(col(\"level\"))) \\ .withColumn(\"response_time_ms\", col(\"response_time\").cast(\"integer\")) \\ .filter(col(\"response_time_ms\").isNotNull()) \\ .groupBy(\"hour\", \"log_level\", \"service_name\") \\ .agg( count(\"*\").alias(\"log_count\"), avg(\"response_time_ms\").alias(\"avg_response_time\"), max(\"response_time_ms\").alias(\"max_response_time\"), countDistinct(\"user_id\").alias(\"unique_users\") ) # Detecci\u00f3n de anomal\u00edas df_anomalies = df_analyzed \\ .filter( (col(\"avg_response_time\") > 5000) | (col(\"log_level\") == \"ERROR\") | (col(\"log_count\") > 10000) ) # Escritura de resultados procesados df_analyzed.write \\ .format(\"mongo\") \\ .option(\"collection\", \"hourly_metrics\") \\ .mode(\"overwrite\") \\ .save() df_anomalies.write \\ .format(\"mongo\") \\ .option(\"collection\", \"detected_anomalies\") \\ .mode(\"append\") \\ .save() Usar Cassandra para almacenar datos de sensores IoT y analizarlos por lotes : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n para Cassandra spark = SparkSession.builder \\ .appName(\"IoT_Sensor_Analytics\") \\ .config(\"spark.cassandra.connection.host\", \"cassandra-cluster.example.com\") \\ .config(\"spark.cassandra.connection.port\", \"9042\") \\ .config(\"spark.cassandra.auth.username\", \"spark_user\") \\ .config(\"spark.cassandra.auth.password\", \"secure_password\") \\ .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\ .getOrCreate() # Lectura de datos de sensores con filtros optimizados df_sensor_data = spark.read \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"sensor_readings\", keyspace=\"iot_data\") \\ .option(\"spark.cassandra.input.split.size_in_mb\", \"512\") \\ .load() \\ .filter(col(\"timestamp\") >= lit(\"2024-01-01\")) \\ .filter(col(\"sensor_type\").isin([\"temperature\", \"humidity\", \"pressure\"])) # An\u00e1lisis de tendencias y agregaciones df_hourly_aggregates = df_sensor_data \\ .withColumn(\"hour_bucket\", date_trunc(\"hour\", col(\"timestamp\"))) \\ .groupBy(\"device_id\", \"sensor_type\", \"location\", \"hour_bucket\") \\ .agg( avg(\"value\").alias(\"avg_value\"), min(\"value\").alias(\"min_value\"), max(\"value\").alias(\"max_value\"), stddev(\"value\").alias(\"stddev_value\"), count(\"*\").alias(\"reading_count\") ) # Detecci\u00f3n de valores an\u00f3malos usando ventanas from pyspark.sql.window import Window window_spec = Window.partitionBy(\"device_id\", \"sensor_type\").orderBy(\"hour_bucket\") df_with_trends = df_hourly_aggregates \\ .withColumn(\"prev_avg\", lag(\"avg_value\").over(window_spec)) \\ .withColumn(\"value_change\", col(\"avg_value\") - col(\"prev_avg\")) \\ .withColumn(\"is_anomaly\", when(abs(col(\"value_change\")) > 3 * col(\"stddev_value\"), True) .otherwise(False)) # Escritura optimizada a Cassandra con TTL df_hourly_aggregates.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"hourly_sensor_metrics\", keyspace=\"iot_analytics\") \\ .option(\"spark.cassandra.output.ttl\", \"2592000\") \\ .mode(\"append\") \\ .save() # Almacenar anomal\u00edas para alertas df_anomalies = df_with_trends.filter(col(\"is_anomaly\") == True) df_anomalies.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .options(table=\"sensor_anomalies\", keyspace=\"iot_alerts\") \\ .mode(\"append\") \\ .save()","title":"Almacenes NoSQL (MongoDB, Cassandra)"},{"location":"tema32/#archivos-planos-csv-json-parquet","text":"Los archivos planos representan una forma com\u00fan de almacenar y intercambiar datos, especialmente en arquitecturas de data lake. Cada formato tiene sus ventajas: CSV para simplicidad e interoperabilidad, JSON para datos semiestructurados, y Parquet para optimizaci\u00f3n de almacenamiento y consultas anal\u00edticas. Spark proporciona APIs nativas optimizadas para estos formatos, con capacidades avanzadas como lectura schema-on-read, particionamiento autom\u00e1tico, y optimizaciones de predicados pushdown en el caso de Parquet. Leer archivos CSV con Spark y aplicar transformaciones : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder \\ .appName(\"CSV_Processing_Pipeline\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Definici\u00f3n de schema para optimizar la lectura sales_schema = StructType([ StructField(\"transaction_id\", StringType(), False), StructField(\"customer_id\", IntegerType(), True), StructField(\"product_id\", StringType(), True), StructField(\"quantity\", IntegerType(), True), StructField(\"unit_price\", DecimalType(10,2), True), StructField(\"discount\", DecimalType(5,2), True), StructField(\"transaction_date\", DateType(), True), StructField(\"store_location\", StringType(), True) ]) # Lectura optimizada de m\u00faltiples archivos CSV df_sales = spark.read \\ .option(\"header\", \"true\") \\ .option(\"inferSchema\", \"false\") \\ .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\ .option(\"multiline\", \"true\") \\ .option(\"escape\", '\"') \\ .schema(sales_schema) \\ .csv(\"s3://data-lake/raw/sales/2024/*/sales_*.csv\") # Limpieza y validaci\u00f3n de datos df_cleaned = df_sales \\ .filter(col(\"quantity\") > 0) \\ .filter(col(\"unit_price\") > 0) \\ .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\") * (1 - col(\"discount\")/100)) \\ .withColumn(\"year_month\", date_format(col(\"transaction_date\"), \"yyyy-MM\")) \\ .withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\"))) \\ .filter(col(\"total_amount\").isNotNull()) # Agregaciones para an\u00e1lisis de ventas df_sales_summary = df_cleaned \\ .groupBy(\"year_month\", \"store_location\", \"day_of_week\") \\ .agg( sum(\"total_amount\").alias(\"total_revenue\"), sum(\"quantity\").alias(\"total_units_sold\"), countDistinct(\"customer_id\").alias(\"unique_customers\"), avg(\"total_amount\").alias(\"avg_transaction_value\"), count(\"transaction_id\").alias(\"transaction_count\") ) \\ .withColumn(\"revenue_per_customer\", col(\"total_revenue\") / col(\"unique_customers\")) # Escritura particionada para optimizar consultas futuras df_sales_summary.write \\ .partitionBy(\"year_month\") \\ .mode(\"overwrite\") \\ .option(\"compression\", \"snappy\") \\ .parquet(\"s3://data-lake/processed/sales_summary/\") # Cache para reutilizaci\u00f3n en m\u00faltiples an\u00e1lisis df_cleaned.cache() # An\u00e1lisis adicional: productos top por ubicaci\u00f3n df_top_products = df_cleaned \\ .groupBy(\"store_location\", \"product_id\") \\ .agg(sum(\"quantity\").alias(\"total_sold\")) \\ .withColumn(\"rank\", row_number().over( Window.partitionBy(\"store_location\") .orderBy(desc(\"total_sold\")) )) \\ .filter(col(\"rank\") <= 10) df_top_products.write \\ .mode(\"overwrite\") \\ .json(\"s3://data-lake/analytics/top_products_by_location/\") Ingerir JSON de logs web para an\u00e1lisis de tr\u00e1fico : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * spark = SparkSession.builder \\ .appName(\"Web_Logs_Traffic_Analysis\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .getOrCreate() # Lectura de logs JSON con schema flexible df_logs = spark.read \\ .option(\"multiline\", \"true\") \\ .option(\"mode\", \"PERMISSIVE\") \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .json(\"s3://logs-bucket/web-logs/2024/*/*/*.json\") # Extracci\u00f3n y procesamiento de campos anidados df_processed = df_logs \\ .withColumn(\"timestamp\", to_timestamp(col(\"@timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")) \\ .withColumn(\"ip_address\", col(\"clientip\")) \\ .withColumn(\"user_agent\", col(\"agent\")) \\ .withColumn(\"request_method\", regexp_extract(col(\"request\"), r'^(\\w+)', 1)) \\ .withColumn(\"request_url\", regexp_extract(col(\"request\"), r'^\\w+\\s+([^\\s]+)', 1)) \\ .withColumn(\"response_code\", col(\"response\").cast(\"integer\")) \\ .withColumn(\"response_size\", col(\"bytes\").cast(\"long\")) \\ .withColumn(\"referrer\", col(\"referrer\")) \\ .withColumn(\"hour\", hour(col(\"timestamp\"))) \\ .withColumn(\"date\", to_date(col(\"timestamp\"))) \\ .filter(col(\"_corrupt_record\").isNull()) \\ .drop(\"_corrupt_record\") # An\u00e1lisis de tr\u00e1fico web df_traffic_analysis = df_processed \\ .groupBy(\"date\", \"hour\", \"request_method\") \\ .agg( count(\"*\").alias(\"request_count\"), countDistinct(\"ip_address\").alias(\"unique_visitors\"), avg(\"response_size\").alias(\"avg_response_size\"), sum(\"response_size\").alias(\"total_bytes_served\"), sum(when(col(\"response_code\") >= 400, 1).otherwise(0)).alias(\"error_count\"), sum(when(col(\"response_code\") == 200, 1).otherwise(0)).alias(\"success_count\") ) \\ .withColumn(\"error_rate\", col(\"error_count\") / col(\"request_count\") * 100) \\ .withColumn(\"success_rate\", col(\"success_count\") / col(\"request_count\") * 100) # An\u00e1lisis de p\u00e1ginas m\u00e1s visitadas df_popular_pages = df_processed \\ .filter(col(\"request_method\") == \"GET\") \\ .filter(col(\"response_code\") == 200) \\ .groupBy(\"date\", \"request_url\") \\ .agg( count(\"*\").alias(\"page_views\"), countDistinct(\"ip_address\").alias(\"unique_visitors\") ) \\ .withColumn(\"rank\", row_number().over( Window.partitionBy(\"date\") .orderBy(desc(\"page_views\")) )) # Detecci\u00f3n de posibles ataques o comportamientos an\u00f3malos df_security_analysis = df_processed \\ .groupBy(\"ip_address\", \"date\") \\ .agg( count(\"*\").alias(\"requests_per_day\"), countDistinct(\"request_url\").alias(\"unique_pages_accessed\"), sum(when(col(\"response_code\") >= 400, 1).otherwise(0)).alias(\"failed_requests\") ) \\ .withColumn(\"suspicious_activity\", when( (col(\"requests_per_day\") > 10000) | (col(\"failed_requests\") > 100) | (col(\"unique_pages_accessed\") < 2), True ).otherwise(False)) # Escritura de resultados en diferentes formatos df_traffic_analysis.write \\ .partitionBy(\"date\") \\ .mode(\"overwrite\") \\ .parquet(\"s3://analytics-bucket/web-traffic-analysis/\") df_popular_pages.filter(col(\"rank\") <= 50).write \\ .partitionBy(\"date\") \\ .mode(\"overwrite\") \\ .json(\"s3://analytics-bucket/popular-pages/\") df_security_analysis.filter(col(\"suspicious_activity\") == True).write \\ .mode(\"append\") \\ .option(\"compression\", \"gzip\") \\ .csv(\"s3://security-bucket/suspicious-activity/\", header=True)","title":"Archivos planos (CSV, JSON, Parquet)"},{"location":"tema32/#apis-y-servicios-web","text":"Las APIs y servicios web permiten acceder a datos en tiempo real o expuestos por terceros, proporcionando informaci\u00f3n din\u00e1mica y actualizada. La integraci\u00f3n requiere manejo de autenticaci\u00f3n, rate limiting, paginaci\u00f3n, y gesti\u00f3n de errores. Airflow es ideal para orquestar estas integraciones mediante HttpHook, mientras que Spark puede procesar los datos obtenidos. Obtener tasas de cambio de divisas desde una API REST : from airflow import DAG from airflow.providers.http.operators.http import SimpleHttpOperator from airflow.providers.http.hooks.http import HttpHook from airflow.operators.python import PythonOperator from airflow.providers.postgres.operators.postgres import PostgresOperator from datetime import datetime, timedelta import json import pandas as pd # Configuraci\u00f3n del DAG default_args = { 'owner': 'finance-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'currency_exchange_etl', default_args=default_args, description='ETL para tasas de cambio de divisas', schedule_interval=timedelta(hours=1), catchup=False ) # Funci\u00f3n para extraer datos de API con manejo de errores def extract_exchange_rates(**context): http_hook = HttpHook(http_conn_id='exchange_api', method='GET') # Configuraci\u00f3n de la petici\u00f3n endpoint = 'v1/latest' headers = { 'Authorization': 'Bearer {{ var.value.exchange_api_key }}', 'Content-Type': 'application/json' } params = { 'base': 'USD', 'symbols': 'EUR,GBP,JPY,CAD,AUD,CHF,CNY,MXN,BRL' } try: # Realizar petici\u00f3n HTTP response = http_hook.run( endpoint=endpoint, headers=headers, data=params ) exchange_data = json.loads(response.content) # Validar respuesta if 'rates' not in exchange_data: raise ValueError(\"API response missing 'rates' field\") # Transformar datos para almacenamiento rates_list = [] base_currency = exchange_data.get('base', 'USD') timestamp = exchange_data.get('timestamp', context['ts']) for currency, rate in exchange_data['rates'].items(): rates_list.append({ 'base_currency': base_currency, 'target_currency': currency, 'exchange_rate': float(rate), 'timestamp': timestamp, 'source': 'exchangerates-api', 'extraction_date': context['ds'] }) # Guardar en archivo temporal para siguiente tarea df = pd.DataFrame(rates_list) temp_file = f\"/tmp/exchange_rates_{context['ds']}.json\" df.to_json(temp_file, orient='records', date_format='iso') return temp_file except Exception as e: print(f\"Error extracting exchange rates: {str(e)}\") raise # Funci\u00f3n para procesar y validar datos def process_exchange_data(**context): temp_file = context['task_instance'].xcom_pull(task_ids='extract_rates') if not temp_file: raise ValueError(\"No data file received from extraction task\") # Cargar y procesar datos df = pd.read_json(temp_file) # Validaciones de calidad de datos df = df.dropna(subset=['exchange_rate']) df = df[df['exchange_rate'] > 0] # Eliminar tasas negativas o cero # Detecci\u00f3n de valores an\u00f3malos (cambios > 10% desde \u00faltima lectura) postgres_hook = PostgresHook(postgres_conn_id='postgres_warehouse') for currency in df['target_currency'].unique(): last_rate_query = f\"\"\" SELECT exchange_rate FROM exchange_rates WHERE target_currency = '{currency}' AND base_currency = 'USD' ORDER BY timestamp DESC LIMIT 1 \"\"\" last_rate = postgres_hook.get_first(last_rate_query) if last_rate: current_rate = df[df['target_currency'] == currency]['exchange_rate'].iloc[0] change_pct = abs((current_rate - last_rate[0]) / last_rate[0] * 100) if change_pct > 10: print(f\"ALERT: {currency} rate changed by {change_pct:.2f}%\") # Guardar datos procesados processed_file = f\"/tmp/processed_rates_{context['ds']}.csv\" df.to_csv(processed_file, index=False) return processed_file # Tareas del DAG extract_rates = PythonOperator( task_id='extract_rates', python_callable=extract_exchange_rates, dag=dag ) process_rates = PythonOperator( task_id='process_rates', python_callable=process_exchange_data, dag=dag ) # Cargar datos a PostgreSQL load_to_db = PostgresOperator( task_id='load_to_database', postgres_conn_id='postgres_warehouse', sql=\"\"\" INSERT INTO exchange_rates (base_currency, target_currency, exchange_rate, timestamp, source, extraction_date) SELECT base_currency, target_currency, exchange_rate, TO_TIMESTAMP(timestamp), source, TO_DATE(extraction_date, 'YYYY-MM-DD') FROM temp_exchange_staging ON CONFLICT (base_currency, target_currency, DATE(timestamp)) DO UPDATE SET exchange_rate = EXCLUDED.exchange_rate, timestamp = EXCLUDED.timestamp; \"\"\", dag=dag ) # Definir dependencias extract_rates >> process_rates >> load_to_db Ingerir datos meteorol\u00f3gicos en tiempo real para an\u00e1lisis predictivo : from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.providers.spark.operators.spark_submit import SparkSubmitOperator from datetime import datetime, timedelta import requests import json import boto3 default_args = { 'owner': 'weather-analytics', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } dag = DAG( 'weather_data_pipeline', default_args=default_args, description='Pipeline de datos meteorol\u00f3gicos en tiempo real', schedule_interval=timedelta(minutes=15), catchup=False ) def fetch_weather_data(**context): \"\"\"Extrae datos meteorol\u00f3gicos de m\u00faltiples APIs\"\"\" # Lista de ciudades para monitorear cities = [ {'name': 'New York', 'lat': 40.7128, 'lon': -74.0060}, {'name': 'London', 'lat': 51.5074, 'lon': -0.1278}, {'name': 'Tokyo', 'lat': 35.6762, 'lon': 139.6503}, {'name': 'Sydney', 'lat': -33.8688, 'lon': 151.2093} ] weather_data = [] api_key = \"{{ var.value.openweather_api_key }}\" for city in cities: try: # Datos meteorol\u00f3gicos actuales current_url = f\"https://api.openweathermap.org/data/2.5/weather\" current_params = { 'lat': city['lat'], 'lon': city['lon'], 'appid': api_key, 'units': 'metric' } current_response = requests.get(current_url, params=current_params) current_response.raise_for_status() current_data = current_response.json() # Pron\u00f3stico extendido forecast_url = f\"https://api.openweathermap.org/data/2.5/forecast\" forecast_response = requests.get(forecast_url, params=current_params) forecast_response.raise_for_status() forecast_data = forecast_response.json() # Estructurar datos actuales current_record = { 'city_name': city['name'], 'latitude': city['lat'], 'longitude': city['lon'], 'timestamp': context['ts'], 'temperature': current_data['main']['temp'], 'feels_like': current_data['main']['feels_like'], 'humidity': current_data['main']['humidity'], 'pressure': current_data['main']['pressure'], 'wind_speed': current_data.get('wind', {}).get('speed', 0), 'wind_direction': current_data.get('wind', {}).get('deg', 0), 'cloud_coverage': current_data['clouds']['all'], 'weather_condition': current_data['weather'][0]['main'], 'weather_description': current_data['weather'][0]['description'], 'visibility': current_data.get('visibility', 0), 'data_type': 'current' } weather_data.append(current_record) # Procesar datos de pron\u00f3stico for forecast_item in forecast_data['list'][:8]: # Pr\u00f3ximas 24 horas forecast_record = { 'city_name': city['name'], 'latitude': city['lat'], 'longitude': city['lon'], 'timestamp': forecast_item['dt_txt'], 'temperature': forecast_item['main']['temp'], 'feels_like': forecast_item['main']['feels_like'], 'humidity': forecast_item['main']['humidity'], 'pressure': forecast_item['main']['pressure'], 'wind_speed': forecast_item.get('wind', {}).get('speed', 0), 'wind_direction': forecast_item.get('wind', {}).get('deg', 0), 'cloud_coverage': forecast_item['clouds']['all'], 'weather_condition': forecast_item['weather'][0]['main'], 'weather_description': forecast_item['weather'][0]['description'], 'visibility': forecast_item.get('visibility', 0), 'data_type': 'forecast', 'precipitation_probability': forecast_item.get('pop', 0) * 100 } weather_data.append(forecast_record) except requests.exceptions.RequestException as e: print(f\"Error fetching data for {city['name']}: {str(e)}\") continue except KeyError as e: print(f\"Missing data field for {city['name']}: {str(e)}\") continue # Guardar datos en S3 para procesamiento con Spark s3_client = boto3.client('s3') file_key = f\"weather-data/raw/{context['ds']}/{context['ts']}/weather_data.json\" s3_client.put_object( Bucket='weather-data-lake', Key=file_key, Body=json.dumps(weather_data, indent=2), ContentType='application/json' ) return f\"s3://weather-data-lake/{file_key}\" # Funci\u00f3n para procesar datos con Spark def create_spark_weather_analysis(): \"\"\"Script de Spark para an\u00e1lisis predictivo de datos meteorol\u00f3gicos\"\"\" spark_script = \"\"\" from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * from pyspark.ml.feature import VectorAssembler from pyspark.ml.regression import RandomForestRegressor from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml import Pipeline # Configuraci\u00f3n de Spark spark = SparkSession.builder \\\\ .appName(\"Weather_Predictive_Analytics\") \\\\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\\\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\\ .getOrCreate() # Schema para datos meteorol\u00f3gicos weather_schema = StructType([ StructField(\"city_name\", StringType(), True), StructField(\"latitude\", DoubleType(), True), StructField(\"longitude\", DoubleType(), True), StructField(\"timestamp\", StringType(), True), StructField(\"temperature\", DoubleType(), True), StructField(\"feels_like\", DoubleType(), True), StructField(\"humidity\", IntegerType(), True), StructField(\"pressure\", DoubleType(), True), StructField(\"wind_speed\", DoubleType(), True), StructField(\"wind_direction\", DoubleType(), True), StructField(\"cloud_coverage\", IntegerType(), True), StructField(\"weather_condition\", StringType(), True), StructField(\"weather_description\", StringType(), True), StructField(\"visibility\", DoubleType(), True), StructField(\"data_type\", StringType(), True), StructField(\"precipitation_probability\", DoubleType(), True) ]) # Lectura de datos actuales y hist\u00f3ricos df_current = spark.read \\\\ .schema(weather_schema) \\\\ .json(\"s3://weather-data-lake/weather-data/raw/*/*/weather_data.json\") # Procesamiento temporal y feature engineering df_processed = df_current \\\\ .withColumn(\"timestamp_parsed\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\\ .withColumn(\"hour\", hour(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"day_of_week\", dayofweek(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"month\", month(col(\"timestamp_parsed\"))) \\\\ .withColumn(\"season\", when(col(\"month\").isin([12, 1, 2]), \"winter\") .when(col(\"month\").isin([3, 4, 5]), \"spring\") .when(col(\"month\").isin([6, 7, 8]), \"summer\") .otherwise(\"autumn\")) \\\\ .withColumn(\"temp_humidity_ratio\", col(\"temperature\") / col(\"humidity\")) \\\\ .withColumn(\"pressure_normalized\", (col(\"pressure\") - 1013.25) / 50) \\\\ .withColumn(\"wind_pressure_interaction\", col(\"wind_speed\") * col(\"pressure_normalized\")) # Crear caracter\u00edsticas de ventana temporal para tendencias window_spec = Window.partitionBy(\"city_name\").orderBy(\"timestamp_parsed\") df_with_trends = df_processed \\\\ .withColumn(\"temp_lag_1h\", lag(\"temperature\", 1).over(window_spec)) \\\\ .withColumn(\"temp_lag_3h\", lag(\"temperature\", 3).over(window_spec)) \\\\ .withColumn(\"pressure_lag_1h\", lag(\"pressure\", 1).over(window_spec)) \\\\ .withColumn(\"temp_change_1h\", col(\"temperature\") - col(\"temp_lag_1h\")) \\\\ .withColumn(\"temp_change_3h\", col(\"temperature\") - col(\"temp_lag_3h\")) \\\\ .withColumn(\"pressure_change_1h\", col(\"pressure\") - col(\"pressure_lag_1h\")) # Filtrar datos v\u00e1lidos para el modelo df_model_ready = df_with_trends \\\\ .filter(col(\"data_type\") == \"current\") \\\\ .filter(col(\"temp_lag_1h\").isNotNull()) \\\\ .filter(col(\"temperature\").between(-50, 60)) \\\\ .filter(col(\"humidity\").between(0, 100)) # Preparar features para modelo predictivo feature_cols = [ \"latitude\", \"longitude\", \"hour\", \"day_of_week\", \"month\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_direction\", \"cloud_coverage\", \"visibility\", \"temp_lag_1h\", \"temp_lag_3h\", \"pressure_lag_1h\", \"temp_change_1h\", \"pressure_change_1h\", \"temp_humidity_ratio\", \"pressure_normalized\", \"wind_pressure_interaction\" ] assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\") rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"temperature\", numTrees=100) # Pipeline de ML pipeline = Pipeline(stages=[assembler, rf]) # Divisi\u00f3n de datos para entrenamiento y validaci\u00f3n train_data, test_data = df_model_ready.randomSplit([0.8, 0.2], seed=42) # Entrenar modelo model = pipeline.fit(train_data) # Predicciones y evaluaci\u00f3n predictions = model.transform(test_data) evaluator = RegressionEvaluator(labelCol=\"temperature\", predictionCol=\"prediction\", metricName=\"rmse\") rmse = evaluator.evaluate(predictions) print(f\"Root Mean Square Error: {rmse}\") # An\u00e1lisis de patrones clim\u00e1ticos df_climate_patterns = df_processed \\\\ .groupBy(\"city_name\", \"season\", \"hour\") \\\\ .agg( avg(\"temperature\").alias(\"avg_temperature\"), avg(\"humidity\").alias(\"avg_humidity\"), avg(\"pressure\").alias(\"avg_pressure\"), avg(\"wind_speed\").alias(\"avg_wind_speed\"), max(\"temperature\").alias(\"max_temperature\"), min(\"temperature\").alias(\"min_temperature\"), stddev(\"temperature\").alias(\"temp_volatility\"), count(\"*\").alias(\"observation_count\") ) # Detecci\u00f3n de eventos clim\u00e1ticos extremos df_extreme_events = df_processed \\\\ .withColumn(\"is_extreme_temp\", when((col(\"temperature\") > 35) | (col(\"temperature\") < -10), True) .otherwise(False)) \\\\ .withColumn(\"is_high_wind\", when(col(\"wind_speed\") > 15, True).otherwise(False)) \\\\ .withColumn(\"is_low_pressure\", when(col(\"pressure\") < 1000, True).otherwise(False)) \\\\ .filter((col(\"is_extreme_temp\") == True) | (col(\"is_high_wind\") == True) | (col(\"is_low_pressure\") == True)) # Predicciones futuras para las pr\u00f3ximas 6 horas df_for_prediction = df_processed \\\\ .filter(col(\"data_type\") == \"current\") \\\\ .orderBy(desc(\"timestamp_parsed\")) \\\\ .limit(4) # Una observaci\u00f3n por ciudad future_predictions = model.transform(df_for_prediction.select(*feature_cols + [\"city_name\", \"timestamp_parsed\"])) # Guardar resultados df_climate_patterns.write \\\\ .partitionBy(\"city_name\", \"season\") \\\\ .mode(\"overwrite\") \\\\ .parquet(\"s3://weather-data-lake/analytics/climate_patterns/\") df_extreme_events.write \\\\ .partitionBy(\"city_name\") \\\\ .mode(\"append\") \\\\ .parquet(\"s3://weather-data-lake/analytics/extreme_events/\") future_predictions.select(\"city_name\", \"timestamp_parsed\", \"prediction\") \\\\ .write \\\\ .mode(\"overwrite\") \\\\ .json(\"s3://weather-data-lake/predictions/temperature_forecast/\") # M\u00e9tricas del modelo para monitoreo model_metrics = spark.createDataFrame([ (\"temperature_prediction_rmse\", rmse, \"{{ ds }}\") ], [\"metric_name\", \"metric_value\", \"date\"]) model_metrics.write \\\\ .mode(\"append\") \\\\ .parquet(\"s3://weather-data-lake/model_metrics/\") spark.stop() \"\"\" # Guardar script en S3 s3_client = boto3.client('s3') s3_client.put_object( Bucket='weather-data-lake', Key='scripts/weather_analysis.py', Body=spark_script, ContentType='text/plain' ) return 's3://weather-data-lake/scripts/weather_analysis.py' # Funci\u00f3n para generar alertas basadas en predicciones def generate_weather_alerts(**context): \"\"\"Genera alertas basadas en condiciones meteorol\u00f3gicas extremas\"\"\" s3_client = boto3.client('s3') try: # Leer predicciones m\u00e1s recientes response = s3_client.get_object( Bucket='weather-data-lake', Key='predictions/temperature_forecast/part-00000-*.json' ) predictions_data = json.loads(response['Body'].read()) alerts = [] for prediction in predictions_data: city = prediction['city_name'] predicted_temp = prediction['prediction'] # Generar alertas por temperatura extrema if predicted_temp > 40: alerts.append({ 'city': city, 'alert_type': 'EXTREME_HEAT', 'severity': 'HIGH', 'predicted_temperature': predicted_temp, 'message': f'Temperatura extrema prevista en {city}: {predicted_temp:.1f}\u00b0C' }) elif predicted_temp < -15: alerts.append({ 'city': city, 'alert_type': 'EXTREME_COLD', 'severity': 'HIGH', 'predicted_temperature': predicted_temp, 'message': f'Temperatura extrema fr\u00eda prevista en {city}: {predicted_temp:.1f}\u00b0C' }) # Enviar alertas si existen if alerts: # Aqu\u00ed se podr\u00eda integrar con servicios de notificaci\u00f3n # como SNS, Slack, email, etc. print(f\"WEATHER ALERTS GENERATED: {len(alerts)} alerts\") for alert in alerts: print(f\"ALERT: {alert['message']}\") return alerts except Exception as e: print(f\"Error generating weather alerts: {str(e)}\") return [] # Definici\u00f3n de tareas del DAG fetch_weather = PythonOperator( task_id='fetch_weather_data', python_callable=fetch_weather_data, dag=dag ) create_spark_script = PythonOperator( task_id='create_spark_script', python_callable=create_spark_weather_analysis, dag=dag ) # Tarea de Spark para an\u00e1lisis predictivo weather_analysis = SparkSubmitOperator( task_id='weather_predictive_analysis', application='s3://weather-data-lake/scripts/weather_analysis.py', conn_id='spark_cluster', conf={ 'spark.executor.memory': '4g', 'spark.executor.cores': '2', 'spark.executor.instances': '4', 'spark.sql.adaptive.enabled': 'true', 'spark.sql.adaptive.coalescePartitions.enabled': 'true' }, dag=dag ) generate_alerts = PythonOperator( task_id='generate_weather_alerts', python_callable=generate_weather_alerts, dag=dag ) # Definir dependencias del pipeline fetch_weather >> create_spark_script >> weather_analysis >> generate_alerts Este pipeline demuestra: Extracci\u00f3n de APIs m\u00faltiples : Obtiene datos actuales y pron\u00f3sticos de OpenWeatherMap Procesamiento distribuido : Utiliza Spark para an\u00e1lisis a gran escala Machine Learning : Implementa modelos predictivos con MLlib Feature Engineering : Crea caracter\u00edsticas temporales y combinadas Detecci\u00f3n de anomal\u00edas : Identifica eventos clim\u00e1ticos extremos Almacenamiento optimizado : Usa particionamiento por ciudad y estaci\u00f3n Sistema de alertas : Genera notificaciones basadas en predicciones Monitoreo de modelos : Rastrea m\u00e9tricas de rendimiento del ML La arquitectura permite escalabilidad horizontal, procesamiento en tiempo real, y an\u00e1lisis predictivo avanzado para aplicaciones meteorol\u00f3gicas cr\u00edticas.","title":"APIs y servicios web"},{"location":"tema32/#322-conectores-spark-jdbc-filesource-delta-etc","text":"Apache Spark proporciona una amplia gama de conectores nativos y de terceros que permiten la integraci\u00f3n fluida con diversas fuentes de datos, desde bases de datos relacionales tradicionales hasta sistemas de almacenamiento distribuido modernos. Estos conectores abstraen la complejidad de acceso a datos y proporcionan APIs unificadas para lectura, escritura y procesamiento de datos a gran escala.","title":"3.2.2 Conectores Spark: JDBC, FileSource, Delta, etc."},{"location":"tema32/#conectores-jdbc","text":"Los conectores JDBC de Spark permiten establecer conexiones directas con bases de datos relacionales utilizando drivers est\u00e1ndar JDBC. Estos conectores soportan paralelizaci\u00f3n autom\u00e1tica de consultas mediante particionado de datos, optimizaci\u00f3n de predicados (predicate pushdown) y gesti\u00f3n eficiente de conexiones para maximizar el rendimiento en entornos distribuidos. Aplicaciones : Migraci\u00f3n de datos desde sistemas legacy Integraci\u00f3n con data warehouses tradicionales Sincronizaci\u00f3n incremental de datos transaccionales Ejecuci\u00f3n de consultas anal\u00edticas distribuidas Conectar Spark a una base de datos MySQL para lectura de tablas normalizadas : from pyspark.sql import SparkSession from pyspark.sql.functions import * # Configuraci\u00f3n de Spark con driver MySQL spark = SparkSession.builder \\ .appName(\"MySQL_JDBC_Connector\") \\ .config(\"spark.jars\", \"/path/to/mysql-connector-java-8.0.33.jar\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n JDBC jdbc_url = \"jdbc:mysql://localhost:3306/ecommerce_db\" connection_properties = { \"user\": \"spark_user\", \"password\": \"secure_password\", \"driver\": \"com.mysql.cj.jdbc.Driver\", \"fetchsize\": \"10000\", # Optimizaci\u00f3n de fetch \"numPartitions\": \"8\", # Paralelizaci\u00f3n \"partitionColumn\": \"customer_id\", \"lowerBound\": \"1\", \"upperBound\": \"1000000\" } # Lectura de tabla completa customers_df = spark.read \\ .jdbc(url=jdbc_url, table=\"customers\", properties=connection_properties) # Lectura con particionado personalizado orders_df = spark.read \\ .jdbc(url=jdbc_url, table=\"orders\", column=\"order_date\", lowerBound=\"2023-01-01\", upperBound=\"2024-12-31\", numPartitions=12, # Una partici\u00f3n por mes properties=connection_properties) # Mostrar esquema y datos customers_df.printSchema() customers_df.show(20, truncate=False) Ejecutar una consulta SQL desde Spark para filtrar solo los datos necesarios : # Consulta SQL personalizada con predicado pushdown custom_query = \"\"\" (SELECT c.customer_id, c.customer_name, c.registration_date, o.order_id, o.order_date, o.total_amount, p.product_name, p.category FROM customers c INNER JOIN orders o ON c.customer_id = o.customer_id INNER JOIN order_items oi ON o.order_id = oi.order_id INNER JOIN products p ON oi.product_id = p.product_id WHERE o.order_date >= '2024-01-01' AND p.category IN ('Electronics', 'Books') AND o.total_amount > 100) AS filtered_data \"\"\" # Ejecuci\u00f3n de consulta optimizada filtered_df = spark.read \\ .jdbc(url=jdbc_url, table=custom_query, properties=connection_properties) # Procesamiento adicional en Spark result_df = filtered_df \\ .groupBy(\"customer_id\", \"customer_name\", \"category\") \\ .agg( count(\"order_id\").alias(\"total_orders\"), sum(\"total_amount\").alias(\"total_spent\"), max(\"order_date\").alias(\"last_purchase_date\") ) \\ .filter(col(\"total_orders\") >= 3) \\ .orderBy(desc(\"total_spent\")) result_df.show() # Escritura de resultados de vuelta a MySQL result_df.write \\ .mode(\"overwrite\") \\ .jdbc(url=jdbc_url, table=\"customer_analytics\", properties=connection_properties)","title":"Conectores JDBC"},{"location":"tema32/#filesource-y-formatos-soportados","text":"Spark FileSource es un conector unificado que proporciona acceso eficiente a sistemas de archivos distribuidos como HDFS, Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, y sistemas de archivos locales. Soporta m\u00faltiples formatos de archivo optimizados para big data, incluyendo formatos columnares (Parquet, ORC), formatos de texto (CSV, JSON, XML) y formatos binarios personalizados. Caracter\u00edsticas : Particionado autom\u00e1tico y manual Compresi\u00f3n transparente (gzip, snappy, lz4, brotli) Schema evolution y schema inference Predicate pushdown para formatos columnares Vectorizaci\u00f3n para mejor rendimiento Leer archivos Parquet de S3 con particiones por fecha : from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * # Configuraci\u00f3n optimizada para S3 spark = SparkSession.builder \\ .appName(\"S3_Parquet_Reader\") \\ .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Ruta base con estructura particionada s3_base_path = \"s3a://data-lake-bucket/sales_data/year=*/month=*/day=*/\" # Lectura con filtros de partici\u00f3n (partition pruning) sales_df = spark.read \\ .option(\"basePath\", \"s3a://data-lake-bucket/sales_data/\") \\ .parquet(s3_base_path) \\ .filter( (col(\"year\") == 2024) & (col(\"month\").isin([10, 11, 12])) & (col(\"day\") >= 1) ) # Lectura con esquema espec\u00edfico para mejor rendimiento schema = StructType([ StructField(\"transaction_id\", StringType(), True), StructField(\"customer_id\", LongType(), True), StructField(\"product_id\", StringType(), True), StructField(\"quantity\", IntegerType(), True), StructField(\"unit_price\", DecimalType(10,2), True), StructField(\"timestamp\", TimestampType(), True), StructField(\"year\", IntegerType(), True), StructField(\"month\", IntegerType(), True), StructField(\"day\", IntegerType(), True) ]) optimized_sales_df = spark.read \\ .schema(schema) \\ .parquet(\"s3a://data-lake-bucket/sales_data/year=2024/month=12/\") # An\u00e1lisis de particiones y estad\u00edsticas print(f\"N\u00famero de particiones: {optimized_sales_df.rdd.getNumPartitions()}\") print(f\"Total de registros: {optimized_sales_df.count()}\") # Agregaciones optimizadas daily_summary = optimized_sales_df \\ .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\ .groupBy(\"year\", \"month\", \"day\") \\ .agg( count(\"transaction_id\").alias(\"total_transactions\"), sum(\"total_amount\").alias(\"daily_revenue\"), countDistinct(\"customer_id\").alias(\"unique_customers\"), avg(\"total_amount\").alias(\"avg_transaction_value\") ) daily_summary.show() Procesar archivos CSV diarios desde un directorio HDFS : import os from datetime import datetime, timedelta from pyspark.sql.functions import * from pyspark.sql.types import * # Esquema expl\u00edcito para archivos CSV csv_schema = StructType([ StructField(\"log_timestamp\", TimestampType(), True), StructField(\"user_id\", StringType(), True), StructField(\"session_id\", StringType(), True), StructField(\"page_url\", StringType(), True), StructField(\"user_agent\", StringType(), True), StructField(\"ip_address\", StringType(), True), StructField(\"response_code\", IntegerType(), True), StructField(\"response_size\", LongType(), True) ]) # Funci\u00f3n para procesar archivos por rango de fechas def process_daily_logs(start_date, end_date, hdfs_base_path): current_date = start_date all_files = [] while current_date <= end_date: date_str = current_date.strftime(\"%Y-%m-%d\") daily_path = f\"{hdfs_base_path}/date={date_str}/*.csv\" all_files.append(daily_path) current_date += timedelta(days=1) return all_files # Configuraci\u00f3n para procesamiento de archivos grandes spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"268435456\") # 256MB spark.conf.set(\"spark.sql.files.openCostInBytes\", \"8388608\") # 8MB # Lectura de m\u00faltiples archivos CSV diarios hdfs_path = \"hdfs://namenode:9000/logs/web_access\" start_date = datetime(2024, 12, 1) end_date = datetime(2024, 12, 7) file_paths = process_daily_logs(start_date, end_date, hdfs_path) # Lectura optimizada con m\u00faltiples opciones web_logs_df = spark.read \\ .schema(csv_schema) \\ .option(\"header\", \"true\") \\ .option(\"multiline\", \"false\") \\ .option(\"escape\", '\"') \\ .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\ .option(\"mode\", \"PERMISSIVE\") \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .csv(file_paths) # Limpieza y enriquecimiento de datos cleaned_logs_df = web_logs_df \\ .filter(col(\"_corrupt_record\").isNull()) \\ .withColumn(\"date\", to_date(col(\"log_timestamp\"))) \\ .withColumn(\"hour\", hour(col(\"log_timestamp\"))) \\ .withColumn(\"is_bot\", when(col(\"user_agent\").rlike(\"(?i)bot|crawler|spider\"), True).otherwise(False)) \\ .withColumn(\"status_category\", when(col(\"response_code\").between(200, 299), \"Success\") .when(col(\"response_code\").between(400, 499), \"Client_Error\") .when(col(\"response_code\").between(500, 599), \"Server_Error\") .otherwise(\"Other\")) # An\u00e1lisis de patrones de tr\u00e1fico traffic_analysis = cleaned_logs_df \\ .filter(~col(\"is_bot\")) \\ .groupBy(\"date\", \"hour\", \"status_category\") \\ .agg( count(\"*\").alias(\"request_count\"), countDistinct(\"user_id\").alias(\"unique_users\"), countDistinct(\"ip_address\").alias(\"unique_ips\"), sum(\"response_size\").alias(\"total_bytes\"), avg(\"response_size\").alias(\"avg_response_size\") ) # Escritura de resultados procesados en formato Parquet particionado traffic_analysis.write \\ .mode(\"overwrite\") \\ .partitionBy(\"date\") \\ .parquet(\"hdfs://namenode:9000/analytics/web_traffic_summary\") traffic_analysis.show(50)","title":"FileSource y formatos soportados"},{"location":"tema32/#conector-delta-lake","text":"Delta Lake es una capa de almacenamiento open-source que proporciona transacciones ACID, manejo de metadatos escalable y unificaci\u00f3n de streaming y batch processing sobre data lakes. El conector Delta para Spark permite operaciones avanzadas como merge, time travel, schema evolution y optimizaciones autom\u00e1ticas, siendo ideal para arquitecturas de datos modernas que requieren consistencia y versionado. Capacidades : Transacciones ACID completas Versionado autom\u00e1tico con time travel Schema enforcement y evolution Optimizaci\u00f3n autom\u00e1tica (Auto Optimize, Z-Ordering) Change Data Feed (CDF) para captura de cambios Vacuum para limpieza de archivos obsoletos Ingerir datos en formato Delta y aplicar MERGE INTO para deduplicaci\u00f3n : from delta import * from pyspark.sql import SparkSession from pyspark.sql.functions import * # Configuraci\u00f3n de Spark con Delta Lake builder = SparkSession.builder \\ .appName(\"Delta_Lake_Upsert\") \\ .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\ .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\ .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\") \\ .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") spark = configure_spark_with_delta_pip(builder).getOrCreate() # Ruta de la tabla Delta delta_table_path = \"/delta/customer_profiles\" # Datos iniciales (simulando carga inicial) initial_data = [ (1, \"John Doe\", \"john.doe@email.com\", \"2024-01-15\", \"Premium\", 5000.0), (2, \"Jane Smith\", \"jane.smith@email.com\", \"2024-01-20\", \"Standard\", 2500.0), (3, \"Bob Johnson\", \"bob.johnson@email.com\", \"2024-02-01\", \"Premium\", 7500.0) ] columns = [\"customer_id\", \"name\", \"email\", \"registration_date\", \"tier\", \"lifetime_value\"] initial_df = spark.createDataFrame(initial_data, columns) # Creaci\u00f3n de tabla Delta inicial initial_df.write \\ .format(\"delta\") \\ .mode(\"overwrite\") \\ .option(\"overwriteSchema\", \"true\") \\ .save(delta_table_path) # Creaci\u00f3n de DeltaTable para operaciones avanzadas delta_table = DeltaTable.forPath(spark, delta_table_path) # Datos nuevos y actualizados (simulando ingesta incremental) new_data = [ (2, \"Jane Smith-Wilson\", \"jane.wilson@email.com\", \"2024-01-20\", \"Premium\", 3500.0), # Actualizaci\u00f3n (4, \"Alice Brown\", \"alice.brown@email.com\", \"2024-03-10\", \"Standard\", 1800.0), # Nuevo (5, \"Charlie Davis\", \"charlie.davis@email.com\", \"2024-03-15\", \"Premium\", 9200.0), # Nuevo (1, \"John Doe\", \"john.doe@newemail.com\", \"2024-01-15\", \"Premium\", 5500.0) # Actualizaci\u00f3n email ] updates_df = spark.createDataFrame(new_data, columns) \\ .withColumn(\"last_updated\", current_timestamp()) # Operaci\u00f3n MERGE con l\u00f3gica de deduplicaci\u00f3n delta_table.alias(\"target\") \\ .merge( updates_df.alias(\"source\"), \"target.customer_id = source.customer_id\" ) \\ .whenMatchedUpdate(set={ \"name\": \"source.name\", \"email\": \"source.email\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .whenNotMatchedInsert(values={ \"customer_id\": \"source.customer_id\", \"name\": \"source.name\", \"email\": \"source.email\", \"registration_date\": \"source.registration_date\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .execute() # Verificaci\u00f3n de resultados print(\"Estado actual de la tabla:\") spark.read.format(\"delta\").load(delta_table_path).show() # MERGE avanzado con condiciones complejas complex_updates = [ (1, \"John Doe Sr.\", \"john.doe.sr@email.com\", \"2024-01-15\", \"Platinum\", 12000.0), (6, \"Diana Prince\", \"diana.prince@email.com\", \"2024-04-01\", \"Premium\", 8500.0), (2, \"Jane Smith-Wilson\", \"jane.wilson@email.com\", \"2024-01-20\", \"Standard\", 2800.0) # Downgrade ] complex_df = spark.createDataFrame(complex_updates, columns) \\ .withColumn(\"last_updated\", current_timestamp()) # MERGE con condiciones de negocio delta_table.alias(\"target\") \\ .merge( complex_df.alias(\"source\"), \"target.customer_id = source.customer_id\" ) \\ .whenMatchedUpdate( condition=\"source.lifetime_value > target.lifetime_value OR source.tier != target.tier\", set={ \"name\": \"source.name\", \"email\": \"source.email\", \"tier\": \"source.tier\", \"lifetime_value\": \"greatest(target.lifetime_value, source.lifetime_value)\", \"last_updated\": \"source.last_updated\" } ) \\ .whenNotMatchedInsert(values={ \"customer_id\": \"source.customer_id\", \"name\": \"source.name\", \"email\": \"source.email\", \"registration_date\": \"source.registration_date\", \"tier\": \"source.tier\", \"lifetime_value\": \"source.lifetime_value\", \"last_updated\": \"source.last_updated\" }) \\ .execute() print(\"Estado despu\u00e9s del MERGE condicional:\") spark.read.format(\"delta\").load(delta_table_path).show() Mantener versiones de datasets para auditor\u00eda : # Consulta del historial de versiones print(\"Historial de versiones de la tabla:\") delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False) # Time Travel - consultar versi\u00f3n espec\u00edfica version_1_df = spark.read \\ .format(\"delta\") \\ .option(\"versionAsOf\", 1) \\ .load(delta_table_path) print(\"Estado en versi\u00f3n 1:\") version_1_df.show() # Time Travel por timestamp from datetime import datetime, timedelta yesterday = datetime.now() - timedelta(days=1) timestamp_query = spark.read \\ .format(\"delta\") \\ .option(\"timestampAsOf\", yesterday.strftime(\"%Y-%m-%d %H:%M:%S\")) \\ .load(delta_table_path) # Auditor\u00eda de cambios entre versiones current_version = spark.read.format(\"delta\").load(delta_table_path) previous_version = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path) # Detectar registros modificados changes_audit = current_version.alias(\"current\") \\ .join(previous_version.alias(\"previous\"), col(\"current.customer_id\") == col(\"previous.customer_id\"), \"full_outer\") \\ .select( coalesce(col(\"current.customer_id\"), col(\"previous.customer_id\")).alias(\"customer_id\"), when(col(\"previous.customer_id\").isNull(), \"INSERTED\") .when(col(\"current.customer_id\").isNull(), \"DELETED\") .when(col(\"current.name\") != col(\"previous.name\") | col(\"current.email\") != col(\"previous.email\") | col(\"current.tier\") != col(\"previous.tier\") | col(\"current.lifetime_value\") != col(\"previous.lifetime_value\"), \"UPDATED\") .otherwise(\"UNCHANGED\").alias(\"change_type\"), col(\"current.name\").alias(\"current_name\"), col(\"previous.name\").alias(\"previous_name\"), col(\"current.email\").alias(\"current_email\"), col(\"previous.email\").alias(\"previous_email\") ) \\ .filter(col(\"change_type\") != \"UNCHANGED\") print(\"Auditor\u00eda de cambios:\") changes_audit.show(truncate=False) # Configuraci\u00f3n de retenci\u00f3n y optimizaci\u00f3n # Vacuum para eliminar archivos antiguos (cuidado en producci\u00f3n) delta_table.vacuum(retentionHours=168) # 7 d\u00edas # Optimize con Z-Ordering para mejor rendimiento de consultas spark.sql(f\"OPTIMIZE delta.`{delta_table_path}` ZORDER BY (customer_id, tier)\") # Habilitar Change Data Feed para captura de cambios spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\") # Consultar cambios usando CDF changes_df = spark.read \\ .format(\"delta\") \\ .option(\"readChangeFeed\", \"true\") \\ .option(\"startingVersion\", 0) \\ .load(delta_table_path) print(\"Change Data Feed:\") changes_df.select(\"customer_id\", \"name\", \"_change_type\", \"_commit_version\", \"_commit_timestamp\").show() # Metadatos y estad\u00edsticas de la tabla print(\"Detalles de la tabla Delta:\") spark.sql(f\"DESCRIBE DETAIL delta.`{delta_table_path}`\").show(truncate=False) # Estad\u00edsticas de archivos print(\"Estad\u00edsticas de archivos:\") print(f\"N\u00famero de archivos: {len(delta_table.detail().collect()[0]['numFiles'])}\") print(f\"Tama\u00f1o total: {delta_table.detail().collect()[0]['sizeInBytes']} bytes\")","title":"Conector Delta Lake"},{"location":"tema32/#323-airflow-uso-de-hooks-y-connections","text":"Apache Airflow proporciona mecanismos reutilizables y seguros para conectarse de forma modular a distintas fuentes de datos y servicios externos. Esta arquitectura permite mantener las credenciales separadas del c\u00f3digo y facilita la reutilizaci\u00f3n de conexiones a trav\u00e9s de m\u00faltiples DAGs y tareas.","title":"3.2.3 Airflow: uso de Hooks y Connections"},{"location":"tema32/#hooks-en-airflow","text":"Los Hooks son interfaces de bajo nivel que proporcionan una capa de abstracci\u00f3n para acceder a sistemas externos como bases de datos, APIs REST, servicios de almacenamiento en la nube (S3, GCS), y otros servicios. Cada operador en Airflow utiliza internamente un Hook espec\u00edfico para realizar sus operaciones, lo que permite encapsular la l\u00f3gica de conexi\u00f3n y las operaciones espec\u00edficas del sistema. Ventajas Reutilizaci\u00f3n : Un mismo Hook puede ser usado por m\u00faltiples operadores y tareas Abstracci\u00f3n : Ocultan la complejidad de las conexiones y protocolos espec\u00edficos Consistencia : Proporcionan una interfaz uniforme para sistemas similares Mantenibilidad : Centralizan la l\u00f3gica de conexi\u00f3n en un solo lugar Usar PostgresHook para ejecutar una consulta y pasar los resultados a otra tarea : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from airflow.operators.email import EmailOperator import pandas as pd default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5) } def extract_sales_data(**context): \"\"\"Extrae datos de ventas desde PostgreSQL y los pasa a la siguiente tarea\"\"\" # Inicializar el Hook de PostgreSQL usando la conexi\u00f3n configurada postgres_hook = PostgresHook(postgres_conn_id='postgres_sales_db') # Ejecutar consulta SQL sql_query = \"\"\" SELECT DATE(sale_date) as fecha, SUM(amount) as total_ventas, COUNT(*) as num_transacciones, AVG(amount) as promedio_venta FROM sales WHERE sale_date >= CURRENT_DATE - INTERVAL '7 days' GROUP BY DATE(sale_date) ORDER BY fecha DESC; \"\"\" # Obtener resultados como DataFrame de pandas df = postgres_hook.get_pandas_df(sql_query) # Convertir a diccionario para pasar entre tareas sales_data = df.to_dict('records') # Guardar en XCom para la siguiente tarea context['task_instance'].xcom_push(key='weekly_sales', value=sales_data) print(f\"Extra\u00eddos {len(sales_data)} registros de ventas\") return sales_data def process_sales_report(**context): \"\"\"Procesa los datos de ventas y genera un reporte\"\"\" # Obtener datos de la tarea anterior sales_data = context['task_instance'].xcom_pull( task_ids='extract_sales_data', key='weekly_sales' ) if not sales_data: raise ValueError(\"No se recibieron datos de ventas\") # Procesar datos total_sales = sum([record['total_ventas'] for record in sales_data]) avg_daily_sales = total_sales / len(sales_data) # Generar reporte report = f\"\"\" Reporte Semanal de Ventas: - Total de ventas: ${total_sales:,.2f} - Promedio diario: ${avg_daily_sales:,.2f} - D\u00edas analizados: {len(sales_data)} \"\"\" print(report) context['task_instance'].xcom_push(key='sales_report', value=report) return report # Definir el DAG dag = DAG( 'sales_reporting_dag', default_args=default_args, description='Pipeline de reporte de ventas usando PostgresHook', schedule_interval='@daily', catchup=False ) # Definir tareas extract_task = PythonOperator( task_id='extract_sales_data', python_callable=extract_sales_data, dag=dag ) process_task = PythonOperator( task_id='process_sales_report', python_callable=process_sales_report, dag=dag ) # Definir dependencias extract_task >> process_task Usar HttpHook para llamar a una API y procesar su JSON : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.operators.postgres import PostgresOperator import json default_args = { 'owner': 'api-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=2) } def fetch_weather_data(**context): \"\"\"Obtiene datos meteorol\u00f3gicos desde una API externa\"\"\" # Inicializar HttpHook con la conexi\u00f3n configurada http_hook = HttpHook( method='GET', http_conn_id='weather_api_conn' # Conexi\u00f3n configurada en Airflow UI ) # Par\u00e1metros para la API endpoint = '/current' headers = { 'Accept': 'application/json', 'User-Agent': 'Airflow-Weather-Pipeline/1.0' } # Obtener ciudad desde variables de Airflow from airflow.models import Variable city = Variable.get(\"weather_city\", default_var=\"Cali\") data = { 'q': city, 'units': 'metric', 'lang': 'es' } try: # Realizar petici\u00f3n HTTP response = http_hook.run( endpoint=endpoint, headers=headers, data=data ) # Procesar respuesta JSON weather_data = json.loads(response.content) # Extraer informaci\u00f3n relevante processed_data = { 'timestamp': datetime.now().isoformat(), 'ciudad': weather_data.get('name'), 'temperatura': weather_data['main']['temp'], 'sensacion_termica': weather_data['main']['feels_like'], 'humedad': weather_data['main']['humidity'], 'presion': weather_data['main']['pressure'], 'descripcion': weather_data['weather'][0]['description'], 'visibilidad': weather_data.get('visibility', 0) / 1000, # km 'viento_velocidad': weather_data.get('wind', {}).get('speed', 0) } print(f\"Datos meteorol\u00f3gicos obtenidos para {processed_data['ciudad']}\") print(f\"Temperatura: {processed_data['temperatura']}\u00b0C\") # Guardar en XCom context['task_instance'].xcom_push( key='weather_data', value=processed_data ) return processed_data except Exception as e: print(f\"Error al obtener datos meteorol\u00f3gicos: {str(e)}\") raise def validate_and_transform_weather(**context): \"\"\"Valida y transforma los datos meteorol\u00f3gicos\"\"\" # Obtener datos de la tarea anterior weather_data = context['task_instance'].xcom_pull( task_ids='fetch_weather_data', key='weather_data' ) if not weather_data: raise ValueError(\"No se recibieron datos meteorol\u00f3gicos\") # Validaciones if weather_data['temperatura'] < -50 or weather_data['temperatura'] > 60: raise ValueError(f\"Temperatura fuera de rango: {weather_data['temperatura']}\u00b0C\") if weather_data['humedad'] < 0 or weather_data['humedad'] > 100: raise ValueError(f\"Humedad fuera de rango: {weather_data['humedad']}%\") # Transformaciones adicionales weather_data['categoria_temperatura'] = ( 'Muy Fr\u00edo' if weather_data['temperatura'] < 10 else 'Fr\u00edo' if weather_data['temperatura'] < 18 else 'Templado' if weather_data['temperatura'] < 25 else 'C\u00e1lido' if weather_data['temperatura'] < 30 else 'Muy C\u00e1lido' ) weather_data['indice_confort'] = ( weather_data['temperatura'] - (weather_data['humedad'] / 100) * 2 - weather_data['viento_velocidad'] * 0.5 ) print(f\"Datos validados y transformados exitosamente\") print(f\"Categor\u00eda de temperatura: {weather_data['categoria_temperatura']}\") print(f\"\u00cdndice de confort: {weather_data['indice_confort']:.1f}\") return weather_data def store_weather_data(**context): \"\"\"Almacena los datos procesados en la base de datos\"\"\" weather_data = context['task_instance'].xcom_pull( task_ids='validate_and_transform_weather' ) # Aqu\u00ed normalmente insertar\u00edas en la base de datos # usando otro Hook como PostgresHook print(\"Datos meteorol\u00f3gicos almacenados exitosamente\") print(json.dumps(weather_data, indent=2, ensure_ascii=False)) # Definir el DAG dag = DAG( 'weather_api_pipeline', default_args=default_args, description='Pipeline para consumir API meteorol\u00f3gica usando HttpHook', schedule_interval=timedelta(hours=3), catchup=False, tags=['api', 'weather', 'http'] ) # Definir tareas fetch_task = PythonOperator( task_id='fetch_weather_data', python_callable=fetch_weather_data, dag=dag ) validate_task = PythonOperator( task_id='validate_and_transform_weather', python_callable=validate_and_transform_weather, dag=dag ) store_task = PythonOperator( task_id='store_weather_data', python_callable=store_weather_data, dag=dag ) # Definir dependencias fetch_task >> validate_task >> store_task","title":"Hooks en Airflow"},{"location":"tema32/#connections-y-variables","text":"Las Connections y Variables de Airflow proporcionan un mecanismo seguro y centralizado para gestionar credenciales, configuraciones y par\u00e1metros sin exponerlos directamente en el c\u00f3digo del DAG. Esto mejora la seguridad, facilita el mantenimiento y permite diferentes configuraciones entre entornos (desarrollo, testing, producci\u00f3n). Connections almacenan informaci\u00f3n de conexi\u00f3n completa (host, puerto, usuario, contrase\u00f1a, esquemas) mientras que Variables almacenan valores individuales que pueden ser utilizados a trav\u00e9s de m\u00faltiples DAGs. Definir una conexi\u00f3n S3 en Airflow UI y usarla desde un S3Hook : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.amazon.aws.hooks.s3 import S3Hook from airflow.models import Variable import pandas as pd import io default_args = { 'owner': 'data-engineering', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } def upload_data_to_s3(**context): \"\"\"Sube datos procesados a Amazon S3 usando conexi\u00f3n configurada\"\"\" # Inicializar S3Hook usando la conexi\u00f3n 'aws_s3_conn' definida en Airflow UI s3_hook = S3Hook(aws_conn_id='aws_s3_conn') # Obtener configuraci\u00f3n desde Variables de Airflow bucket_name = Variable.get(\"s3_data_bucket\", default_var=\"mi-bucket-datos\") s3_prefix = Variable.get(\"s3_data_prefix\", default_var=\"processed-data\") # Generar datos de ejemplo (normalmente vendr\u00edan de una tarea anterior) sample_data = { 'fecha': pd.date_range('2024-01-01', periods=100, freq='D'), 'ventas': pd.Series(range(100)) * 150.5, 'region': ['Norte', 'Sur', 'Este', 'Oeste'] * 25, 'producto': ['A', 'B', 'C'] * 33 + ['A'] } df = pd.DataFrame(sample_data) # Convertir DataFrame a CSV en memoria csv_buffer = io.StringIO() df.to_csv(csv_buffer, index=False, encoding='utf-8') csv_content = csv_buffer.getvalue() # Generar nombre de archivo con timestamp timestamp = context['execution_date'].strftime('%Y%m%d_%H%M%S') s3_key = f\"{s3_prefix}/ventas_diarias_{timestamp}.csv\" try: # Subir archivo a S3 s3_hook.load_string( string_data=csv_content, key=s3_key, bucket_name=bucket_name, replace=True, content_type='text/csv' ) print(f\"Archivo subido exitosamente a S3:\") print(f\"Bucket: {bucket_name}\") print(f\"Key: {s3_key}\") print(f\"Registros: {len(df)}\") # Verificar que el archivo existe if s3_hook.check_for_key(key=s3_key, bucket_name=bucket_name): print(\"\u2713 Verificaci\u00f3n exitosa: El archivo existe en S3\") else: raise Exception(\"Error: El archivo no se encontr\u00f3 en S3 despu\u00e9s de la subida\") # Guardar informaci\u00f3n del archivo en XCom file_info = { 'bucket': bucket_name, 's3_key': s3_key, 'size_bytes': len(csv_content.encode('utf-8')), 'records_count': len(df), 'upload_timestamp': timestamp } context['task_instance'].xcom_push(key='s3_file_info', value=file_info) return file_info except Exception as e: print(f\"Error al subir archivo a S3: {str(e)}\") raise def process_s3_files(**context): \"\"\"Lee y procesa archivos desde S3\"\"\" s3_hook = S3Hook(aws_conn_id='aws_s3_conn') bucket_name = Variable.get(\"s3_data_bucket\") s3_prefix = Variable.get(\"s3_data_prefix\") # Listar archivos en el bucket con el prefijo especificado file_keys = s3_hook.list_keys( bucket_name=bucket_name, prefix=s3_prefix ) print(f\"Archivos encontrados en S3: {len(file_keys)}\") # Procesar el archivo m\u00e1s reciente if file_keys: latest_file = sorted(file_keys)[-1] # Leer archivo desde S3 file_content = s3_hook.read_key( key=latest_file, bucket_name=bucket_name ) # Convertir a DataFrame df = pd.read_csv(io.StringIO(file_content)) # Realizar an\u00e1lisis b\u00e1sico analysis = { 'total_records': len(df), 'total_sales': df['ventas'].sum(), 'avg_sales': df['ventas'].mean(), 'unique_regions': df['region'].nunique(), 'date_range': f\"{df['fecha'].min()} to {df['fecha'].max()}\" } print(\"An\u00e1lisis del archivo procesado:\") for key, value in analysis.items(): print(f\" {key}: {value}\") return analysis else: print(\"No se encontraron archivos para procesar\") return {} # Definir el DAG dag = DAG( 's3_data_pipeline', default_args=default_args, description='Pipeline de datos usando S3Hook y conexiones configuradas', schedule_interval='@daily', catchup=False, tags=['s3', 'aws', 'data-pipeline'] ) # Definir tareas upload_task = PythonOperator( task_id='upload_data_to_s3', python_callable=upload_data_to_s3, dag=dag ) process_task = PythonOperator( task_id='process_s3_files', python_callable=process_s3_files, dag=dag ) # Definir dependencias upload_task >> process_task Almacenar claves API en variables cifradas para ser usadas desde el DAG : from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.http.hooks.http import HttpHook from airflow.models import Variable from airflow.providers.postgres.hooks.postgres import PostgresHook import requests import json import hashlib default_args = { 'owner': 'api-integration-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=5) } def fetch_secure_api_data(**context): \"\"\"Consume API externa usando claves cifradas almacenadas en Variables\"\"\" # Obtener claves API cifradas desde Variables de Airflow # Estas variables se configuran en Airflow UI marcadas como \"encrypt\" api_key = Variable.get(\"external_api_key\") # Variable cifrada api_secret = Variable.get(\"external_api_secret\") # Variable cifrada # Obtener configuraciones no sensibles api_base_url = Variable.get(\"external_api_base_url\", default_var=\"https://api.example.com\") max_records = int(Variable.get(\"api_max_records\", default_var=\"1000\")) # Generar token de autenticaci\u00f3n (ejemplo con HMAC) timestamp = str(int(datetime.now().timestamp())) message = f\"{api_key}{timestamp}\" signature = hashlib.hmac( api_secret.encode('utf-8'), message.encode('utf-8'), hashlib.sha256 ).hexdigest() # Configurar headers de autenticaci\u00f3n headers = { 'Authorization': f'Bearer {api_key}', 'X-API-Timestamp': timestamp, 'X-API-Signature': signature, 'Content-Type': 'application/json', 'User-Agent': 'Airflow-Data-Pipeline/2.0' } # Par\u00e1metros de la consulta params = { 'limit': max_records, 'format': 'json', 'date_from': context['execution_date'].strftime('%Y-%m-%d'), 'include_metadata': True } try: # Realizar petici\u00f3n usando requests directamente para mayor control response = requests.get( f\"{api_base_url}/v1/data\", headers=headers, params=params, timeout=30 ) response.raise_for_status() # Lanza excepci\u00f3n si status != 2xx data = response.json() # Validar respuesta if 'error' in data: raise Exception(f\"Error en API: {data['error']}\") records = data.get('records', []) metadata = data.get('metadata', {}) print(f\"Datos obtenidos exitosamente:\") print(f\" Registros: {len(records)}\") print(f\" P\u00e1gina: {metadata.get('page', 1)}\") print(f\" Total disponible: {metadata.get('total_count', 'N/A')}\") # Procesar y limpiar datos processed_records = [] for record in records: processed_record = { 'id': record.get('id'), 'timestamp': record.get('created_at'), 'value': float(record.get('value', 0)), 'category': record.get('category', 'unknown'), 'status': record.get('status', 'active'), 'metadata': json.dumps(record.get('additional_data', {})) } processed_records.append(processed_record) # Guardar en XCom para tareas posteriores context['task_instance'].xcom_push( key='api_data', value=processed_records ) context['task_instance'].xcom_push( key='api_metadata', value=metadata ) return { 'records_count': len(processed_records), 'api_response_time': response.elapsed.total_seconds(), 'status_code': response.status_code } except requests.exceptions.RequestException as e: print(f\"Error de conexi\u00f3n con la API: {str(e)}\") raise except json.JSONDecodeError as e: print(f\"Error al decodificar respuesta JSON: {str(e)}\") raise except Exception as e: print(f\"Error general al consumir API: {str(e)}\") raise def validate_and_enrich_data(**context): \"\"\"Valida y enriquece los datos obtenidos de la API\"\"\" # Obtener datos de la tarea anterior api_data = context['task_instance'].xcom_pull( task_ids='fetch_secure_api_data', key='api_data' ) if not api_data: raise ValueError(\"No se recibieron datos de la API\") # Obtener configuraciones de validaci\u00f3n desde Variables min_value = float(Variable.get(\"validation_min_value\", default_var=\"0\")) max_value = float(Variable.get(\"validation_max_value\", default_var=\"10000\")) valid_categories = Variable.get(\"valid_categories\", default_var=\"A,B,C\").split(',') validated_records = [] validation_errors = [] for i, record in enumerate(api_data): errors = [] # Validaciones if record['value'] < min_value or record['value'] > max_value: errors.append(f\"Valor fuera de rango: {record['value']}\") if record['category'] not in valid_categories: errors.append(f\"Categor\u00eda inv\u00e1lida: {record['category']}\") if not record['id']: errors.append(\"ID faltante\") # Si hay errores, registrar y omitir registro if errors: validation_errors.append({ 'record_index': i, 'record_id': record.get('id', 'N/A'), 'errors': errors }) continue # Enriquecer datos v\u00e1lidos enriched_record = record.copy() enriched_record['validation_timestamp'] = datetime.now().isoformat() enriched_record['value_category'] = ( 'low' if record['value'] < 100 else 'medium' if record['value'] < 1000 else 'high' ) enriched_record['processing_batch'] = context['run_id'] validated_records.append(enriched_record) print(f\"Validaci\u00f3n completada:\") print(f\" Registros v\u00e1lidos: {len(validated_records)}\") print(f\" Registros con errores: {len(validation_errors)}\") if validation_errors: print(\"Errores de validaci\u00f3n encontrados:\") for error in validation_errors[:5]: # Mostrar solo los primeros 5 print(f\" Record {error['record_index']}: {', '.join(error['errors'])}\") # Guardar resultados context['task_instance'].xcom_push( key='validated_data', value=validated_records ) context['task_instance'].xcom_push( key='validation_errors', value=validation_errors ) return { 'valid_records': len(validated_records), 'invalid_records': len(validation_errors), 'validation_success_rate': len(validated_records) / len(api_data) * 100 } def store_processed_data(**context): \"\"\"Almacena los datos procesados en la base de datos usando conexi\u00f3n configurada\"\"\" # Obtener datos validados validated_data = context['task_instance'].xcom_pull( task_ids='validate_and_enrich_data', key='validated_data' ) if not validated_data: print(\"No hay datos v\u00e1lidos para almacenar\") return # Usar PostgresHook con conexi\u00f3n configurada postgres_hook = PostgresHook(postgres_conn_id='postgres_data_warehouse') # Obtener configuraci\u00f3n de tabla desde Variables target_table = Variable.get(\"target_table_name\", default_var=\"api_data\") try: # Preparar datos para inserci\u00f3n masiva insert_sql = f\"\"\" INSERT INTO {target_table} (external_id, timestamp, value, category, status, metadata, validation_timestamp, value_category, processing_batch) VALUES %s ON CONFLICT (external_id) DO UPDATE SET value = EXCLUDED.value, category = EXCLUDED.category, status = EXCLUDED.status, metadata = EXCLUDED.metadata, validation_timestamp = EXCLUDED.validation_timestamp, value_category = EXCLUDED.value_category, processing_batch = EXCLUDED.processing_batch; \"\"\" # Preparar tuplas de datos data_tuples = [] for record in validated_data: data_tuple = ( record['id'], record['timestamp'], record['value'], record['category'], record['status'], record['metadata'], record['validation_timestamp'], record['value_category'], record['processing_batch'] ) data_tuples.append(data_tuple) # Ejecutar inserci\u00f3n masiva usando extras de psycopg2 from psycopg2.extras import execute_values with postgres_hook.get_conn() as conn: with conn.cursor() as cursor: execute_values(cursor, insert_sql, data_tuples) conn.commit() print(f\"Almacenados {len(data_tuples)} registros en {target_table}\") # Registrar estad\u00edsticas stats_sql = f\"\"\" SELECT COUNT(*) as total_records, AVG(value) as avg_value, MIN(timestamp) as min_timestamp, MAX(timestamp) as max_timestamp FROM {target_table} WHERE processing_batch = %s; \"\"\" stats = postgres_hook.get_first(stats_sql, parameters=[context['run_id']]) print(\"Estad\u00edsticas del lote procesado:\") print(f\" Total de registros: {stats[0]}\") print(f\" Valor promedio: {stats[1]:.2f}\") print(f\" Rango temporal: {stats[2]} - {stats[3]}\") return { 'records_stored': len(data_tuples), 'table_name': target_table, 'batch_id': context['run_id'] } except Exception as e: print(f\"Error al almacenar datos: {str(e)}\") raise # Definir el DAG dag = DAG( 'secure_api_integration', default_args=default_args, description='Pipeline seguro para integraci\u00f3n de API externa con variables cifradas', schedule_interval=timedelta(hours=6), catchup=False, tags=['api', 'security', 'encryption', 'variables'] ) # Definir tareas fetch_task = PythonOperator( task_id='fetch_secure_api_data', python_callable=fetch_secure_api_data, dag=dag ) validate_task = PythonOperator( task_id='validate_and_enrich_data', python_callable=validate_and_enrich_data, dag=dag ) store_task = PythonOperator( task_id='store_processed_data', python_callable=store_processed_data, dag=dag ) # Definir dependencias fetch_task >> validate_task >> store_task Configuraci\u00f3n de Connections y Variables en Airflow UI : Para que estos ejemplos funcionen correctamente, es necesario configurar las siguientes conexiones y variables en la interfaz de Airflow: Connections (Admin \u2192 Connections) : postgres_sales_db : Conexi\u00f3n PostgreSQL para base de datos de ventas weather_api_conn : Conexi\u00f3n HTTP para API meteorol\u00f3gica aws_s3_conn : Conexi\u00f3n AWS para acceso a S3 postgres_data_warehouse : Conexi\u00f3n PostgreSQL para almac\u00e9n de datos Variables (Admin \u2192 Variables) : weather_city : Ciudad para datos meteorol\u00f3gicos s3_data_bucket : Nombre del bucket S3 s3_data_prefix : Prefijo para archivos en S3 external_api_key : Clave API (marcada como cifrada) external_api_secret : Secreto API (marcada como cifrada) validation_min_value : Valor m\u00ednimo para validaci\u00f3n validation_max_value : Valor m\u00e1ximo para validaci\u00f3n valid_categories : Categor\u00edas v\u00e1lidas separadas por coma target_table_name : Nombre de la tabla destino","title":"Connections y Variables"},{"location":"tema32/#mejores-practicas-para-hooks-y-connections","text":"Gesti\u00f3n de Conexiones : Separaci\u00f3n por entorno : Utiliza diferentes conexiones para desarrollo, testing y producci\u00f3n Nomenclatura consistente : Usa un patr\u00f3n claro como {env}_{service}_{purpose}_conn Principio de menor privilegio : Configura conexiones con los permisos m\u00ednimos necesarios Rotaci\u00f3n de credenciales : Implementa un proceso regular de rotaci\u00f3n de passwords y tokens Monitoreo : Registra y monitorea el uso de conexiones para detectar anomal\u00edas Gesti\u00f3n de Variables : Cifrado de datos sensibles : Siempre marca como \"encrypt\" las variables que contienen informaci\u00f3n sensible Versionado : Mant\u00e9n un registro de cambios en variables cr\u00edticas Valores por defecto : Proporciona valores por defecto sensatos para variables opcionales Documentaci\u00f3n : Documenta el prop\u00f3sito y formato esperado de cada variable Validaci\u00f3n : Implementa validaci\u00f3n de formato y rango para variables cr\u00edticas Optimizaci\u00f3n de Rendimiento : Reutilizaci\u00f3n de conexiones : Los Hooks autom\u00e1ticamente reutilizan conexiones dentro de una tarea Pool de conexiones : Configura pools de conexiones para sistemas de alta concurrencia Timeouts apropiados : Establece timeouts adecuados para evitar tareas colgadas Batch operations : Prefiere operaciones por lotes sobre m\u00faltiples operaciones individuales Limpieza de recursos : Aseg\u00farate de que las conexiones se cierren adecuadamente Manejo de Errores y Reintentos : Excepciones espec\u00edficas : Captura y maneja excepciones espec\u00edficas seg\u00fan el tipo de error Logging detallado : Registra informaci\u00f3n suficiente para diagnosticar problemas Reintentos inteligentes : Configura diferentes estrategias de reintento seg\u00fan el tipo de falla Alertas apropiadas : Configura alertas para fallos cr\u00edticos vs. fallos recuperables Fallback mechanisms : Implementa mecanismos de respaldo cuando sea posible Seguridad : Validaci\u00f3n de entrada : Siempre valida y sanitiza datos de entrada Auditor\u00eda : Mant\u00e9n logs de auditor\u00eda para accesos a sistemas sensibles Cifrado en tr\u00e1nsito : Usa siempre conexiones cifradas (HTTPS, SSL/TLS) Segregaci\u00f3n de redes : Utiliza redes privadas para comunicaci\u00f3n entre servicios Secretos externos : Considera usar servicios externos de gesti\u00f3n de secretos (AWS Secrets Manager, HashiCorp Vault)","title":"Mejores Pr\u00e1cticas para Hooks y Connections"},{"location":"tema32/#ejemplo-avanzado-pipeline-completo-con-multiples-hooks","text":"Este ejemplo avanzado demuestra c\u00f3mo integrar m\u00faltiples Hooks y Connections en un pipeline completo de datos, incluyendo manejo de errores, notificaciones, y mejores pr\u00e1cticas de producci\u00f3n. from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.operators.bash import BashOperator from airflow.providers.postgres.hooks.postgres import PostgresHook from airflow.providers.amazon.aws.hooks.s3 import S3Hook from airflow.providers.http.hooks.http import HttpHook from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook from airflow.models import Variable from airflow.utils.task_group import TaskGroup import pandas as pd import json import logging # Configurar logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) default_args = { 'owner': 'data-platform-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': False, # Usaremos Slack en su lugar 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5), 'on_failure_callback': lambda context: send_failure_notification(context) } def send_failure_notification(context): \"\"\"Env\u00eda notificaci\u00f3n de fallo a Slack\"\"\" try: slack_hook = SlackWebhookHook( http_conn_id='slack_webhook_conn', webhook_token=Variable.get('slack_webhook_token') ) message = f\"\"\" *Fallo en Pipeline de Datos* *DAG:* {context['dag'].dag_id} *Tarea:* {context['task'].task_id} *Fecha de ejecuci\u00f3n:* {context['execution_date']} *Error:* {context.get('exception', 'Error desconocido')} Por favor revisar los logs para m\u00e1s detalles. \"\"\" slack_hook.send_text(message) except Exception as e: logger.error(f\"Error enviando notificaci\u00f3n a Slack: {str(e)}\") def extract_from_multiple_sources(**context): \"\"\"Extrae datos de m\u00faltiples fuentes usando diferentes Hooks\"\"\" results = {} # 1. Extraer desde PostgreSQL try: postgres_hook = PostgresHook(postgres_conn_id='postgres_source_db') sql_query = \"\"\" SELECT id, created_date, amount, customer_id, product_category FROM transactions WHERE DATE(created_date) = CURRENT_DATE - INTERVAL '1 day' \"\"\" transactions_df = postgres_hook.get_pandas_df(sql_query) results['transactions'] = { 'count': len(transactions_df), 'data': transactions_df.to_dict('records') } logger.info(f\"Extra\u00eddas {len(transactions_df)} transacciones de PostgreSQL\") except Exception as e: logger.error(f\"Error extrayendo de PostgreSQL: {str(e)}\") results['transactions'] = {'count': 0, 'data': [], 'error': str(e)} # 2. Extraer desde API externa try: http_hook = HttpHook( method='GET', http_conn_id='external_api_conn' ) # Obtener datos de referencia (cat\u00e1logo de productos) response = http_hook.run( endpoint='/api/v1/products', headers={ 'Authorization': f\"Bearer {Variable.get('api_token')}\", 'Content-Type': 'application/json' } ) api_data = json.loads(response.content) results['products'] = { 'count': len(api_data.get('products', [])), 'data': api_data.get('products', []) } logger.info(f\"Extra\u00eddos {len(api_data.get('products', []))} productos de API\") except Exception as e: logger.error(f\"Error extrayendo de API: {str(e)}\") results['products'] = {'count': 0, 'data': [], 'error': str(e)} # 3. Extraer archivo desde S3 try: s3_hook = S3Hook(aws_conn_id='aws_s3_conn') bucket_name = Variable.get('source_data_bucket') # Buscar archivo m\u00e1s reciente yesterday = (context['execution_date'] - timedelta(days=1)).strftime('%Y%m%d') s3_key = f\"customer-data/customers_{yesterday}.csv\" if s3_hook.check_for_key(key=s3_key, bucket_name=bucket_name): file_content = s3_hook.read_key(key=s3_key, bucket_name=bucket_name) customers_df = pd.read_csv(pd.io.common.StringIO(file_content)) results['customers'] = { 'count': len(customers_df), 'data': customers_df.to_dict('records') } logger.info(f\"Extra\u00eddos {len(customers_df)} clientes de S3\") else: logger.warning(f\"Archivo {s3_key} no encontrado en S3\") results['customers'] = {'count': 0, 'data': [], 'error': 'Archivo no encontrado'} except Exception as e: logger.error(f\"Error extrayendo de S3: {str(e)}\") results['customers'] = {'count': 0, 'data': [], 'error': str(e)} # Guardar resultados en XCom context['task_instance'].xcom_push(key='extraction_results', value=results) return results def transform_and_join_data(**context): \"\"\"Transforma y combina datos de m\u00faltiples fuentes\"\"\" # Obtener datos extra\u00eddos extraction_results = context['task_instance'].xcom_pull( task_ids='extract_from_multiple_sources', key='extraction_results' ) if not extraction_results: raise ValueError(\"No se recibieron datos de extracci\u00f3n\") # Convertir a DataFrames transactions_df = pd.DataFrame(extraction_results['transactions']['data']) products_df = pd.DataFrame(extraction_results['products']['data']) customers_df = pd.DataFrame(extraction_results['customers']['data']) logger.info(f\"Procesando {len(transactions_df)} transacciones\") if transactions_df.empty: logger.warning(\"No hay transacciones para procesar\") return {'processed_records': 0} # Realizar joins if not products_df.empty: # Join con productos para obtener informaci\u00f3n adicional transactions_df = transactions_df.merge( products_df[['id', 'name', 'price', 'margin']], left_on='product_category', right_on='id', how='left', suffixes=('', '_product') ) logger.info(\"Join con productos completado\") if not customers_df.empty: # Join con clientes para segmentaci\u00f3n transactions_df = transactions_df.merge( customers_df[['id', 'segment', 'region', 'registration_date']], left_on='customer_id', right_on='id', how='left', suffixes=('', '_customer') ) logger.info(\"Join con clientes completado\") # Transformaciones adicionales transactions_df['profit'] = transactions_df['amount'] * transactions_df.get('margin', 0.2) transactions_df['transaction_date'] = pd.to_datetime(transactions_df['created_date']) transactions_df['month_year'] = transactions_df['transaction_date'].dt.to_period('M') # Calcular m\u00e9tricas agregadas summary_stats = { 'total_transactions': len(transactions_df), 'total_revenue': transactions_df['amount'].sum(), 'total_profit': transactions_df['profit'].sum(), 'avg_transaction_amount': transactions_df['amount'].mean(), 'unique_customers': transactions_df['customer_id'].nunique() if 'customer_id' in transactions_df.columns else 0, 'processing_timestamp': datetime.now().isoformat() } logger.info(f\"Transformaci\u00f3n completada: {summary_stats}\") # Guardar datos transformados processed_data = transactions_df.to_dict('records') context['task_instance'].xcom_push(key='processed_data', value=processed_data) context['task_instance'].xcom_push(key='summary_stats', value=summary_stats) return summary_stats def load_to_data_warehouse(**context): \"\"\"Carga datos transformados al data warehouse\"\"\" # Obtener datos procesados processed_data = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='processed_data' ) summary_stats = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='summary_stats' ) if not processed_data: logger.warning(\"No hay datos procesados para cargar\") return # Conectar al data warehouse dw_hook = PostgresHook(postgres_conn_id='postgres_data_warehouse') try: # Crear tabla temporal para la carga temp_table = f\"temp_transactions_{context['run_id'].replace('-', '_')}\" create_temp_table_sql = f\"\"\" CREATE TEMP TABLE {temp_table} ( transaction_id BIGINT, transaction_date TIMESTAMP, amount DECIMAL(10,2), profit DECIMAL(10,2), customer_id BIGINT, customer_segment VARCHAR(50), customer_region VARCHAR(50), product_name VARCHAR(200), product_category VARCHAR(100), month_year VARCHAR(10), processing_timestamp TIMESTAMP ); \"\"\" dw_hook.run(create_temp_table_sql) logger.info(f\"Tabla temporal {temp_table} creada\") # Insertar datos en tabla temporal insert_count = 0 batch_size = 1000 for i in range(0, len(processed_data), batch_size): batch = processed_data[i:i + batch_size] # Preparar valores para inserci\u00f3n values = [] for record in batch: values.append(( record.get('id'), record.get('created_date'), record.get('amount', 0), record.get('profit', 0), record.get('customer_id'), record.get('segment'), record.get('region'), record.get('name'), record.get('product_category'), record.get('month_year'), summary_stats['processing_timestamp'] )) # Inserci\u00f3n por lotes insert_sql = f\"\"\" INSERT INTO {temp_table} VALUES %s \"\"\" dw_hook.run(insert_sql, parameters=[values]) insert_count += len(batch) logger.info(f\"Insertados {insert_count}/{len(processed_data)} registros\") # Mover datos de tabla temporal a tabla final merge_sql = f\"\"\" INSERT INTO fact_transactions SELECT * FROM {temp_table} ON CONFLICT (transaction_id) DO UPDATE SET amount = EXCLUDED.amount, profit = EXCLUDED.profit, processing_timestamp = EXCLUDED.processing_timestamp; \"\"\" dw_hook.run(merge_sql) # Actualizar tabla de m\u00e9tricas diarias metrics_sql = \"\"\" INSERT INTO daily_metrics (date, total_transactions, total_revenue, total_profit, avg_transaction_amount) VALUES (CURRENT_DATE, %s, %s, %s, %s) ON CONFLICT (date) DO UPDATE SET total_transactions = EXCLUDED.total_transactions, total_revenue = EXCLUDED.total_revenue, total_profit = EXCLUDED.total_profit, avg_transaction_amount = EXCLUDED.avg_transaction_amount; \"\"\" dw_hook.run(metrics_sql, parameters=[ summary_stats['total_transactions'], summary_stats['total_revenue'], summary_stats['total_profit'], summary_stats['avg_transaction_amount'] ]) logger.info(f\"Carga completada: {insert_count} registros procesados\") return { 'loaded_records': insert_count, 'load_timestamp': datetime.now().isoformat() } except Exception as e: logger.error(f\"Error en carga al data warehouse: {str(e)}\") raise def send_success_notification(**context): \"\"\"Env\u00eda notificaci\u00f3n de \u00e9xito con resumen del pipeline\"\"\" summary_stats = context['task_instance'].xcom_pull( task_ids='transform_and_join_data', key='summary_stats' ) try: slack_hook = SlackWebhookHook( http_conn_id='slack_webhook_conn', webhook_token=Variable.get('slack_webhook_token') ) message = f\"\"\" \u2705 *Pipeline de Datos Completado Exitosamente* *DAG:* {context['dag'].dag_id} *Fecha de ejecuci\u00f3n:* {context['execution_date']} *Resumen:* \u2022 Transacciones procesadas: {summary_stats.get('total_transactions', 0):,} \u2022 Ingresos totales: ${summary_stats.get('total_revenue', 0):,.2f} \u2022 Ganancia total: ${summary_stats.get('total_profit', 0):,.2f} \u2022 Promedio por transacci\u00f3n: ${summary_stats.get('avg_transaction_amount', 0):,.2f} \u2022 Clientes \u00fanicos: {summary_stats.get('unique_customers', 0):,} El pipeline se ejecut\u00f3 sin errores. \u2728 \"\"\" slack_hook.send_text(message) logger.info(\"Notificaci\u00f3n de \u00e9xito enviada a Slack\") except Exception as e: logger.error(f\"Error enviando notificaci\u00f3n de \u00e9xito: {str(e)}\") # Definir el DAG dag = DAG( 'comprehensive_data_pipeline', default_args=default_args, description='Pipeline completo de datos usando m\u00faltiples Hooks y Connections', schedule_interval='@daily', catchup=False, max_active_runs=1, tags=['etl', 'multi-source', 'production', 'comprehensive'] ) # Definir tareas principales extract_task = PythonOperator( task_id='extract_from_multiple_sources', python_callable=extract_from_multiple_sources, dag=dag ) transform_task = PythonOperator( task_id='transform_and_join_data', python_callable=transform_and_join_data, dag=dag ) load_task = PythonOperator( task_id='load_to_data_warehouse', python_callable=load_to_data_warehouse, dag=dag ) # Tarea de validaci\u00f3n de calidad de datos quality_check = BashOperator( task_id='data_quality_check', bash_command=\"\"\" python /opt/airflow/scripts/data_quality_check.py \\ --date {{ ds }} \\ --threshold 0.95 \"\"\", dag=dag ) notify_success = PythonOperator( task_id='send_success_notification', python_callable=send_success_notification, dag=dag, trigger_rule='all_success' ) # Definir dependencias extract_task >> transform_task >> load_task >> quality_check >> notify_success","title":"Ejemplo Avanzado: Pipeline Completo con M\u00faltiples Hooks"},{"location":"tema32/#324-lectura-incremental-vs-completa","text":"La estrategia de carga de datos es fundamental para optimizar el rendimiento, minimizar el uso de recursos y garantizar la consistencia de los datos. La elecci\u00f3n entre lectura completa e incremental depende de factores como el volumen de datos, la frecuencia de cambios, los recursos disponibles y los requisitos de latencia del negocio.","title":"3.2.4 Lectura incremental vs. completa"},{"location":"tema32/#lectura-completa-full-load","text":"La lectura completa implica extraer y procesar todo el conjunto de datos desde la fuente en cada ejecuci\u00f3n, independientemente de si los datos han cambiado o no. Esta estrategia es apropiada cuando el volumen de datos es manejable, no existe un mecanismo confiable para identificar cambios, o cuando se requiere una reconstrucci\u00f3n completa del dataset por motivos de integridad o correcci\u00f3n de errores. Ventajas Desventajas Simplicidad en la implementaci\u00f3n y l\u00f3gica de procesamiento Alto consumo de recursos computacionales y de red Garantiza consistencia completa de los datos Mayor tiempo de procesamiento No requiere mecanismos de control de cambios en la fuente Impacto en sistemas fuente por carga completa repetitiva Ideal para datasets peque\u00f1os a medianos Cargar todo un cat\u00e1logo de productos cada noche \u00datil cuando el cat\u00e1logo es relativamente peque\u00f1o y se requiere una vista completa y consistente para an\u00e1lisis o sincronizaci\u00f3n con otros sistemas. # Ejemplo con PySpark - Carga completa de cat\u00e1logo de productos from pyspark.sql import SparkSession from pyspark.sql.functions import current_timestamp def full_load_product_catalog(): spark = SparkSession.builder \\ .appName(\"ProductCatalogFullLoad\") \\ .getOrCreate() # Configuraci\u00f3n de conexi\u00f3n a base de datos fuente jdbc_url = \"jdbc:postgresql://prod-db:5432/ecommerce\" connection_properties = { \"user\": \"etl_user\", \"password\": \"secure_password\", \"driver\": \"org.postgresql.Driver\" } # Lectura completa del cat\u00e1logo products_df = spark.read \\ .jdbc(url=jdbc_url, table=\"products\", properties=connection_properties) # Agregar timestamp de procesamiento products_with_timestamp = products_df.withColumn( \"load_timestamp\", current_timestamp() ) # Escribir a data lake (sobrescribiendo completamente) products_with_timestamp.write \\ .mode(\"overwrite\") \\ .option(\"path\", \"s3a://datalake/bronze/products/\") \\ .saveAsTable(\"bronze.products\") print(f\"Carga completa finalizada. Registros procesados: {products_df.count()}\") spark.stop() # Programaci\u00f3n en Airflow from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime, timedelta default_args = { 'owner': 'data-team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'product_catalog_full_load', default_args=default_args, description='Carga completa diaria del cat\u00e1logo de productos', schedule_interval='0 2 * * *', # Diario a las 2 AM catchup=False ) full_load_task = PythonOperator( task_id='load_complete_catalog', python_callable=full_load_product_catalog, dag=dag ) Reprocesar un hist\u00f3rico completo de transacciones por correcci\u00f3n Necesario cuando se detectan errores en el procesamiento previo o cuando se implementan nuevas reglas de negocio que requieren recalcular todo el hist\u00f3rico. # Ejemplo con PySpark - Reprocesamiento completo de transacciones from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * def reprocess_complete_transaction_history(): spark = SparkSession.builder \\ .appName(\"TransactionHistoryReprocess\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .getOrCreate() # Lectura completa del hist\u00f3rico de transacciones transactions_df = spark.read \\ .option(\"multiline\", \"true\") \\ .option(\"inferSchema\", \"true\") \\ .parquet(\"s3a://datalake/raw/transactions/\") # Aplicar nuevas reglas de negocio y correcciones corrected_transactions = transactions_df \\ .withColumn(\"amount_usd\", when(col(\"currency\") == \"EUR\", col(\"amount\") * 1.1) .when(col(\"currency\") == \"GBP\", col(\"amount\") * 1.25) .otherwise(col(\"amount\"))) \\ .withColumn(\"transaction_category\", when(col(\"merchant_category\").isin([\"grocery\", \"supermarket\"]), \"essential\") .when(col(\"merchant_category\").isin([\"entertainment\", \"dining\"]), \"lifestyle\") .otherwise(\"other\")) \\ .withColumn(\"reprocessed_at\", current_timestamp()) \\ .withColumn(\"processing_version\", lit(\"v2.1\")) # Escribir datos corregidos (particionado por a\u00f1o-mes para mejor performance) corrected_transactions \\ .withColumn(\"year_month\", date_format(col(\"transaction_date\"), \"yyyy-MM\")) \\ .write \\ .mode(\"overwrite\") \\ .partitionBy(\"year_month\") \\ .option(\"path\", \"s3a://datalake/silver/transactions_corrected/\") \\ .saveAsTable(\"silver.transactions_corrected\") # Generar m\u00e9tricas de reprocesamiento total_records = corrected_transactions.count() date_range = corrected_transactions.agg( min(\"transaction_date\").alias(\"min_date\"), max(\"transaction_date\").alias(\"max_date\") ).collect()[0] print(f\"Reprocesamiento completo finalizado:\") print(f\"- Registros procesados: {total_records}\") print(f\"- Rango de fechas: {date_range['min_date']} a {date_range['max_date']}\") spark.stop() # DAG de Airflow para reprocesamiento manual reprocess_dag = DAG( 'transaction_history_reprocess', default_args=default_args, description='Reprocesamiento completo del hist\u00f3rico de transacciones', schedule_interval=None, # Ejecuci\u00f3n manual solamente catchup=False ) reprocess_task = PythonOperator( task_id='reprocess_transaction_history', python_callable=reprocess_complete_transaction_history, dag=reprocess_dag )","title":"Lectura completa (full load)"},{"location":"tema32/#lectura-incremental","text":"La lectura incremental procesa \u00fanicamente los datos nuevos o modificados desde la \u00faltima ejecuci\u00f3n, utilizando mecanismos de control como timestamps, IDs auto-incrementales, flags de estado o columnas de versionado. Esta estrategia es esencial para sistemas de alto volumen donde procesar todos los datos en cada ejecuci\u00f3n ser\u00eda ineficiente o impracticable. Ventajas Desventajas Significativa reducci\u00f3n en tiempo de procesamiento y uso de recursos Mayor complejidad en la implementaci\u00f3n Menor impacto en sistemas fuente Requiere mecanismos confiables de control de cambios Permite procesamiento en tiempo real o near-real-time Necesidad de manejar datos duplicados o fuera de orden Escalabilidad mejorada para grandes vol\u00famenes de datos Potencial p\u00e9rdida de datos si falla el mecanismo de control Cargar \u00f3rdenes nuevas con base en un campo created_at Estrategia com\u00fan para sistemas transaccionales donde se procesan solo las \u00f3rdenes creadas desde la \u00faltima ejecuci\u00f3n. # Ejemplo con PySpark - Carga incremental de \u00f3rdenes from pyspark.sql import SparkSession from pyspark.sql.functions import * from datetime import datetime, timedelta import boto3 def incremental_orders_load(): spark = SparkSession.builder \\ .appName(\"OrdersIncrementalLoad\") \\ .getOrCreate() # Obtener \u00faltimo timestamp procesado desde metastore o archivo de control def get_last_processed_timestamp(): try: last_run_df = spark.read.table(\"control.incremental_loads\") \\ .filter(col(\"table_name\") == \"orders\") \\ .orderBy(col(\"last_processed_timestamp\").desc()) \\ .limit(1) if last_run_df.count() > 0: return last_run_df.collect()[0][\"last_processed_timestamp\"] else: # Si es la primera ejecuci\u00f3n, usar fecha de inicio del proyecto return datetime(2024, 1, 1) except: return datetime(2024, 1, 1) last_timestamp = get_last_processed_timestamp() current_timestamp = datetime.now() print(f\"Procesando \u00f3rdenes desde: {last_timestamp}\") # Configuraci\u00f3n de conexi\u00f3n jdbc_url = \"jdbc:mysql://orders-db:3306/ecommerce\" connection_properties = { \"user\": \"etl_user\", \"password\": \"secure_password\", \"driver\": \"com.mysql.cj.jdbc.Driver\" } # Query incremental con filtro por timestamp incremental_query = f\"\"\" (SELECT order_id, customer_id, order_date, total_amount, status, created_at, updated_at FROM orders WHERE created_at > '{last_timestamp}' AND created_at <= '{current_timestamp}' ORDER BY created_at) as incremental_orders \"\"\" # Lectura incremental new_orders_df = spark.read \\ .jdbc(url=jdbc_url, table=incremental_query, properties=connection_properties) if new_orders_df.count() > 0: # Procesamiento y enriquecimiento de datos processed_orders = new_orders_df \\ .withColumn(\"load_timestamp\", lit(current_timestamp)) \\ .withColumn(\"order_year_month\", date_format(col(\"order_date\"), \"yyyy-MM\")) # Escribir nuevos datos (append mode) processed_orders.write \\ .mode(\"append\") \\ .partitionBy(\"order_year_month\") \\ .option(\"path\", \"s3a://datalake/bronze/orders/\") \\ .saveAsTable(\"bronze.orders\") # Actualizar tabla de control control_record = spark.createDataFrame([{ \"table_name\": \"orders\", \"last_processed_timestamp\": current_timestamp, \"records_processed\": new_orders_df.count(), \"processing_timestamp\": datetime.now() }]) control_record.write \\ .mode(\"append\") \\ .saveAsTable(\"control.incremental_loads\") print(f\"Carga incremental completada. Nuevos registros: {new_orders_df.count()}\") else: print(\"No hay nuevos registros para procesar\") spark.stop() # DAG de Airflow para carga incremental incremental_dag = DAG( 'orders_incremental_load', default_args=default_args, description='Carga incremental de \u00f3rdenes cada 15 minutos', schedule_interval='*/15 * * * *', # Cada 15 minutos catchup=True # Permite recuperar ejecuciones perdidas ) incremental_task = PythonOperator( task_id='load_incremental_orders', python_callable=incremental_orders_load, dag=incremental_dag ) Usar watermark en Spark para carga de eventos recientes desde Kafka T\u00e9cnica avanzada para procesamiento de streaming que maneja eventos tard\u00edos y garantiza procesamiento exactly-once. # Ejemplo con Spark Structured Streaming - Watermark desde Kafka from pyspark.sql import SparkSession from pyspark.sql.functions import * from pyspark.sql.types import * def kafka_streaming_with_watermark(): spark = SparkSession.builder \\ .appName(\"KafkaStreamingWatermark\") \\ .config(\"spark.sql.streaming.checkpointLocation\", \"s3a://checkpoints/kafka-events/\") \\ .getOrCreate() # Schema para eventos de Kafka event_schema = StructType([ StructField(\"event_id\", StringType(), True), StructField(\"user_id\", LongType(), True), StructField(\"event_type\", StringType(), True), StructField(\"event_timestamp\", TimestampType(), True), StructField(\"properties\", MapType(StringType(), StringType()), True) ]) # Lectura desde Kafka con configuraci\u00f3n de consumer kafka_stream = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"kafka-cluster:9092\") \\ .option(\"subscribe\", \"user-events\") \\ .option(\"startingOffsets\", \"latest\") \\ .option(\"maxOffsetsPerTrigger\", 10000) \\ .option(\"kafka.group.id\", \"spark-streaming-consumer\") \\ .load() # Parsear JSON y extraer timestamp parsed_events = kafka_stream \\ .select( from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"event\"), col(\"timestamp\").alias(\"kafka_timestamp\"), col(\"offset\"), col(\"partition\") ) \\ .select(\"event.*\", \"kafka_timestamp\", \"offset\", \"partition\") # Aplicar watermark para manejar eventos tard\u00edos # Permite eventos hasta 5 minutos tarde watermarked_events = parsed_events \\ .withWatermark(\"event_timestamp\", \"5 minutes\") # Agregaciones por ventana de tiempo con watermark windowed_aggregations = watermarked_events \\ .groupBy( window(col(\"event_timestamp\"), \"1 minute\", \"30 seconds\"), # Ventana deslizante col(\"event_type\") ) \\ .agg( count(\"*\").alias(\"event_count\"), countDistinct(\"user_id\").alias(\"unique_users\"), collect_set(\"user_id\").alias(\"user_list\") ) \\ .withColumn(\"window_start\", col(\"window.start\")) \\ .withColumn(\"window_end\", col(\"window.end\")) \\ .drop(\"window\") # Configurar sink para escritura incremental query = windowed_aggregations.writeStream \\ .outputMode(\"append\") \\ .format(\"delta\") \\ .option(\"path\", \"s3a://datalake/silver/event_aggregations/\") \\ .option(\"checkpointLocation\", \"s3a://checkpoints/event-aggregations/\") \\ .partitionBy(\"event_type\") \\ .trigger(processingTime='30 seconds') \\ .start() # Tambi\u00e9n escribir eventos raw para auditor\u00eda raw_events_query = parsed_events.writeStream \\ .outputMode(\"append\") \\ .format(\"delta\") \\ .option(\"path\", \"s3a://datalake/bronze/raw_events/\") \\ .option(\"checkpointLocation\", \"s3a://checkpoints/raw-events/\") \\ .partitionBy(date_format(col(\"event_timestamp\"), \"yyyy-MM-dd\")) \\ .trigger(processingTime='10 seconds') \\ .start() # Monitoreo del stream print(\"Streaming iniciado. Estad\u00edsticas del stream:\") print(f\"- Checkpoint location: {query.lastProgress}\") return query, raw_events_query # Integraci\u00f3n con Airflow para monitoreo y control from airflow.operators.bash import BashOperator from airflow.sensors.filesystem import FileSensor streaming_dag = DAG( 'kafka_streaming_monitor', default_args=default_args, description='Monitoreo y control de streaming desde Kafka', schedule_interval='*/5 * * * *', # Verificaci\u00f3n cada 5 minutos catchup=False ) # Sensor para verificar que el checkpoint existe checkpoint_sensor = FileSensor( task_id='check_streaming_checkpoint', filepath='s3a://checkpoints/kafka-events/_metadata', fs_conn_id='aws_default', poke_interval=60, timeout=300, dag=streaming_dag ) # Tarea para verificar salud del streaming job health_check = BashOperator( task_id='streaming_health_check', bash_command=''' # Verificar que los archivos de checkpoint se est\u00e1n actualizando latest_checkpoint=$(aws s3 ls s3://checkpoints/kafka-events/commits/ --recursive | tail -1 | awk '{print $1\" \"$2}') current_time=$(date -u +\"%Y-%m-%d %H:%M:%S\") # Calcular diferencia en minutos checkpoint_age=$(( ($(date -d \"$current_time\" +%s) - $(date -d \"$latest_checkpoint\" +%s)) / 60 )) if [ $checkpoint_age -gt 10 ]; then echo \"WARNING: Streaming job checkpoint is $checkpoint_age minutes old\" exit 1 else echo \"Streaming job is healthy. Checkpoint age: $checkpoint_age minutes\" fi ''', dag=streaming_dag ) checkpoint_sensor >> health_check Esta implementaci\u00f3n completa muestra c\u00f3mo ambas estrategias pueden ser efectivamente utilizadas seg\u00fan los requisitos espec\u00edficos del caso de uso, considerando factores como volumen de datos, latencia requerida, y recursos disponibles.","title":"Lectura incremental"},{"location":"tema32/#325-gestion-de-autenticacion-y-configuracion-segura","text":"Proteger accesos y configurar credenciales de forma segura es una pr\u00e1ctica obligatoria en entornos productivos. La gesti\u00f3n inadecuada de secretos y credenciales representa uno de los principales vectores de ataque en sistemas de datos modernos, por lo que implementar estrategias robustas de autenticaci\u00f3n es fundamental para mantener la integridad y confidencialidad de los pipelines de datos.","title":"3.2.5 Gesti\u00f3n de autenticaci\u00f3n y configuraci\u00f3n segura"},{"location":"tema32/#uso-de-variables-y-secretos","text":"La externalizaci\u00f3n de credenciales mediante sistemas especializados de gesti\u00f3n de secretos permite eliminar completamente las credenciales hardcodeadas del c\u00f3digo fuente. Estas herramientas proporcionan cifrado en reposo y en tr\u00e1nsito, control de acceso granular, rotaci\u00f3n autom\u00e1tica de credenciales y auditor\u00eda completa de accesos. Airflow ofrece m\u00faltiples backends de secretos que se integran nativamente con servicios como AWS Secrets Manager, Azure Key Vault, Google Secret Manager y HashiCorp Vault. Guardar contrase\u00f1as de bases de datos en Airflow usando Variable.get(\"db_password\") . from airflow import DAG from airflow.operators.python import PythonOperator from airflow.models import Variable from airflow.providers.postgres.hooks.postgres import PostgresHook from datetime import datetime, timedelta import logging def connect_to_database(**context): \"\"\" Funci\u00f3n que establece conexi\u00f3n segura a PostgreSQL usando variables de Airflow \"\"\" try: # Obtener credenciales de forma segura desde Variables de Airflow db_host = Variable.get(\"postgres_host\") db_port = Variable.get(\"postgres_port\", default_var=\"5432\") db_name = Variable.get(\"postgres_database\") db_user = Variable.get(\"postgres_username\") db_password = Variable.get(\"postgres_password\") # Marcada como sensitive en UI # Crear conexi\u00f3n usando PostgresHook postgres_hook = PostgresHook( postgres_conn_id='postgres_default', schema=db_name ) # Alternativa: conexi\u00f3n manual con credenciales desde variables connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\" # Ejecutar consulta de prueba records = postgres_hook.get_records(\"SELECT version();\") logging.info(f\"Conexi\u00f3n exitosa. Versi\u00f3n de PostgreSQL: {records[0][0]}\") return {\"status\": \"success\", \"connection\": \"established\"} except Exception as e: logging.error(f\"Error conectando a la base de datos: {str(e)}\") raise # Configuraci\u00f3n segura usando Airflow Secrets Backend default_args = { 'owner': 'data_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'email_on_retry': False, 'retries': 3, 'retry_delay': timedelta(minutes=5) } dag = DAG( 'secure_database_connection', default_args=default_args, description='Ejemplo de conexi\u00f3n segura a base de datos', schedule_interval='@daily', catchup=False, tags=['security', 'database'] ) # Task que utiliza credenciales seguras secure_db_task = PythonOperator( task_id='connect_secure_database', python_callable=connect_to_database, dag=dag ) Usar Azure Key Vault para proteger secretos de conexi\u00f3n a Blob Storage. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.microsoft.azure.hooks.key_vault import AzureKeyVaultHook from azure.storage.blob import BlobServiceClient from azure.identity import DefaultAzureCredential from datetime import datetime, timedelta import logging def process_blob_data_securely(**context): \"\"\" Funci\u00f3n que accede a Azure Blob Storage usando secretos desde Key Vault \"\"\" try: # Configurar Azure Key Vault Hook key_vault_hook = AzureKeyVaultHook( key_vault_name=\"your-key-vault-name\", key_vault_conn_id=\"azure_key_vault_default\" ) # Obtener secretos del Key Vault storage_account_name = key_vault_hook.get_secret(\"storage-account-name\") storage_account_key = key_vault_hook.get_secret(\"storage-account-key\") container_name = key_vault_hook.get_secret(\"blob-container-name\") # Crear cliente de Blob Storage con credenciales seguras connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\" blob_service_client = BlobServiceClient.from_connection_string(connection_string) # Listar blobs en el contenedor container_client = blob_service_client.get_container_client(container_name) blob_list = list(container_client.list_blobs()) logging.info(f\"Encontrados {len(blob_list)} blobs en el contenedor {container_name}\") # Procesar cada blob (ejemplo: leer archivos CSV) processed_files = [] for blob in blob_list[:5]: # Procesar solo los primeros 5 archivos if blob.name.endswith('.csv'): blob_client = blob_service_client.get_blob_client( container=container_name, blob=blob.name ) # Descargar contenido del blob blob_data = blob_client.download_blob() content = blob_data.readall().decode('utf-8') logging.info(f\"Procesado archivo: {blob.name}, tama\u00f1o: {len(content)} caracteres\") processed_files.append({ 'filename': blob.name, 'size': blob.size, 'last_modified': blob.last_modified }) return { \"status\": \"success\", \"processed_files\": processed_files, \"total_blobs\": len(blob_list) } except Exception as e: logging.error(f\"Error accediendo a Azure Blob Storage: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'data_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=3) } dag = DAG( 'azure_blob_secure_access', default_args=default_args, description='Acceso seguro a Azure Blob Storage con Key Vault', schedule_interval='@hourly', catchup=False, tags=['azure', 'security', 'blob-storage'] ) # Task principal secure_blob_task = PythonOperator( task_id='process_blob_data_secure', python_callable=process_blob_data_securely, dag=dag )","title":"Uso de variables y secretos"},{"location":"tema32/#autenticacion-en-servicios-y-apis","text":"La autenticaci\u00f3n en servicios externos requiere implementar protocolos est\u00e1ndar como OAuth2, JWT, o sistemas de roles basados en la nube (IAM). Estos mecanismos proporcionan acceso granular y temporal, permitiendo la rotaci\u00f3n autom\u00e1tica de credenciales y el principio de menor privilegio. La integraci\u00f3n con proveedores de identidad corporativos facilita la gesti\u00f3n centralizada de accesos y el cumplimiento de pol\u00edticas de seguridad organizacionales. Conectarse a un API de terceros mediante OAuth2. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.models import Variable from requests_oauthlib import OAuth2Session import requests import json from datetime import datetime, timedelta import logging def authenticate_and_call_api(**context): \"\"\" Funci\u00f3n que implementa flujo OAuth2 para acceder a API externa \"\"\" try: # Obtener credenciales OAuth2 desde Variables de Airflow client_id = Variable.get(\"oauth2_client_id\") client_secret = Variable.get(\"oauth2_client_secret\") token_url = Variable.get(\"oauth2_token_url\") api_base_url = Variable.get(\"api_base_url\") # Configurar datos para solicitud de token token_data = { 'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret, 'scope': 'read write' # Scopes requeridos } # Solicitar token de acceso token_response = requests.post( token_url, data=token_data, headers={'Content-Type': 'application/x-www-form-urlencoded'} ) if token_response.status_code != 200: raise Exception(f\"Error obteniendo token: {token_response.text}\") token_info = token_response.json() access_token = token_info['access_token'] logging.info(\"Token OAuth2 obtenido exitosamente\") # Usar token para llamadas a la API headers = { 'Authorization': f'Bearer {access_token}', 'Content-Type': 'application/json' } # Ejemplo: obtener datos de usuarios users_response = requests.get( f\"{api_base_url}/users\", headers=headers ) if users_response.status_code == 200: users_data = users_response.json() logging.info(f\"Obtenidos {len(users_data)} usuarios de la API\") # Ejemplo: crear un nuevo recurso new_resource = { 'name': 'Data Pipeline Resource', 'type': 'automated', 'created_by': 'airflow' } create_response = requests.post( f\"{api_base_url}/resources\", headers=headers, json=new_resource ) if create_response.status_code == 201: created_resource = create_response.json() logging.info(f\"Recurso creado con ID: {created_resource.get('id')}\") return { \"status\": \"success\", \"users_count\": len(users_data), \"resource_created\": create_response.status_code == 201 } else: raise Exception(f\"Error en API call: {users_response.text}\") except Exception as e: logging.error(f\"Error en autenticaci\u00f3n OAuth2: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'api_team', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 3, 'retry_delay': timedelta(minutes=2) } dag = DAG( 'oauth2_api_integration', default_args=default_args, description='Integraci\u00f3n con API externa usando OAuth2', schedule_interval='@daily', catchup=False, tags=['api', 'oauth2', 'integration'] ) oauth2_task = PythonOperator( task_id='authenticate_and_call_external_api', python_callable=authenticate_and_call_api, dag=dag ) Usar roles IAM en AWS para acceso a buckets S3 sin credenciales expl\u00edcitas. from airflow import DAG from airflow.operators.python import PythonOperator from airflow.providers.amazon.aws.hooks.s3 import S3Hook import boto3 from botocore.exceptions import ClientError, NoCredentialsError import pandas as pd from datetime import datetime, timedelta import logging import io def process_s3_data_with_iam_role(**context): \"\"\" Funci\u00f3n que accede a S3 usando roles IAM sin credenciales expl\u00edcitas \"\"\" try: # Usar S3Hook de Airflow (utiliza roles IAM autom\u00e1ticamente) s3_hook = S3Hook(aws_conn_id='aws_default') # Configuraci\u00f3n de bucket y prefijos bucket_name = 'your-data-bucket' input_prefix = 'raw-data/' output_prefix = 'processed-data/' # Listar objetos en el bucket objects = s3_hook.list_keys( bucket_name=bucket_name, prefix=input_prefix ) logging.info(f\"Encontrados {len(objects)} objetos en {bucket_name}/{input_prefix}\") # Procesar archivos CSV processed_files = [] for obj_key in objects: if obj_key.endswith('.csv'): try: # Leer archivo CSV desde S3 csv_content = s3_hook.read_key( key=obj_key, bucket_name=bucket_name ) # Procesar con pandas df = pd.read_csv(io.StringIO(csv_content)) # Realizar transformaciones (ejemplo) df_processed = df.copy() df_processed['processed_date'] = datetime.now().strftime('%Y-%m-%d') df_processed['record_count'] = len(df) # Guardar archivo procesado de vuelta a S3 output_key = obj_key.replace(input_prefix, output_prefix).replace('.csv', '_processed.csv') csv_buffer = io.StringIO() df_processed.to_csv(csv_buffer, index=False) s3_hook.load_string( string_data=csv_buffer.getvalue(), key=output_key, bucket_name=bucket_name, replace=True ) logging.info(f\"Procesado: {obj_key} -> {output_key}\") processed_files.append({ 'input_file': obj_key, 'output_file': output_key, 'records_processed': len(df), 'file_size_bytes': len(csv_content) }) except Exception as file_error: logging.error(f\"Error procesando {obj_key}: {str(file_error)}\") continue # Crear reporte de procesamiento report_data = { 'processing_date': datetime.now().isoformat(), 'bucket': bucket_name, 'total_files_processed': len(processed_files), 'files_detail': processed_files } # Guardar reporte en S3 report_key = f\"reports/processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\" s3_hook.load_string( string_data=json.dumps(report_data, indent=2), key=report_key, bucket_name=bucket_name, replace=True ) return { 'status': 'success', 'files_processed': len(processed_files), 'report_location': f's3://{bucket_name}/{report_key}' } except NoCredentialsError: logging.error(\"No se encontraron credenciales AWS. Verificar configuraci\u00f3n de roles IAM.\") raise except ClientError as e: logging.error(f\"Error de cliente AWS: {e.response['Error']['Message']}\") raise except Exception as e: logging.error(f\"Error general procesando datos S3: {str(e)}\") raise def verify_iam_permissions(**context): \"\"\" Funci\u00f3n auxiliar para verificar permisos IAM \"\"\" try: # Crear cliente STS para verificar identidad sts_client = boto3.client('sts') identity = sts_client.get_caller_identity() logging.info(f\"Ejecutando como: {identity.get('Arn')}\") logging.info(f\"Account ID: {identity.get('Account')}\") # Verificar acceso b\u00e1sico a S3 s3_client = boto3.client('s3') response = s3_client.list_buckets() logging.info(f\"Acceso a S3 verificado. Buckets accesibles: {len(response['Buckets'])}\") return { 'iam_role': identity.get('Arn'), 'account_id': identity.get('Account'), 's3_access': True } except Exception as e: logging.error(f\"Error verificando permisos IAM: {str(e)}\") raise # Configuraci\u00f3n del DAG default_args = { 'owner': 'data_engineering', 'depends_on_past': False, 'start_date': datetime(2024, 1, 1), 'email_on_failure': True, 'retries': 2, 'retry_delay': timedelta(minutes=5) } dag = DAG( 's3_iam_role_processing', default_args=default_args, description='Procesamiento de datos S3 usando roles IAM', schedule_interval='@daily', catchup=False, tags=['aws', 's3', 'iam', 'data-processing'] ) # Tasks del pipeline verify_permissions_task = PythonOperator( task_id='verify_iam_permissions', python_callable=verify_iam_permissions, dag=dag ) process_s3_data_task = PythonOperator( task_id='process_s3_data_with_iam', python_callable=process_s3_data_with_iam_role, dag=dag ) # Definir dependencias verify_permissions_task >> process_s3_data_task","title":"Autenticaci\u00f3n en servicios y APIs"},{"location":"tema32/#tarea","text":"Desarrolla los siguientes ejercicios pr\u00e1cticos relacionados con el tema 3.2 : Crea un pipeline en Apache Spark que lea datos de una base de datos PostgreSQL usando JDBC y los almacene como archivos Parquet. Implementa un DAG en Airflow que consuma una API REST con HttpHook y almacene los resultados en un archivo JSON en S3. Compara el rendimiento entre una lectura completa y una lectura incremental sobre una tabla con campo updated_at en MySQL. Configura una conexi\u00f3n segura en Airflow utilizando variables y crea un DAG que lea de MongoDB. Dise\u00f1a un pipeline en Spark que combine datos de una API y archivos CSV, y cargue el resultado en un Delta Lake particionado por fecha.","title":"Tarea"},{"location":"tema33/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.3. Procesamiento Escalable y Particionamiento Objetivo : Asegurar eficiencia y rendimiento en grandes vol\u00famenes de datos mediante el dise\u00f1o de flujos ETL que aprovechen t\u00e9cnicas de escalabilidad y particionamiento, optimizando recursos de c\u00f3mputo y distribuci\u00f3n de carga. Introducci\u00f3n : En contextos de Big Data, la capacidad de procesar grandes vol\u00famenes de informaci\u00f3n de forma eficiente es cr\u00edtica. A medida que los conjuntos de datos crecen, tambi\u00e9n lo hacen los desaf\u00edos asociados al rendimiento y al uso \u00f3ptimo de los recursos. El dise\u00f1o de procesos escalables, junto con una estrategia adecuada de particionamiento, es esencial para garantizar una ejecuci\u00f3n r\u00e1pida, segura y econ\u00f3mica de los pipelines ETL. Desarrollo : Este tema aborda los fundamentos y t\u00e9cnicas para construir pipelines ETL escalables. Se analizar\u00e1n los tipos de escalabilidad, t\u00e9cnicas espec\u00edficas en Apache Spark como el particionamiento y tuning de par\u00e1metros, y se comparar\u00e1n enfoques de procesamiento batch y streaming. Adem\u00e1s, se presentar\u00e1n ejemplos pr\u00e1cticos que ilustran c\u00f3mo controlar el volumen y balancear la carga computacional en entornos distribuidos. 3.3.1 Concepto de escalabilidad horizontal y vertical La escalabilidad se refiere a la capacidad de un sistema para manejar un aumento de carga o datos sin comprometer el rendimiento. Escalabilidad vertical (scale-up) Consiste en mejorar un solo nodo, aumentando sus capacidades de hardware (m\u00e1s CPU, m\u00e1s RAM, almacenamiento m\u00e1s r\u00e1pido). Incrementar la memoria RAM en un nodo Spark para permitir cargas mayores en memoria. Usar m\u00e1quinas con discos SSD de alta velocidad para mejorar I/O en procesos locales. Escalabilidad horizontal (scale-out) Consiste en distribuir la carga entre m\u00faltiples nodos para procesamiento paralelo. A\u00f1adir m\u00e1s nodos a un cl\u00faster Spark en Databricks para reducir el tiempo total de ejecuci\u00f3n de un ETL masivo. Usar auto-scaling en Amazon EMR para ajustar din\u00e1micamente los recursos al volumen de datos procesado. 3.3.2 T\u00e9cnicas de particionamiento en Spark El particionamiento eficiente permite dividir los datos para procesarlos en paralelo, reduciendo tiempos de ejecuci\u00f3n. T\u00e9cnicas clave PartitionBy : organiza los datos seg\u00fan columnas clave, \u00fatil al escribir DataFrames en formatos como Parquet. Bucketing : divide datos en buckets predefinidos; \u00fatil para joins. Repartition : redistribuye datos generando m\u00e1s particiones (shuffle costoso). Coalesce : reduce particiones sin movimiento de datos (optimizaci\u00f3n de salida). Guardar datos de ventas particionados por pa\u00eds y mes , para mejorar la velocidad de consultas agregadas sobre ventas regionales.: from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"Partitioned Sales Data\").getOrCreate() # Simulaci\u00f3n de datos de ventas data = [(\"USA\", \"2025-06\", 5000), (\"Canada\", \"2025-06\", 3500), (\"USA\", \"2025-07\", 7000), (\"Mexico\", \"2025-07\", 4200)] columns = [\"country\", \"month\", \"sales\"] df = spark.createDataFrame(data, columns) # Guardar datos particionados por pa\u00eds y mes df.write.partitionBy(\"country\", \"month\").parquet(\"sales_partitioned\") Aplicar bucketing sobre ID de usuario para facilitar joins , para mejorar la eficiencia de joins entre registros de actividad y detalles de usuario: # Guardar datos con bucketing sobre ID de usuario df.write.bucketBy(10, \"user_id\").saveAsTable(\"user_logs_bucketed\") Aqu\u00ed, los datos de user_logs_bucketed se almacenar\u00e1n en 10 buckets seg\u00fan user_id , lo que agiliza joins con otras tablas. Particionar datos de transacciones bancarias por a\u00f1o y tipo de transacci\u00f3n , para permitir consultas eficientes sobre historiales de transacciones: df.write.partitionBy(\"year\", \"transaction_type\").parquet(\"bank_transactions_partitioned\") Aplicar bucketing sobre ID de producto para mejorar joins , para optimizar combinaciones entre ventas y detalles de productos en grandes vol\u00famenes de datos: df.write.bucketBy(20, \"product_id\").saveAsTable(\"products_bucketed\") Cu\u00e1ndo usar Repartition vs. Coalesce Usar repartition(100) antes de un join masivo entre DataFrames , para distribuir uniformemente los datos antes de un join masivo, evitando particiones desbalanceadas: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Repartition Example\").getOrCreate() # Simulaci\u00f3n de DataFrames df1 = spark.range(100000).withColumnRenamed(\"id\", \"key\") df2 = spark.range(100000).withColumnRenamed(\"id\", \"key\") # Redistribuir los datos antes del join df1 = df1.repartition(100, \"key\") df2 = df2.repartition(100, \"key\") # Realizar el join df_joined = df1.join(df2, \"key\") Esto equilibra el procesamiento en m\u00faltiples nodos antes del join, mejorando el rendimiento. Usar coalesce(1) para consolidar archivos en un pipeline ETL , para reducir la cantidad de archivos generados al escribir resultados finales en almacenamiento: df_final = df_joined.coalesce(1) # Reduce las particiones al m\u00ednimo df_final.write.mode(\"overwrite\").parquet(\"final_results.parquet\") Aqu\u00ed, los datos se consolidan en un \u00fanico archivo antes de la exportaci\u00f3n, ideal para resultados de consumo humano o reportes. Usar repartition(50) antes de una agregaci\u00f3n intensiva , para garantizar una distribuci\u00f3n uniforme de los datos para evitar que una sola partici\u00f3n haga todo el trabajo: from pyspark.sql.functions import sum df_large = spark.range(1000000).withColumnRenamed(\"id\", \"transaction_id\") # Redistribuir antes de la agregaci\u00f3n df_large = df_large.repartition(50, \"transaction_id\") # Calcular la suma de valores df_aggregated = df_large.groupBy(\"transaction_id\").agg(sum(\"transaction_id\").alias(\"total\")) Repartir los datos equilibra la carga entre los workers de Spark, evitando problemas de desempe\u00f1o. Usar coalesce(5) tras una transformaci\u00f3n intensiva , para optimizar la escritura sin alterar demasiado la distribuci\u00f3n de datos: df_transformed = df_large.withColumn(\"transaction_type\", df_large.transaction_id % 3) # Reducir las particiones antes de escribir df_transformed = df_transformed.coalesce(5) df_transformed.write.mode(\"overwrite\").parquet(\"optimized_transactions.parquet\") Esto minimiza la fragmentaci\u00f3n en almacenamiento pero mantiene algo de paralelismo en la escritura. Cada t\u00e9cnica tiene su aplicaci\u00f3n espec\u00edfica, dependiendo de la necesidad de redistribuir o consolidar los datos. 3.3.3 Tuning de par\u00e1metros en Spark La optimizaci\u00f3n de los par\u00e1metros de Spark es esencial para mejorar el rendimiento y la eficiencia de los trabajos distribuidos. Aqu\u00ed te dejo una lista extendida de par\u00e1metros clave, junto con estrategias para su ajuste adecuado. Par\u00e1metros clave spark.executor.memory : Define la cantidad de memoria asignada a cada executor en el cluster. Si el trabajo involucra operaciones intensivas en memoria, como transformaciones con groupBy y join , aumenta el valor. Evita asignar m\u00e1s memoria de la disponible en el nodo, ya que puede provocar Out of Memory (OOM) . Usa spark.memory.fraction para controlar cu\u00e1nto de esta memoria se usa para almacenamiento frente a ejecuci\u00f3n. spark.sql.shuffle.partitions : N\u00famero de particiones creadas en operaciones de shuffle. Aumenta si el dataset es grande y quieres reducir la carga en cada partici\u00f3n. Para conjuntos de datos peque\u00f1os, reducir el n\u00famero puede evitar sobrecarga de peque\u00f1os archivos ( tiny partitions ). Generalmente, valores entre 200-400 funcionan bien para datos medianos. spark.executor.instances : N\u00famero de ejecutores disponibles. Incrementarlo mejora la paralelizaci\u00f3n pero requiere suficiente recursos en el cluster. Un buen punto de partida es asignar (n\u00famero de n\u00facleos del cluster / n\u00facleos por executor) , asegurando que cada executor tenga al menos 1-2 GB de RAM por n\u00facleo . spark.default.parallelism : N\u00famero de tareas paralelas por defecto. Ajusta a 2 * n\u00famero de n\u00facleos totales para un paralelismo eficiente. Si los datos son peque\u00f1os, reducir este valor puede evitar sobreasignaci\u00f3n innecesaria de tareas. spark.executor.cores : N\u00famero de n\u00facleos asignados a cada executor. Mayor n\u00famero de cores aumenta la concurrencia, pero puede sobrecargar la memoria si no se gestiona bien. Generalmente 4-8 cores por executor funciona bien en clusters grandes. spark.memory.fraction : Fracci\u00f3n de memoria de ejecuci\u00f3n disponible para almacenamiento de datos internos. Si los trabajos requieren muchas operaciones en memoria, aumentar este valor puede mejorar la eficiencia. Mantener un equilibrio con spark.memory.storageFraction para evitar presi\u00f3n sobre la memoria disponible. spark.sql.autoBroadcastJoinThreshold : Tama\u00f1o m\u00e1ximo de datos para que una tabla se transmita ( broadcast ) en joins. Si se hacen muchos joins en datasets peque\u00f1os, aumentar el valor permite optimizar los broadcast joins . Si los datasets son grandes, desactivar broadcast puede evitar consumo excesivo de memoria. spark.task.cpus : N\u00famero de n\u00facleos usados por cada tarea dentro de un executor. Para cargas de trabajo CPU-intensivas, asignar m\u00e1s n\u00facleos puede mejorar el rendimiento. Para tareas I/O-intensivas, mantener un valor bajo es mejor para paralelismo. spark.dynamicAllocation.enabled : Permite que Spark ajuste din\u00e1micamente el n\u00famero de ejecutores en funci\u00f3n de la carga de trabajo. Habilitar en entornos con variabilidad en cargas de trabajo mejora la eficiencia. Desactivarlo en clusters con recursos fijos evita fluctuaciones inesperadas. Estrategia General de Tuning Analizar la carga de trabajo : Si el trabajo es CPU-intensivo, aumentar n\u00facleos; si es memoria-intensivo, ajustar spark.executor.memory . Optimizar particiones : Reducir el n\u00famero si hay demasiados archivos peque\u00f1os, aumentarlo si las tareas tardan demasiado. Monitorear m\u00e9tricas en Spark UI : Identificar cuellos de botella con storage , execution y shuffle . Iterar pruebas : No hay valores absolutos, cada sistema tiene su configuraci\u00f3n \u00f3ptima seg\u00fan los datos y recursos disponibles. 3.3.4 Comparaci\u00f3n de procesamiento batch vs. streaming (ej. Spark Structured Streaming) Procesamiento por lotes (batch) El procesamiento por lotes es un paradigma que maneja grandes vol\u00famenes de datos almacenados de forma est\u00e1tica, ejecut\u00e1ndose en intervalos predefinidos como horarios, diarios, semanales o mensuales. Este enfoque es ideal cuando la latencia no es cr\u00edtica y se necesita procesar datos hist\u00f3ricos completos para obtener insights profundos. Spark excede en este tipo de procesamiento gracias a su capacidad de distribuci\u00f3n masiva y optimizaciones como Catalyst Optimizer y Tungsten. Las caracter\u00edsticas principales incluyen alta throughput, procesamiento de datasets completos, tolerancia a fallos robusta mediante lineage de RDDs, y la capacidad de manejar transformaciones complejas que requieren m\u00faltiples pasadas sobre los datos. El procesamiento batch permite optimizaciones avanzadas como particionado inteligente, caching estrat\u00e9gico y ejecuci\u00f3n lazy evaluation. ETL nocturno que consolida logs de ventas y los carga al DWH - Transformaci\u00f3n y limpieza de millones de registros de transacciones diarias para an\u00e1lisis de negocio. Agrupamiento semanal de datos para generaci\u00f3n de KPIs - C\u00e1lculo de m\u00e9tricas de rendimiento empresarial agregando datos de m\u00faltiples fuentes. Procesamiento de datos de machine learning para entrenamiento de modelos - Preparaci\u00f3n de datasets masivos con feature engineering, normalizaci\u00f3n y particionado para algoritmos de ML distribuidos usando MLlib. An\u00e1lisis forense de logs de seguridad - Procesamiento retrospectivo de terabytes de logs de red para identificar patrones de ataques, correlacionar eventos sospechosos y generar reportes de amenazas. Migraci\u00f3n y reconciliaci\u00f3n de bases de datos legacy - Transformaci\u00f3n masiva de esquemas antiguos, validaci\u00f3n de integridad referencial y carga incremental hacia arquitecturas modernas de data lakes. Procesamiento en tiempo real (streaming) El procesamiento en streaming maneja flujos continuos de datos conforme van llegando, proporcionando capacidades de an\u00e1lisis y respuesta con latencia ultra-baja (milisegundos a segundos). Spark Structured Streaming ofrece un modelo unificado que trata los streams como tablas infinitas, permitiendo usar la misma API de SQL y DataFrame tanto para batch como streaming, garantizando exactly-once processing y manejo autom\u00e1tico de late data. Este paradigma es fundamental para casos de uso que requieren decisiones inmediatas, monitoreo en tiempo real, alertas proactivas y experiencias interactivas. Spark Structured Streaming proporciona checkpointing autom\u00e1tico, recuperaci\u00f3n de fallos sin p\u00e9rdida de datos, y la capacidad de manejar m\u00faltiples fuentes de datos simult\u00e1neamente (Kafka, Kinesis, TCP sockets, etc.). Detecci\u00f3n en tiempo real de fraudes bancarios con Spark Structured Streaming - An\u00e1lisis instant\u00e1neo de patrones transaccionales para bloquear operaciones sospechosas antes de su ejecuci\u00f3n. Pipeline de logs de sensores IoT para monitoreo ambiental - Procesamiento continuo de m\u00e9tricas de temperatura, humedad y calidad del aire para alertas autom\u00e1ticas. An\u00e1lisis de clickstream en plataformas de e-commerce - Seguimiento en tiempo real del comportamiento de usuarios para personalizaci\u00f3n din\u00e1mica, recomendaciones instant\u00e1neas y optimizaci\u00f3n de conversion rates. Monitoreo de infraestructura y DevOps - Procesamiento continuo de m\u00e9tricas de servidores, aplicaciones y redes para detecci\u00f3n proactiva de anomal\u00edas, auto-scaling y resoluci\u00f3n autom\u00e1tica de incidentes. Trading algor\u00edtmico y an\u00e1lisis financiero - Procesamiento de feeds de mercados financieros en tiempo real para ejecutar estrategias de trading, calcular riesgos din\u00e1micamente y generar alertas de volatilidad extrema. 3.3.5 Ejemplos de control de volumen y distribuci\u00f3n de carga El dise\u00f1o efectivo de pipelines en Spark requiere estrategias sofisticadas para distribuir la carga de trabajo de manera \u00f3ptima y evitar cuellos de botella que pueden degradar significativamente el rendimiento. La gesti\u00f3n adecuada del volumen de datos y su distribuci\u00f3n a trav\u00e9s del cluster es fundamental para maximizar el paralelismo, minimizar el shuffle de datos y optimizar el uso de recursos computacionales. Esto incluye t\u00e9cnicas de pre-procesamiento, particionado inteligente, y balanceamiento din\u00e1mico de workloads. T\u00e9cnicas de control de volumen Las t\u00e9cnicas de control de volumen se enfocan en reducir la cantidad de datos que deben procesarse en las etapas m\u00e1s costosas del pipeline, aplicando principios de optimizaci\u00f3n temprana que pueden resultar en mejoras de rendimiento de \u00f3rdenes de magnitud. Filtrado temprano de datos irrelevantes antes de operaciones costosas - Aplicar filter antes de join para reducir drasticamente el volumen de datos en operaciones de shuffle intensivo, minimizando el tr\u00e1fico de red y uso de memoria. Muestreo ( sample ) para pruebas y validaciones antes del procesamiento completo - Utilizar subconjuntos representativos para validar l\u00f3gica de negocio, ajustar par\u00e1metros de configuraci\u00f3n y estimar recursos necesarios. Pushdown de predicados y proyecciones - Aprovechar optimizaciones de Catalyst Optimizer para ejecutar filtros y selecciones directamente en el storage layer (Parquet, Delta Lake), reduciendo I/O y transferencia de datos. Compresi\u00f3n y codificaci\u00f3n adaptativa - Implementar esquemas de compresi\u00f3n espec\u00edficos por tipo de dato (dictionary encoding para strings, delta encoding para timestamps) para minimizar footprint de memoria y acelerar serializaci\u00f3n. Lazy evaluation estrat\u00e9gica - Dise\u00f1ar transformaciones que aprovechan la evaluaci\u00f3n perezosa de Spark para combinar m\u00faltiples operaciones en stages optimizados, reduciendo materializaciones intermedias innecesarias. Estrategias de distribuci\u00f3n de carga La distribuci\u00f3n eficiente de carga requiere un entendimiento profundo de los patrones de acceso a datos, caracter\u00edsticas de las claves de particionado, y topolog\u00eda del cluster para evitar data skew y hotspots de procesamiento. Pre-particionar archivos de entrada en HDFS/Parquet para alinear con claves de join - Organizar datos f\u00edsicamente seg\u00fan las claves m\u00e1s frecuentemente utilizadas en joins, eliminando shuffle operations y mejorando locality. Balancear workloads entre particiones usando claves con distribuci\u00f3n uniforme - Evitar data skew seleccionando claves de particionado con cardinalidad alta y distribuci\u00f3n equilibrada, implementando t\u00e9cnicas como salting o bucketing. Dynamic partition pruning y predicate pushdown - Configurar Spark para eliminar din\u00e1micamente particiones irrelevantes durante la ejecuci\u00f3n, reduciendo significativamente el conjunto de datos a procesar. Coalescing y repartitioning inteligente - Aplicar coalesce() para consolidar particiones peque\u00f1as sin shuffle, y repartition() para redistribuir datos cuando se requiere cambiar el esquema de particionado. Broadcasting de datasets peque\u00f1os - Utilizar broadcast joins para tablas de dimensiones peque\u00f1as (<200MB), eliminando shuffle operations y mejorando dramatically el rendimiento de joins. Casos reales Pipeline de ingesti\u00f3n de logs web optimizado - En un sistema de an\u00e1lisis de logs web procesando 50TB diarios, se aplic\u00f3 filter temprano para eliminar bots y requests irrelevantes, seguido de repartition por tipo de evento y timestamp, resultando en 70% reducci\u00f3n en tiempo de procesamiento. ETL de transacciones financieras acelerado - Un ETL procesando millones de registros de transacciones fue optimizado dividiendo los datos por regi\u00f3n geogr\u00e1fica y ejecutando pipelines paralelos, implementando particionado por hash de customer_id para evitar skew, logrando 5x mejora en throughput. An\u00e1lisis de sensores IoT con particionado temporal - Sistema de procesamiento de datos de sensores industriales implement\u00f3 particionado por device_id y timestamp, con pre-agregaci\u00f3n por ventanas de tiempo, utilizando Delta Lake para optimizar upserts y reducir small files problem. Data warehouse de e-commerce con bucketing estrat\u00e9gico - Pipeline ETL para plataforma de comercio electr\u00f3nico implement\u00f3 bucketing por customer_id en tablas de facts, combinado con Z-ordering en product_id, resultando en 80% reducci\u00f3n en tiempo de queries anal\u00edticas y mejora significativa en concurrent user experience. Streaming analytics con watermarking avanzado - Sistema de an\u00e1lisis en tiempo real de eventos de aplicaciones m\u00f3viles implement\u00f3 watermarking din\u00e1mico y particionado por session_id con salting, manejando efectivamente late-arriving data y achieving sub-second latency para alertas cr\u00edticas de negocio. Tarea Desarrolla los siguientes ejercicios para aplicar los conocimientos adquiridos: Implementa un pipeline en PySpark que lea datos de transacciones, los particione por pa\u00eds y mes, y los escriba en formato Parquet. Ajusta par\u00e1metros de Spark ( executor memory , shuffle partitions ) para mejorar el rendimiento de un ETL sobre 1 mill\u00f3n de registros con m\u00faltiples joins. Compara dos pipelines: uno en batch y otro en streaming usando Spark Structured Streaming, que consuman datos simulados de sensores de temperatura. Dise\u00f1a una estrategia de particionamiento y bucketing para un sistema de recomendaciones que une ratings de usuarios y metadatos de pel\u00edculas. Realiza pruebas de escalabilidad horizontal en un entorno local o en la nube (por ejemplo, Databricks Community Edition) incrementando el n\u00famero de nodos y midiendo el tiempo de ejecuci\u00f3n de un pipeline.","title":"Procesamiento Escalable y Particionamiento"},{"location":"tema33/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema33/#tema-33-procesamiento-escalable-y-particionamiento","text":"Objetivo : Asegurar eficiencia y rendimiento en grandes vol\u00famenes de datos mediante el dise\u00f1o de flujos ETL que aprovechen t\u00e9cnicas de escalabilidad y particionamiento, optimizando recursos de c\u00f3mputo y distribuci\u00f3n de carga. Introducci\u00f3n : En contextos de Big Data, la capacidad de procesar grandes vol\u00famenes de informaci\u00f3n de forma eficiente es cr\u00edtica. A medida que los conjuntos de datos crecen, tambi\u00e9n lo hacen los desaf\u00edos asociados al rendimiento y al uso \u00f3ptimo de los recursos. El dise\u00f1o de procesos escalables, junto con una estrategia adecuada de particionamiento, es esencial para garantizar una ejecuci\u00f3n r\u00e1pida, segura y econ\u00f3mica de los pipelines ETL. Desarrollo : Este tema aborda los fundamentos y t\u00e9cnicas para construir pipelines ETL escalables. Se analizar\u00e1n los tipos de escalabilidad, t\u00e9cnicas espec\u00edficas en Apache Spark como el particionamiento y tuning de par\u00e1metros, y se comparar\u00e1n enfoques de procesamiento batch y streaming. Adem\u00e1s, se presentar\u00e1n ejemplos pr\u00e1cticos que ilustran c\u00f3mo controlar el volumen y balancear la carga computacional en entornos distribuidos.","title":"Tema 3.3. Procesamiento Escalable y Particionamiento"},{"location":"tema33/#331-concepto-de-escalabilidad-horizontal-y-vertical","text":"La escalabilidad se refiere a la capacidad de un sistema para manejar un aumento de carga o datos sin comprometer el rendimiento.","title":"3.3.1 Concepto de escalabilidad horizontal y vertical"},{"location":"tema33/#escalabilidad-vertical-scale-up","text":"Consiste en mejorar un solo nodo, aumentando sus capacidades de hardware (m\u00e1s CPU, m\u00e1s RAM, almacenamiento m\u00e1s r\u00e1pido). Incrementar la memoria RAM en un nodo Spark para permitir cargas mayores en memoria. Usar m\u00e1quinas con discos SSD de alta velocidad para mejorar I/O en procesos locales.","title":"Escalabilidad vertical (scale-up)"},{"location":"tema33/#escalabilidad-horizontal-scale-out","text":"Consiste en distribuir la carga entre m\u00faltiples nodos para procesamiento paralelo. A\u00f1adir m\u00e1s nodos a un cl\u00faster Spark en Databricks para reducir el tiempo total de ejecuci\u00f3n de un ETL masivo. Usar auto-scaling en Amazon EMR para ajustar din\u00e1micamente los recursos al volumen de datos procesado.","title":"Escalabilidad horizontal (scale-out)"},{"location":"tema33/#332-tecnicas-de-particionamiento-en-spark","text":"El particionamiento eficiente permite dividir los datos para procesarlos en paralelo, reduciendo tiempos de ejecuci\u00f3n.","title":"3.3.2 T\u00e9cnicas de particionamiento en Spark"},{"location":"tema33/#tecnicas-clave","text":"PartitionBy : organiza los datos seg\u00fan columnas clave, \u00fatil al escribir DataFrames en formatos como Parquet. Bucketing : divide datos en buckets predefinidos; \u00fatil para joins. Repartition : redistribuye datos generando m\u00e1s particiones (shuffle costoso). Coalesce : reduce particiones sin movimiento de datos (optimizaci\u00f3n de salida). Guardar datos de ventas particionados por pa\u00eds y mes , para mejorar la velocidad de consultas agregadas sobre ventas regionales.: from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"Partitioned Sales Data\").getOrCreate() # Simulaci\u00f3n de datos de ventas data = [(\"USA\", \"2025-06\", 5000), (\"Canada\", \"2025-06\", 3500), (\"USA\", \"2025-07\", 7000), (\"Mexico\", \"2025-07\", 4200)] columns = [\"country\", \"month\", \"sales\"] df = spark.createDataFrame(data, columns) # Guardar datos particionados por pa\u00eds y mes df.write.partitionBy(\"country\", \"month\").parquet(\"sales_partitioned\") Aplicar bucketing sobre ID de usuario para facilitar joins , para mejorar la eficiencia de joins entre registros de actividad y detalles de usuario: # Guardar datos con bucketing sobre ID de usuario df.write.bucketBy(10, \"user_id\").saveAsTable(\"user_logs_bucketed\") Aqu\u00ed, los datos de user_logs_bucketed se almacenar\u00e1n en 10 buckets seg\u00fan user_id , lo que agiliza joins con otras tablas. Particionar datos de transacciones bancarias por a\u00f1o y tipo de transacci\u00f3n , para permitir consultas eficientes sobre historiales de transacciones: df.write.partitionBy(\"year\", \"transaction_type\").parquet(\"bank_transactions_partitioned\") Aplicar bucketing sobre ID de producto para mejorar joins , para optimizar combinaciones entre ventas y detalles de productos en grandes vol\u00famenes de datos: df.write.bucketBy(20, \"product_id\").saveAsTable(\"products_bucketed\")","title":"T\u00e9cnicas clave"},{"location":"tema33/#cuando-usar-repartition-vs-coalesce","text":"Usar repartition(100) antes de un join masivo entre DataFrames , para distribuir uniformemente los datos antes de un join masivo, evitando particiones desbalanceadas: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Repartition Example\").getOrCreate() # Simulaci\u00f3n de DataFrames df1 = spark.range(100000).withColumnRenamed(\"id\", \"key\") df2 = spark.range(100000).withColumnRenamed(\"id\", \"key\") # Redistribuir los datos antes del join df1 = df1.repartition(100, \"key\") df2 = df2.repartition(100, \"key\") # Realizar el join df_joined = df1.join(df2, \"key\") Esto equilibra el procesamiento en m\u00faltiples nodos antes del join, mejorando el rendimiento. Usar coalesce(1) para consolidar archivos en un pipeline ETL , para reducir la cantidad de archivos generados al escribir resultados finales en almacenamiento: df_final = df_joined.coalesce(1) # Reduce las particiones al m\u00ednimo df_final.write.mode(\"overwrite\").parquet(\"final_results.parquet\") Aqu\u00ed, los datos se consolidan en un \u00fanico archivo antes de la exportaci\u00f3n, ideal para resultados de consumo humano o reportes. Usar repartition(50) antes de una agregaci\u00f3n intensiva , para garantizar una distribuci\u00f3n uniforme de los datos para evitar que una sola partici\u00f3n haga todo el trabajo: from pyspark.sql.functions import sum df_large = spark.range(1000000).withColumnRenamed(\"id\", \"transaction_id\") # Redistribuir antes de la agregaci\u00f3n df_large = df_large.repartition(50, \"transaction_id\") # Calcular la suma de valores df_aggregated = df_large.groupBy(\"transaction_id\").agg(sum(\"transaction_id\").alias(\"total\")) Repartir los datos equilibra la carga entre los workers de Spark, evitando problemas de desempe\u00f1o. Usar coalesce(5) tras una transformaci\u00f3n intensiva , para optimizar la escritura sin alterar demasiado la distribuci\u00f3n de datos: df_transformed = df_large.withColumn(\"transaction_type\", df_large.transaction_id % 3) # Reducir las particiones antes de escribir df_transformed = df_transformed.coalesce(5) df_transformed.write.mode(\"overwrite\").parquet(\"optimized_transactions.parquet\") Esto minimiza la fragmentaci\u00f3n en almacenamiento pero mantiene algo de paralelismo en la escritura. Cada t\u00e9cnica tiene su aplicaci\u00f3n espec\u00edfica, dependiendo de la necesidad de redistribuir o consolidar los datos.","title":"Cu\u00e1ndo usar Repartition vs. Coalesce"},{"location":"tema33/#333-tuning-de-parametros-en-spark","text":"La optimizaci\u00f3n de los par\u00e1metros de Spark es esencial para mejorar el rendimiento y la eficiencia de los trabajos distribuidos. Aqu\u00ed te dejo una lista extendida de par\u00e1metros clave, junto con estrategias para su ajuste adecuado.","title":"3.3.3 Tuning de par\u00e1metros en Spark"},{"location":"tema33/#parametros-clave","text":"spark.executor.memory : Define la cantidad de memoria asignada a cada executor en el cluster. Si el trabajo involucra operaciones intensivas en memoria, como transformaciones con groupBy y join , aumenta el valor. Evita asignar m\u00e1s memoria de la disponible en el nodo, ya que puede provocar Out of Memory (OOM) . Usa spark.memory.fraction para controlar cu\u00e1nto de esta memoria se usa para almacenamiento frente a ejecuci\u00f3n. spark.sql.shuffle.partitions : N\u00famero de particiones creadas en operaciones de shuffle. Aumenta si el dataset es grande y quieres reducir la carga en cada partici\u00f3n. Para conjuntos de datos peque\u00f1os, reducir el n\u00famero puede evitar sobrecarga de peque\u00f1os archivos ( tiny partitions ). Generalmente, valores entre 200-400 funcionan bien para datos medianos. spark.executor.instances : N\u00famero de ejecutores disponibles. Incrementarlo mejora la paralelizaci\u00f3n pero requiere suficiente recursos en el cluster. Un buen punto de partida es asignar (n\u00famero de n\u00facleos del cluster / n\u00facleos por executor) , asegurando que cada executor tenga al menos 1-2 GB de RAM por n\u00facleo . spark.default.parallelism : N\u00famero de tareas paralelas por defecto. Ajusta a 2 * n\u00famero de n\u00facleos totales para un paralelismo eficiente. Si los datos son peque\u00f1os, reducir este valor puede evitar sobreasignaci\u00f3n innecesaria de tareas. spark.executor.cores : N\u00famero de n\u00facleos asignados a cada executor. Mayor n\u00famero de cores aumenta la concurrencia, pero puede sobrecargar la memoria si no se gestiona bien. Generalmente 4-8 cores por executor funciona bien en clusters grandes. spark.memory.fraction : Fracci\u00f3n de memoria de ejecuci\u00f3n disponible para almacenamiento de datos internos. Si los trabajos requieren muchas operaciones en memoria, aumentar este valor puede mejorar la eficiencia. Mantener un equilibrio con spark.memory.storageFraction para evitar presi\u00f3n sobre la memoria disponible. spark.sql.autoBroadcastJoinThreshold : Tama\u00f1o m\u00e1ximo de datos para que una tabla se transmita ( broadcast ) en joins. Si se hacen muchos joins en datasets peque\u00f1os, aumentar el valor permite optimizar los broadcast joins . Si los datasets son grandes, desactivar broadcast puede evitar consumo excesivo de memoria. spark.task.cpus : N\u00famero de n\u00facleos usados por cada tarea dentro de un executor. Para cargas de trabajo CPU-intensivas, asignar m\u00e1s n\u00facleos puede mejorar el rendimiento. Para tareas I/O-intensivas, mantener un valor bajo es mejor para paralelismo. spark.dynamicAllocation.enabled : Permite que Spark ajuste din\u00e1micamente el n\u00famero de ejecutores en funci\u00f3n de la carga de trabajo. Habilitar en entornos con variabilidad en cargas de trabajo mejora la eficiencia. Desactivarlo en clusters con recursos fijos evita fluctuaciones inesperadas.","title":"Par\u00e1metros clave"},{"location":"tema33/#estrategia-general-de-tuning","text":"Analizar la carga de trabajo : Si el trabajo es CPU-intensivo, aumentar n\u00facleos; si es memoria-intensivo, ajustar spark.executor.memory . Optimizar particiones : Reducir el n\u00famero si hay demasiados archivos peque\u00f1os, aumentarlo si las tareas tardan demasiado. Monitorear m\u00e9tricas en Spark UI : Identificar cuellos de botella con storage , execution y shuffle . Iterar pruebas : No hay valores absolutos, cada sistema tiene su configuraci\u00f3n \u00f3ptima seg\u00fan los datos y recursos disponibles.","title":"Estrategia General de Tuning"},{"location":"tema33/#334-comparacion-de-procesamiento-batch-vs-streaming-ej-spark-structured-streaming","text":"","title":"3.3.4 Comparaci\u00f3n de procesamiento batch vs. streaming (ej. Spark Structured Streaming)"},{"location":"tema33/#procesamiento-por-lotes-batch","text":"El procesamiento por lotes es un paradigma que maneja grandes vol\u00famenes de datos almacenados de forma est\u00e1tica, ejecut\u00e1ndose en intervalos predefinidos como horarios, diarios, semanales o mensuales. Este enfoque es ideal cuando la latencia no es cr\u00edtica y se necesita procesar datos hist\u00f3ricos completos para obtener insights profundos. Spark excede en este tipo de procesamiento gracias a su capacidad de distribuci\u00f3n masiva y optimizaciones como Catalyst Optimizer y Tungsten. Las caracter\u00edsticas principales incluyen alta throughput, procesamiento de datasets completos, tolerancia a fallos robusta mediante lineage de RDDs, y la capacidad de manejar transformaciones complejas que requieren m\u00faltiples pasadas sobre los datos. El procesamiento batch permite optimizaciones avanzadas como particionado inteligente, caching estrat\u00e9gico y ejecuci\u00f3n lazy evaluation. ETL nocturno que consolida logs de ventas y los carga al DWH - Transformaci\u00f3n y limpieza de millones de registros de transacciones diarias para an\u00e1lisis de negocio. Agrupamiento semanal de datos para generaci\u00f3n de KPIs - C\u00e1lculo de m\u00e9tricas de rendimiento empresarial agregando datos de m\u00faltiples fuentes. Procesamiento de datos de machine learning para entrenamiento de modelos - Preparaci\u00f3n de datasets masivos con feature engineering, normalizaci\u00f3n y particionado para algoritmos de ML distribuidos usando MLlib. An\u00e1lisis forense de logs de seguridad - Procesamiento retrospectivo de terabytes de logs de red para identificar patrones de ataques, correlacionar eventos sospechosos y generar reportes de amenazas. Migraci\u00f3n y reconciliaci\u00f3n de bases de datos legacy - Transformaci\u00f3n masiva de esquemas antiguos, validaci\u00f3n de integridad referencial y carga incremental hacia arquitecturas modernas de data lakes.","title":"Procesamiento por lotes (batch)"},{"location":"tema33/#procesamiento-en-tiempo-real-streaming","text":"El procesamiento en streaming maneja flujos continuos de datos conforme van llegando, proporcionando capacidades de an\u00e1lisis y respuesta con latencia ultra-baja (milisegundos a segundos). Spark Structured Streaming ofrece un modelo unificado que trata los streams como tablas infinitas, permitiendo usar la misma API de SQL y DataFrame tanto para batch como streaming, garantizando exactly-once processing y manejo autom\u00e1tico de late data. Este paradigma es fundamental para casos de uso que requieren decisiones inmediatas, monitoreo en tiempo real, alertas proactivas y experiencias interactivas. Spark Structured Streaming proporciona checkpointing autom\u00e1tico, recuperaci\u00f3n de fallos sin p\u00e9rdida de datos, y la capacidad de manejar m\u00faltiples fuentes de datos simult\u00e1neamente (Kafka, Kinesis, TCP sockets, etc.). Detecci\u00f3n en tiempo real de fraudes bancarios con Spark Structured Streaming - An\u00e1lisis instant\u00e1neo de patrones transaccionales para bloquear operaciones sospechosas antes de su ejecuci\u00f3n. Pipeline de logs de sensores IoT para monitoreo ambiental - Procesamiento continuo de m\u00e9tricas de temperatura, humedad y calidad del aire para alertas autom\u00e1ticas. An\u00e1lisis de clickstream en plataformas de e-commerce - Seguimiento en tiempo real del comportamiento de usuarios para personalizaci\u00f3n din\u00e1mica, recomendaciones instant\u00e1neas y optimizaci\u00f3n de conversion rates. Monitoreo de infraestructura y DevOps - Procesamiento continuo de m\u00e9tricas de servidores, aplicaciones y redes para detecci\u00f3n proactiva de anomal\u00edas, auto-scaling y resoluci\u00f3n autom\u00e1tica de incidentes. Trading algor\u00edtmico y an\u00e1lisis financiero - Procesamiento de feeds de mercados financieros en tiempo real para ejecutar estrategias de trading, calcular riesgos din\u00e1micamente y generar alertas de volatilidad extrema.","title":"Procesamiento en tiempo real (streaming)"},{"location":"tema33/#335-ejemplos-de-control-de-volumen-y-distribucion-de-carga","text":"El dise\u00f1o efectivo de pipelines en Spark requiere estrategias sofisticadas para distribuir la carga de trabajo de manera \u00f3ptima y evitar cuellos de botella que pueden degradar significativamente el rendimiento. La gesti\u00f3n adecuada del volumen de datos y su distribuci\u00f3n a trav\u00e9s del cluster es fundamental para maximizar el paralelismo, minimizar el shuffle de datos y optimizar el uso de recursos computacionales. Esto incluye t\u00e9cnicas de pre-procesamiento, particionado inteligente, y balanceamiento din\u00e1mico de workloads.","title":"3.3.5 Ejemplos de control de volumen y distribuci\u00f3n de carga"},{"location":"tema33/#tecnicas-de-control-de-volumen","text":"Las t\u00e9cnicas de control de volumen se enfocan en reducir la cantidad de datos que deben procesarse en las etapas m\u00e1s costosas del pipeline, aplicando principios de optimizaci\u00f3n temprana que pueden resultar en mejoras de rendimiento de \u00f3rdenes de magnitud. Filtrado temprano de datos irrelevantes antes de operaciones costosas - Aplicar filter antes de join para reducir drasticamente el volumen de datos en operaciones de shuffle intensivo, minimizando el tr\u00e1fico de red y uso de memoria. Muestreo ( sample ) para pruebas y validaciones antes del procesamiento completo - Utilizar subconjuntos representativos para validar l\u00f3gica de negocio, ajustar par\u00e1metros de configuraci\u00f3n y estimar recursos necesarios. Pushdown de predicados y proyecciones - Aprovechar optimizaciones de Catalyst Optimizer para ejecutar filtros y selecciones directamente en el storage layer (Parquet, Delta Lake), reduciendo I/O y transferencia de datos. Compresi\u00f3n y codificaci\u00f3n adaptativa - Implementar esquemas de compresi\u00f3n espec\u00edficos por tipo de dato (dictionary encoding para strings, delta encoding para timestamps) para minimizar footprint de memoria y acelerar serializaci\u00f3n. Lazy evaluation estrat\u00e9gica - Dise\u00f1ar transformaciones que aprovechan la evaluaci\u00f3n perezosa de Spark para combinar m\u00faltiples operaciones en stages optimizados, reduciendo materializaciones intermedias innecesarias.","title":"T\u00e9cnicas de control de volumen"},{"location":"tema33/#estrategias-de-distribucion-de-carga","text":"La distribuci\u00f3n eficiente de carga requiere un entendimiento profundo de los patrones de acceso a datos, caracter\u00edsticas de las claves de particionado, y topolog\u00eda del cluster para evitar data skew y hotspots de procesamiento. Pre-particionar archivos de entrada en HDFS/Parquet para alinear con claves de join - Organizar datos f\u00edsicamente seg\u00fan las claves m\u00e1s frecuentemente utilizadas en joins, eliminando shuffle operations y mejorando locality. Balancear workloads entre particiones usando claves con distribuci\u00f3n uniforme - Evitar data skew seleccionando claves de particionado con cardinalidad alta y distribuci\u00f3n equilibrada, implementando t\u00e9cnicas como salting o bucketing. Dynamic partition pruning y predicate pushdown - Configurar Spark para eliminar din\u00e1micamente particiones irrelevantes durante la ejecuci\u00f3n, reduciendo significativamente el conjunto de datos a procesar. Coalescing y repartitioning inteligente - Aplicar coalesce() para consolidar particiones peque\u00f1as sin shuffle, y repartition() para redistribuir datos cuando se requiere cambiar el esquema de particionado. Broadcasting de datasets peque\u00f1os - Utilizar broadcast joins para tablas de dimensiones peque\u00f1as (<200MB), eliminando shuffle operations y mejorando dramatically el rendimiento de joins.","title":"Estrategias de distribuci\u00f3n de carga"},{"location":"tema33/#casos-reales","text":"Pipeline de ingesti\u00f3n de logs web optimizado - En un sistema de an\u00e1lisis de logs web procesando 50TB diarios, se aplic\u00f3 filter temprano para eliminar bots y requests irrelevantes, seguido de repartition por tipo de evento y timestamp, resultando en 70% reducci\u00f3n en tiempo de procesamiento. ETL de transacciones financieras acelerado - Un ETL procesando millones de registros de transacciones fue optimizado dividiendo los datos por regi\u00f3n geogr\u00e1fica y ejecutando pipelines paralelos, implementando particionado por hash de customer_id para evitar skew, logrando 5x mejora en throughput. An\u00e1lisis de sensores IoT con particionado temporal - Sistema de procesamiento de datos de sensores industriales implement\u00f3 particionado por device_id y timestamp, con pre-agregaci\u00f3n por ventanas de tiempo, utilizando Delta Lake para optimizar upserts y reducir small files problem. Data warehouse de e-commerce con bucketing estrat\u00e9gico - Pipeline ETL para plataforma de comercio electr\u00f3nico implement\u00f3 bucketing por customer_id en tablas de facts, combinado con Z-ordering en product_id, resultando en 80% reducci\u00f3n en tiempo de queries anal\u00edticas y mejora significativa en concurrent user experience. Streaming analytics con watermarking avanzado - Sistema de an\u00e1lisis en tiempo real de eventos de aplicaciones m\u00f3viles implement\u00f3 watermarking din\u00e1mico y particionado por session_id con salting, manejando efectivamente late-arriving data y achieving sub-second latency para alertas cr\u00edticas de negocio.","title":"Casos reales"},{"location":"tema33/#tarea","text":"Desarrolla los siguientes ejercicios para aplicar los conocimientos adquiridos: Implementa un pipeline en PySpark que lea datos de transacciones, los particione por pa\u00eds y mes, y los escriba en formato Parquet. Ajusta par\u00e1metros de Spark ( executor memory , shuffle partitions ) para mejorar el rendimiento de un ETL sobre 1 mill\u00f3n de registros con m\u00faltiples joins. Compara dos pipelines: uno en batch y otro en streaming usando Spark Structured Streaming, que consuman datos simulados de sensores de temperatura. Dise\u00f1a una estrategia de particionamiento y bucketing para un sistema de recomendaciones que une ratings de usuarios y metadatos de pel\u00edculas. Realiza pruebas de escalabilidad horizontal en un entorno local o en la nube (por ejemplo, Databricks Community Edition) incrementando el n\u00famero de nodos y midiendo el tiempo de ejecuci\u00f3n de un pipeline.","title":"Tarea"},{"location":"tema34/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.4. Manejo de Esquemas y Calidad de Datos Objetivo : Prevenir fallos estructurales y mantener la integridad de los datos mediante el dise\u00f1o, control y validaci\u00f3n de esquemas, as\u00ed como la aplicaci\u00f3n de t\u00e9cnicas de calidad y monitoreo de datos durante los procesos ETL. Introducci\u00f3n : En los sistemas de Big Data, los datos provienen de m\u00faltiples fuentes, formatos y estructuras. Asegurar que los esquemas de datos sean consistentes, flexibles ante cambios, y que los datos cumplan con criterios de calidad definidos, es esencial para garantizar flujos ETL robustos, escalables y seguros. Este tema aborda los conceptos y herramientas clave que permiten controlar la estructura de los datos y mitigar riesgos comunes como fallos por incompatibilidad de esquemas, errores de transformaci\u00f3n o baja calidad de los datos. Desarrollo : El manejo de esquemas en flujos ETL implica definir reglas claras sobre la estructura de los datos que se procesan, ya sea de forma expl\u00edcita (definida por el ingeniero de datos) o inferida autom\u00e1ticamente por herramientas como Apache Spark. A medida que los sistemas evolucionan, tambi\u00e9n lo hacen los esquemas, lo que obliga a gestionar adecuadamente su evoluci\u00f3n sin interrumpir los pipelines. Adem\u00e1s, es indispensable validar la calidad de los datos mediante reglas autom\u00e1ticas, controlar los errores y registrar las excepciones de transformaci\u00f3n. Este tema presenta t\u00e9cnicas, herramientas y ejemplos pr\u00e1cticos para el manejo avanzado de esquemas y la calidad de datos en arquitecturas modernas. 3.4.1 Esquemas expl\u00edcitos vs. inferidos La definici\u00f3n del esquema de datos puede realizarse de manera expl\u00edcita o ser inferida autom\u00e1ticamente por las herramientas de procesamiento como Spark. Cada enfoque tiene implicaciones directas sobre la robustez, flexibilidad y trazabilidad de los pipelines ETL. Ventajas y riesgos de los esquemas expl\u00edcitos Un esquema expl\u00edcito proporciona una estructura rigurosa y bien definida para los datos, lo que es esencial en entornos productivos y cr\u00edticos donde la integridad es clave. Al establecer de antemano los tipos de datos, nombres de columnas y estructuras esperadas, se minimizan errores derivados de datos mal formateados o inconsistencias, permitiendo validaciones m\u00e1s estrictas y optimizaci\u00f3n en el procesamiento. Esto mejora el rendimiento en consultas y operaciones, ya que el motor de ejecuci\u00f3n puede aprovechar la informaci\u00f3n estructural para optimizar los planes de ejecuci\u00f3n. Adem\u00e1s, facilita la interoperabilidad entre sistemas al garantizar que los datos siempre siguen un formato preestablecido. Sin embargo, esta rigidez tambi\u00e9n puede representar desaf\u00edos. En entornos donde la flexibilidad es esencial, como procesamiento de datos semi-estructurados o ingesti\u00f3n de datos din\u00e1micos, un esquema expl\u00edcito puede ser limitante. Cambios en la estructura de los datos pueden requerir modificaciones en el esquema, lo que conlleva tiempo y esfuerzo de mantenimiento. Adem\u00e1s, en grandes vol\u00famenes de datos, definir un esquema fijo sin conocer completamente la naturaleza de los datos puede generar problemas de compatibilidad y p\u00e9rdida de informaci\u00f3n. Por estas razones, en casos de exploraci\u00f3n de datos, se puede preferir un enfoque m\u00e1s adaptable como el esquema inferido. Aspecto Ventajas Riesgos Integridad y validaci\u00f3n Garantiza datos consistentes y previene errores de formato Puede bloquear datos que no se ajusten al esquema, limitando flexibilidad Optimizaci\u00f3n del rendimiento Mejora la velocidad de consultas y procesamiento al aprovechar la estructura definida Puede generar sobrecarga en modificaciones o adaptaciones futuras Interoperabilidad Facilita integraci\u00f3n con otros sistemas mediante formatos predefinidos Requiere coordinaci\u00f3n rigurosa para cambios y actualizaciones Adaptabilidad Ideal para datos estructurados y fuentes confiables Poco adecuado para datos semi-estructurados o en evoluci\u00f3n constante Mantenimiento Reduce la posibilidad de errores operacionales y facilita auditor\u00eda Necesita cambios manuales si se modifica la fuente de datos Si el contexto demanda estabilidad y predictibilidad, un esquema expl\u00edcito es una gran ventaja. Sin embargo, si el objetivo es manejar datos cambiantes o no estructurados, es recomendable evaluar alternativas m\u00e1s flexibles. Pipeline de Datos de IoT en Manufactura Garantiza que las alertas cr\u00edticas de temperatura y presi\u00f3n siempre tengan tipos correctos, evitando fallos en sistemas de seguridad industrial. from pyspark.sql.types import * # Esquema expl\u00edcito para sensores industriales sensor_schema = StructType([ StructField(\"sensor_id\", StringType(), False), StructField(\"timestamp\", TimestampType(), False), StructField(\"temperature\", DoubleType(), False), StructField(\"pressure\", DoubleType(), False), StructField(\"vibration\", DoubleType(), True), StructField(\"status\", StringType(), False) ]) df_sensors = spark.read.schema(sensor_schema).json(\"hdfs://sensors/data/\") Data Warehouse de E-commerce Evita que montos con formato incorrecto contaminen reportes financieros cr\u00edticos. # Esquema para tabla de \u00f3rdenes con validaciones estrictas orders_schema = StructType([ StructField(\"order_id\", LongType(), False), StructField(\"customer_id\", LongType(), False), StructField(\"order_date\", DateType(), False), StructField(\"total_amount\", DecimalType(10,2), False), StructField(\"currency\", StringType(), False), StructField(\"payment_method\", StringType(), False) ]) # Cualquier registro que no cumpla el esquema ser\u00e1 rechazado df_orders = spark.read.schema(orders_schema).option(\"mode\", \"FAILFAST\").parquet(\"s3://orders/\") Pipeline de Logs de Aplicaci\u00f3n Garantiza que campos cr\u00edticos como timestamp y level siempre est\u00e9n presentes para monitoreo y alertas. # Esquema para logs estructurados con campos obligatorios log_schema = StructType([ StructField(\"timestamp\", TimestampType(), False), StructField(\"level\", StringType(), False), StructField(\"service\", StringType(), False), StructField(\"message\", StringType(), False), StructField(\"user_id\", StringType(), True), StructField(\"request_id\", StringType(), True), StructField(\"duration_ms\", IntegerType(), True) ]) df_logs = spark.readStream.schema(log_schema).json(\"kafka://logs-topic\") Ventajas y riesgos de los esquemas inferidos Los esquemas inferidos ofrecen una soluci\u00f3n flexible y \u00e1gil en escenarios donde la naturaleza de los datos puede variar o evolucionar con el tiempo. En entornos de exploraci\u00f3n y prototipado, permiten cargar y procesar datos sin una definici\u00f3n estricta, lo que agiliza el desarrollo y facilita la integraci\u00f3n con fuentes heterog\u00e9neas. Esta adaptabilidad es especialmente \u00fatil en sistemas que reciben datos de m\u00faltiples or\u00edgenes o formatos desconocidos, permitiendo ajustes autom\u00e1ticos sin intervenci\u00f3n manual. Adem\u00e1s, en arquitecturas de big data como PySpark, el esquema inferido puede mejorar la facilidad de uso al eliminar la necesidad de definir expl\u00edcitamente cada estructura antes de su procesamiento. Sin embargo, esta flexibilidad conlleva ciertos riesgos. La falta de control en la definici\u00f3n de los datos puede generar errores silenciosos si los valores cambian inesperadamente, afectando la integridad de los procesos downstream. Asimismo, si los datos presentan inconsistencias, el motor de ejecuci\u00f3n podr\u00eda inferir tipos incorrectos, generando problemas de compatibilidad y fallos dif\u00edciles de detectar. En sistemas de producci\u00f3n, depender de esquemas inferidos puede provocar ineficiencias al requerir validaciones posteriores y ajustes constantes. Por ello, es fundamental evaluar el contexto antes de optar por este enfoque, combin\u00e1ndolo con estrategias que mitiguen posibles inconvenientes. Aspecto Ventajas Riesgos Flexibilidad Admite datos din\u00e1micos sin definir esquemas r\u00edgidos Puede derivar en inconsistencias si los datos cambian inesperadamente Rapidez de implementaci\u00f3n Reduce la necesidad de configuraci\u00f3n manual en entornos de exploraci\u00f3n Puede generar errores dif\u00edciles de detectar en sistemas cr\u00edticos Interoperabilidad Facilita la ingesti\u00f3n de m\u00faltiples formatos sin restricciones previas Menor control sobre el formato y calidad de los datos Adaptabilidad Ideal para prototipado y an\u00e1lisis de datos desconocidos Puede ocasionar problemas de compatibilidad en procesos posteriores Mantenimiento Minimiza esfuerzo inicial en definici\u00f3n de datos Puede generar costos adicionales de correcci\u00f3n y validaci\u00f3n en producci\u00f3n Un esquema inferido puede ser invaluable para exploraci\u00f3n y modelos de datos altamente cambiantes, pero es crucial establecer controles para minimizar errores ocultos. An\u00e1lisis de Redes Sociales Permite procesar datos de m\u00faltiples plataformas sociales sin definir esquemas espec\u00edficos para cada una. # Esquema inferido para datos variables de APIs sociales df_social = spark.read.option(\"multiline\", \"true\").json(\"s3://social-data/*/\") # Los campos pueden variar seg\u00fan la plataforma (Twitter, Facebook, Instagram) df_social.printSchema() # Muestra la estructura inferida din\u00e1micamente # Manejo seguro de campos opcionales df_processed = df_social.select( col(\"user_id\"), col(\"timestamp\"), col(\"text\").alias(\"content\"), col(\"likes\").cast(\"int\").alias(\"engagement_likes\"), col(\"shares\").cast(\"int\").alias(\"engagement_shares\") # Puede no existir en todas las plataformas ) Migraci\u00f3n de Bases de Datos Legacy Facilita la migraci\u00f3n de sistemas legacy donde la documentaci\u00f3n del esquema puede estar desactualizada o perdida. # Inferencia autom\u00e1tica para tablas con esquemas desconocidos df_legacy = spark.read.format(\"jdbc\") \\ .option(\"url\", \"jdbc:mysql://legacy-db:3306/old_system\") \\ .option(\"dbtable\", \"unknown_table\") \\ .option(\"inferSchema\", \"true\") \\ .load() # Exploraci\u00f3n r\u00e1pida de estructura df_legacy.describe().show() df_legacy.dtypes # Verificar tipos inferidos # Transformaci\u00f3n adaptativa for col_name, col_type in df_legacy.dtypes: if col_type == 'string' and 'date' in col_name.lower(): df_legacy = df_legacy.withColumn(col_name, to_date(col(col_name))) Prototipado de ML con Datos Externos Permite iniciar r\u00e1pidamente experimentos de ML sin invertir tiempo en definici\u00f3n manual de esquemas. # Carga r\u00e1pida para experimentaci\u00f3n con datasets p\u00fablicos df_experiment = spark.read.option(\"header\", \"true\") \\ .option(\"inferSchema\", \"true\") \\ .csv(\"s3://public-datasets/kaggle-competition/\") # An\u00e1lisis exploratorio inmediato df_experiment.summary().show() # Feature engineering r\u00e1pido sin conocimiento previo del esquema numeric_columns = [col_name for col_name, col_type in df_experiment.dtypes if col_type in ['int', 'double', 'float']] df_scaled = df_experiment.select( *[col(c) for c in numeric_columns] ).fillna(0) Enfoque H\u00edbrido El enfoque h\u00edbrido combina lo mejor de los esquemas expl\u00edcitos e inferidos, equilibrando estabilidad y flexibilidad. La estrategia consiste en definir un esquema expl\u00edcito para los aspectos cr\u00edticos del procesamiento de datos, garantizando integridad y eficiencia en las consultas, mientras se permite la inferencia de ciertos campos menos estructurados o de datos con variabilidad impredecible. Esto facilita la compatibilidad con datos din\u00e1micos sin comprometer la robustez del sistema. En entornos de big data y procesamiento distribuido como Spark, este enfoque puede aprovechar las ventajas de optimizaci\u00f3n y validaci\u00f3n de los esquemas expl\u00edcitos mientras mantiene adaptabilidad a cambios inesperados en los datos. Para implementarlo, se pueden definir estructuras clave con esquemas preestablecidos y, al mismo tiempo, permitir la inferencia en \u00e1reas donde la predictibilidad no es esencial. Por ejemplo, en la ingesti\u00f3n de logs, los campos cr\u00edticos como identificadores y fechas pueden tener tipos de datos definidos, mientras que los mensajes o metadatos pueden inferirse para mantener flexibilidad. Este modelo reduce el riesgo de errores silenciosos al mismo tiempo que permite eficiencia en exploraci\u00f3n y expansi\u00f3n de datos, lo que lo convierte en una opci\u00f3n ideal para sistemas que necesitan adaptabilidad sin comprometer la estabilidad operativa. # Esquema base expl\u00edcito para campos cr\u00edticos base_schema = StructType([ StructField(\"id\", StringType(), False), StructField(\"timestamp\", TimestampType(), False), StructField(\"amount\", DecimalType(10,2), False) ]) # Lectura con esquema parcial + inferencia para campos adicionales df = spark.read.schema(base_schema) \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .json(\"data/transactions/\") # Manejo de campos adicionales inferidos additional_fields = [c for c in df.columns if c not in [\"id\", \"timestamp\", \"amount\", \"_corrupt_record\"]] Esta estrategia h\u00edbrida maximiza tanto la robustez como la flexibilidad del pipeline. 3.4.2 Schema evolution en Spark (Avro, Parquet, Delta) La evoluci\u00f3n de esquemas en Spark es una caracter\u00edstica fundamental para manejar cambios en la estructura de datos sin afectar la compatibilidad con versiones anteriores. Esto es particularmente \u00fatil en sistemas de big data donde las fuentes de datos pueden cambiar con el tiempo, pero a\u00fan se requiere accesibilidad a datos hist\u00f3ricos. Spark ofrece soporte para la evoluci\u00f3n de esquemas en formatos como Avro, Parquet y Delta Lake, cada uno con su propia manera de gestionar modificaciones sin comprometer la integridad del sistema. En Avro, la evoluci\u00f3n de esquemas se basa en el concepto de compatibilidad mediante reglas predefinidas. Un esquema nuevo debe ser compatible con las versiones anteriores, lo que significa que los cambios pueden incluir la adici\u00f3n de nuevos campos opcionales, cambios en nombres de campos con alias, o reordenaci\u00f3n de elementos sin afectar la legibilidad de los datos. Avro mantiene metadatos estructurados que permiten interpretar versiones antiguas de datos con esquemas m\u00e1s recientes, lo que facilita la interoperabilidad entre diferentes versiones de la misma estructura. Parquet, por otro lado, soporta la evoluci\u00f3n de esquemas principalmente mediante la adici\u00f3n de columnas. Debido a su estructura basada en columnas, Parquet permite agregar nuevos atributos sin alterar los datos existentes. Sin embargo, la modificaci\u00f3n de tipos de datos o la eliminaci\u00f3n de columnas puede generar incompatibilidades, lo que obliga a realizar transformaciones adicionales para garantizar que las aplicaciones sean capaces de leer los datos correctamente. La metadata de Parquet juega un papel clave en la compatibilidad, ya que almacena informaci\u00f3n detallada sobre las versiones de esquema utilizadas. Delta Lake ofrece una evoluci\u00f3n de esquemas m\u00e1s avanzada al permitir modificaciones como la adici\u00f3n o eliminaci\u00f3n de columnas, cambios en tipos de datos y adaptaciones estructurales sin necesidad de reescribir por completo los datos almacenados. Esto se logra mediante la gesti\u00f3n de versiones dentro de un registro transaccional que mantiene la coherencia del esquema en diferentes momentos del tiempo. Con Delta Lake, Spark puede leer y escribir datos bajo m\u00faltiples versiones sin perder compatibilidad, ofreciendo una flexibilidad notable en entornos donde los datos evolucionan r\u00e1pidamente. Estos enfoques de evoluci\u00f3n de esquemas en Spark proporcionan soluciones efectivas para manejar cambios estructurales sin afectar el acceso a datos hist\u00f3ricos, lo que resulta esencial en sistemas que deben ser robustos frente a modificaciones sin comprometer la integridad ni la eficiencia del procesamiento. Compatibilidad de evoluci\u00f3n: adici\u00f3n, eliminaci\u00f3n y cambios de tipo La compatibilidad en la evoluci\u00f3n de esquemas permite modificaciones estructurales en los datos sin afectar la capacidad de lectura de versiones previas, siempre que se respeten ciertas reglas de compatibilidad hacia atr\u00e1s. Esto significa que es posible agregar nuevas columnas sin afectar consultas existentes, reordenar campos sin alterar su interpretaci\u00f3n, e incluso cambiar tipos de datos si la transformaci\u00f3n es segura (por ejemplo, de int a bigint o de string a text ). Sin embargo, eliminaciones y cambios de tipo m\u00e1s dr\u00e1sticos pueden generar problemas de compatibilidad, por lo que deben manejarse con estrategias como versiones de esquema o transformaciones expl\u00edcitas. Adici\u00f3n de columna en Avro: Se agrega un nuevo campo opcional email a un esquema de usuarios, asegurando que versiones anteriores a\u00fan puedan interpretar los datos sin errores. Cambio de tipo en Parquet: Se transforma una columna edad de int a bigint para admitir valores m\u00e1s grandes sin afectar consultas existentes. Eliminaci\u00f3n en Delta Lake: Se elimina una columna obsoleta direcci\u00f3n y los datos hist\u00f3ricos siguen accesibles mediante versiones anteriores registradas en el log transaccional de Delta. Estos cambios reflejan c\u00f3mo Spark maneja la evoluci\u00f3n sin comprometer la estabilidad del sistema. Herramientas para manejar la evoluci\u00f3n de esquemas Las herramientas de evoluci\u00f3n de esquemas en Spark permiten manejar cambios estructurales sin comprometer la integridad de los datos. Par\u00e1metros como spark.sql.parquet.mergeSchema=true en Parquet y --mergeSchema en Delta Lake facilitan la integraci\u00f3n de nuevas columnas o estructuras sin requerir una migraci\u00f3n completa, manteniendo compatibilidad con versiones anteriores. En Avro, la evoluci\u00f3n de esquemas mediante AvroSchema asegura que los datos hist\u00f3ricos y actuales puedan coexistir sin afectar procesos anal\u00edticos. Una plataforma de monitoreo ambiental activa mergeSchema en Delta para incorporar sensores adicionales sin interrupciones operativas. Un equipo de ingenier\u00eda de datos usa Avro con evoluci\u00f3n de esquemas para adaptar su pipeline de predicciones sin reescribir datos antiguos. Una empresa de retail ajusta autom\u00e1ticamente su modelo Parquet para agregar atributos nuevos sobre tendencias de compra sin afectar reportes hist\u00f3ricos. Estas soluciones garantizan adaptabilidad y estabilidad en entornos din\u00e1micos de big data. 3.4.3 Control de versiones de esquemas En sistemas de procesamiento distribuido, el control de versiones de esquemas es un mecanismo clave para garantizar la trazabilidad, validaci\u00f3n y compatibilidad de los datos a lo largo del tiempo. Esto es especialmente relevante en arquitecturas de big data, donde los datos evolucionan constantemente y deben seguir siendo accesibles sin interrupciones. Sin un sistema de versionado adecuado, los cambios en los esquemas pueden generar errores al momento de consumir o transformar datos, afectando la confiabilidad del sistema. Existen dos enfoques principales para gestionar la evoluci\u00f3n de esquemas: el uso de Schema Registry , que permite registrar y validar los esquemas de manera centralizada, y el versionado de tablas en Delta Lake , que ofrece un historial de cambios con soporte para consultas en diferentes puntos del tiempo. Uso de Schema Registry Un Schema Registry es un servicio que almacena y gestiona las versiones de los esquemas de datos utilizados en un sistema. Soluciones como Confluent Schema Registry (para Apache Kafka) o AWS Glue Schema Registry permiten registrar, versionar y validar esquemas al momento de producir o consumir datos. Esto asegura que los productores de datos sigan un formato predefinido y que los consumidores puedan interpretar los datos correctamente, incluso si se han realizado cambios en la estructura. El Schema Registry act\u00faa como un intermediario que evita incompatibilidades entre diferentes versiones de datos. Al registrar un esquema, el sistema verifica que los cambios sean compatibles con versiones anteriores, permitiendo agregar nuevos campos opcionales sin afectar la lectura de datos hist\u00f3ricos. Adem\u00e1s, si un consumidor utiliza una versi\u00f3n anterior del esquema, el registro puede proporcionar un mecanismo de compatibilidad que transforma los datos para que sigan siendo accesibles. Ejemplo : Supongamos que una empresa de telecomunicaciones utiliza Kafka para procesar registros de llamadas. Cada mensaje en Kafka sigue un esquema Avro gestionado por Confluent Schema Registry . Cuando se a\u00f1ade un nuevo campo ubicaci\u00f3n a los datos de llamadas, el Schema Registry asegura que los consumidores existentes puedan seguir procesando los registros sin errores, manteniendo compatibilidad con versiones anteriores. Versionado de tablas con Delta Lake Delta Lake ofrece un sistema de versionado de esquemas incorporado en su arquitectura transaccional. A diferencia de otros formatos como Parquet o Avro, Delta Lake permite el \"time travel\" , es decir, la capacidad de consultar versiones previas de una tabla y revertir cambios si es necesario. Este mecanismo es esencial para garantizar la estabilidad en sistemas donde los datos cambian con frecuencia. Las operaciones estructurales como ADD COLUMN , CHANGE TYPE y DROP COLUMN son registradas en el log de transacciones de Delta, lo que permite rastrear c\u00f3mo evolucion\u00f3 el esquema a lo largo del tiempo. Si un cambio genera problemas en los consumidores de datos, se pueden restaurar versiones anteriores sin afectar la integridad de los datos almacenados. Ejemplo : Una startup que trabaja con datos de sensores usa Delta Lake para almacenar registros de mediciones. Para incorporar nuevas m\u00e9tricas sin afectar sistemas que dependen de datos hist\u00f3ricos, habilitan --mergeSchema en Delta Lake. As\u00ed, los nuevos datos con m\u00e9tricas adicionales son integrados sin necesidad de realizar migraciones complejas y sin impactar consultas anteriores. 3.4.4 Validaci\u00f3n y limpieza de datos La validaci\u00f3n y limpieza de datos son procesos fundamentales para garantizar que la informaci\u00f3n utilizada en sistemas anal\u00edticos o de negocio sea precisa y confiable. La validaci\u00f3n implica la verificaci\u00f3n de la integridad de los datos mediante reglas predefinidas, como la comprobaci\u00f3n de formatos, rangos aceptables y valores nulos. Este proceso asegura que los datos ingresados cumplan con las expectativas y requisitos del sistema antes de ser utilizados en c\u00e1lculos o reportes. Por otro lado, la limpieza de datos aborda problemas como valores duplicados, inconsistencias, errores tipogr\u00e1ficos y registros incompletos, permitiendo transformar datos crudos en informaci\u00f3n estructurada y libre de anomal\u00edas. Sin estos mecanismos, los sistemas corren el riesgo de generar an\u00e1lisis err\u00f3neos, afectar la toma de decisiones y comprometer la confiabilidad de modelos predictivos. Implementar una estrategia efectiva de validaci\u00f3n y limpieza requiere el uso de herramientas especializadas, como reglas definidas en bases de datos, funciones en frameworks de procesamiento de datos como PySpark y SparkSQL, o soluciones avanzadas como Data Quality Services en entornos empresariales. La automatizaci\u00f3n de estos procesos es clave para mantener la escalabilidad y eficiencia, reduciendo la carga operativa en equipos de datos y garantizando que la informaci\u00f3n est\u00e9 siempre en condiciones \u00f3ptimas para su uso. Adem\u00e1s, estrategias como la detecci\u00f3n de valores at\u00edpicos, normalizaci\u00f3n de formatos y enriquecimiento de datos pueden mejorar la calidad de la informaci\u00f3n disponible y potenciar la capacidad de los sistemas para ofrecer insights precisos y relevantes. Reglas de calidad: nulos, tipos, rangos, unicidad Las reglas de calidad de datos son validaciones sistem\u00e1ticas que garantizan que los datos cumplan con est\u00e1ndares espec\u00edficos antes de ser procesados o almacenados. En Spark, estas reglas se pueden implementar como filtros (que excluyen registros no v\u00e1lidos) o como excepciones (que detienen el procesamiento cuando se detectan problemas). 1. Reglas de Nulos Las reglas de nulos verifican que los campos cr\u00edticos no contengan valores faltantes o que los campos opcionales manejen correctamente los valores nulos. Ejemplo PySpark : from pyspark.sql import SparkSession from pyspark.sql.functions import col, isnan, isnull, when, count # Datos de ejemplo data = [ (\"001\", \"Juan P\u00e9rez\", 25, \"juan@email.com\"), (\"002\", None, 30, \"ana@email.com\"), (\"003\", \"Carlos L\u00f3pez\", None, None), (\"004\", \"Mar\u00eda Garc\u00eda\", 28, \"maria@email.com\") ] df = spark.createDataFrame(data, [\"id\", \"nombre\", \"edad\", \"email\"]) # Regla: nombre y email son obligatorios df_clean = df.filter( col(\"nombre\").isNotNull() & col(\"email\").isNotNull() ) print(\"Registros v\u00e1lidos:\") df_clean.show() # Registros rechazados para auditor\u00eda df_rejected = df.filter( col(\"nombre\").isNull() | col(\"email\").isNull() ) print(\"Registros rechazados:\") df_rejected.show() Ejemplo SparkSQL : -- Crear vista temporal CREATE OR REPLACE TEMPORARY VIEW usuarios AS SELECT * FROM VALUES ('001', 'Juan P\u00e9rez', 25, 'juan@email.com'), ('002', NULL, 30, 'ana@email.com'), ('003', 'Carlos L\u00f3pez', NULL, NULL), ('004', 'Mar\u00eda Garc\u00eda', 28, 'maria@email.com') AS t(id, nombre, edad, email); -- Filtrar registros v\u00e1lidos SELECT * FROM usuarios WHERE nombre IS NOT NULL AND email IS NOT NULL; -- Contar registros con problemas de calidad SELECT COUNT(*) as total_registros, SUM(CASE WHEN nombre IS NULL THEN 1 ELSE 0 END) as nombres_nulos, SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as emails_nulos FROM usuarios; Ejemplo PySpark : def validate_nulls(df, required_columns): \"\"\"Valida que las columnas requeridas no tengan nulos\"\"\" for col_name in required_columns: null_count = df.filter(col(col_name).isNull()).count() if null_count > 0: raise ValueError(f\"Columna '{col_name}' tiene {null_count} valores nulos\") return df # Aplicar validaci\u00f3n estricta try: df_validated = validate_nulls(df, [\"nombre\", \"email\"]) print(\"Validaci\u00f3n exitosa\") except ValueError as e: print(f\"Error de calidad: {e}\") 2. Reglas de Tipos Las reglas de tipos verifican que los datos tengan el formato correcto seg\u00fan su tipo esperado (n\u00fameros, fechas, emails, etc.). Ejemplo PySpark : from pyspark.sql.functions import regexp_match, length, when from pyspark.sql.types import IntegerType # Datos con problemas de tipos data = [ (\"001\", \"25\", \"2023-12-01\", \"juan@email.com\"), (\"002\", \"treinta\", \"2023/12/02\", \"ana.email.com\"), (\"003\", \"28\", \"invalid_date\", \"carlos@email.com\"), (\"004\", \"-5\", \"2023-12-03\", \"maria@domain\") ] df = spark.createDataFrame(data, [\"id\", \"edad_str\", \"fecha_str\", \"email\"]) # Regla 1: Edad debe ser num\u00e9rica y positiva df_with_edad = df.withColumn( \"edad_valida\", when(col(\"edad_str\").rlike(\"^[0-9]+$\") & (col(\"edad_str\").cast(IntegerType()) > 0), col(\"edad_str\").cast(IntegerType()) ).otherwise(None) ) # Regla 2: Email debe tener formato v\u00e1lido df_with_email = df_with_edad.withColumn( \"email_valido\", when(col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"), col(\"email\") ).otherwise(None) ) # Regla 3: Fecha debe tener formato ISO df_with_fecha = df_with_email.withColumn( \"fecha_valida\", when(col(\"fecha_str\").rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"), to_date(col(\"fecha_str\")) ).otherwise(None) ) # Filtrar solo registros completamente v\u00e1lidos df_clean = df_with_fecha.filter( col(\"edad_valida\").isNotNull() & col(\"email_valido\").isNotNull() & col(\"fecha_valida\").isNotNull() ) df_clean.select(\"id\", \"edad_valida\", \"fecha_valida\", \"email_valido\").show() Ejemplo SparkSQL : -- Crear datos de prueba CREATE OR REPLACE TEMPORARY VIEW datos_raw AS SELECT * FROM VALUES ('001', '25', '2023-12-01', 'juan@email.com'), ('002', 'treinta', '2023/12/02', 'ana.email.com'), ('003', '28', 'invalid_date', 'carlos@email.com'), ('004', '-5', '2023-12-03', 'maria@domain') AS t(id, edad_str, fecha_str, email); -- Validar tipos y aplicar reglas SELECT id, CASE WHEN edad_str RLIKE '^[0-9]+$' AND CAST(edad_str AS INT) > 0 THEN CAST(edad_str AS INT) ELSE NULL END as edad_valida, CASE WHEN fecha_str RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$' THEN TO_DATE(fecha_str) ELSE NULL END as fecha_valida, CASE WHEN email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$' THEN email ELSE NULL END as email_valido FROM datos_raw WHERE edad_str RLIKE '^[0-9]+$' AND CAST(edad_str AS INT) > 0 AND fecha_str RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$' AND email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'; 3. Reglas de Rangos Las reglas de rangos verifican que los valores num\u00e9ricos, fechas o strings est\u00e9n dentro de l\u00edmites aceptables. Ejemplo PySpark : from pyspark.sql.functions import col, when, current_date, datediff from datetime import datetime, date # Datos de empleados data = [ (\"001\", \"Juan\", 25, 50000, \"2020-01-15\"), (\"002\", \"Ana\", 17, 30000, \"2025-06-01\"), # Menor de edad (\"003\", \"Carlos\", 45, -5000, \"2019-03-10\"), # Salario negativo (\"004\", \"Mar\u00eda\", 150, 80000, \"1800-12-25\"), # Edad imposible, fecha muy antigua (\"005\", \"Luis\", 30, 60000, \"2022-05-20\") ] df = spark.createDataFrame(data, [\"id\", \"nombre\", \"edad\", \"salario\", \"fecha_ingreso\"]) df = df.withColumn(\"fecha_ingreso\", to_date(col(\"fecha_ingreso\"))) # Regla 1: Edad entre 18 y 65 a\u00f1os df_edad_valida = df.withColumn( \"edad_en_rango\", when((col(\"edad\") >= 18) & (col(\"edad\") <= 65), True).otherwise(False) ) # Regla 2: Salario positivo y menor a 200,000 df_salario_valido = df_edad_valida.withColumn( \"salario_en_rango\", when((col(\"salario\") > 0) & (col(\"salario\") <= 200000), True).otherwise(False) ) # Regla 3: Fecha de ingreso no futura y no anterior a 2000 fecha_minima = date(2000, 1, 1) df_fecha_valida = df_salario_valido.withColumn( \"fecha_en_rango\", when((col(\"fecha_ingreso\") >= lit(fecha_minima)) & (col(\"fecha_ingreso\") <= current_date()), True).otherwise(False) ) # Filtrar registros v\u00e1lidos df_valid = df_fecha_valida.filter( col(\"edad_en_rango\") & col(\"salario_en_rango\") & col(\"fecha_en_rango\") ) print(\"Empleados v\u00e1lidos:\") df_valid.select(\"id\", \"nombre\", \"edad\", \"salario\", \"fecha_ingreso\").show() # Reporte de calidad quality_report = df_fecha_valida.agg( count(\"*\").alias(\"total_registros\"), sum(when(col(\"edad_en_rango\"), 1).otherwise(0)).alias(\"edad_valida\"), sum(when(col(\"salario_en_rango\"), 1).otherwise(0)).alias(\"salario_valido\"), sum(when(col(\"fecha_en_rango\"), 1).otherwise(0)).alias(\"fecha_valida\") ) quality_report.show() Ejemplo SparkSQL : -- Crear datos de empleados CREATE OR REPLACE TEMPORARY VIEW empleados AS SELECT * FROM VALUES ('001', 'Juan', 25, 50000, '2020-01-15'), ('002', 'Ana', 17, 30000, '2025-06-01'), ('003', 'Carlos', 45, -5000, '2019-03-10'), ('004', 'Mar\u00eda', 150, 80000, '1800-12-25'), ('005', 'Luis', 30, 60000, '2022-05-20') AS t(id, nombre, edad, salario, fecha_ingreso_str); -- Aplicar reglas de rango SELECT id, nombre, edad, salario, TO_DATE(fecha_ingreso_str) as fecha_ingreso, -- Validaciones de rango CASE WHEN edad BETWEEN 18 AND 65 THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as edad_estado, CASE WHEN salario > 0 AND salario <= 200000 THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as salario_estado, CASE WHEN TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE() THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as fecha_estado FROM empleados WHERE edad BETWEEN 18 AND 65 AND salario > 0 AND salario <= 200000 AND TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE(); -- Estad\u00edsticas de calidad por rango SELECT COUNT(*) as total_empleados, SUM(CASE WHEN edad BETWEEN 18 AND 65 THEN 1 ELSE 0 END) as edad_valida, SUM(CASE WHEN salario > 0 AND salario <= 200000 THEN 1 ELSE 0 END) as salario_valido, SUM(CASE WHEN TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE() THEN 1 ELSE 0 END) as fecha_valida FROM empleados; 4. Reglas de Unicidad Las reglas de unicidad verifican que no existan duplicados en campos que deben ser \u00fanicos (IDs, emails, c\u00f3digos, etc.). Ejemplo PySpark : from pyspark.sql.functions import col, count, desc from pyspark.sql.window import Window # Datos con duplicados data = [ (\"001\", \"juan@email.com\", \"12345678\", \"Juan P\u00e9rez\"), (\"002\", \"ana@email.com\", \"87654321\", \"Ana Garc\u00eda\"), (\"003\", \"carlos@email.com\", \"12345678\", \"Carlos L\u00f3pez\"), # DNI duplicado (\"004\", \"juan@email.com\", \"11111111\", \"Juan Mart\u00ednez\"), # Email duplicado (\"005\", \"maria@email.com\", \"22222222\", \"Mar\u00eda Rodr\u00edguez\"), (\"001\", \"pedro@email.com\", \"33333333\", \"Pedro S\u00e1nchez\") # ID duplicado ] df = spark.createDataFrame(data, [\"id\", \"email\", \"dni\", \"nombre\"]) # M\u00e9todo 1: Identificar duplicados por campo print(\"=== An\u00e1lisis de Duplicados ===\") # Duplicados por ID duplicados_id = df.groupBy(\"id\").count().filter(col(\"count\") > 1) print(\"IDs duplicados:\") duplicados_id.show() # Duplicados por Email duplicados_email = df.groupBy(\"email\").count().filter(col(\"count\") > 1) print(\"Emails duplicados:\") duplicados_email.show() # Duplicados por DNI duplicados_dni = df.groupBy(\"dni\").count().filter(col(\"count\") > 1) print(\"DNIs duplicados:\") duplicados_dni.show() # M\u00e9todo 2: Marcar registros duplicados window_id = Window.partitionBy(\"id\") window_email = Window.partitionBy(\"email\") window_dni = Window.partitionBy(\"dni\") df_marked = df.withColumn(\"count_id\", count(\"*\").over(window_id)) \\ .withColumn(\"count_email\", count(\"*\").over(window_email)) \\ .withColumn(\"count_dni\", count(\"*\").over(window_dni)) # Identificar registros con alg\u00fan tipo de duplicado df_duplicates = df_marked.filter( (col(\"count_id\") > 1) | (col(\"count_email\") > 1) | (col(\"count_dni\") > 1) ) print(\"Registros con duplicados:\") df_duplicates.orderBy(\"id\").show() # M\u00e9todo 3: Mantener solo registros \u00fanicos (primera ocurrencia) df_unique = df.dropDuplicates([\"id\", \"email\", \"dni\"]) print(\"Registros \u00fanicos:\") df_unique.show() # M\u00e9todo 4: Validaci\u00f3n estricta con excepci\u00f3n def validate_uniqueness(df, unique_columns): \"\"\"Valida que las columnas especificadas sean \u00fanicas\"\"\" for col_name in unique_columns: duplicate_count = df.groupBy(col_name).count().filter(col(\"count\") > 1).count() if duplicate_count > 0: duplicates = df.groupBy(col_name).count().filter(col(\"count\") > 1).collect() raise ValueError(f\"Columna '{col_name}' tiene duplicados: {[row[col_name] for row in duplicates]}\") return True try: validate_uniqueness(df_unique, [\"id\", \"email\", \"dni\"]) print(\"Validaci\u00f3n de unicidad exitosa\") except ValueError as e: print(f\"Error de unicidad: {e}\") Ejemplo SparkSQL : -- Crear datos con duplicados CREATE OR REPLACE TEMPORARY VIEW usuarios_duplicados AS SELECT * FROM VALUES ('001', 'juan@email.com', '12345678', 'Juan P\u00e9rez'), ('002', 'ana@email.com', '87654321', 'Ana Garc\u00eda'), ('003', 'carlos@email.com', '12345678', 'Carlos L\u00f3pez'), ('004', 'juan@email.com', '11111111', 'Juan Mart\u00ednez'), ('005', 'maria@email.com', '22222222', 'Mar\u00eda Rodr\u00edguez'), ('001', 'pedro@email.com', '33333333', 'Pedro S\u00e1nchez') AS t(id, email, dni, nombre); -- An\u00e1lisis de duplicados por campo SELECT 'ID' as campo, id as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY id HAVING COUNT(*) > 1 UNION ALL SELECT 'EMAIL' as campo, email as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY email HAVING COUNT(*) > 1 UNION ALL SELECT 'DNI' as campo, dni as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY dni HAVING COUNT(*) > 1; -- Identificar registros duplicados con detalles WITH duplicados_marcados AS ( SELECT *, COUNT(*) OVER (PARTITION BY id) as count_id, COUNT(*) OVER (PARTITION BY email) as count_email, COUNT(*) OVER (PARTITION BY dni) as count_dni, ROW_NUMBER() OVER (PARTITION BY id ORDER BY nombre) as rn_id FROM usuarios_duplicados ) SELECT id, email, dni, nombre, CASE WHEN count_id > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_id, CASE WHEN count_email > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_email, CASE WHEN count_dni > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_dni FROM duplicados_marcados WHERE count_id > 1 OR count_email > 1 OR count_dni > 1 ORDER BY id; -- Obtener registros \u00fanicos (primera ocurrencia por cada campo) WITH ranked_records AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY nombre) as rn_id, ROW_NUMBER() OVER (PARTITION BY email ORDER BY nombre) as rn_email, ROW_NUMBER() OVER (PARTITION BY dni ORDER BY nombre) as rn_dni FROM usuarios_duplicados ) SELECT id, email, dni, nombre FROM ranked_records WHERE rn_id = 1 AND rn_email = 1 AND rn_dni = 1; -- Reporte de calidad de unicidad SELECT COUNT(*) as total_registros, COUNT(DISTINCT id) as ids_unicos, COUNT(DISTINCT email) as emails_unicos, COUNT(DISTINCT dni) as dnis_unicos, COUNT(*) - COUNT(DISTINCT id) as duplicados_id, COUNT(*) - COUNT(DISTINCT email) as duplicados_email, COUNT(*) - COUNT(DISTINCT dni) as duplicados_dni FROM usuarios_duplicados; Pipeline Integrado de Calidad Ejemplo PySpark : def apply_data_quality_rules(df): \"\"\"Aplica todas las reglas de calidad de datos\"\"\" from pyspark.sql.functions import * # 1. Reglas de nulos df = df.filter(col(\"id\").isNotNull() & col(\"email\").isNotNull()) # 2. Reglas de tipos df = df.filter(col(\"edad\").rlike(\"^[0-9]+$\")) \\ .withColumn(\"edad\", col(\"edad\").cast(\"int\")) # 3. Reglas de rangos df = df.filter((col(\"edad\") >= 18) & (col(\"edad\") <= 65)) df = df.filter((col(\"salario\") > 0) & (col(\"salario\") <= 200000)) # 4. Reglas de unicidad df = df.dropDuplicates([\"id\", \"email\"]) return df # Aplicar pipeline de calidad df_clean = apply_data_quality_rules(df_raw) Ejemplo SparkSQL : -- Pipeline completo de calidad de datos WITH quality_pipeline AS ( SELECT * FROM raw_data WHERE -- Reglas de nulos id IS NOT NULL AND email IS NOT NULL -- Reglas de tipos AND edad RLIKE '^[0-9]+$' -- Reglas de rangos AND CAST(edad AS INT) BETWEEN 18 AND 65 AND salario > 0 AND salario <= 200000 -- Fecha v\u00e1lida AND fecha_ingreso BETWEEN '2000-01-01' AND CURRENT_DATE() ), unique_records AS ( SELECT DISTINCT * -- Regla de unicidad b\u00e1sica FROM quality_pipeline ) SELECT * FROM unique_records; Estos ejemplos muestran c\u00f3mo implementar sistem\u00e1ticamente reglas de calidad de datos en Spark, tanto usando la API de Python como SparkSQL, proporcionando robustez y confiabilidad a los pipelines de datos. Detecci\u00f3n y manejo de duplicados La detecci\u00f3n y manejo de duplicados es crucial para mantener la integridad de los datos y evitar problemas como conteos err\u00f3neos, cargas incorrectas y an\u00e1lisis sesgados. En Spark, podemos implementar diferentes estrategias de detecci\u00f3n usando claves primarias, combinaciones \u00fanicas o funciones hash. 1. Detecci\u00f3n por Claves Primarias Las claves primarias son identificadores \u00fanicos que no deben repetirse en un dataset. Su duplicaci\u00f3n indica problemas serios de calidad de datos. Ejemplo PySpark : from pyspark.sql import SparkSession from pyspark.sql.functions import col, count, desc, first, last, max as spark_max, min as spark_min from pyspark.sql.window import Window # Datos de ejemplo con duplicados en clave primaria data = [ (\"USR001\", \"Juan P\u00e9rez\", \"juan@email.com\", \"2023-01-15\", 1000), (\"USR002\", \"Ana Garc\u00eda\", \"ana@email.com\", \"2023-01-16\", 1500), (\"USR001\", \"Juan P\u00e9rez\", \"juan.perez@email.com\", \"2023-01-17\", 1200), # ID duplicado (\"USR003\", \"Carlos L\u00f3pez\", \"carlos@email.com\", \"2023-01-18\", 800), (\"USR002\", \"Ana Garc\u00eda\", \"ana.garcia@email.com\", \"2023-01-19\", 1600), # ID duplicado (\"USR004\", \"Mar\u00eda Rodr\u00edguez\", \"maria@email.com\", \"2023-01-20\", 2000) ] df = spark.createDataFrame(data, [\"user_id\", \"nombre\", \"email\", \"fecha_registro\", \"saldo\"]) # Detectar duplicados por clave primaria print(\"=== Detecci\u00f3n de Duplicados por Clave Primaria ===\") duplicados_pk = df.groupBy(\"user_id\") \\ .count() \\ .filter(col(\"count\") > 1) \\ .orderBy(desc(\"count\")) print(\"Claves primarias duplicadas:\") duplicados_pk.show() # Mostrar todos los registros duplicados df_duplicados = df.join(duplicados_pk.select(\"user_id\"), [\"user_id\"]) print(\"Registros con claves primarias duplicadas:\") df_duplicados.orderBy(\"user_id\", \"fecha_registro\").show() # Estrategia 1: Mantener el registro m\u00e1s reciente window_spec = Window.partitionBy(\"user_id\").orderBy(desc(\"fecha_registro\")) df_latest = df.withColumn(\"rn\", row_number().over(window_spec)) \\ .filter(col(\"rn\") == 1) \\ .drop(\"rn\") print(\"Registros \u00fanicos (m\u00e1s recientes):\") df_latest.orderBy(\"user_id\").show() # Estrategia 2: Mantener el registro con mayor saldo window_spec_saldo = Window.partitionBy(\"user_id\").orderBy(desc(\"saldo\")) df_max_saldo = df.withColumn(\"rn\", row_number().over(window_spec_saldo)) \\ .filter(col(\"rn\") == 1) \\ .drop(\"rn\") print(\"Registros \u00fanicos (mayor saldo):\") df_max_saldo.orderBy(\"user_id\").show() # Estrategia 3: Consolidar informaci\u00f3n de duplicados df_consolidated = df.groupBy(\"user_id\") \\ .agg( first(\"nombre\").alias(\"nombre\"), first(\"email\").alias(\"email_principal\"), spark_max(\"fecha_registro\").alias(\"ultima_fecha\"), spark_max(\"saldo\").alias(\"saldo_maximo\"), count(\"*\").alias(\"num_registros\") ) print(\"Registros consolidados:\") df_consolidated.show() Ejemplo SparkSQL : -- Crear tabla con duplicados en clave primaria CREATE OR REPLACE TEMPORARY VIEW usuarios_duplicados AS SELECT * FROM VALUES ('USR001', 'Juan P\u00e9rez', 'juan@email.com', '2023-01-15', 1000), ('USR002', 'Ana Garc\u00eda', 'ana@email.com', '2023-01-16', 1500), ('USR001', 'Juan P\u00e9rez', 'juan.perez@email.com', '2023-01-17', 1200), ('USR003', 'Carlos L\u00f3pez', 'carlos@email.com', '2023-01-18', 800), ('USR002', 'Ana Garc\u00eda', 'ana.garcia@email.com', '2023-01-19', 1600), ('USR004', 'Mar\u00eda Rodr\u00edguez', 'maria@email.com', '2023-01-20', 2000) AS t(user_id, nombre, email, fecha_registro, saldo); -- Detectar duplicados por clave primaria SELECT user_id, COUNT(*) as num_duplicados FROM usuarios_duplicados GROUP BY user_id HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- Mostrar registros duplicados con detalles WITH duplicados AS ( SELECT user_id FROM usuarios_duplicados GROUP BY user_id HAVING COUNT(*) > 1 ) SELECT u.*, 'DUPLICADO' as estado FROM usuarios_duplicados u INNER JOIN duplicados d ON u.user_id = d.user_id ORDER BY u.user_id, u.fecha_registro; -- Estrategia 1: Mantener registro m\u00e1s reciente WITH ranked_records AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY fecha_registro DESC) as rn FROM usuarios_duplicados ) SELECT user_id, nombre, email, fecha_registro, saldo FROM ranked_records WHERE rn = 1 ORDER BY user_id; -- Estrategia 2: Consolidar informaci\u00f3n SELECT user_id, FIRST(nombre) as nombre, FIRST(email) as email_principal, MAX(fecha_registro) as ultima_fecha, MAX(saldo) as saldo_maximo, COUNT(*) as num_registros_originales FROM usuarios_duplicados GROUP BY user_id ORDER BY user_id; 2. Detecci\u00f3n por Combinaciones \u00danicas Las combinaciones \u00fanicas involucran m\u00faltiples campos que juntos deben ser \u00fanicos, como combinaciones de nombre+email, producto+fecha, etc. Ejemplo PySpark : from pyspark.sql.functions import concat_ws, md5, col, count, desc, collect_list # Datos de transacciones con duplicados data = [ (\"TXN001\", \"USR001\", \"2023-01-15\", \"COMPRA\", 100.50, \"Producto A\"), (\"TXN002\", \"USR002\", \"2023-01-15\", \"COMPRA\", 200.00, \"Producto B\"), (\"TXN003\", \"USR001\", \"2023-01-15\", \"COMPRA\", 100.50, \"Producto A\"), # Duplicado potencial (\"TXN004\", \"USR003\", \"2023-01-16\", \"VENTA\", 150.00, \"Producto C\"), (\"TXN005\", \"USR002\", \"2023-01-15\", \"COMPRA\", 200.00, \"Producto B\"), # Duplicado potencial (\"TXN006\", \"USR001\", \"2023-01-17\", \"COMPRA\", 100.50, \"Producto A\") # Mismo usuario y producto, fecha diferente ] df = spark.createDataFrame(data, [\"txn_id\", \"user_id\", \"fecha\", \"tipo\", \"monto\", \"producto\"]) print(\"=== Detecci\u00f3n por Combinaciones \u00danicas ===\") # Combinaci\u00f3n 1: user_id + fecha + producto (transacciones id\u00e9nticas) duplicados_combo1 = df.groupBy(\"user_id\", \"fecha\", \"producto\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por usuario + fecha + producto:\") duplicados_combo1.show(truncate=False) # Combinaci\u00f3n 2: user_id + tipo + monto (transacciones similares) duplicados_combo2 = df.groupBy(\"user_id\", \"tipo\", \"monto\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\"), collect_list(\"fecha\").alias(\"fechas\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por usuario + tipo + monto:\") duplicados_combo2.show(truncate=False) # Crear clave compuesta para an\u00e1lisis df_with_key = df.withColumn(\"clave_compuesta\", concat_ws(\"|\", col(\"user_id\"), col(\"fecha\"), col(\"producto\"))) # Detectar duplicados exactos duplicados_exactos = df_with_key.groupBy(\"clave_compuesta\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados exactos (clave compuesta):\") duplicados_exactos.show(truncate=False) # Estrategia: Eliminar duplicados manteniendo el primer registro df_sin_duplicados = df.dropDuplicates([\"user_id\", \"fecha\", \"producto\"]) print(\"Registros sin duplicados:\") df_sin_duplicados.orderBy(\"user_id\", \"fecha\").show() # Estrategia: Marcar duplicados para auditor\u00eda window_spec = Window.partitionBy(\"user_id\", \"fecha\", \"producto\").orderBy(\"txn_id\") df_marked = df.withColumn(\"es_duplicado\", when(row_number().over(window_spec) > 1, True).otherwise(False)) print(\"Registros marcados (duplicados identificados):\") df_marked.orderBy(\"user_id\", \"fecha\", \"txn_id\").show() Ejemplo SparkSQL : -- Crear tabla de transacciones CREATE OR REPLACE TEMPORARY VIEW transacciones AS SELECT * FROM VALUES ('TXN001', 'USR001', '2023-01-15', 'COMPRA', 100.50, 'Producto A'), ('TXN002', 'USR002', '2023-01-15', 'COMPRA', 200.00, 'Producto B'), ('TXN003', 'USR001', '2023-01-15', 'COMPRA', 100.50, 'Producto A'), ('TXN004', 'USR003', '2023-01-16', 'VENTA', 150.00, 'Producto C'), ('TXN005', 'USR002', '2023-01-15', 'COMPRA', 200.00, 'Producto B'), ('TXN006', 'USR001', '2023-01-17', 'COMPRA', 100.50, 'Producto A') AS t(txn_id, user_id, fecha, tipo, monto, producto); -- Detectar duplicados por combinaciones \u00fanicas SELECT user_id, fecha, producto, tipo, monto, COUNT(*) as num_duplicados, COLLECT_LIST(txn_id) as txn_ids FROM transacciones GROUP BY user_id, fecha, producto, tipo, monto HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- An\u00e1lisis de duplicados con detalles WITH duplicados_identificados AS ( SELECT user_id, fecha, producto, COUNT(*) as count_duplicados FROM transacciones GROUP BY user_id, fecha, producto HAVING COUNT(*) > 1 ) SELECT t.*, 'DUPLICADO' as estado, ROW_NUMBER() OVER (PARTITION BY t.user_id, t.fecha, t.producto ORDER BY t.txn_id) as orden_duplicado FROM transacciones t INNER JOIN duplicados_identificados d ON t.user_id = d.user_id AND t.fecha = d.fecha AND t.producto = d.producto ORDER BY t.user_id, t.fecha, t.producto, t.txn_id; -- Eliminar duplicados manteniendo el primero WITH ranked_transactions AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id, fecha, producto ORDER BY txn_id) as rn FROM transacciones ) SELECT txn_id, user_id, fecha, tipo, monto, producto FROM ranked_transactions WHERE rn = 1 ORDER BY user_id, fecha; -- Crear clave compuesta para an\u00e1lisis SELECT *, CONCAT(user_id, '|', fecha, '|', producto) as clave_compuesta, COUNT(*) OVER (PARTITION BY user_id, fecha, producto) as count_grupo FROM transacciones ORDER BY clave_compuesta; 3. Detecci\u00f3n por Funciones Hash Las funciones hash permiten detectar duplicados de manera eficiente, especialmente \u00fatil para registros con muchos campos o contenido textual. Ejemplo PySpark : from pyspark.sql.functions import md5, sha1, sha2, concat_ws, col, count, collect_set # Datos de documentos con contenido similar data = [ (\"DOC001\", \"Contrato de Servicios\", \"Este es un contrato para servicios de consultor\u00eda\", \"2023-01-15\", \"Juan P\u00e9rez\"), (\"DOC002\", \"Propuesta Comercial\", \"Propuesta para desarrollo de software\", \"2023-01-16\", \"Ana Garc\u00eda\"), (\"DOC003\", \"Contrato de Servicios\", \"Este es un contrato para servicios de consultor\u00eda\", \"2023-01-17\", \"Carlos L\u00f3pez\"), # Contenido duplicado (\"DOC004\", \"Manual de Usuario\", \"Gu\u00eda completa para el uso del sistema\", \"2023-01-18\", \"Mar\u00eda Rodr\u00edguez\"), (\"DOC005\", \"Propuesta Comercial\", \"Propuesta para desarrollo de software\", \"2023-01-19\", \"Luis Mart\u00edn\"), # Contenido duplicado (\"DOC006\", \"Contrato Modificado\", \"Este es un contrato para servicios de consultor\u00eda especializada\", \"2023-01-20\", \"Pedro S\u00e1nchez\") # Contenido similar ] df = spark.createDataFrame(data, [\"doc_id\", \"titulo\", \"contenido\", \"fecha\", \"autor\"]) print(\"=== Detecci\u00f3n por Funciones Hash ===\") # Hash del contenido completo df_with_hash = df.withColumn(\"hash_contenido\", md5(col(\"contenido\"))) \\ .withColumn(\"hash_titulo\", md5(col(\"titulo\"))) \\ .withColumn(\"hash_completo\", md5(concat_ws(\"|\", col(\"titulo\"), col(\"contenido\")))) # Detectar duplicados exactos por contenido duplicados_contenido = df_with_hash.groupBy(\"hash_contenido\") \\ .agg(count(\"*\").alias(\"count\"), collect_set(\"doc_id\").alias(\"doc_ids\"), first(\"contenido\").alias(\"contenido_ejemplo\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por contenido (hash MD5):\") duplicados_contenido.show(truncate=False) # Detectar duplicados por t\u00edtulo duplicados_titulo = df_with_hash.groupBy(\"hash_titulo\") \\ .agg(count(\"*\").alias(\"count\"), collect_set(\"doc_id\").alias(\"doc_ids\"), first(\"titulo\").alias(\"titulo_ejemplo\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por t\u00edtulo:\") duplicados_titulo.show(truncate=False) # Hash de m\u00faltiples algoritmos para comparaci\u00f3n df_multi_hash = df.withColumn(\"md5_hash\", md5(col(\"contenido\"))) \\ .withColumn(\"sha1_hash\", sha1(col(\"contenido\"))) \\ .withColumn(\"sha256_hash\", sha2(col(\"contenido\"), 256)) print(\"Hashes m\u00faltiples:\") df_multi_hash.select(\"doc_id\", \"titulo\", \"md5_hash\", \"sha1_hash\").show(truncate=False) # Detectar duplicados con diferentes criterios de hash print(\"An\u00e1lisis de duplicados por diferentes hash:\") df_multi_hash.groupBy(\"md5_hash\") \\ .agg(count(\"*\").alias(\"count_md5\"), collect_set(\"doc_id\").alias(\"docs_md5\")) \\ .filter(col(\"count_md5\") > 1) \\ .show(truncate=False) # Estrategia: Eliminar duplicados basado en hash df_unique_content = df_with_hash.dropDuplicates([\"hash_contenido\"]) print(\"Documentos \u00fanicos por contenido:\") df_unique_content.select(\"doc_id\", \"titulo\", \"autor\", \"fecha\").show() # Estrategia: Crear registro de duplicados para auditor\u00eda window_hash = Window.partitionBy(\"hash_contenido\").orderBy(\"fecha\") df_audit = df_with_hash.withColumn(\"es_original\", when(row_number().over(window_hash) == 1, True).otherwise(False)) \\ .withColumn(\"orden_duplicado\", row_number().over(window_hash)) print(\"Auditor\u00eda de duplicados:\") df_audit.select(\"doc_id\", \"titulo\", \"autor\", \"fecha\", \"es_original\", \"orden_duplicado\").show() Ejemplo SparkSQL : -- Crear tabla de documentos CREATE OR REPLACE TEMPORARY VIEW documentos AS SELECT * FROM VALUES ('DOC001', 'Contrato de Servicios', 'Este es un contrato para servicios de consultor\u00eda', '2023-01-15', 'Juan P\u00e9rez'), ('DOC002', 'Propuesta Comercial', 'Propuesta para desarrollo de software', '2023-01-16', 'Ana Garc\u00eda'), ('DOC003', 'Contrato de Servicios', 'Este es un contrato para servicios de consultor\u00eda', '2023-01-17', 'Carlos L\u00f3pez'), ('DOC004', 'Manual de Usuario', 'Gu\u00eda completa para el uso del sistema', '2023-01-18', 'Mar\u00eda Rodr\u00edguez'), ('DOC005', 'Propuesta Comercial', 'Propuesta para desarrollo de software', '2023-01-19', 'Luis Mart\u00edn'), ('DOC006', 'Contrato Modificado', 'Este es un contrato para servicios de consultor\u00eda especializada', '2023-01-20', 'Pedro S\u00e1nchez') AS t(doc_id, titulo, contenido, fecha, autor); -- Crear hashes para detecci\u00f3n de duplicados CREATE OR REPLACE TEMPORARY VIEW documentos_hash AS SELECT *, MD5(contenido) as hash_contenido, MD5(titulo) as hash_titulo, MD5(CONCAT(titulo, '|', contenido)) as hash_completo, SHA1(contenido) as sha1_contenido, SHA2(contenido, 256) as sha256_contenido FROM documentos; -- Detectar duplicados por contenido SELECT hash_contenido, COUNT(*) as num_duplicados, COLLECT_SET(doc_id) as documentos_duplicados, FIRST(contenido) as contenido_ejemplo FROM documentos_hash GROUP BY hash_contenido HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- Detectar duplicados por t\u00edtulo SELECT hash_titulo, COUNT(*) as num_duplicados, COLLECT_SET(doc_id) as documentos_duplicados, FIRST(titulo) as titulo_ejemplo FROM documentos_hash GROUP BY hash_titulo HAVING COUNT(*) > 1; -- An\u00e1lisis comparativo de diferentes algoritmos hash SELECT doc_id, titulo, autor, hash_contenido as md5_hash, sha1_contenido, SUBSTR(sha256_contenido, 1, 16) as sha256_preview FROM documentos_hash ORDER BY hash_contenido; -- Eliminar duplicados manteniendo el documento m\u00e1s antiguo WITH ranked_docs AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY hash_contenido ORDER BY fecha ASC) as rn FROM documentos_hash ) SELECT doc_id, titulo, contenido, fecha, autor FROM ranked_docs WHERE rn = 1 ORDER BY fecha; -- Crear reporte de auditor\u00eda de duplicados WITH duplicate_analysis AS ( SELECT doc_id, titulo, autor, fecha, hash_contenido, COUNT(*) OVER (PARTITION BY hash_contenido) as total_duplicados, ROW_NUMBER() OVER (PARTITION BY hash_contenido ORDER BY fecha) as orden_cronologico FROM documentos_hash ) SELECT doc_id, titulo, autor, fecha, CASE WHEN total_duplicados = 1 THEN '\u00daNICO' WHEN orden_cronologico = 1 THEN 'ORIGINAL' ELSE 'DUPLICADO' END as estado_documento, total_duplicados, orden_cronologico FROM duplicate_analysis ORDER BY hash_contenido, orden_cronologico; 4. Estrategias Avanzadas de Manejo de Duplicados Ejemplo PySpark : def comprehensive_duplicate_detection(df, primary_key_cols, unique_combination_cols, content_cols): \"\"\" Pipeline completo para detecci\u00f3n y manejo de duplicados \"\"\" from pyspark.sql.functions import * print(\"=== Pipeline Completo de Detecci\u00f3n de Duplicados ===\") # 1. Duplicados por clave primaria if primary_key_cols: pk_duplicates = df.groupBy(*primary_key_cols).count().filter(col(\"count\") > 1) pk_count = pk_duplicates.count() print(f\"Duplicados por clave primaria: {pk_count}\") # 2. Duplicados por combinaci\u00f3n \u00fanica if unique_combination_cols: combo_duplicates = df.groupBy(*unique_combination_cols).count().filter(col(\"count\") > 1) combo_count = combo_duplicates.count() print(f\"Duplicados por combinaci\u00f3n \u00fanica: {combo_count}\") # 3. Duplicados por contenido (hash) if content_cols: content_hash = concat_ws(\"|\", *[col(c) for c in content_cols]) df_hash = df.withColumn(\"content_hash\", md5(content_hash)) hash_duplicates = df_hash.groupBy(\"content_hash\").count().filter(col(\"count\") > 1) hash_count = hash_duplicates.count() print(f\"Duplicados por contenido: {hash_count}\") # 4. Crear dataset limpio con m\u00faltiples estrategias df_clean = df # Eliminar duplicados por clave primaria (mantener m\u00e1s reciente si hay fecha) if primary_key_cols and \"fecha\" in df.columns: window_pk = Window.partitionBy(*primary_key_cols).orderBy(desc(\"fecha\")) df_clean = df_clean.withColumn(\"rn_pk\", row_number().over(window_pk)) \\ .filter(col(\"rn_pk\") == 1).drop(\"rn_pk\") # Eliminar duplicados por combinaci\u00f3n \u00fanica if unique_combination_cols: df_clean = df_clean.dropDuplicates(unique_combination_cols) # Eliminar duplicados por contenido if content_cols: content_hash = concat_ws(\"|\", *[col(c) for c in content_cols]) df_clean = df_clean.withColumn(\"content_hash\", md5(content_hash)) \\ .dropDuplicates([\"content_hash\"]) \\ .drop(\"content_hash\") print(f\"Registros originales: {df.count()}\") print(f\"Registros despu\u00e9s de limpiar: {df_clean.count()}\") print(f\"Registros eliminados: {df.count() - df_clean.count()}\") return df_clean # Aplicar pipeline completo df_clean = comprehensive_duplicate_detection( df_original, primary_key_cols=[\"doc_id\"], unique_combination_cols=[\"titulo\", \"autor\"], content_cols=[\"contenido\"] ) Ejemplo SparkSQL : -- Reporte completo de an\u00e1lisis de duplicados WITH duplicate_summary AS ( -- Duplicados por clave primaria SELECT 'PRIMARY_KEY' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT doc_id, COUNT(*) as count_duplicados FROM documentos GROUP BY doc_id HAVING COUNT(*) > 1 ) UNION ALL -- Duplicados por combinaci\u00f3n \u00fanica SELECT 'UNIQUE_COMBINATION' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT titulo, autor, COUNT(*) as count_duplicados FROM documentos GROUP BY titulo, autor HAVING COUNT(*) > 1 ) UNION ALL -- Duplicados por contenido SELECT 'CONTENT_HASH' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT MD5(contenido) as hash_contenido, COUNT(*) as count_duplicados FROM documentos GROUP BY MD5(contenido) HAVING COUNT(*) > 1 ) ), total_records AS ( SELECT COUNT(*) as total_registros FROM documentos ) SELECT ds.*, ROUND((ds.registros_duplicados * 100.0) / tr.total_registros, 2) as porcentaje_duplicados FROM duplicate_summary ds CROSS JOIN total_records tr ORDER BY ds.registros_duplicados DESC; -- Pipeline de limpieza completo CREATE OR REPLACE TEMPORARY VIEW documentos_limpios AS WITH paso1_pk_clean AS ( -- Eliminar duplicados por clave primaria (mantener m\u00e1s reciente) SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY doc_id ORDER BY fecha DESC) as rn FROM documentos ) WHERE rn = 1 ), paso2_combo_clean AS ( -- Eliminar duplicados por combinaci\u00f3n \u00fanica SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY titulo, autor ORDER BY fecha ASC) as rn FROM paso1_pk_clean ) WHERE rn = 1 ), paso3_content_clean AS ( -- Eliminar duplicados por contenido SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY MD5(contenido) ORDER BY fecha ASC) as rn FROM paso2_combo_clean ) WHERE rn = 1 ) SELECT doc_id, titulo, contenido, fecha, autor FROM paso3_content_clean; -- Verificar resultado final SELECT 'ORIGINAL' as dataset, COUNT(*) as total_registros FROM documentos UNION ALL SELECT 'LIMPIO' as dataset, COUNT(*) as total_registros FROM documentos_limpios; 5. Mejores Pr\u00e1cticas para Manejo de Duplicados Estrategia de Detecci\u00f3n Progresiva : def progressive_duplicate_detection(df): \"\"\"Detecci\u00f3n progresiva de duplicados con diferentes niveles de strictness\"\"\" # Nivel 1: Duplicados exactos (m\u00e1s estricto) exact_duplicates = df.groupBy(*df.columns).count().filter(col(\"count\") > 1) # Nivel 2: Duplicados por campos clave key_duplicates = df.groupBy(\"id\", \"email\").count().filter(col(\"count\") > 1) # Nivel 3: Duplicados por similitud (menos estricto) content_duplicates = df.withColumn(\"content_hash\", md5(col(\"content\"))) \\ .groupBy(\"content_hash\").count().filter(col(\"count\") > 1) return { \"exact\": exact_duplicates.count(), \"key\": key_duplicates.count(), \"content\": content_duplicates.count() } Audit Trail de Duplicados : def create_duplicate_audit_trail(df): \"\"\"Crear rastro de auditor\u00eda para duplicados eliminados\"\"\" # Identificar duplicados antes de eliminar duplicates_info = df.groupBy(\"id\").agg( count(\"*\").alias(\"count_duplicates\"), collect_list(struct(*df.columns)).alias(\"all_versions\") ).filter(col(\"count_duplicates\") > 1) # Guardar informaci\u00f3n de auditor\u00eda duplicates_info.write.mode(\"overwrite\").parquet(\"audit/duplicates_removed/\") return duplicates_info Estas estrategias permiten un manejo robusto y trazable de duplicados, garantizando la integridad de los datos mientras se mantiene un registro completo de las transformaciones realizadas. 3.4.5 Logging de errores y manejo de excepciones durante la transformaci\u00f3n El registro y manejo de errores durante la transformaci\u00f3n de datos es fundamental para garantizar la estabilidad y confiabilidad de un pipeline. Sin un sistema de logging adecuado, los fallos pueden pasar desapercibidos, generando inconsistencias en los datos procesados y afectando decisiones estrat\u00e9gicas. Un enfoque bien estructurado permite detectar y solucionar problemas r\u00e1pidamente, minimizando el impacto en los sistemas y asegurando una trazabilidad clara para an\u00e1lisis posteriores. Adem\u00e1s, integrar estrategias de recuperaci\u00f3n evita interrupciones innecesarias y mejora la resiliencia del sistema ante fallos inesperados. Logging estructurado de errores Un sistema de logging estructurado facilita la identificaci\u00f3n y resoluci\u00f3n de problemas al proporcionar informaci\u00f3n detallada sobre el contexto del fallo. En lugar de registrar mensajes gen\u00e9ricos, se recomienda incluir detalles como el identificador de la transformaci\u00f3n, el tipo de error, el origen de los datos y los valores espec\u00edficos que causaron la falla. Esto permite a los ingenieros de datos rastrear con precisi\u00f3n el origen del problema y aplicar correcciones eficientes. Por ejemplo, si una conversi\u00f3n de tipos en PySpark falla debido a un valor inesperado en una columna, el sistema de logging deber\u00eda registrar el nombre de la columna, el valor conflictivo y la operaci\u00f3n en la que ocurri\u00f3 el fallo. Este enfoque es crucial para evitar diagn\u00f3sticos err\u00f3neos y reducir el tiempo de respuesta ante incidentes. Herramientas como log4j en Spark o sistemas centralizados como Elastic Stack pueden almacenar logs estructurados con niveles de severidad y trazabilidad para an\u00e1lisis en tiempo real. Estrategias de recuperaci\u00f3n y manejo de excepciones En lugar de detener todo el pipeline por un error inesperado, es recomendable implementar t\u00e9cnicas de manejo de excepciones que permitan continuar con el procesamiento sin comprometer la calidad de los datos. Una estrategia com\u00fan es el uso de bloques try-catch , que capturan errores espec\u00edficos y permiten definir acciones correctivas. Otra t\u00e9cnica es la implementaci\u00f3n de rutas alternas , donde los registros problem\u00e1ticos se redirigen a un \u00e1rea de revisi\u00f3n sin afectar el flujo principal del procesamiento. Esto es \u00fatil en sistemas que requieren alta disponibilidad, como plataformas de an\u00e1lisis en tiempo real. Adem\u00e1s, marcar registros inv\u00e1lidos con etiquetas espec\u00edficas en lugar de eliminarlos permite que los analistas revisen y corrijan problemas sin perder informaci\u00f3n valiosa. Por ejemplo, en un pipeline de Spark, los datos con valores inconsistentes pueden enviarse a una tabla de auditor\u00eda en Delta Lake, donde se almacenan con informaci\u00f3n adicional sobre el error para su posterior revisi\u00f3n. Esto no solo mejora la calidad del procesamiento, sino que tambi\u00e9n facilita la correcci\u00f3n proactiva de datos y el aprendizaje sobre patrones de fallos recurrentes. Tarea Implementa un esquema expl\u00edcito en PySpark para una tabla de pedidos. Agrega validaciones para tipos y valores. Simula la evoluci\u00f3n de esquema en una tabla Delta agregando columnas nuevas, eliminando otras y cambiando tipos. Eval\u00faa los efectos. Dise\u00f1a un proceso de control de calidad que incluya detecci\u00f3n de duplicados y validaci\u00f3n de rangos para un conjunto de datos de clientes. Implementa un log estructurado de errores en un flujo Spark. Incluye tipo de error, registro afectado y timestamp. Integra un esquema en Avro a un Schema Registry local o en la nube. Genera dos versiones del esquema y simula un caso de incompatibilidad.","title":"Manejo de Esquemas y Calidad de Datos"},{"location":"tema34/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema34/#tema-34-manejo-de-esquemas-y-calidad-de-datos","text":"Objetivo : Prevenir fallos estructurales y mantener la integridad de los datos mediante el dise\u00f1o, control y validaci\u00f3n de esquemas, as\u00ed como la aplicaci\u00f3n de t\u00e9cnicas de calidad y monitoreo de datos durante los procesos ETL. Introducci\u00f3n : En los sistemas de Big Data, los datos provienen de m\u00faltiples fuentes, formatos y estructuras. Asegurar que los esquemas de datos sean consistentes, flexibles ante cambios, y que los datos cumplan con criterios de calidad definidos, es esencial para garantizar flujos ETL robustos, escalables y seguros. Este tema aborda los conceptos y herramientas clave que permiten controlar la estructura de los datos y mitigar riesgos comunes como fallos por incompatibilidad de esquemas, errores de transformaci\u00f3n o baja calidad de los datos. Desarrollo : El manejo de esquemas en flujos ETL implica definir reglas claras sobre la estructura de los datos que se procesan, ya sea de forma expl\u00edcita (definida por el ingeniero de datos) o inferida autom\u00e1ticamente por herramientas como Apache Spark. A medida que los sistemas evolucionan, tambi\u00e9n lo hacen los esquemas, lo que obliga a gestionar adecuadamente su evoluci\u00f3n sin interrumpir los pipelines. Adem\u00e1s, es indispensable validar la calidad de los datos mediante reglas autom\u00e1ticas, controlar los errores y registrar las excepciones de transformaci\u00f3n. Este tema presenta t\u00e9cnicas, herramientas y ejemplos pr\u00e1cticos para el manejo avanzado de esquemas y la calidad de datos en arquitecturas modernas.","title":"Tema 3.4. Manejo de Esquemas y Calidad de Datos"},{"location":"tema34/#341-esquemas-explicitos-vs-inferidos","text":"La definici\u00f3n del esquema de datos puede realizarse de manera expl\u00edcita o ser inferida autom\u00e1ticamente por las herramientas de procesamiento como Spark. Cada enfoque tiene implicaciones directas sobre la robustez, flexibilidad y trazabilidad de los pipelines ETL.","title":"3.4.1 Esquemas expl\u00edcitos vs. inferidos"},{"location":"tema34/#ventajas-y-riesgos-de-los-esquemas-explicitos","text":"Un esquema expl\u00edcito proporciona una estructura rigurosa y bien definida para los datos, lo que es esencial en entornos productivos y cr\u00edticos donde la integridad es clave. Al establecer de antemano los tipos de datos, nombres de columnas y estructuras esperadas, se minimizan errores derivados de datos mal formateados o inconsistencias, permitiendo validaciones m\u00e1s estrictas y optimizaci\u00f3n en el procesamiento. Esto mejora el rendimiento en consultas y operaciones, ya que el motor de ejecuci\u00f3n puede aprovechar la informaci\u00f3n estructural para optimizar los planes de ejecuci\u00f3n. Adem\u00e1s, facilita la interoperabilidad entre sistemas al garantizar que los datos siempre siguen un formato preestablecido. Sin embargo, esta rigidez tambi\u00e9n puede representar desaf\u00edos. En entornos donde la flexibilidad es esencial, como procesamiento de datos semi-estructurados o ingesti\u00f3n de datos din\u00e1micos, un esquema expl\u00edcito puede ser limitante. Cambios en la estructura de los datos pueden requerir modificaciones en el esquema, lo que conlleva tiempo y esfuerzo de mantenimiento. Adem\u00e1s, en grandes vol\u00famenes de datos, definir un esquema fijo sin conocer completamente la naturaleza de los datos puede generar problemas de compatibilidad y p\u00e9rdida de informaci\u00f3n. Por estas razones, en casos de exploraci\u00f3n de datos, se puede preferir un enfoque m\u00e1s adaptable como el esquema inferido. Aspecto Ventajas Riesgos Integridad y validaci\u00f3n Garantiza datos consistentes y previene errores de formato Puede bloquear datos que no se ajusten al esquema, limitando flexibilidad Optimizaci\u00f3n del rendimiento Mejora la velocidad de consultas y procesamiento al aprovechar la estructura definida Puede generar sobrecarga en modificaciones o adaptaciones futuras Interoperabilidad Facilita integraci\u00f3n con otros sistemas mediante formatos predefinidos Requiere coordinaci\u00f3n rigurosa para cambios y actualizaciones Adaptabilidad Ideal para datos estructurados y fuentes confiables Poco adecuado para datos semi-estructurados o en evoluci\u00f3n constante Mantenimiento Reduce la posibilidad de errores operacionales y facilita auditor\u00eda Necesita cambios manuales si se modifica la fuente de datos Si el contexto demanda estabilidad y predictibilidad, un esquema expl\u00edcito es una gran ventaja. Sin embargo, si el objetivo es manejar datos cambiantes o no estructurados, es recomendable evaluar alternativas m\u00e1s flexibles. Pipeline de Datos de IoT en Manufactura Garantiza que las alertas cr\u00edticas de temperatura y presi\u00f3n siempre tengan tipos correctos, evitando fallos en sistemas de seguridad industrial. from pyspark.sql.types import * # Esquema expl\u00edcito para sensores industriales sensor_schema = StructType([ StructField(\"sensor_id\", StringType(), False), StructField(\"timestamp\", TimestampType(), False), StructField(\"temperature\", DoubleType(), False), StructField(\"pressure\", DoubleType(), False), StructField(\"vibration\", DoubleType(), True), StructField(\"status\", StringType(), False) ]) df_sensors = spark.read.schema(sensor_schema).json(\"hdfs://sensors/data/\") Data Warehouse de E-commerce Evita que montos con formato incorrecto contaminen reportes financieros cr\u00edticos. # Esquema para tabla de \u00f3rdenes con validaciones estrictas orders_schema = StructType([ StructField(\"order_id\", LongType(), False), StructField(\"customer_id\", LongType(), False), StructField(\"order_date\", DateType(), False), StructField(\"total_amount\", DecimalType(10,2), False), StructField(\"currency\", StringType(), False), StructField(\"payment_method\", StringType(), False) ]) # Cualquier registro que no cumpla el esquema ser\u00e1 rechazado df_orders = spark.read.schema(orders_schema).option(\"mode\", \"FAILFAST\").parquet(\"s3://orders/\") Pipeline de Logs de Aplicaci\u00f3n Garantiza que campos cr\u00edticos como timestamp y level siempre est\u00e9n presentes para monitoreo y alertas. # Esquema para logs estructurados con campos obligatorios log_schema = StructType([ StructField(\"timestamp\", TimestampType(), False), StructField(\"level\", StringType(), False), StructField(\"service\", StringType(), False), StructField(\"message\", StringType(), False), StructField(\"user_id\", StringType(), True), StructField(\"request_id\", StringType(), True), StructField(\"duration_ms\", IntegerType(), True) ]) df_logs = spark.readStream.schema(log_schema).json(\"kafka://logs-topic\")","title":"Ventajas y riesgos de los esquemas expl\u00edcitos"},{"location":"tema34/#ventajas-y-riesgos-de-los-esquemas-inferidos","text":"Los esquemas inferidos ofrecen una soluci\u00f3n flexible y \u00e1gil en escenarios donde la naturaleza de los datos puede variar o evolucionar con el tiempo. En entornos de exploraci\u00f3n y prototipado, permiten cargar y procesar datos sin una definici\u00f3n estricta, lo que agiliza el desarrollo y facilita la integraci\u00f3n con fuentes heterog\u00e9neas. Esta adaptabilidad es especialmente \u00fatil en sistemas que reciben datos de m\u00faltiples or\u00edgenes o formatos desconocidos, permitiendo ajustes autom\u00e1ticos sin intervenci\u00f3n manual. Adem\u00e1s, en arquitecturas de big data como PySpark, el esquema inferido puede mejorar la facilidad de uso al eliminar la necesidad de definir expl\u00edcitamente cada estructura antes de su procesamiento. Sin embargo, esta flexibilidad conlleva ciertos riesgos. La falta de control en la definici\u00f3n de los datos puede generar errores silenciosos si los valores cambian inesperadamente, afectando la integridad de los procesos downstream. Asimismo, si los datos presentan inconsistencias, el motor de ejecuci\u00f3n podr\u00eda inferir tipos incorrectos, generando problemas de compatibilidad y fallos dif\u00edciles de detectar. En sistemas de producci\u00f3n, depender de esquemas inferidos puede provocar ineficiencias al requerir validaciones posteriores y ajustes constantes. Por ello, es fundamental evaluar el contexto antes de optar por este enfoque, combin\u00e1ndolo con estrategias que mitiguen posibles inconvenientes. Aspecto Ventajas Riesgos Flexibilidad Admite datos din\u00e1micos sin definir esquemas r\u00edgidos Puede derivar en inconsistencias si los datos cambian inesperadamente Rapidez de implementaci\u00f3n Reduce la necesidad de configuraci\u00f3n manual en entornos de exploraci\u00f3n Puede generar errores dif\u00edciles de detectar en sistemas cr\u00edticos Interoperabilidad Facilita la ingesti\u00f3n de m\u00faltiples formatos sin restricciones previas Menor control sobre el formato y calidad de los datos Adaptabilidad Ideal para prototipado y an\u00e1lisis de datos desconocidos Puede ocasionar problemas de compatibilidad en procesos posteriores Mantenimiento Minimiza esfuerzo inicial en definici\u00f3n de datos Puede generar costos adicionales de correcci\u00f3n y validaci\u00f3n en producci\u00f3n Un esquema inferido puede ser invaluable para exploraci\u00f3n y modelos de datos altamente cambiantes, pero es crucial establecer controles para minimizar errores ocultos. An\u00e1lisis de Redes Sociales Permite procesar datos de m\u00faltiples plataformas sociales sin definir esquemas espec\u00edficos para cada una. # Esquema inferido para datos variables de APIs sociales df_social = spark.read.option(\"multiline\", \"true\").json(\"s3://social-data/*/\") # Los campos pueden variar seg\u00fan la plataforma (Twitter, Facebook, Instagram) df_social.printSchema() # Muestra la estructura inferida din\u00e1micamente # Manejo seguro de campos opcionales df_processed = df_social.select( col(\"user_id\"), col(\"timestamp\"), col(\"text\").alias(\"content\"), col(\"likes\").cast(\"int\").alias(\"engagement_likes\"), col(\"shares\").cast(\"int\").alias(\"engagement_shares\") # Puede no existir en todas las plataformas ) Migraci\u00f3n de Bases de Datos Legacy Facilita la migraci\u00f3n de sistemas legacy donde la documentaci\u00f3n del esquema puede estar desactualizada o perdida. # Inferencia autom\u00e1tica para tablas con esquemas desconocidos df_legacy = spark.read.format(\"jdbc\") \\ .option(\"url\", \"jdbc:mysql://legacy-db:3306/old_system\") \\ .option(\"dbtable\", \"unknown_table\") \\ .option(\"inferSchema\", \"true\") \\ .load() # Exploraci\u00f3n r\u00e1pida de estructura df_legacy.describe().show() df_legacy.dtypes # Verificar tipos inferidos # Transformaci\u00f3n adaptativa for col_name, col_type in df_legacy.dtypes: if col_type == 'string' and 'date' in col_name.lower(): df_legacy = df_legacy.withColumn(col_name, to_date(col(col_name))) Prototipado de ML con Datos Externos Permite iniciar r\u00e1pidamente experimentos de ML sin invertir tiempo en definici\u00f3n manual de esquemas. # Carga r\u00e1pida para experimentaci\u00f3n con datasets p\u00fablicos df_experiment = spark.read.option(\"header\", \"true\") \\ .option(\"inferSchema\", \"true\") \\ .csv(\"s3://public-datasets/kaggle-competition/\") # An\u00e1lisis exploratorio inmediato df_experiment.summary().show() # Feature engineering r\u00e1pido sin conocimiento previo del esquema numeric_columns = [col_name for col_name, col_type in df_experiment.dtypes if col_type in ['int', 'double', 'float']] df_scaled = df_experiment.select( *[col(c) for c in numeric_columns] ).fillna(0)","title":"Ventajas y riesgos de los esquemas inferidos"},{"location":"tema34/#enfoque-hibrido","text":"El enfoque h\u00edbrido combina lo mejor de los esquemas expl\u00edcitos e inferidos, equilibrando estabilidad y flexibilidad. La estrategia consiste en definir un esquema expl\u00edcito para los aspectos cr\u00edticos del procesamiento de datos, garantizando integridad y eficiencia en las consultas, mientras se permite la inferencia de ciertos campos menos estructurados o de datos con variabilidad impredecible. Esto facilita la compatibilidad con datos din\u00e1micos sin comprometer la robustez del sistema. En entornos de big data y procesamiento distribuido como Spark, este enfoque puede aprovechar las ventajas de optimizaci\u00f3n y validaci\u00f3n de los esquemas expl\u00edcitos mientras mantiene adaptabilidad a cambios inesperados en los datos. Para implementarlo, se pueden definir estructuras clave con esquemas preestablecidos y, al mismo tiempo, permitir la inferencia en \u00e1reas donde la predictibilidad no es esencial. Por ejemplo, en la ingesti\u00f3n de logs, los campos cr\u00edticos como identificadores y fechas pueden tener tipos de datos definidos, mientras que los mensajes o metadatos pueden inferirse para mantener flexibilidad. Este modelo reduce el riesgo de errores silenciosos al mismo tiempo que permite eficiencia en exploraci\u00f3n y expansi\u00f3n de datos, lo que lo convierte en una opci\u00f3n ideal para sistemas que necesitan adaptabilidad sin comprometer la estabilidad operativa. # Esquema base expl\u00edcito para campos cr\u00edticos base_schema = StructType([ StructField(\"id\", StringType(), False), StructField(\"timestamp\", TimestampType(), False), StructField(\"amount\", DecimalType(10,2), False) ]) # Lectura con esquema parcial + inferencia para campos adicionales df = spark.read.schema(base_schema) \\ .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\ .json(\"data/transactions/\") # Manejo de campos adicionales inferidos additional_fields = [c for c in df.columns if c not in [\"id\", \"timestamp\", \"amount\", \"_corrupt_record\"]] Esta estrategia h\u00edbrida maximiza tanto la robustez como la flexibilidad del pipeline.","title":"Enfoque H\u00edbrido"},{"location":"tema34/#342-schema-evolution-en-spark-avro-parquet-delta","text":"La evoluci\u00f3n de esquemas en Spark es una caracter\u00edstica fundamental para manejar cambios en la estructura de datos sin afectar la compatibilidad con versiones anteriores. Esto es particularmente \u00fatil en sistemas de big data donde las fuentes de datos pueden cambiar con el tiempo, pero a\u00fan se requiere accesibilidad a datos hist\u00f3ricos. Spark ofrece soporte para la evoluci\u00f3n de esquemas en formatos como Avro, Parquet y Delta Lake, cada uno con su propia manera de gestionar modificaciones sin comprometer la integridad del sistema. En Avro, la evoluci\u00f3n de esquemas se basa en el concepto de compatibilidad mediante reglas predefinidas. Un esquema nuevo debe ser compatible con las versiones anteriores, lo que significa que los cambios pueden incluir la adici\u00f3n de nuevos campos opcionales, cambios en nombres de campos con alias, o reordenaci\u00f3n de elementos sin afectar la legibilidad de los datos. Avro mantiene metadatos estructurados que permiten interpretar versiones antiguas de datos con esquemas m\u00e1s recientes, lo que facilita la interoperabilidad entre diferentes versiones de la misma estructura. Parquet, por otro lado, soporta la evoluci\u00f3n de esquemas principalmente mediante la adici\u00f3n de columnas. Debido a su estructura basada en columnas, Parquet permite agregar nuevos atributos sin alterar los datos existentes. Sin embargo, la modificaci\u00f3n de tipos de datos o la eliminaci\u00f3n de columnas puede generar incompatibilidades, lo que obliga a realizar transformaciones adicionales para garantizar que las aplicaciones sean capaces de leer los datos correctamente. La metadata de Parquet juega un papel clave en la compatibilidad, ya que almacena informaci\u00f3n detallada sobre las versiones de esquema utilizadas. Delta Lake ofrece una evoluci\u00f3n de esquemas m\u00e1s avanzada al permitir modificaciones como la adici\u00f3n o eliminaci\u00f3n de columnas, cambios en tipos de datos y adaptaciones estructurales sin necesidad de reescribir por completo los datos almacenados. Esto se logra mediante la gesti\u00f3n de versiones dentro de un registro transaccional que mantiene la coherencia del esquema en diferentes momentos del tiempo. Con Delta Lake, Spark puede leer y escribir datos bajo m\u00faltiples versiones sin perder compatibilidad, ofreciendo una flexibilidad notable en entornos donde los datos evolucionan r\u00e1pidamente. Estos enfoques de evoluci\u00f3n de esquemas en Spark proporcionan soluciones efectivas para manejar cambios estructurales sin afectar el acceso a datos hist\u00f3ricos, lo que resulta esencial en sistemas que deben ser robustos frente a modificaciones sin comprometer la integridad ni la eficiencia del procesamiento.","title":"3.4.2 Schema evolution en Spark (Avro, Parquet, Delta)"},{"location":"tema34/#compatibilidad-de-evolucion-adicion-eliminacion-y-cambios-de-tipo","text":"La compatibilidad en la evoluci\u00f3n de esquemas permite modificaciones estructurales en los datos sin afectar la capacidad de lectura de versiones previas, siempre que se respeten ciertas reglas de compatibilidad hacia atr\u00e1s. Esto significa que es posible agregar nuevas columnas sin afectar consultas existentes, reordenar campos sin alterar su interpretaci\u00f3n, e incluso cambiar tipos de datos si la transformaci\u00f3n es segura (por ejemplo, de int a bigint o de string a text ). Sin embargo, eliminaciones y cambios de tipo m\u00e1s dr\u00e1sticos pueden generar problemas de compatibilidad, por lo que deben manejarse con estrategias como versiones de esquema o transformaciones expl\u00edcitas. Adici\u00f3n de columna en Avro: Se agrega un nuevo campo opcional email a un esquema de usuarios, asegurando que versiones anteriores a\u00fan puedan interpretar los datos sin errores. Cambio de tipo en Parquet: Se transforma una columna edad de int a bigint para admitir valores m\u00e1s grandes sin afectar consultas existentes. Eliminaci\u00f3n en Delta Lake: Se elimina una columna obsoleta direcci\u00f3n y los datos hist\u00f3ricos siguen accesibles mediante versiones anteriores registradas en el log transaccional de Delta. Estos cambios reflejan c\u00f3mo Spark maneja la evoluci\u00f3n sin comprometer la estabilidad del sistema.","title":"Compatibilidad de evoluci\u00f3n: adici\u00f3n, eliminaci\u00f3n y cambios de tipo"},{"location":"tema34/#herramientas-para-manejar-la-evolucion-de-esquemas","text":"Las herramientas de evoluci\u00f3n de esquemas en Spark permiten manejar cambios estructurales sin comprometer la integridad de los datos. Par\u00e1metros como spark.sql.parquet.mergeSchema=true en Parquet y --mergeSchema en Delta Lake facilitan la integraci\u00f3n de nuevas columnas o estructuras sin requerir una migraci\u00f3n completa, manteniendo compatibilidad con versiones anteriores. En Avro, la evoluci\u00f3n de esquemas mediante AvroSchema asegura que los datos hist\u00f3ricos y actuales puedan coexistir sin afectar procesos anal\u00edticos. Una plataforma de monitoreo ambiental activa mergeSchema en Delta para incorporar sensores adicionales sin interrupciones operativas. Un equipo de ingenier\u00eda de datos usa Avro con evoluci\u00f3n de esquemas para adaptar su pipeline de predicciones sin reescribir datos antiguos. Una empresa de retail ajusta autom\u00e1ticamente su modelo Parquet para agregar atributos nuevos sobre tendencias de compra sin afectar reportes hist\u00f3ricos. Estas soluciones garantizan adaptabilidad y estabilidad en entornos din\u00e1micos de big data.","title":"Herramientas para manejar la evoluci\u00f3n de esquemas"},{"location":"tema34/#343-control-de-versiones-de-esquemas","text":"En sistemas de procesamiento distribuido, el control de versiones de esquemas es un mecanismo clave para garantizar la trazabilidad, validaci\u00f3n y compatibilidad de los datos a lo largo del tiempo. Esto es especialmente relevante en arquitecturas de big data, donde los datos evolucionan constantemente y deben seguir siendo accesibles sin interrupciones. Sin un sistema de versionado adecuado, los cambios en los esquemas pueden generar errores al momento de consumir o transformar datos, afectando la confiabilidad del sistema. Existen dos enfoques principales para gestionar la evoluci\u00f3n de esquemas: el uso de Schema Registry , que permite registrar y validar los esquemas de manera centralizada, y el versionado de tablas en Delta Lake , que ofrece un historial de cambios con soporte para consultas en diferentes puntos del tiempo.","title":"3.4.3 Control de versiones de esquemas"},{"location":"tema34/#uso-de-schema-registry","text":"Un Schema Registry es un servicio que almacena y gestiona las versiones de los esquemas de datos utilizados en un sistema. Soluciones como Confluent Schema Registry (para Apache Kafka) o AWS Glue Schema Registry permiten registrar, versionar y validar esquemas al momento de producir o consumir datos. Esto asegura que los productores de datos sigan un formato predefinido y que los consumidores puedan interpretar los datos correctamente, incluso si se han realizado cambios en la estructura. El Schema Registry act\u00faa como un intermediario que evita incompatibilidades entre diferentes versiones de datos. Al registrar un esquema, el sistema verifica que los cambios sean compatibles con versiones anteriores, permitiendo agregar nuevos campos opcionales sin afectar la lectura de datos hist\u00f3ricos. Adem\u00e1s, si un consumidor utiliza una versi\u00f3n anterior del esquema, el registro puede proporcionar un mecanismo de compatibilidad que transforma los datos para que sigan siendo accesibles. Ejemplo : Supongamos que una empresa de telecomunicaciones utiliza Kafka para procesar registros de llamadas. Cada mensaje en Kafka sigue un esquema Avro gestionado por Confluent Schema Registry . Cuando se a\u00f1ade un nuevo campo ubicaci\u00f3n a los datos de llamadas, el Schema Registry asegura que los consumidores existentes puedan seguir procesando los registros sin errores, manteniendo compatibilidad con versiones anteriores.","title":"Uso de Schema Registry"},{"location":"tema34/#versionado-de-tablas-con-delta-lake","text":"Delta Lake ofrece un sistema de versionado de esquemas incorporado en su arquitectura transaccional. A diferencia de otros formatos como Parquet o Avro, Delta Lake permite el \"time travel\" , es decir, la capacidad de consultar versiones previas de una tabla y revertir cambios si es necesario. Este mecanismo es esencial para garantizar la estabilidad en sistemas donde los datos cambian con frecuencia. Las operaciones estructurales como ADD COLUMN , CHANGE TYPE y DROP COLUMN son registradas en el log de transacciones de Delta, lo que permite rastrear c\u00f3mo evolucion\u00f3 el esquema a lo largo del tiempo. Si un cambio genera problemas en los consumidores de datos, se pueden restaurar versiones anteriores sin afectar la integridad de los datos almacenados. Ejemplo : Una startup que trabaja con datos de sensores usa Delta Lake para almacenar registros de mediciones. Para incorporar nuevas m\u00e9tricas sin afectar sistemas que dependen de datos hist\u00f3ricos, habilitan --mergeSchema en Delta Lake. As\u00ed, los nuevos datos con m\u00e9tricas adicionales son integrados sin necesidad de realizar migraciones complejas y sin impactar consultas anteriores.","title":"Versionado de tablas con Delta Lake"},{"location":"tema34/#344-validacion-y-limpieza-de-datos","text":"La validaci\u00f3n y limpieza de datos son procesos fundamentales para garantizar que la informaci\u00f3n utilizada en sistemas anal\u00edticos o de negocio sea precisa y confiable. La validaci\u00f3n implica la verificaci\u00f3n de la integridad de los datos mediante reglas predefinidas, como la comprobaci\u00f3n de formatos, rangos aceptables y valores nulos. Este proceso asegura que los datos ingresados cumplan con las expectativas y requisitos del sistema antes de ser utilizados en c\u00e1lculos o reportes. Por otro lado, la limpieza de datos aborda problemas como valores duplicados, inconsistencias, errores tipogr\u00e1ficos y registros incompletos, permitiendo transformar datos crudos en informaci\u00f3n estructurada y libre de anomal\u00edas. Sin estos mecanismos, los sistemas corren el riesgo de generar an\u00e1lisis err\u00f3neos, afectar la toma de decisiones y comprometer la confiabilidad de modelos predictivos. Implementar una estrategia efectiva de validaci\u00f3n y limpieza requiere el uso de herramientas especializadas, como reglas definidas en bases de datos, funciones en frameworks de procesamiento de datos como PySpark y SparkSQL, o soluciones avanzadas como Data Quality Services en entornos empresariales. La automatizaci\u00f3n de estos procesos es clave para mantener la escalabilidad y eficiencia, reduciendo la carga operativa en equipos de datos y garantizando que la informaci\u00f3n est\u00e9 siempre en condiciones \u00f3ptimas para su uso. Adem\u00e1s, estrategias como la detecci\u00f3n de valores at\u00edpicos, normalizaci\u00f3n de formatos y enriquecimiento de datos pueden mejorar la calidad de la informaci\u00f3n disponible y potenciar la capacidad de los sistemas para ofrecer insights precisos y relevantes.","title":"3.4.4 Validaci\u00f3n y limpieza de datos"},{"location":"tema34/#reglas-de-calidad-nulos-tipos-rangos-unicidad","text":"Las reglas de calidad de datos son validaciones sistem\u00e1ticas que garantizan que los datos cumplan con est\u00e1ndares espec\u00edficos antes de ser procesados o almacenados. En Spark, estas reglas se pueden implementar como filtros (que excluyen registros no v\u00e1lidos) o como excepciones (que detienen el procesamiento cuando se detectan problemas). 1. Reglas de Nulos Las reglas de nulos verifican que los campos cr\u00edticos no contengan valores faltantes o que los campos opcionales manejen correctamente los valores nulos. Ejemplo PySpark : from pyspark.sql import SparkSession from pyspark.sql.functions import col, isnan, isnull, when, count # Datos de ejemplo data = [ (\"001\", \"Juan P\u00e9rez\", 25, \"juan@email.com\"), (\"002\", None, 30, \"ana@email.com\"), (\"003\", \"Carlos L\u00f3pez\", None, None), (\"004\", \"Mar\u00eda Garc\u00eda\", 28, \"maria@email.com\") ] df = spark.createDataFrame(data, [\"id\", \"nombre\", \"edad\", \"email\"]) # Regla: nombre y email son obligatorios df_clean = df.filter( col(\"nombre\").isNotNull() & col(\"email\").isNotNull() ) print(\"Registros v\u00e1lidos:\") df_clean.show() # Registros rechazados para auditor\u00eda df_rejected = df.filter( col(\"nombre\").isNull() | col(\"email\").isNull() ) print(\"Registros rechazados:\") df_rejected.show() Ejemplo SparkSQL : -- Crear vista temporal CREATE OR REPLACE TEMPORARY VIEW usuarios AS SELECT * FROM VALUES ('001', 'Juan P\u00e9rez', 25, 'juan@email.com'), ('002', NULL, 30, 'ana@email.com'), ('003', 'Carlos L\u00f3pez', NULL, NULL), ('004', 'Mar\u00eda Garc\u00eda', 28, 'maria@email.com') AS t(id, nombre, edad, email); -- Filtrar registros v\u00e1lidos SELECT * FROM usuarios WHERE nombre IS NOT NULL AND email IS NOT NULL; -- Contar registros con problemas de calidad SELECT COUNT(*) as total_registros, SUM(CASE WHEN nombre IS NULL THEN 1 ELSE 0 END) as nombres_nulos, SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as emails_nulos FROM usuarios; Ejemplo PySpark : def validate_nulls(df, required_columns): \"\"\"Valida que las columnas requeridas no tengan nulos\"\"\" for col_name in required_columns: null_count = df.filter(col(col_name).isNull()).count() if null_count > 0: raise ValueError(f\"Columna '{col_name}' tiene {null_count} valores nulos\") return df # Aplicar validaci\u00f3n estricta try: df_validated = validate_nulls(df, [\"nombre\", \"email\"]) print(\"Validaci\u00f3n exitosa\") except ValueError as e: print(f\"Error de calidad: {e}\") 2. Reglas de Tipos Las reglas de tipos verifican que los datos tengan el formato correcto seg\u00fan su tipo esperado (n\u00fameros, fechas, emails, etc.). Ejemplo PySpark : from pyspark.sql.functions import regexp_match, length, when from pyspark.sql.types import IntegerType # Datos con problemas de tipos data = [ (\"001\", \"25\", \"2023-12-01\", \"juan@email.com\"), (\"002\", \"treinta\", \"2023/12/02\", \"ana.email.com\"), (\"003\", \"28\", \"invalid_date\", \"carlos@email.com\"), (\"004\", \"-5\", \"2023-12-03\", \"maria@domain\") ] df = spark.createDataFrame(data, [\"id\", \"edad_str\", \"fecha_str\", \"email\"]) # Regla 1: Edad debe ser num\u00e9rica y positiva df_with_edad = df.withColumn( \"edad_valida\", when(col(\"edad_str\").rlike(\"^[0-9]+$\") & (col(\"edad_str\").cast(IntegerType()) > 0), col(\"edad_str\").cast(IntegerType()) ).otherwise(None) ) # Regla 2: Email debe tener formato v\u00e1lido df_with_email = df_with_edad.withColumn( \"email_valido\", when(col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"), col(\"email\") ).otherwise(None) ) # Regla 3: Fecha debe tener formato ISO df_with_fecha = df_with_email.withColumn( \"fecha_valida\", when(col(\"fecha_str\").rlike(\"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"), to_date(col(\"fecha_str\")) ).otherwise(None) ) # Filtrar solo registros completamente v\u00e1lidos df_clean = df_with_fecha.filter( col(\"edad_valida\").isNotNull() & col(\"email_valido\").isNotNull() & col(\"fecha_valida\").isNotNull() ) df_clean.select(\"id\", \"edad_valida\", \"fecha_valida\", \"email_valido\").show() Ejemplo SparkSQL : -- Crear datos de prueba CREATE OR REPLACE TEMPORARY VIEW datos_raw AS SELECT * FROM VALUES ('001', '25', '2023-12-01', 'juan@email.com'), ('002', 'treinta', '2023/12/02', 'ana.email.com'), ('003', '28', 'invalid_date', 'carlos@email.com'), ('004', '-5', '2023-12-03', 'maria@domain') AS t(id, edad_str, fecha_str, email); -- Validar tipos y aplicar reglas SELECT id, CASE WHEN edad_str RLIKE '^[0-9]+$' AND CAST(edad_str AS INT) > 0 THEN CAST(edad_str AS INT) ELSE NULL END as edad_valida, CASE WHEN fecha_str RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$' THEN TO_DATE(fecha_str) ELSE NULL END as fecha_valida, CASE WHEN email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$' THEN email ELSE NULL END as email_valido FROM datos_raw WHERE edad_str RLIKE '^[0-9]+$' AND CAST(edad_str AS INT) > 0 AND fecha_str RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$' AND email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'; 3. Reglas de Rangos Las reglas de rangos verifican que los valores num\u00e9ricos, fechas o strings est\u00e9n dentro de l\u00edmites aceptables. Ejemplo PySpark : from pyspark.sql.functions import col, when, current_date, datediff from datetime import datetime, date # Datos de empleados data = [ (\"001\", \"Juan\", 25, 50000, \"2020-01-15\"), (\"002\", \"Ana\", 17, 30000, \"2025-06-01\"), # Menor de edad (\"003\", \"Carlos\", 45, -5000, \"2019-03-10\"), # Salario negativo (\"004\", \"Mar\u00eda\", 150, 80000, \"1800-12-25\"), # Edad imposible, fecha muy antigua (\"005\", \"Luis\", 30, 60000, \"2022-05-20\") ] df = spark.createDataFrame(data, [\"id\", \"nombre\", \"edad\", \"salario\", \"fecha_ingreso\"]) df = df.withColumn(\"fecha_ingreso\", to_date(col(\"fecha_ingreso\"))) # Regla 1: Edad entre 18 y 65 a\u00f1os df_edad_valida = df.withColumn( \"edad_en_rango\", when((col(\"edad\") >= 18) & (col(\"edad\") <= 65), True).otherwise(False) ) # Regla 2: Salario positivo y menor a 200,000 df_salario_valido = df_edad_valida.withColumn( \"salario_en_rango\", when((col(\"salario\") > 0) & (col(\"salario\") <= 200000), True).otherwise(False) ) # Regla 3: Fecha de ingreso no futura y no anterior a 2000 fecha_minima = date(2000, 1, 1) df_fecha_valida = df_salario_valido.withColumn( \"fecha_en_rango\", when((col(\"fecha_ingreso\") >= lit(fecha_minima)) & (col(\"fecha_ingreso\") <= current_date()), True).otherwise(False) ) # Filtrar registros v\u00e1lidos df_valid = df_fecha_valida.filter( col(\"edad_en_rango\") & col(\"salario_en_rango\") & col(\"fecha_en_rango\") ) print(\"Empleados v\u00e1lidos:\") df_valid.select(\"id\", \"nombre\", \"edad\", \"salario\", \"fecha_ingreso\").show() # Reporte de calidad quality_report = df_fecha_valida.agg( count(\"*\").alias(\"total_registros\"), sum(when(col(\"edad_en_rango\"), 1).otherwise(0)).alias(\"edad_valida\"), sum(when(col(\"salario_en_rango\"), 1).otherwise(0)).alias(\"salario_valido\"), sum(when(col(\"fecha_en_rango\"), 1).otherwise(0)).alias(\"fecha_valida\") ) quality_report.show() Ejemplo SparkSQL : -- Crear datos de empleados CREATE OR REPLACE TEMPORARY VIEW empleados AS SELECT * FROM VALUES ('001', 'Juan', 25, 50000, '2020-01-15'), ('002', 'Ana', 17, 30000, '2025-06-01'), ('003', 'Carlos', 45, -5000, '2019-03-10'), ('004', 'Mar\u00eda', 150, 80000, '1800-12-25'), ('005', 'Luis', 30, 60000, '2022-05-20') AS t(id, nombre, edad, salario, fecha_ingreso_str); -- Aplicar reglas de rango SELECT id, nombre, edad, salario, TO_DATE(fecha_ingreso_str) as fecha_ingreso, -- Validaciones de rango CASE WHEN edad BETWEEN 18 AND 65 THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as edad_estado, CASE WHEN salario > 0 AND salario <= 200000 THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as salario_estado, CASE WHEN TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE() THEN 'V\u00c1LIDO' ELSE 'INV\u00c1LIDO' END as fecha_estado FROM empleados WHERE edad BETWEEN 18 AND 65 AND salario > 0 AND salario <= 200000 AND TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE(); -- Estad\u00edsticas de calidad por rango SELECT COUNT(*) as total_empleados, SUM(CASE WHEN edad BETWEEN 18 AND 65 THEN 1 ELSE 0 END) as edad_valida, SUM(CASE WHEN salario > 0 AND salario <= 200000 THEN 1 ELSE 0 END) as salario_valido, SUM(CASE WHEN TO_DATE(fecha_ingreso_str) BETWEEN '2000-01-01' AND CURRENT_DATE() THEN 1 ELSE 0 END) as fecha_valida FROM empleados; 4. Reglas de Unicidad Las reglas de unicidad verifican que no existan duplicados en campos que deben ser \u00fanicos (IDs, emails, c\u00f3digos, etc.). Ejemplo PySpark : from pyspark.sql.functions import col, count, desc from pyspark.sql.window import Window # Datos con duplicados data = [ (\"001\", \"juan@email.com\", \"12345678\", \"Juan P\u00e9rez\"), (\"002\", \"ana@email.com\", \"87654321\", \"Ana Garc\u00eda\"), (\"003\", \"carlos@email.com\", \"12345678\", \"Carlos L\u00f3pez\"), # DNI duplicado (\"004\", \"juan@email.com\", \"11111111\", \"Juan Mart\u00ednez\"), # Email duplicado (\"005\", \"maria@email.com\", \"22222222\", \"Mar\u00eda Rodr\u00edguez\"), (\"001\", \"pedro@email.com\", \"33333333\", \"Pedro S\u00e1nchez\") # ID duplicado ] df = spark.createDataFrame(data, [\"id\", \"email\", \"dni\", \"nombre\"]) # M\u00e9todo 1: Identificar duplicados por campo print(\"=== An\u00e1lisis de Duplicados ===\") # Duplicados por ID duplicados_id = df.groupBy(\"id\").count().filter(col(\"count\") > 1) print(\"IDs duplicados:\") duplicados_id.show() # Duplicados por Email duplicados_email = df.groupBy(\"email\").count().filter(col(\"count\") > 1) print(\"Emails duplicados:\") duplicados_email.show() # Duplicados por DNI duplicados_dni = df.groupBy(\"dni\").count().filter(col(\"count\") > 1) print(\"DNIs duplicados:\") duplicados_dni.show() # M\u00e9todo 2: Marcar registros duplicados window_id = Window.partitionBy(\"id\") window_email = Window.partitionBy(\"email\") window_dni = Window.partitionBy(\"dni\") df_marked = df.withColumn(\"count_id\", count(\"*\").over(window_id)) \\ .withColumn(\"count_email\", count(\"*\").over(window_email)) \\ .withColumn(\"count_dni\", count(\"*\").over(window_dni)) # Identificar registros con alg\u00fan tipo de duplicado df_duplicates = df_marked.filter( (col(\"count_id\") > 1) | (col(\"count_email\") > 1) | (col(\"count_dni\") > 1) ) print(\"Registros con duplicados:\") df_duplicates.orderBy(\"id\").show() # M\u00e9todo 3: Mantener solo registros \u00fanicos (primera ocurrencia) df_unique = df.dropDuplicates([\"id\", \"email\", \"dni\"]) print(\"Registros \u00fanicos:\") df_unique.show() # M\u00e9todo 4: Validaci\u00f3n estricta con excepci\u00f3n def validate_uniqueness(df, unique_columns): \"\"\"Valida que las columnas especificadas sean \u00fanicas\"\"\" for col_name in unique_columns: duplicate_count = df.groupBy(col_name).count().filter(col(\"count\") > 1).count() if duplicate_count > 0: duplicates = df.groupBy(col_name).count().filter(col(\"count\") > 1).collect() raise ValueError(f\"Columna '{col_name}' tiene duplicados: {[row[col_name] for row in duplicates]}\") return True try: validate_uniqueness(df_unique, [\"id\", \"email\", \"dni\"]) print(\"Validaci\u00f3n de unicidad exitosa\") except ValueError as e: print(f\"Error de unicidad: {e}\") Ejemplo SparkSQL : -- Crear datos con duplicados CREATE OR REPLACE TEMPORARY VIEW usuarios_duplicados AS SELECT * FROM VALUES ('001', 'juan@email.com', '12345678', 'Juan P\u00e9rez'), ('002', 'ana@email.com', '87654321', 'Ana Garc\u00eda'), ('003', 'carlos@email.com', '12345678', 'Carlos L\u00f3pez'), ('004', 'juan@email.com', '11111111', 'Juan Mart\u00ednez'), ('005', 'maria@email.com', '22222222', 'Mar\u00eda Rodr\u00edguez'), ('001', 'pedro@email.com', '33333333', 'Pedro S\u00e1nchez') AS t(id, email, dni, nombre); -- An\u00e1lisis de duplicados por campo SELECT 'ID' as campo, id as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY id HAVING COUNT(*) > 1 UNION ALL SELECT 'EMAIL' as campo, email as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY email HAVING COUNT(*) > 1 UNION ALL SELECT 'DNI' as campo, dni as valor, COUNT(*) as cantidad FROM usuarios_duplicados GROUP BY dni HAVING COUNT(*) > 1; -- Identificar registros duplicados con detalles WITH duplicados_marcados AS ( SELECT *, COUNT(*) OVER (PARTITION BY id) as count_id, COUNT(*) OVER (PARTITION BY email) as count_email, COUNT(*) OVER (PARTITION BY dni) as count_dni, ROW_NUMBER() OVER (PARTITION BY id ORDER BY nombre) as rn_id FROM usuarios_duplicados ) SELECT id, email, dni, nombre, CASE WHEN count_id > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_id, CASE WHEN count_email > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_email, CASE WHEN count_dni > 1 THEN 'DUPLICADO' ELSE '\u00daNICO' END as estado_dni FROM duplicados_marcados WHERE count_id > 1 OR count_email > 1 OR count_dni > 1 ORDER BY id; -- Obtener registros \u00fanicos (primera ocurrencia por cada campo) WITH ranked_records AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY nombre) as rn_id, ROW_NUMBER() OVER (PARTITION BY email ORDER BY nombre) as rn_email, ROW_NUMBER() OVER (PARTITION BY dni ORDER BY nombre) as rn_dni FROM usuarios_duplicados ) SELECT id, email, dni, nombre FROM ranked_records WHERE rn_id = 1 AND rn_email = 1 AND rn_dni = 1; -- Reporte de calidad de unicidad SELECT COUNT(*) as total_registros, COUNT(DISTINCT id) as ids_unicos, COUNT(DISTINCT email) as emails_unicos, COUNT(DISTINCT dni) as dnis_unicos, COUNT(*) - COUNT(DISTINCT id) as duplicados_id, COUNT(*) - COUNT(DISTINCT email) as duplicados_email, COUNT(*) - COUNT(DISTINCT dni) as duplicados_dni FROM usuarios_duplicados;","title":"Reglas de calidad: nulos, tipos, rangos, unicidad"},{"location":"tema34/#pipeline-integrado-de-calidad","text":"Ejemplo PySpark : def apply_data_quality_rules(df): \"\"\"Aplica todas las reglas de calidad de datos\"\"\" from pyspark.sql.functions import * # 1. Reglas de nulos df = df.filter(col(\"id\").isNotNull() & col(\"email\").isNotNull()) # 2. Reglas de tipos df = df.filter(col(\"edad\").rlike(\"^[0-9]+$\")) \\ .withColumn(\"edad\", col(\"edad\").cast(\"int\")) # 3. Reglas de rangos df = df.filter((col(\"edad\") >= 18) & (col(\"edad\") <= 65)) df = df.filter((col(\"salario\") > 0) & (col(\"salario\") <= 200000)) # 4. Reglas de unicidad df = df.dropDuplicates([\"id\", \"email\"]) return df # Aplicar pipeline de calidad df_clean = apply_data_quality_rules(df_raw) Ejemplo SparkSQL : -- Pipeline completo de calidad de datos WITH quality_pipeline AS ( SELECT * FROM raw_data WHERE -- Reglas de nulos id IS NOT NULL AND email IS NOT NULL -- Reglas de tipos AND edad RLIKE '^[0-9]+$' -- Reglas de rangos AND CAST(edad AS INT) BETWEEN 18 AND 65 AND salario > 0 AND salario <= 200000 -- Fecha v\u00e1lida AND fecha_ingreso BETWEEN '2000-01-01' AND CURRENT_DATE() ), unique_records AS ( SELECT DISTINCT * -- Regla de unicidad b\u00e1sica FROM quality_pipeline ) SELECT * FROM unique_records; Estos ejemplos muestran c\u00f3mo implementar sistem\u00e1ticamente reglas de calidad de datos en Spark, tanto usando la API de Python como SparkSQL, proporcionando robustez y confiabilidad a los pipelines de datos.","title":"Pipeline Integrado de Calidad"},{"location":"tema34/#deteccion-y-manejo-de-duplicados","text":"La detecci\u00f3n y manejo de duplicados es crucial para mantener la integridad de los datos y evitar problemas como conteos err\u00f3neos, cargas incorrectas y an\u00e1lisis sesgados. En Spark, podemos implementar diferentes estrategias de detecci\u00f3n usando claves primarias, combinaciones \u00fanicas o funciones hash. 1. Detecci\u00f3n por Claves Primarias Las claves primarias son identificadores \u00fanicos que no deben repetirse en un dataset. Su duplicaci\u00f3n indica problemas serios de calidad de datos. Ejemplo PySpark : from pyspark.sql import SparkSession from pyspark.sql.functions import col, count, desc, first, last, max as spark_max, min as spark_min from pyspark.sql.window import Window # Datos de ejemplo con duplicados en clave primaria data = [ (\"USR001\", \"Juan P\u00e9rez\", \"juan@email.com\", \"2023-01-15\", 1000), (\"USR002\", \"Ana Garc\u00eda\", \"ana@email.com\", \"2023-01-16\", 1500), (\"USR001\", \"Juan P\u00e9rez\", \"juan.perez@email.com\", \"2023-01-17\", 1200), # ID duplicado (\"USR003\", \"Carlos L\u00f3pez\", \"carlos@email.com\", \"2023-01-18\", 800), (\"USR002\", \"Ana Garc\u00eda\", \"ana.garcia@email.com\", \"2023-01-19\", 1600), # ID duplicado (\"USR004\", \"Mar\u00eda Rodr\u00edguez\", \"maria@email.com\", \"2023-01-20\", 2000) ] df = spark.createDataFrame(data, [\"user_id\", \"nombre\", \"email\", \"fecha_registro\", \"saldo\"]) # Detectar duplicados por clave primaria print(\"=== Detecci\u00f3n de Duplicados por Clave Primaria ===\") duplicados_pk = df.groupBy(\"user_id\") \\ .count() \\ .filter(col(\"count\") > 1) \\ .orderBy(desc(\"count\")) print(\"Claves primarias duplicadas:\") duplicados_pk.show() # Mostrar todos los registros duplicados df_duplicados = df.join(duplicados_pk.select(\"user_id\"), [\"user_id\"]) print(\"Registros con claves primarias duplicadas:\") df_duplicados.orderBy(\"user_id\", \"fecha_registro\").show() # Estrategia 1: Mantener el registro m\u00e1s reciente window_spec = Window.partitionBy(\"user_id\").orderBy(desc(\"fecha_registro\")) df_latest = df.withColumn(\"rn\", row_number().over(window_spec)) \\ .filter(col(\"rn\") == 1) \\ .drop(\"rn\") print(\"Registros \u00fanicos (m\u00e1s recientes):\") df_latest.orderBy(\"user_id\").show() # Estrategia 2: Mantener el registro con mayor saldo window_spec_saldo = Window.partitionBy(\"user_id\").orderBy(desc(\"saldo\")) df_max_saldo = df.withColumn(\"rn\", row_number().over(window_spec_saldo)) \\ .filter(col(\"rn\") == 1) \\ .drop(\"rn\") print(\"Registros \u00fanicos (mayor saldo):\") df_max_saldo.orderBy(\"user_id\").show() # Estrategia 3: Consolidar informaci\u00f3n de duplicados df_consolidated = df.groupBy(\"user_id\") \\ .agg( first(\"nombre\").alias(\"nombre\"), first(\"email\").alias(\"email_principal\"), spark_max(\"fecha_registro\").alias(\"ultima_fecha\"), spark_max(\"saldo\").alias(\"saldo_maximo\"), count(\"*\").alias(\"num_registros\") ) print(\"Registros consolidados:\") df_consolidated.show() Ejemplo SparkSQL : -- Crear tabla con duplicados en clave primaria CREATE OR REPLACE TEMPORARY VIEW usuarios_duplicados AS SELECT * FROM VALUES ('USR001', 'Juan P\u00e9rez', 'juan@email.com', '2023-01-15', 1000), ('USR002', 'Ana Garc\u00eda', 'ana@email.com', '2023-01-16', 1500), ('USR001', 'Juan P\u00e9rez', 'juan.perez@email.com', '2023-01-17', 1200), ('USR003', 'Carlos L\u00f3pez', 'carlos@email.com', '2023-01-18', 800), ('USR002', 'Ana Garc\u00eda', 'ana.garcia@email.com', '2023-01-19', 1600), ('USR004', 'Mar\u00eda Rodr\u00edguez', 'maria@email.com', '2023-01-20', 2000) AS t(user_id, nombre, email, fecha_registro, saldo); -- Detectar duplicados por clave primaria SELECT user_id, COUNT(*) as num_duplicados FROM usuarios_duplicados GROUP BY user_id HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- Mostrar registros duplicados con detalles WITH duplicados AS ( SELECT user_id FROM usuarios_duplicados GROUP BY user_id HAVING COUNT(*) > 1 ) SELECT u.*, 'DUPLICADO' as estado FROM usuarios_duplicados u INNER JOIN duplicados d ON u.user_id = d.user_id ORDER BY u.user_id, u.fecha_registro; -- Estrategia 1: Mantener registro m\u00e1s reciente WITH ranked_records AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY fecha_registro DESC) as rn FROM usuarios_duplicados ) SELECT user_id, nombre, email, fecha_registro, saldo FROM ranked_records WHERE rn = 1 ORDER BY user_id; -- Estrategia 2: Consolidar informaci\u00f3n SELECT user_id, FIRST(nombre) as nombre, FIRST(email) as email_principal, MAX(fecha_registro) as ultima_fecha, MAX(saldo) as saldo_maximo, COUNT(*) as num_registros_originales FROM usuarios_duplicados GROUP BY user_id ORDER BY user_id; 2. Detecci\u00f3n por Combinaciones \u00danicas Las combinaciones \u00fanicas involucran m\u00faltiples campos que juntos deben ser \u00fanicos, como combinaciones de nombre+email, producto+fecha, etc. Ejemplo PySpark : from pyspark.sql.functions import concat_ws, md5, col, count, desc, collect_list # Datos de transacciones con duplicados data = [ (\"TXN001\", \"USR001\", \"2023-01-15\", \"COMPRA\", 100.50, \"Producto A\"), (\"TXN002\", \"USR002\", \"2023-01-15\", \"COMPRA\", 200.00, \"Producto B\"), (\"TXN003\", \"USR001\", \"2023-01-15\", \"COMPRA\", 100.50, \"Producto A\"), # Duplicado potencial (\"TXN004\", \"USR003\", \"2023-01-16\", \"VENTA\", 150.00, \"Producto C\"), (\"TXN005\", \"USR002\", \"2023-01-15\", \"COMPRA\", 200.00, \"Producto B\"), # Duplicado potencial (\"TXN006\", \"USR001\", \"2023-01-17\", \"COMPRA\", 100.50, \"Producto A\") # Mismo usuario y producto, fecha diferente ] df = spark.createDataFrame(data, [\"txn_id\", \"user_id\", \"fecha\", \"tipo\", \"monto\", \"producto\"]) print(\"=== Detecci\u00f3n por Combinaciones \u00danicas ===\") # Combinaci\u00f3n 1: user_id + fecha + producto (transacciones id\u00e9nticas) duplicados_combo1 = df.groupBy(\"user_id\", \"fecha\", \"producto\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por usuario + fecha + producto:\") duplicados_combo1.show(truncate=False) # Combinaci\u00f3n 2: user_id + tipo + monto (transacciones similares) duplicados_combo2 = df.groupBy(\"user_id\", \"tipo\", \"monto\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\"), collect_list(\"fecha\").alias(\"fechas\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por usuario + tipo + monto:\") duplicados_combo2.show(truncate=False) # Crear clave compuesta para an\u00e1lisis df_with_key = df.withColumn(\"clave_compuesta\", concat_ws(\"|\", col(\"user_id\"), col(\"fecha\"), col(\"producto\"))) # Detectar duplicados exactos duplicados_exactos = df_with_key.groupBy(\"clave_compuesta\") \\ .agg(count(\"*\").alias(\"count\"), collect_list(\"txn_id\").alias(\"txn_ids\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados exactos (clave compuesta):\") duplicados_exactos.show(truncate=False) # Estrategia: Eliminar duplicados manteniendo el primer registro df_sin_duplicados = df.dropDuplicates([\"user_id\", \"fecha\", \"producto\"]) print(\"Registros sin duplicados:\") df_sin_duplicados.orderBy(\"user_id\", \"fecha\").show() # Estrategia: Marcar duplicados para auditor\u00eda window_spec = Window.partitionBy(\"user_id\", \"fecha\", \"producto\").orderBy(\"txn_id\") df_marked = df.withColumn(\"es_duplicado\", when(row_number().over(window_spec) > 1, True).otherwise(False)) print(\"Registros marcados (duplicados identificados):\") df_marked.orderBy(\"user_id\", \"fecha\", \"txn_id\").show() Ejemplo SparkSQL : -- Crear tabla de transacciones CREATE OR REPLACE TEMPORARY VIEW transacciones AS SELECT * FROM VALUES ('TXN001', 'USR001', '2023-01-15', 'COMPRA', 100.50, 'Producto A'), ('TXN002', 'USR002', '2023-01-15', 'COMPRA', 200.00, 'Producto B'), ('TXN003', 'USR001', '2023-01-15', 'COMPRA', 100.50, 'Producto A'), ('TXN004', 'USR003', '2023-01-16', 'VENTA', 150.00, 'Producto C'), ('TXN005', 'USR002', '2023-01-15', 'COMPRA', 200.00, 'Producto B'), ('TXN006', 'USR001', '2023-01-17', 'COMPRA', 100.50, 'Producto A') AS t(txn_id, user_id, fecha, tipo, monto, producto); -- Detectar duplicados por combinaciones \u00fanicas SELECT user_id, fecha, producto, tipo, monto, COUNT(*) as num_duplicados, COLLECT_LIST(txn_id) as txn_ids FROM transacciones GROUP BY user_id, fecha, producto, tipo, monto HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- An\u00e1lisis de duplicados con detalles WITH duplicados_identificados AS ( SELECT user_id, fecha, producto, COUNT(*) as count_duplicados FROM transacciones GROUP BY user_id, fecha, producto HAVING COUNT(*) > 1 ) SELECT t.*, 'DUPLICADO' as estado, ROW_NUMBER() OVER (PARTITION BY t.user_id, t.fecha, t.producto ORDER BY t.txn_id) as orden_duplicado FROM transacciones t INNER JOIN duplicados_identificados d ON t.user_id = d.user_id AND t.fecha = d.fecha AND t.producto = d.producto ORDER BY t.user_id, t.fecha, t.producto, t.txn_id; -- Eliminar duplicados manteniendo el primero WITH ranked_transactions AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id, fecha, producto ORDER BY txn_id) as rn FROM transacciones ) SELECT txn_id, user_id, fecha, tipo, monto, producto FROM ranked_transactions WHERE rn = 1 ORDER BY user_id, fecha; -- Crear clave compuesta para an\u00e1lisis SELECT *, CONCAT(user_id, '|', fecha, '|', producto) as clave_compuesta, COUNT(*) OVER (PARTITION BY user_id, fecha, producto) as count_grupo FROM transacciones ORDER BY clave_compuesta; 3. Detecci\u00f3n por Funciones Hash Las funciones hash permiten detectar duplicados de manera eficiente, especialmente \u00fatil para registros con muchos campos o contenido textual. Ejemplo PySpark : from pyspark.sql.functions import md5, sha1, sha2, concat_ws, col, count, collect_set # Datos de documentos con contenido similar data = [ (\"DOC001\", \"Contrato de Servicios\", \"Este es un contrato para servicios de consultor\u00eda\", \"2023-01-15\", \"Juan P\u00e9rez\"), (\"DOC002\", \"Propuesta Comercial\", \"Propuesta para desarrollo de software\", \"2023-01-16\", \"Ana Garc\u00eda\"), (\"DOC003\", \"Contrato de Servicios\", \"Este es un contrato para servicios de consultor\u00eda\", \"2023-01-17\", \"Carlos L\u00f3pez\"), # Contenido duplicado (\"DOC004\", \"Manual de Usuario\", \"Gu\u00eda completa para el uso del sistema\", \"2023-01-18\", \"Mar\u00eda Rodr\u00edguez\"), (\"DOC005\", \"Propuesta Comercial\", \"Propuesta para desarrollo de software\", \"2023-01-19\", \"Luis Mart\u00edn\"), # Contenido duplicado (\"DOC006\", \"Contrato Modificado\", \"Este es un contrato para servicios de consultor\u00eda especializada\", \"2023-01-20\", \"Pedro S\u00e1nchez\") # Contenido similar ] df = spark.createDataFrame(data, [\"doc_id\", \"titulo\", \"contenido\", \"fecha\", \"autor\"]) print(\"=== Detecci\u00f3n por Funciones Hash ===\") # Hash del contenido completo df_with_hash = df.withColumn(\"hash_contenido\", md5(col(\"contenido\"))) \\ .withColumn(\"hash_titulo\", md5(col(\"titulo\"))) \\ .withColumn(\"hash_completo\", md5(concat_ws(\"|\", col(\"titulo\"), col(\"contenido\")))) # Detectar duplicados exactos por contenido duplicados_contenido = df_with_hash.groupBy(\"hash_contenido\") \\ .agg(count(\"*\").alias(\"count\"), collect_set(\"doc_id\").alias(\"doc_ids\"), first(\"contenido\").alias(\"contenido_ejemplo\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por contenido (hash MD5):\") duplicados_contenido.show(truncate=False) # Detectar duplicados por t\u00edtulo duplicados_titulo = df_with_hash.groupBy(\"hash_titulo\") \\ .agg(count(\"*\").alias(\"count\"), collect_set(\"doc_id\").alias(\"doc_ids\"), first(\"titulo\").alias(\"titulo_ejemplo\")) \\ .filter(col(\"count\") > 1) print(\"Duplicados por t\u00edtulo:\") duplicados_titulo.show(truncate=False) # Hash de m\u00faltiples algoritmos para comparaci\u00f3n df_multi_hash = df.withColumn(\"md5_hash\", md5(col(\"contenido\"))) \\ .withColumn(\"sha1_hash\", sha1(col(\"contenido\"))) \\ .withColumn(\"sha256_hash\", sha2(col(\"contenido\"), 256)) print(\"Hashes m\u00faltiples:\") df_multi_hash.select(\"doc_id\", \"titulo\", \"md5_hash\", \"sha1_hash\").show(truncate=False) # Detectar duplicados con diferentes criterios de hash print(\"An\u00e1lisis de duplicados por diferentes hash:\") df_multi_hash.groupBy(\"md5_hash\") \\ .agg(count(\"*\").alias(\"count_md5\"), collect_set(\"doc_id\").alias(\"docs_md5\")) \\ .filter(col(\"count_md5\") > 1) \\ .show(truncate=False) # Estrategia: Eliminar duplicados basado en hash df_unique_content = df_with_hash.dropDuplicates([\"hash_contenido\"]) print(\"Documentos \u00fanicos por contenido:\") df_unique_content.select(\"doc_id\", \"titulo\", \"autor\", \"fecha\").show() # Estrategia: Crear registro de duplicados para auditor\u00eda window_hash = Window.partitionBy(\"hash_contenido\").orderBy(\"fecha\") df_audit = df_with_hash.withColumn(\"es_original\", when(row_number().over(window_hash) == 1, True).otherwise(False)) \\ .withColumn(\"orden_duplicado\", row_number().over(window_hash)) print(\"Auditor\u00eda de duplicados:\") df_audit.select(\"doc_id\", \"titulo\", \"autor\", \"fecha\", \"es_original\", \"orden_duplicado\").show() Ejemplo SparkSQL : -- Crear tabla de documentos CREATE OR REPLACE TEMPORARY VIEW documentos AS SELECT * FROM VALUES ('DOC001', 'Contrato de Servicios', 'Este es un contrato para servicios de consultor\u00eda', '2023-01-15', 'Juan P\u00e9rez'), ('DOC002', 'Propuesta Comercial', 'Propuesta para desarrollo de software', '2023-01-16', 'Ana Garc\u00eda'), ('DOC003', 'Contrato de Servicios', 'Este es un contrato para servicios de consultor\u00eda', '2023-01-17', 'Carlos L\u00f3pez'), ('DOC004', 'Manual de Usuario', 'Gu\u00eda completa para el uso del sistema', '2023-01-18', 'Mar\u00eda Rodr\u00edguez'), ('DOC005', 'Propuesta Comercial', 'Propuesta para desarrollo de software', '2023-01-19', 'Luis Mart\u00edn'), ('DOC006', 'Contrato Modificado', 'Este es un contrato para servicios de consultor\u00eda especializada', '2023-01-20', 'Pedro S\u00e1nchez') AS t(doc_id, titulo, contenido, fecha, autor); -- Crear hashes para detecci\u00f3n de duplicados CREATE OR REPLACE TEMPORARY VIEW documentos_hash AS SELECT *, MD5(contenido) as hash_contenido, MD5(titulo) as hash_titulo, MD5(CONCAT(titulo, '|', contenido)) as hash_completo, SHA1(contenido) as sha1_contenido, SHA2(contenido, 256) as sha256_contenido FROM documentos; -- Detectar duplicados por contenido SELECT hash_contenido, COUNT(*) as num_duplicados, COLLECT_SET(doc_id) as documentos_duplicados, FIRST(contenido) as contenido_ejemplo FROM documentos_hash GROUP BY hash_contenido HAVING COUNT(*) > 1 ORDER BY num_duplicados DESC; -- Detectar duplicados por t\u00edtulo SELECT hash_titulo, COUNT(*) as num_duplicados, COLLECT_SET(doc_id) as documentos_duplicados, FIRST(titulo) as titulo_ejemplo FROM documentos_hash GROUP BY hash_titulo HAVING COUNT(*) > 1; -- An\u00e1lisis comparativo de diferentes algoritmos hash SELECT doc_id, titulo, autor, hash_contenido as md5_hash, sha1_contenido, SUBSTR(sha256_contenido, 1, 16) as sha256_preview FROM documentos_hash ORDER BY hash_contenido; -- Eliminar duplicados manteniendo el documento m\u00e1s antiguo WITH ranked_docs AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY hash_contenido ORDER BY fecha ASC) as rn FROM documentos_hash ) SELECT doc_id, titulo, contenido, fecha, autor FROM ranked_docs WHERE rn = 1 ORDER BY fecha; -- Crear reporte de auditor\u00eda de duplicados WITH duplicate_analysis AS ( SELECT doc_id, titulo, autor, fecha, hash_contenido, COUNT(*) OVER (PARTITION BY hash_contenido) as total_duplicados, ROW_NUMBER() OVER (PARTITION BY hash_contenido ORDER BY fecha) as orden_cronologico FROM documentos_hash ) SELECT doc_id, titulo, autor, fecha, CASE WHEN total_duplicados = 1 THEN '\u00daNICO' WHEN orden_cronologico = 1 THEN 'ORIGINAL' ELSE 'DUPLICADO' END as estado_documento, total_duplicados, orden_cronologico FROM duplicate_analysis ORDER BY hash_contenido, orden_cronologico; 4. Estrategias Avanzadas de Manejo de Duplicados Ejemplo PySpark : def comprehensive_duplicate_detection(df, primary_key_cols, unique_combination_cols, content_cols): \"\"\" Pipeline completo para detecci\u00f3n y manejo de duplicados \"\"\" from pyspark.sql.functions import * print(\"=== Pipeline Completo de Detecci\u00f3n de Duplicados ===\") # 1. Duplicados por clave primaria if primary_key_cols: pk_duplicates = df.groupBy(*primary_key_cols).count().filter(col(\"count\") > 1) pk_count = pk_duplicates.count() print(f\"Duplicados por clave primaria: {pk_count}\") # 2. Duplicados por combinaci\u00f3n \u00fanica if unique_combination_cols: combo_duplicates = df.groupBy(*unique_combination_cols).count().filter(col(\"count\") > 1) combo_count = combo_duplicates.count() print(f\"Duplicados por combinaci\u00f3n \u00fanica: {combo_count}\") # 3. Duplicados por contenido (hash) if content_cols: content_hash = concat_ws(\"|\", *[col(c) for c in content_cols]) df_hash = df.withColumn(\"content_hash\", md5(content_hash)) hash_duplicates = df_hash.groupBy(\"content_hash\").count().filter(col(\"count\") > 1) hash_count = hash_duplicates.count() print(f\"Duplicados por contenido: {hash_count}\") # 4. Crear dataset limpio con m\u00faltiples estrategias df_clean = df # Eliminar duplicados por clave primaria (mantener m\u00e1s reciente si hay fecha) if primary_key_cols and \"fecha\" in df.columns: window_pk = Window.partitionBy(*primary_key_cols).orderBy(desc(\"fecha\")) df_clean = df_clean.withColumn(\"rn_pk\", row_number().over(window_pk)) \\ .filter(col(\"rn_pk\") == 1).drop(\"rn_pk\") # Eliminar duplicados por combinaci\u00f3n \u00fanica if unique_combination_cols: df_clean = df_clean.dropDuplicates(unique_combination_cols) # Eliminar duplicados por contenido if content_cols: content_hash = concat_ws(\"|\", *[col(c) for c in content_cols]) df_clean = df_clean.withColumn(\"content_hash\", md5(content_hash)) \\ .dropDuplicates([\"content_hash\"]) \\ .drop(\"content_hash\") print(f\"Registros originales: {df.count()}\") print(f\"Registros despu\u00e9s de limpiar: {df_clean.count()}\") print(f\"Registros eliminados: {df.count() - df_clean.count()}\") return df_clean # Aplicar pipeline completo df_clean = comprehensive_duplicate_detection( df_original, primary_key_cols=[\"doc_id\"], unique_combination_cols=[\"titulo\", \"autor\"], content_cols=[\"contenido\"] ) Ejemplo SparkSQL : -- Reporte completo de an\u00e1lisis de duplicados WITH duplicate_summary AS ( -- Duplicados por clave primaria SELECT 'PRIMARY_KEY' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT doc_id, COUNT(*) as count_duplicados FROM documentos GROUP BY doc_id HAVING COUNT(*) > 1 ) UNION ALL -- Duplicados por combinaci\u00f3n \u00fanica SELECT 'UNIQUE_COMBINATION' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT titulo, autor, COUNT(*) as count_duplicados FROM documentos GROUP BY titulo, autor HAVING COUNT(*) > 1 ) UNION ALL -- Duplicados por contenido SELECT 'CONTENT_HASH' as tipo_duplicado, COUNT(*) as grupos_duplicados, SUM(count_duplicados - 1) as registros_duplicados FROM ( SELECT MD5(contenido) as hash_contenido, COUNT(*) as count_duplicados FROM documentos GROUP BY MD5(contenido) HAVING COUNT(*) > 1 ) ), total_records AS ( SELECT COUNT(*) as total_registros FROM documentos ) SELECT ds.*, ROUND((ds.registros_duplicados * 100.0) / tr.total_registros, 2) as porcentaje_duplicados FROM duplicate_summary ds CROSS JOIN total_records tr ORDER BY ds.registros_duplicados DESC; -- Pipeline de limpieza completo CREATE OR REPLACE TEMPORARY VIEW documentos_limpios AS WITH paso1_pk_clean AS ( -- Eliminar duplicados por clave primaria (mantener m\u00e1s reciente) SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY doc_id ORDER BY fecha DESC) as rn FROM documentos ) WHERE rn = 1 ), paso2_combo_clean AS ( -- Eliminar duplicados por combinaci\u00f3n \u00fanica SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY titulo, autor ORDER BY fecha ASC) as rn FROM paso1_pk_clean ) WHERE rn = 1 ), paso3_content_clean AS ( -- Eliminar duplicados por contenido SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY MD5(contenido) ORDER BY fecha ASC) as rn FROM paso2_combo_clean ) WHERE rn = 1 ) SELECT doc_id, titulo, contenido, fecha, autor FROM paso3_content_clean; -- Verificar resultado final SELECT 'ORIGINAL' as dataset, COUNT(*) as total_registros FROM documentos UNION ALL SELECT 'LIMPIO' as dataset, COUNT(*) as total_registros FROM documentos_limpios; 5. Mejores Pr\u00e1cticas para Manejo de Duplicados Estrategia de Detecci\u00f3n Progresiva : def progressive_duplicate_detection(df): \"\"\"Detecci\u00f3n progresiva de duplicados con diferentes niveles de strictness\"\"\" # Nivel 1: Duplicados exactos (m\u00e1s estricto) exact_duplicates = df.groupBy(*df.columns).count().filter(col(\"count\") > 1) # Nivel 2: Duplicados por campos clave key_duplicates = df.groupBy(\"id\", \"email\").count().filter(col(\"count\") > 1) # Nivel 3: Duplicados por similitud (menos estricto) content_duplicates = df.withColumn(\"content_hash\", md5(col(\"content\"))) \\ .groupBy(\"content_hash\").count().filter(col(\"count\") > 1) return { \"exact\": exact_duplicates.count(), \"key\": key_duplicates.count(), \"content\": content_duplicates.count() } Audit Trail de Duplicados : def create_duplicate_audit_trail(df): \"\"\"Crear rastro de auditor\u00eda para duplicados eliminados\"\"\" # Identificar duplicados antes de eliminar duplicates_info = df.groupBy(\"id\").agg( count(\"*\").alias(\"count_duplicates\"), collect_list(struct(*df.columns)).alias(\"all_versions\") ).filter(col(\"count_duplicates\") > 1) # Guardar informaci\u00f3n de auditor\u00eda duplicates_info.write.mode(\"overwrite\").parquet(\"audit/duplicates_removed/\") return duplicates_info Estas estrategias permiten un manejo robusto y trazable de duplicados, garantizando la integridad de los datos mientras se mantiene un registro completo de las transformaciones realizadas.","title":"Detecci\u00f3n y manejo de duplicados"},{"location":"tema34/#345-logging-de-errores-y-manejo-de-excepciones-durante-la-transformacion","text":"El registro y manejo de errores durante la transformaci\u00f3n de datos es fundamental para garantizar la estabilidad y confiabilidad de un pipeline. Sin un sistema de logging adecuado, los fallos pueden pasar desapercibidos, generando inconsistencias en los datos procesados y afectando decisiones estrat\u00e9gicas. Un enfoque bien estructurado permite detectar y solucionar problemas r\u00e1pidamente, minimizando el impacto en los sistemas y asegurando una trazabilidad clara para an\u00e1lisis posteriores. Adem\u00e1s, integrar estrategias de recuperaci\u00f3n evita interrupciones innecesarias y mejora la resiliencia del sistema ante fallos inesperados.","title":"3.4.5 Logging de errores y manejo de excepciones durante la transformaci\u00f3n"},{"location":"tema34/#logging-estructurado-de-errores","text":"Un sistema de logging estructurado facilita la identificaci\u00f3n y resoluci\u00f3n de problemas al proporcionar informaci\u00f3n detallada sobre el contexto del fallo. En lugar de registrar mensajes gen\u00e9ricos, se recomienda incluir detalles como el identificador de la transformaci\u00f3n, el tipo de error, el origen de los datos y los valores espec\u00edficos que causaron la falla. Esto permite a los ingenieros de datos rastrear con precisi\u00f3n el origen del problema y aplicar correcciones eficientes. Por ejemplo, si una conversi\u00f3n de tipos en PySpark falla debido a un valor inesperado en una columna, el sistema de logging deber\u00eda registrar el nombre de la columna, el valor conflictivo y la operaci\u00f3n en la que ocurri\u00f3 el fallo. Este enfoque es crucial para evitar diagn\u00f3sticos err\u00f3neos y reducir el tiempo de respuesta ante incidentes. Herramientas como log4j en Spark o sistemas centralizados como Elastic Stack pueden almacenar logs estructurados con niveles de severidad y trazabilidad para an\u00e1lisis en tiempo real.","title":"Logging estructurado de errores"},{"location":"tema34/#estrategias-de-recuperacion-y-manejo-de-excepciones","text":"En lugar de detener todo el pipeline por un error inesperado, es recomendable implementar t\u00e9cnicas de manejo de excepciones que permitan continuar con el procesamiento sin comprometer la calidad de los datos. Una estrategia com\u00fan es el uso de bloques try-catch , que capturan errores espec\u00edficos y permiten definir acciones correctivas. Otra t\u00e9cnica es la implementaci\u00f3n de rutas alternas , donde los registros problem\u00e1ticos se redirigen a un \u00e1rea de revisi\u00f3n sin afectar el flujo principal del procesamiento. Esto es \u00fatil en sistemas que requieren alta disponibilidad, como plataformas de an\u00e1lisis en tiempo real. Adem\u00e1s, marcar registros inv\u00e1lidos con etiquetas espec\u00edficas en lugar de eliminarlos permite que los analistas revisen y corrijan problemas sin perder informaci\u00f3n valiosa. Por ejemplo, en un pipeline de Spark, los datos con valores inconsistentes pueden enviarse a una tabla de auditor\u00eda en Delta Lake, donde se almacenan con informaci\u00f3n adicional sobre el error para su posterior revisi\u00f3n. Esto no solo mejora la calidad del procesamiento, sino que tambi\u00e9n facilita la correcci\u00f3n proactiva de datos y el aprendizaje sobre patrones de fallos recurrentes.","title":"Estrategias de recuperaci\u00f3n y manejo de excepciones"},{"location":"tema34/#tarea","text":"Implementa un esquema expl\u00edcito en PySpark para una tabla de pedidos. Agrega validaciones para tipos y valores. Simula la evoluci\u00f3n de esquema en una tabla Delta agregando columnas nuevas, eliminando otras y cambiando tipos. Eval\u00faa los efectos. Dise\u00f1a un proceso de control de calidad que incluya detecci\u00f3n de duplicados y validaci\u00f3n de rangos para un conjunto de datos de clientes. Implementa un log estructurado de errores en un flujo Spark. Incluye tipo de error, registro afectado y timestamp. Integra un esquema en Avro a un Schema Registry local o en la nube. Genera dos versiones del esquema y simula un caso de incompatibilidad.","title":"Tarea"},{"location":"tema35/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.5. Monitorizaci\u00f3n y Troubleshooting de Pipelines Objetivo : Detectar, analizar y resolver errores operativos en pipelines ETL mediante herramientas de observabilidad, m\u00e9tricas clave, interfaces gr\u00e1ficas y estrategias de recuperaci\u00f3n automatizada, asegurando la confiabilidad y eficiencia del procesamiento de datos. Introducci\u00f3n : La creaci\u00f3n de pipelines ETL escalables y funcionales no es suficiente si no se cuenta con mecanismos adecuados para su supervisi\u00f3n y mantenimiento. La monitorizaci\u00f3n permite identificar cuellos de botella, errores y problemas de rendimiento, mientras que el troubleshooting proporciona los medios para analizarlos y resolverlos. Este tema aborda herramientas como Spark UI y Airflow UI, el uso de logs y m\u00e9tricas, y estrategias pr\u00e1cticas para prevenir y remediar fallos comunes. Desarrollo : La observabilidad de pipelines ETL es un componente cr\u00edtico en cualquier arquitectura de datos moderna. Los ingenieros de datos deben dominar no solo la construcci\u00f3n de flujos, sino tambi\u00e9n su operaci\u00f3n continua. Este tema desarrolla las competencias necesarias para leer y configurar logs, interpretar interfaces de usuario de ejecuci\u00f3n, utilizar m\u00e9tricas relevantes, automatizar alertas y aplicar soluciones efectivas a problemas frecuentes que afectan el desempe\u00f1o de los pipelines. 3.5.1 Configuraci\u00f3n y lectura de logs en Spark y Airflow La inspecci\u00f3n de logs es crucial para entender el comportamiento interno de tus jobs de Spark y DAGs de Airflow . Al monitorear los logs, puedes identificar r\u00e1pidamente errores, cuellos de botella de rendimiento y problemas de configuraci\u00f3n que podr\u00edan afectar la eficiencia de tus pipelines de datos. Esto te permite mantener la estabilidad y el rendimiento de tus sistemas de Big Data. Configuraci\u00f3n de logs en Apache Spark y Airflow Tanto Spark como Airflow ofrecen opciones robustas para personalizar el nivel de logging, lo que te permite ajustar la verbosidad de los logs seg\u00fan tus necesidades espec\u00edficas (por ejemplo, INFO , DEBUG , WARN , ERROR , FATAL ). Una configuraci\u00f3n adecuada asegura que obtengas la trazabilidad necesaria sin sobrecargar tus sistemas de almacenamiento de logs. Configuraci\u00f3n de logs en Apache Spark : El archivo log4j.properties es fundamental para controlar el comportamiento del logging en Spark. Puedes ubicarlo en el directorio conf de tu instalaci\u00f3n de Spark. Dentro de este archivo, puedes definir los niveles de logging para diferentes paquetes de Spark (por ejemplo, org.apache.spark , org.eclipse.jetty ) y configurar los appenders para especificar d\u00f3nde se escribir\u00e1n los logs (por ejemplo, consola, archivos, syslog). Para reducir la verbosidad de los logs de Spark a WARN (solo advertencias y errores) y ver los logs en la consola, a\u00f1adir\u00edas las siguientes l\u00edneas a log4j.properties : log4j.rootCategory=WARN, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n Para habilitar el logging a nivel DEBUG para un paquete espec\u00edfico, por ejemplo, para la configuraci\u00f3n de Spark SQL: log4j.logger.org.apache.spark.sql.execution.SparkSqlParser=DEBUG Configuraci\u00f3n de logs en Apache Airflow : El archivo airflow.cfg es el coraz\u00f3n de la configuraci\u00f3n de Airflow y te permite definir c\u00f3mo y d\u00f3nde se almacenan los logs de tus DAGs y tareas. Dentro de la secci\u00f3n [logging] , puedes especificar el remote_logging para enviar logs a servicios de almacenamiento en la nube, el log_level global y la ruta base_log_folder para los logs locales. Para un control m\u00e1s granular, puedes definir una clase Python personalizada logging_config_class que herede de airflow.utils.log.logging_mixin.LoggingMixin y especifique reglas de logging avanzadas. Esto es especialmente \u00fatil para integrar sistemas de logging personalizados o para configurar loggers de forma din\u00e1mica. En airflow.cfg , puedes establecer el nivel de log global y la ruta local: [logging] log_level = INFO base_log_folder = /opt/airflow/logs Para centralizar los logs en un servicio como Amazon CloudWatch, puedes configurar el remote_logging y especificar la clase de logging: [logging] remote_logging = True remote_base_log_folder = s3://your-airflow-logs-bucket/ remote_log_conn_id = aws_default log_level = INFO logging_config_class = my_project.config.cloud_logging.CloudLoggingConfig Donde CloudLoggingConfig ser\u00eda una clase Python personalizada que manejar\u00eda la integraci\u00f3n con CloudWatch o un servicio similar. Centralizaci\u00f3n de logs en entornos en la nube : En entornos de producci\u00f3n basados en la nube, es una pr\u00e1ctica recomendada centralizar los logs para facilitar su an\u00e1lisis, monitoreo y archivo. Amazon CloudWatch / S3 : Para AWS, puedes enviar logs de Spark directamente a CloudWatch Logs o almacenar los logs de Airflow en S3, lo que permite un acceso unificado y el uso de servicios como CloudWatch Logs Insights para consultas. Google Cloud Logging / Google Cloud Storage : En GCP, los logs pueden ser enviados a Cloud Logging, que ofrece capacidades de b\u00fasqueda y an\u00e1lisis potentes, y los logs a largo plazo pueden archivarse en Google Cloud Storage. Elastic Stack (ELK) : Para una soluci\u00f3n autoalojada o en la nube, Elasticsearch, Logstash y Kibana proporcionan una plataforma robusta para la ingesta, almacenamiento, indexaci\u00f3n y visualizaci\u00f3n de logs de Spark y Airflow. Lectura e interpretaci\u00f3n de logs Interpretar logs de Spark y Airflow requiere familiaridad con los patrones de mensajes y la estructura jer\u00e1rquica de la ejecuci\u00f3n de trabajos y tareas. Reconocer los errores comunes y sus causas subyacentes te ayudar\u00e1 a diagnosticar problemas de manera m\u00e1s eficiente. Identificaci\u00f3n de errores comunes en Spark : Un error Stage Failed indica que una etapa (Stage) de tu job de Spark no pudo completarse. Esto puede ser debido a fallos en las tareas, problemas de memoria, errores de datos o configuraci\u00f3n incorrecta. Si ves un mensaje como Stage 2 failed in 20.0 s , debes buscar en los logs de las tareas (Tasks) dentro de esa etapa para identificar el error espec\u00edfico, como java.lang.OutOfMemoryError o org.apache.spark.SparkException: Task failed while writing rows . El error* TaskKilled ocurre cuando una tarea es terminada prematuramente. Las razones pueden ser un tiempo de espera excedido ( timeout ), errores de memoria en el executor, o si el propio executor fue terminado. TaskKilled (killed intentionally) o TaskKilled (killed due to memory limit exceeding) indican que la tarea fue eliminada. Debes revisar los logs del executor para entender la causa ra\u00edz, como la configuraci\u00f3n de spark.executor.memory . El error Executor Lost significa que un nodo de trabajo que aloja a un executor de Spark se ha perdido o ha fallado. Esto puede ser causado por problemas de red, fallos de hardware, problemas de memoria en el nodo o configuraciones incorrectas de recursos. ERROR Driver: Lost executor 1 on <hostname> sugiere que el executor se desconect\u00f3. Necesitas investigar los logs del nodo donde se ejecutaba el executor para encontrar la causa, como falta de memoria RAM o problemas de conectividad de red. Identificaci\u00f3n de errores comunes en Airflow : Logs de operadores (tasks) con fallos de conexi\u00f3n : Muchos DAGs interact\u00faan con bases de datos, APIs o sistemas de archivos remotos. Los errores de conexi\u00f3n son comunes y suelen aparecer como excepciones de red o autenticaci\u00f3n. Un log de una tarea de PostgresOperator mostrando psycopg2.OperationalError: could not connect to server: Connection refused indica un problema con las credenciales, la IP o el puerto de la base de datos. Retries fallidos ( retries exhausted ) : Airflow permite configurar reintentos para las tareas. Si una tarea agota todos sus reintentos, su estado final ser\u00e1 failed . Ver Task \"my_task\" failed after 3 retries en los logs del scheduler o worker significa que la tarea no pudo completarse con \u00e9xito despu\u00e9s de m\u00faltiples intentos. Debes analizar los logs de cada intento para identificar el error recurrente. Fallos por timeout : Si una tarea excede el tiempo de ejecuci\u00f3n configurado ( execution_timeout ), Airflow la terminar\u00e1. [2025-06-06 19:13:51,123] {taskinstance.py:1150} ERROR - Task timed out: my_long_running_task indica que la tarea tard\u00f3 m\u00e1s de lo permitido. Es posible que necesites optimizar la tarea o aumentar el execution_timeout si el tiempo de ejecuci\u00f3n es aceptable. B\u00fasqueda de trazas que indiquen problemas con la configuraci\u00f3n del cluster o variables mal definidas : Errores de classpath : Problemas al cargar librer\u00edas o dependencias pueden causar que Spark jobs fallen al inicio. java.lang.ClassNotFoundException: com.example.MyCustomClass en los logs del driver o executor indica que una clase necesaria no est\u00e1 disponible en el classpath del cluster. Variables de entorno incorrectas : Una variable de entorno mal configurada puede afectar tanto a Spark como a Airflow. Un DAG de Airflow que depende de una variable de entorno DB_PASSWORD y falla con un AuthenticationFailed o similar, podr\u00eda indicar que la variable no est\u00e1 definida o es incorrecta. Configuraci\u00f3n de recursos ( memory , cores ) : Valores incorrectos en la configuraci\u00f3n de memoria o CPU para Spark executors pueden llevar a errores de OutOfMemoryError o a un rendimiento deficiente. Mensajes como Container killed by YARN for exceeding memory limits sugieren que los recursos asignados no son suficientes para la carga de trabajo, y necesitas ajustar spark.executor.memory o spark.driver.memory . La clave para una interpretaci\u00f3n efectiva de los logs es la pr\u00e1ctica y la familiaridad con tus propios pipelines y la infraestructura subyacente. Herramientas de visualizaci\u00f3n y agregaci\u00f3n de logs pueden ser de gran ayuda para identificar patrones y correlacionar eventos. 3.5.2 Uso de interfaces gr\u00e1ficas (Airflow UI, Spark UI) para diagn\u00f3stico Las interfaces visuales son herramientas cruciales para el monitoreo, diagn\u00f3stico y optimizaci\u00f3n de flujos de trabajo de Big Data. Proporcionan una vista estructurada y f\u00e1cilmente navegable del estado de ejecuci\u00f3n, los tiempos de procesamiento, las dependencias entre componentes y la identificaci\u00f3n de posibles errores o cuellos de botella. Estas interfaces transforman datos complejos de logs y m\u00e9tricas en representaciones gr\u00e1ficas intuitivas, lo que facilita la toma de decisiones y la resoluci\u00f3n de problemas. Airflow UI La interfaz de usuario de Apache Airflow es una herramienta esencial para la orquestaci\u00f3n y monitoreo de flujos de trabajo (DAGs). Permite a los desarrolladores y operadores una visibilidad completa sobre el ciclo de vida de las tareas, desde su programaci\u00f3n hasta su finalizaci\u00f3n o fallo. Utilizar el Tree View y Graph View para entender dependencias entre tareas. Tree View : Esta vista ofrece una perspectiva cronol\u00f3gica del estado de cada instancia de tarea a lo largo del tiempo. Es ideal para identificar patrones de fallos recurrentes o para ver el historial de ejecuciones de un DAG. Por ejemplo, puedes observar si una tarea espec\u00edfica siempre falla en la misma hora del d\u00eda o despu\u00e9s de ciertas condiciones, lo que podr\u00eda indicar un problema de recursos o una dependencia externa. Graph View : Proporciona una representaci\u00f3n visual de las relaciones de dependencia entre las tareas dentro de un DAG. Es fundamental para entender el flujo l\u00f3gico del trabajo. Si una tarea est\u00e1 tardando m\u00e1s de lo esperado, puedes ver r\u00e1pidamente qu\u00e9 tareas est\u00e1n esperando su finalizaci\u00f3n y c\u00f3mo esto impacta el flujo completo. Por ejemplo, si la tarea \"transformar_datos\" falla, el Graph View mostrar\u00e1 claramente que las tareas \"cargar_a_dw\" y \"generar_reporte\" no se ejecutar\u00e1n hasta que \"transformar_datos\" se complete exitosamente. Revisar logs por tarea desde la interfaz y aplicar retries manuales. Cada instancia de tarea en Airflow genera logs detallados que son accesibles directamente desde la UI. Estos logs son vitales para depurar errores, ya que contienen la salida est\u00e1ndar y los mensajes de error generados por la tarea. Por ejemplo, si una tarea de carga de datos falla, los logs podr\u00edan indicar un problema de conectividad con la base de datos o un error en el formato de los datos. Cuando una tarea falla debido a un problema transitorio (por ejemplo, un problema de red moment\u00e1neo), Airflow permite realizar retries manuales directamente desde la interfaz. Esto evita tener que esperar a la pr\u00f3xima ejecuci\u00f3n programada o modificar el DAG, agilizando la recuperaci\u00f3n de flujos de trabajo. Verificar el SLA Misses y la duraci\u00f3n de ejecuci\u00f3n por instancia. SLA Misses (Service Level Agreement Misses) : Airflow permite definir SLAs para las tareas, lo que significa que esperas que una tarea se complete dentro de un tiempo determinado. Si una tarea excede este tiempo, Airflow lo marca como un \"SLA Miss\". La UI te permite ver un resumen de estas infracciones, lo que es crucial para identificar tareas que est\u00e1n afectando el rendimiento general o que necesitan optimizaci\u00f3n urgente. Por ejemplo, si la tarea \"procesar_ventas_diarias\" tiene un SLA de 30 minutos y consistentemente tarda 45 minutos, ver\u00e1s un SLA Miss, indicando que el proceso est\u00e1 tardando demasiado. Duraci\u00f3n de ejecuci\u00f3n por instancia : La Airflow UI muestra el tiempo que cada instancia de una tarea tard\u00f3 en completarse. Al analizar estas duraciones a lo largo de varias ejecuciones, puedes identificar tendencias de rendimiento. Si una tarea que sol\u00eda tardar 5 minutos ahora tarda 20, podr\u00eda indicar un aumento en el volumen de datos, una degradaci\u00f3n del rendimiento del sistema subyacente o un problema en el c\u00f3digo de la tarea. Spark UI La Spark UI es la herramienta de monitoreo y diagn\u00f3stico por excelencia para aplicaciones Apache Spark. Proporciona una visi\u00f3n granular del progreso de los jobs, las etapas, las tareas y la utilizaci\u00f3n de recursos en el cl\u00faster, lo que es indispensable para la optimizaci\u00f3n del rendimiento y la depuraci\u00f3n de problemas en el procesamiento de Big Data. Analizar el DAG f\u00edsico y l\u00f3gico del job para identificar etapas lentas. DAG L\u00f3gico : La Spark UI muestra el plan l\u00f3gico de ejecuci\u00f3n de tu job, representando las transformaciones de RDDs o DataFrames. Esto te ayuda a entender c\u00f3mo Spark interpreta tu c\u00f3digo. DAG F\u00edsico : Esta es una representaci\u00f3n de las etapas y tareas reales que Spark generar\u00e1 para ejecutar el plan l\u00f3gico. Cada etapa se corresponde con una o m\u00e1s operaciones de \"shuffle\" o con la lectura de datos. La Spark UI permite visualizar el tiempo de ejecuci\u00f3n de cada etapa. Si identificas una etapa que consume una cantidad desproporcionada de tiempo (por ejemplo, una etapa de join o groupByKey ), esto indica un posible cuello de botella. Por ejemplo, una etapa de shuffle excepcionalmente lenta puede indicar un problema de particionamiento de datos o una red saturada. Identificar tareas que consumen m\u00e1s recursos o que est\u00e1n desbalanceadas. Dentro de cada etapa, la Spark UI desglosa el rendimiento a nivel de tarea . Puedes ver m\u00e9tricas como el tiempo de ejecuci\u00f3n de cada tarea, la cantidad de datos le\u00eddos/escritos, y la utilizaci\u00f3n de memoria y CPU. Tareas desbalanceadas (Skew) : Si observas que la mayor\u00eda de las tareas en una etapa se completan r\u00e1pidamente, pero unas pocas tardan significativamente m\u00e1s (por ejemplo, una tarea tarda 5 minutos mientras que el resto tarda 10 segundos), esto es un claro indicio de sesgo de datos (data skew) . Esto significa que algunos ejecutores est\u00e1n procesando una cantidad desproporcionadamente grande de datos debido a valores clave no distribuidos uniformemente. La Spark UI te permite identificar estas tareas \"lentas\" para luego aplicar t\u00e9cnicas de optimizaci\u00f3n como salting o re-particionamiento. Revisar el Event Timeline para ver eventos cr\u00edticos de ejecuci\u00f3n. El Event Timeline de la Spark UI es una representaci\u00f3n visual cronol\u00f3gica de los eventos clave que ocurren durante la ejecuci\u00f3n de un job Spark. Esto incluye el inicio y fin de ejecutores, la asignaci\u00f3n y liberaci\u00f3n de recursos, el inicio y fin de etapas, y los eventos de shuffle. Analizar esta l\u00ednea de tiempo es crucial para detectar problemas a nivel de cl\u00faster. Por ejemplo, si observas un gran n\u00famero de eventos de \"Executor Lost\" (ejecutor perdido), podr\u00eda indicar problemas de memoria en los nodos del cl\u00faster o inestabilidad de la red. Si hay largos periodos de inactividad entre etapas, podr\u00eda sugerir problemas con la asignaci\u00f3n de recursos o con la disponibilidad de datos. Esta vista te ayuda a entender c\u00f3mo los recursos del cl\u00faster est\u00e1n siendo utilizados a lo largo del tiempo y a identificar periodos de inactividad o contenci\u00f3n. 3.5.3 M\u00e9tricas clave: tiempo de ejecuci\u00f3n, throughput, retries El seguimiento de m\u00e9tricas clave es esencial para establecer la salud operativa, la eficiencia y la confiabilidad de los pipelines de datos. Estas m\u00e9tricas nos permiten identificar cuellos de botella, predecir fallos y optimizar el uso de recursos. M\u00e9tricas de rendimiento Las m\u00e9tricas de rendimiento nos indican qu\u00e9 tan r\u00e1pido y eficientemente se est\u00e1n ejecutando nuestros pipelines. Tiempo de ejecuci\u00f3n total del DAG/job y por tarea individual : Se refiere al tiempo que tarda un flujo completo de trabajo (DAG o job) en finalizar, as\u00ed como el tiempo que consume cada una de las tareas que lo componen. Monitorear ambos niveles nos permite identificar tareas que est\u00e1n tardando m\u00e1s de lo esperado y que podr\u00edan ser optimizadas. Un pipeline de ETL que carga datos de un sistema transaccional a un data warehouse. Si el tiempo total de ejecuci\u00f3n aumenta de 30 minutos a 2 horas, necesitamos investigar. Al revisar los tiempos individuales de las tareas, podr\u00edamos descubrir que una tarea de JOIN en Spark ha pasado de 5 minutos a 1 hora. En Spark, el Spark UI proporciona detalles sobre el tiempo de ejecuci\u00f3n de cada etapa y tarea. A nivel de c\u00f3digo, puedes registrar el tiempo: from datetime import datetime start_time_job = datetime.now() # ... c\u00f3digo de tu job Spark ... df_transformed = spark.sql(\"SELECT ... FROM large_table JOIN another_table ON ...\") df_transformed.write.parquet(\"s3://data-lake/processed/data.parquet\") # ... end_time_job = datetime.now() print(f\"Tiempo total de ejecuci\u00f3n del job: {end_time_job - start_time_job}\") # Para una tarea espec\u00edfica (ej. una transformaci\u00f3n costosa) start_time_task_join = datetime.now() df_joined = large_df.join(small_df, \"key\") end_time_task_join = datetime.now() print(f\"Tiempo de ejecuci\u00f3n de la tarea JOIN: {end_time_task_join - start_time_task_join}\") Throughput (datos procesados por unidad de tiempo) : Representa la cantidad de datos que el pipeline es capaz de procesar en un determinado per\u00edodo. Es una m\u00e9trica crucial para entender la capacidad y la escalabilidad del sistema. Se puede medir en filas por segundo, megabytes por segundo, etc. Un pipeline de ingesta de logs en tiempo real que procesa 10 GB de datos por hora. Si este throughput disminuye a 2 GB por hora, indica un problema en la ingesta o en el procesamiento. Para una carga de datos: # Suponiendo que 'rows_processed' es el n\u00famero total de filas procesadas # y 'execution_time_seconds' es el tiempo total de ejecuci\u00f3n en segundos rows_processed = 1_000_000 execution_time_seconds = 60 # 1 minuto throughput_rows_per_second = rows_processed / execution_time_seconds print(f\"Throughput: {throughput_rows_per_second:.2f} filas/segundo\") # Para datos en bytes (ej. leyendo un archivo grande) file_size_bytes = 10 * 1024 * 1024 # 10 MB throughput_mb_per_second = (file_size_bytes / (1024 * 1024)) / execution_time_seconds print(f\"Throughput: {throughput_mb_per_second:.2f} MB/segundo\") Latencia entre tareas, importante en flujos encadenados : Se refiere al tiempo de espera entre la finalizaci\u00f3n de una tarea y el inicio de la siguiente. En pipelines donde las tareas dependen secuencialmente unas de otras, una alta latencia puede acumularse y prolongar significativamente el tiempo total de ejecuci\u00f3n del DAG. Esto es especialmente cr\u00edtico en arquitecturas de microservicios o pipelines de streaming. En un pipeline de Airflow, si la Task A finaliza a las 10:00 AM y la Task B (que depende de Task A ) comienza a las 10:15 AM, la latencia entre tareas es de 15 minutos. Un monitoreo de esta m\u00e9trica puede revelar que los recursos del worker o el scheduler est\u00e1n saturados. Airflow registra los tiempos de inicio y fin de cada tarea. La latencia se calcular\u00eda como: start_time(Task B) - end_time(Task A) Puedes obtener estos datos de la base de datos de metadatos de Airflow o a trav\u00e9s de su API. Herramientas de monitoreo como Prometheus y Grafana se integrar\u00edan para visualizar estas brechas. M\u00e9tricas de confiabilidad y robustez Estas m\u00e9tricas nos informan sobre la estabilidad y la capacidad de nuestro sistema para manejar errores y recuperarse de ellos. N\u00famero de reintentos (retries) y su tasa de \u00e9xito : Indica cu\u00e1ntas veces una tarea tuvo que ser reejecutada debido a un fallo transitorio. Un alto n\u00famero de reintentos puede indicar problemas subyacentes (ej. inestabilidad de la red, picos de carga en sistemas externos) que, aunque se recuperen, afectan la eficiencia y la latencia. La tasa de \u00e9xito de los reintentos (cu\u00e1ntos reintentos finalmente lograron que la tarea se completara con \u00e9xito) es crucial para entender la eficacia de nuestra estrategia de reintentos. Si una tarea que escribe a una base de datos externa falla intermitentemente y necesita 3 reintentos para completarse en el 80% de los casos, esto es aceptable. Sin embargo, si en el 50% de los casos agota todos los reintentos y falla definitivamente, indica un problema grave. from airflow import DAG from airflow.operators.dummy import DummyOperator from datetime import datetime, timedelta with DAG( dag_id='example_retries', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 3, # N\u00famero de reintentos 'retry_delay': timedelta(minutes=5), # Retraso entre reintentos } ) as dag: start_task = DummyOperator(task_id='start') failing_task = DummyOperator(task_id='potentially_failing_task') # Esta tarea simular\u00eda un fallo end_task = DummyOperator(task_id='end') start_task >> failing_task >> end_task Las plataformas como Airflow exponen el n\u00famero de reintentos en su interfaz de usuario y en sus logs. Errores por tipo (conexi\u00f3n, permisos, timeouts) : Clasificar los errores por su naturaleza es fundamental para una depuraci\u00f3n efectiva y para identificar patrones. Errores de conexi\u00f3n : Indican problemas de red, bases de datos no disponibles o servicios externos ca\u00eddos. Connection refused al intentar conectar con una API, o Network unreachable al acceder a un bucket S3. Errores de permisos : Apuntan a problemas de autorizaci\u00f3n o configuraci\u00f3n de seguridad. Access Denied al intentar leer de un bucket de S3 sin los permisos adecuados, o Permission denied al escribir en un directorio. Timeouts : Sugieren que una operaci\u00f3n est\u00e1 tardando demasiado en responder, lo que podr\u00eda deberse a cuellos de botella en el sistema llamado, a una alta latencia de red o a una configuraci\u00f3n de timeout demasiado baja. Una consulta a una base de datos que excede el tiempo l\u00edmite de espera configurado ( Query timeout expired ). Estos errores se capturan en los logs de las aplicaciones (Spark, Python scripts, etc.). Utilizar sistemas de log management (ELK Stack, Splunk, Datadog) con an\u00e1lisis de patrones y alertas es crucial. import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) try: # Simulaci\u00f3n de un error de conexi\u00f3n # requests.get(\"http://nonexistent-service.com\", timeout=1) raise ConnectionError(\"Fallo de conexi\u00f3n al servicio externo.\") except ConnectionError as e: logger.error(f\"Error de conexi\u00f3n: {e}\") except PermissionError as e: logger.error(f\"Error de permisos: {e}\") except Exception as e: logger.error(f\"Error inesperado: {type(e).__name__} - {e}\") Disponibilidad y uptime del scheduler (Airflow) : En sistemas orquestadores como Airflow, la disponibilidad del scheduler es cr\u00edtica, ya que es el componente encargado de determinar cu\u00e1ndo deben ejecutarse los DAGs y sus tareas. Un scheduler ca\u00eddo o inestable detendr\u00e1 por completo la ejecuci\u00f3n de todos los pipelines. El uptime es el porcentaje de tiempo que el scheduler ha estado operativo. Si el scheduler de Airflow estuvo ca\u00eddo durante 2 horas en un d\u00eda, su disponibilidad ese d\u00eda fue de (22/24) * 100 = 91.67% , lo cual es inaceptable para la mayor\u00eda de entornos de producci\u00f3n. Monitoreo a nivel de sistema operativo (CPU, memoria, procesos) del servidor donde corre el scheduler. Herramientas de monitoreo de infraestructura (Prometheus, Grafana, Datadog) pueden rastrear el estado del proceso del scheduler y generar alertas si se detiene o si sus m\u00e9tricas de salud (como la cola de tareas pendientes) son inusuales. Airflow tambi\u00e9n tiene m\u00e9tricas internas que pueden exportarse a sistemas de monitoreo. 3.5.4 Alertas, notificaciones y automatizaci\u00f3n de recuperaci\u00f3n El monitoreo proactivo es fundamental en cualquier pipeline de datos robusto. Permite no solo detectar problemas de manera temprana, sino tambi\u00e9n reaccionar ante errores de forma automatizada, minimizando la intervenci\u00f3n humana directa y reduciendo el tiempo de inactividad. Esto es crucial para mantener la integridad y disponibilidad de los datos. Configuraci\u00f3n de alertas y notificaciones La configuraci\u00f3n de alertas y notificaciones es el primer paso para un monitoreo proactivo eficaz. Permite que los equipos sean informados de inmediato sobre cualquier anomal\u00eda o fallo en los pipelines de datos. Integraci\u00f3n de notificaciones desde Airflow : Airflow, siendo una orquestador de ETL ampliamente utilizado, ofrece operadores predefinidos para enviar notificaciones a diferentes canales. Notificaciones v\u00eda correo electr\u00f3nico (EmailOperator) : Es una forma est\u00e1ndar y efectiva de alertar a los equipos. Se puede configurar para enviar correos electr\u00f3nicos al fracaso o \u00e9xito de una tarea o DAG. from airflow.operators.email import EmailOperator from airflow.utils.email import send_email # ... dentro de la definici\u00f3n de un DAG o tarea send_email_on_failure = EmailOperator( task_id='email_on_failure', to='equipo_data@example.com', subject='Alerta Airflow: Tarea {{ ti.task_id }} fall\u00f3 en DAG {{ ti.dag_id }}', html_content=\"\"\" <h3>Error en Pipeline de Datos</h3> <p>La tarea <b>{{ ti.task_id }}</b> del DAG <b>{{ ti.dag_id }}</b> ha fallado.</p> <p>Revisar logs para m\u00e1s detalles: {{ ti.log_url }}</p> \"\"\", dag=my_dag, # Asociar al DAG si es una tarea separada, o parte de la configuraci\u00f3n de default_args trigger_rule='one_failed' # Se dispara solo si alguna tarea en el DAG falla ) Alternativamente, se pueden configurar default_args para enviar correos electr\u00f3nicos en caso de fallo: default_args = { 'owner': 'airflow', 'start_date': days_ago(1), 'email': ['equipo_data@example.com'], 'email_on_failure': True, 'email_on_retry': False, 'email_on_success': False, } Notificaciones v\u00eda Slack (SlackWebhookOperator) : Slack es una herramienta de comunicaci\u00f3n muy com\u00fan en equipos de desarrollo y operaciones. Airflow permite enviar mensajes directamente a canales de Slack, lo que facilita una comunicaci\u00f3n m\u00e1s inmediata y contextualizada. from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator # Configurar la conexi\u00f3n de Slack en Airflow (Admin -> Connections) # conn_id: slack_connection, host: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX send_slack_notification = SlackWebhookOperator( task_id='slack_notification', slack_webhook_conn_id='slack_connection', # ID de la conexi\u00f3n configurada en Airflow message=\"\"\" :red_circle: *Alerta en Pipeline de Datos*: La tarea `{{ ti.task_id }}` del DAG `{{ ti.dag_id }}` ha fallado. Por favor, revisa los logs en: {{ ti.log_url }} \"\"\", channel='#alerts_data_pipeline', # Canal de Slack al que se enviar\u00e1 el mensaje dag=my_dag, trigger_rule='one_failed' ) Notificaciones v\u00eda Webhook : Para integraciones con sistemas personalizados o herramientas no directamente soportadas por operadores de Airflow, los webhooks ofrecen una gran flexibilidad. Permiten enviar una petici\u00f3n HTTP (POST) a una URL espec\u00edfica con un payload de datos. from airflow.operators.http import SimpleHttpOperator import json send_custom_alert = SimpleHttpOperator( task_id='send_custom_alert', http_conn_id='custom_alert_api', # Conexi\u00f3n HTTP configurada en Airflow endpoint='/api/v1/alerts', method='POST', headers={\"Content-Type\": \"application/json\"}, data=json.dumps({ \"severity\": \"critical\", \"message\": \"Fallo en el procesamiento de datos del DAG {{ ti.dag_id }}\", \"dag_id\": \"{{ ti.dag_id }}\", \"task_id\": \"{{ ti.task_id }}\", \"log_url\": \"{{ ti.log_url }}\" }), dag=my_dag, trigger_rule='one_failed' ) Configuraci\u00f3n de alertas de m\u00e9tricas an\u00f3malas en herramientas externas (Grafana, Datadog) : M\u00e1s all\u00e1 de las fallas de ejecuci\u00f3n, es vital monitorear el rendimiento y la calidad de los datos. Herramientas de monitoreo como Grafana y Datadog permiten configurar alertas basadas en m\u00e9tricas recolectadas de los pipelines. Grafana : Se pueden configurar paneles en Grafana para visualizar m\u00e9tricas como el tiempo de ejecuci\u00f3n de las tareas, el n\u00famero de registros procesados, la latencia de ingesti\u00f3n, etc. Luego, se definen umbrales y reglas de alerta. Por ejemplo, si el tiempo de ejecuci\u00f3n de un DAG excede un umbral predefinido (indicando un cuello de botella o problema de rendimiento), Grafana puede enviar una notificaci\u00f3n a Slack o correo electr\u00f3nico. M\u00e9tricas a monitorear : airflow.task_duration , airflow.dag_runs_succeeded , airflow.dag_runs_failed , spark.driver.memoryUsage , spark.executor.bytesRead . Regla de Alerta : \"Si el airflow.task_duration promedio para el DAG 'data_ingestion' en los \u00faltimos 15 minutos es > 300 segundos, enviar alerta\". Datadog : Datadog ofrece agentes que recolectan m\u00e9tricas de sistemas, bases de datos y aplicaciones (incluyendo Airflow y Spark). Se pueden crear \"Monitors\" para detectar anomal\u00edas. Monitoreo de latencia : \"Alertar si la latencia de los datos en la tabla 'clientes' (medida por la diferencia entre la marca de tiempo de ingesti\u00f3n y la marca de tiempo actual) es superior a 1 hora durante 5 minutos consecutivos.\" Monitoreo de volumen de datos : \"Alertar si el n\u00famero de registros procesados por el job de Spark 'transform_sales' cae un 20% respecto a la media de la \u00faltima semana.\" Automatizaci\u00f3n de recuperaci\u00f3n La automatizaci\u00f3n de la recuperaci\u00f3n es clave para construir pipelines resilientes que puedan manejar fallos de manera aut\u00f3noma, reduciendo la necesidad de intervenci\u00f3n manual y el impacto en la disponibilidad de los datos. Configurar retries autom\u00e1ticos y escalonados (exponential backoff) : Los reintentos son un mecanismo fundamental para manejar fallos transitorios (ej. problemas de red, saturaci\u00f3n de la base de datos). El \"exponential backoff\" (retroceso exponencial) es una estrategia inteligente para los reintentos. En lugar de reintentar inmediatamente, se espera un per\u00edodo de tiempo que aumenta exponencialmente con cada reintento, lo que da m\u00e1s tiempo al sistema para recuperarse y evita sobrecargar un servicio ya inestable. from airflow.models.dag import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='example_retries_dag', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 5, # N\u00famero de reintentos 'retry_delay': timedelta(minutes=2), # Retraso inicial entre reintentos 'retry_exponential_backoff': True, # Habilitar el retroceso exponencial 'max_retry_delay': timedelta(hours=1) # Retraso m\u00e1ximo entre reintentos } ) as dag: failing_task = BashOperator( task_id='simulate_failing_task', # Este comando fallar\u00e1 intencionadamente para demostrar los reintentos bash_command='exit 1', ) En este ejemplo, la tarea intentar\u00e1 ejecutarse 5 veces. El primer reintento ocurrir\u00e1 despu\u00e9s de 2 minutos, el segundo despu\u00e9s de 4 minutos, el tercero despu\u00e9s de 8 minutos, y as\u00ed sucesivamente (hasta un m\u00e1ximo de 1 hora). Aplicar mecanismos de fallback (ej. cargar datos alternativos) : En situaciones donde un componente cr\u00edtico del pipeline falla y los reintentos no son suficientes, un mecanismo de fallback permite que el pipeline contin\u00fae operando, aunque sea con un conjunto de datos alternativo o con un rendimiento reducido. Esto es particularmente \u00fatil para dashboards o aplicaciones que requieren disponibilidad constante. Si el proceso ETL principal para cargar datos de ventas en tiempo real falla debido a un problema con el API de origen, se podr\u00eda configurar un fallback para cargar los datos de ventas del d\u00eda anterior desde un archivo plano o una base de datos de respaldo ya procesada. Esto asegura que los reportes de ventas no est\u00e9n completamente vac\u00edos, aunque no sean los m\u00e1s actualizados. En Airflow, esto se podr\u00eda lograr con BranchPythonOperator o usando un trigger_rule espec\u00edfico en una tarea de fallback. from airflow.operators.python import PythonOperator, BranchPythonOperator from airflow.utils.trigger_rule import TriggerRule def check_main_etl_status(**kwargs): ti = kwargs['ti'] # Obtener el estado de la tarea principal de ETL main_etl_success = ti.xcom_pull(task_ids='main_sales_etl', key='return_value') if main_etl_success: return 'continue_with_main_data' else: return 'load_fallback_data' with DAG( dag_id='sales_data_pipeline_with_fallback', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(hours=1), catchup=False ) as dag: main_sales_etl = BashOperator( task_id='main_sales_etl', bash_command='python /app/scripts/run_realtime_sales_etl.py', # Simula una tarea que puede fallar retries=3, retry_delay=timedelta(minutes=1), do_xcom_push=True # Para que el estado de \u00e9xito se pueda leer ) check_status = BranchPythonOperator( task_id='check_main_etl_status', python_callable=check_main_etl_status, provide_context=True, ) continue_with_main_data = BashOperator( task_id='continue_with_main_data', bash_command='echo \"Procesando datos de ventas en tiempo real...\"', trigger_rule=TriggerRule.ONE_SUCCESS # Se ejecuta si check_status devuelve este ID ) load_fallback_data = BashOperator( task_id='load_fallback_data', bash_command='python /app/scripts/load_yesterday_sales_data.py', # Script para cargar datos alternativos trigger_rule=TriggerRule.ONE_SUCCESS # Se ejecuta si check_status devuelve este ID ) main_sales_etl >> check_status check_status >> [continue_with_main_data, load_fallback_data] Pausar DAGs autom\u00e1ticamente ante errores cr\u00edticos : En situaciones donde un error es persistente y fundamental (ej. un problema de autenticaci\u00f3n con una base de datos cr\u00edtica que no se resuelve con reintentos), es mejor pausar el DAG autom\u00e1ticamente. Esto previene la ejecuci\u00f3n continua de tareas que inevitablemente fallar\u00e1n, generando ruido en los logs y consumiendo recursos innecesariamente. Airflow : Airflow no tiene un operador nativo para \"pausar DAGs autom\u00e1ticamente\". Sin embargo, se puede implementar esta l\u00f3gica utilizando un PythonOperator que interact\u00fae con la API de Airflow o directamente con la base de datos de Airflow (con precauci\u00f3n). from airflow.operators.python import PythonOperator from airflow.api.client.local_client import Client # Necesitar\u00edas la configuraci\u00f3n adecuada para usar el cliente local import logging log = logging.getLogger(__name__) def pause_dag_on_critical_error(**kwargs): dag_id = kwargs['dag_run'].dag_id ti = kwargs['ti'] # Condici\u00f3n para pausar: Si la tarea fall\u00f3 y ya se agotaron los reintentos (o es un error espec\u00edfico) if ti.current_retries >= ti.max_tries: # O alguna otra l\u00f3gica para determinar un error cr\u00edtico log.info(f\"Error cr\u00edtico detectado en DAG {dag_id}. Pausando DAG...\") # Instanciar el cliente de Airflow para interactuar con la API # Aseg\u00farate de que Airflow est\u00e9 configurado para permitir esta operaci\u00f3n. # En un entorno de producci\u00f3n, es preferible usar la API REST de Airflow con autenticaci\u00f3n. try: client = Client(None, None) # Placeholder. En producci\u00f3n, configurar conexi\u00f3n a Airflow API. client.set_dag_paused(dag_id=dag_id, is_paused=True) log.info(f\"DAG '{dag_id}' pausado exitosamente.\") except Exception as e: log.error(f\"Error al intentar pausar el DAG '{dag_id}': {e}\") else: log.info(f\"Error en DAG {dag_id}, pero no cr\u00edtico para pausar.\") with DAG( dag_id='critical_error_handling_dag', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 3, 'retry_delay': timedelta(minutes=5), } ) as dag: critical_task = BashOperator( task_id='critical_database_access', bash_command='python /app/scripts/access_critical_db.py', # Simula una falla recurrente de DB ) check_and_pause_dag = PythonOperator( task_id='check_and_pause_dag', python_callable=pause_dag_on_critical_error, provide_context=True, trigger_rule='all_failed' # Se ejecuta solo si la tarea anterior falla y no tiene m\u00e1s reintentos ) critical_task >> check_and_pause_dag Es importante destacar que la interacci\u00f3n con la API de Airflow desde dentro de un DAG requiere una configuraci\u00f3n de seguridad adecuada y permisos. Para entornos productivos, es m\u00e1s robusto usar el cliente REST de Airflow con autenticaci\u00f3n, o un sensor externo que monitoree los estados de los DAGs y act\u00fae en consecuencia. 3.5.5 Troubleshooting com\u00fan Dominar la resoluci\u00f3n de fallos t\u00edpicos acelera la recuperaci\u00f3n y mejora la resiliencia del sistema, minimizando el impacto en la operaci\u00f3n y los tiempos de inactividad. Tiempos largos de ejecuci\u00f3n Cuando los jobs de Spark tardan m\u00e1s de lo esperado, es crucial identificar la causa ra\u00edz. Diagn\u00f3stico de cuellos de botella en etapas espec\u00edficas del DAG (Directed Acyclic Graph) : El DAG de Spark visualiza las transformaciones y acciones del job. Un an\u00e1lisis detallado permite identificar qu\u00e9 etapas est\u00e1n consumiendo m\u00e1s tiempo. Esto se puede lograr a trav\u00e9s de la interfaz de usuario de Spark (Spark UI) o analizando los logs. Si observamos que la etapa de shuffle es la que m\u00e1s tiempo toma, podr\u00eda indicar una gran cantidad de datos siendo transferidos entre los nodos, lo que sugiere una posible necesidad de optimizaci\u00f3n de las operaciones de agregaci\u00f3n o joins. La pesta\u00f1a \"Stages\" en la Spark UI proporciona una visi\u00f3n detallada del tiempo de ejecuci\u00f3n de cada etapa, el tiempo de CPU, los datos le\u00eddos/escritos, etc. Optimizaci\u00f3n del paralelismo y tuning de recursos en Spark : Ajustar el n\u00famero de ejecutores, n\u00facleos por ejecutor y la memoria asignada puede tener un impacto significativo. Un paralelismo insuficiente puede llevar a que los recursos no se utilicen por completo, mientras que un paralelismo excesivo puede generar una sobrecarga de tareas y una gesti\u00f3n ineficiente. Se puede ajustar el spark.sql.shuffle.partitions para operaciones como joins o agregaciones. Tuning de recursos : spark.executor.instances : N\u00famero de ejecutores. spark.executor.cores : N\u00famero de n\u00facleos por ejecutor. spark.executor.memory : Memoria asignada a cada ejecutor. spark.driver.memory : Memoria asignada al driver. from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"LongRunningJobOptimization\") \\ .config(\"spark.executor.instances\", \"10\") \\ .config(\"spark.executor.cores\", \"4\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.sql.shuffle.partitions\", \"200\") \\ .getOrCreate() En este ejemplo, estamos configurando Spark para usar 10 ejecutores, cada uno con 4 n\u00facleos y 8 GB de memoria, y estableciendo el n\u00famero de particiones de shuffle en 200. Fallos por particiones desbalanceadas Las particiones desbalanceadas (data skew) pueden causar problemas de rendimiento severos, donde uno o pocos ejecutores terminan haciendo la mayor parte del trabajo, convirti\u00e9ndose en cuellos de botella. Evaluaci\u00f3n del tama\u00f1o y distribuci\u00f3n de particiones : Es fundamental entender c\u00f3mo se distribuyen los datos entre las particiones. Esto se puede hacer inspeccionando los logs de Spark o utilizando herramientas como df.rdd.getNumPartitions() y df.groupBy(spark_partition_id()).count().orderBy(\"count\", ascending=False).show() . Si un count() en una columna espec\u00edfica muestra un valor muy alto para una \u00fanica partici\u00f3n, es un claro indicador de desbalanceo. from pyspark.sql.functions import spark_partition_id # Suponiendo que 'df' es tu DataFrame df.withColumn(\"partitionId\", spark_partition_id()) \\ .groupBy(\"partitionId\") \\ .count() \\ .orderBy(\"count\", ascending=False) \\ .show() Esto mostrar\u00e1 el n\u00famero de registros en cada partici\u00f3n, permitiendo identificar r\u00e1pidamente las particiones con un tama\u00f1o desproporcionado. Uso de repartition() o coalesce() para mejorar el rendimiento : repartition(numPartitions, *cols) : Redistribuye los datos entre un n\u00famero especificado de particiones. Es una operaci\u00f3n de shuffle completa, lo que significa que mueve todos los datos entre los nodos. Es \u00fatil cuando se necesita aumentar o disminuir dr\u00e1sticamente el n\u00famero de particiones o cuando se desea re-distribuir los datos basados en una o m\u00e1s columnas para evitar el desbalanceo. # Suponiendo que 'df' tiene una columna 'customer_id' que est\u00e1 causando skew df_repartitioned = df.repartition(200, \"customer_id\") # Ahora, las operaciones subsiguientes en customer_id deber\u00edan ser m\u00e1s balanceadas coalesce(numPartitions) : Reduce el n\u00famero de particiones de un DataFrame de forma m\u00e1s eficiente que repartition() porque evita un shuffle completo de los datos. Solo puede reducir el n\u00famero de particiones. Es \u00fatil cuando se han realizado muchas transformaciones que han creado un gran n\u00famero de particiones peque\u00f1as. # Si tienes demasiadas particiones peque\u00f1as despu\u00e9s de algunas transformaciones df_coalesced = df.coalesce(50) Consideraciones : Elegir entre repartition y coalesce depende del escenario. Si el desbalanceo es severo y necesitas una redistribuci\u00f3n completa o si necesitas un mayor n\u00famero de particiones, repartition es la elecci\u00f3n. Si solo necesitas reducir el n\u00famero de particiones sin un shuffle costoso, coalesce es m\u00e1s eficiente. Problemas de red o acceso a fuentes Los problemas de conectividad o acceso a datos externos son comunes en entornos de Big Data y pueden llevar a fallos de jobs aparentemente inexplicables. Verificaci\u00f3n de credenciales, endpoints y latencias : Credenciales : Asegurarse de que las credenciales de acceso (claves API, nombres de usuario/contrase\u00f1as, tokens OAuth) para las fuentes de datos (bases de datos, S3, HDFS, Kafka, etc.) sean correctas y tengan los permisos adecuados. Endpoints : Confirmar que los URLs, IPs o nombres de host de las fuentes de datos sean correctos y accesibles desde los nodos del cluster Spark. Latencias : Las altas latencias de red pueden ralentizar significativamente la lectura de datos o la escritura de resultados. Se pueden usar herramientas como ping , traceroute o herramientas de monitoreo de red para diagnosticar esto. Ejemplo : Un job que falla al intentar leer datos de un bucket S3 puede deberse a que las credenciales IAM no son v\u00e1lidas o el rol asumido no tiene permisos de lectura para ese bucket. Implementaci\u00f3n de retries y validaciones previas de conectividad : Retries (Reintentos) : Configurar el cliente o conector de la fuente de datos para reintentar autom\u00e1ticamente la conexi\u00f3n o la operaci\u00f3n en caso de fallos transitorios (ej., timeouts de red). Muchos conectores de Spark (JDBC, S3) tienen opciones de reintento configurables. Validaciones previas de conectividad : Antes de iniciar operaciones de lectura/escritura a gran escala, realizar una peque\u00f1a prueba de conectividad. Esto puede ser un simple ping a la base de datos o un intento de listar un peque\u00f1o n\u00famero de archivos en un directorio de S3. # Ejemplo de lectura desde una base de datos con reintentos configurados jdbc_url = \"jdbc:postgresql://your_database_host:5432/your_database\" connection_properties = { \"user\": \"your_user\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\", \"reconnect\": \"true\", # Algunas bases de datos soportan esto \"retries\": \"3\", # Configuraci\u00f3n a nivel de conector o driver \"loginTimeout\": \"30\" # Timeout de conexi\u00f3n } try: df_jdbc = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties) df_jdbc.show() except Exception as e: print(f\"Error al conectar o leer desde la base de datos: {e}\") # L\u00f3gica para manejar el fallo, quiz\u00e1s enviar una alerta Validaci\u00f3n de conectividad a S3 (ejemplo conceptual) : import boto3 from botocore.exceptions import ClientError # Suponiendo que ya tienes tus credenciales AWS configuradas s3_client = boto3.client('s3') bucket_name = \"your-s3-bucket\" try: # Intentar listar un prefijo o verificar la existencia del bucket s3_client.head_bucket(Bucket=bucket_name) print(f\"Conexi\u00f3n exitosa al bucket S3: {bucket_name}\") # Proceder con la lectura de datos con Spark df_s3 = spark.read.parquet(f\"s3a://{bucket_name}/path/to/data/\") df_s3.show() except ClientError as e: error_code = e.response['Error']['Code'] if error_code == '404': print(f\"Bucket S3 '{bucket_name}' no encontrado o no accesible.\") elif error_code == '403': print(f\"Permisos insuficientes para acceder al bucket S3 '{bucket_name}'.\") else: print(f\"Error de S3 desconocido: {e}\") # L\u00f3gica para manejar el fallo, evitar la ejecuci\u00f3n del job de Spark except Exception as e: print(f\"Error general al intentar acceder a S3: {e}\") Tarea Configura un DAG en Airflow con logging personalizado y verifica la salida de logs para una ejecuci\u00f3n exitosa y una fallida. Ejecuta un job en Apache Spark que procese datos particionados de forma desigual. Revisa el Spark UI e identifica cuellos de botella. Simula un error de red al conectar con una fuente de datos externa en un DAG de Airflow. Implementa un retry autom\u00e1tico y registra el comportamiento en logs. Configura alertas por correo para fallos cr\u00edticos en un pipeline ETL usando EmailOperator en Airflow. Analiza m\u00e9tricas de throughput y retries de un flujo ETL hist\u00f3rico y propone ajustes de recursos o paralelismo para mejorar su eficiencia.","title":"Monitorizaci\u00f3n y Troubleshooting de Pipelines"},{"location":"tema35/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema35/#tema-35-monitorizacion-y-troubleshooting-de-pipelines","text":"Objetivo : Detectar, analizar y resolver errores operativos en pipelines ETL mediante herramientas de observabilidad, m\u00e9tricas clave, interfaces gr\u00e1ficas y estrategias de recuperaci\u00f3n automatizada, asegurando la confiabilidad y eficiencia del procesamiento de datos. Introducci\u00f3n : La creaci\u00f3n de pipelines ETL escalables y funcionales no es suficiente si no se cuenta con mecanismos adecuados para su supervisi\u00f3n y mantenimiento. La monitorizaci\u00f3n permite identificar cuellos de botella, errores y problemas de rendimiento, mientras que el troubleshooting proporciona los medios para analizarlos y resolverlos. Este tema aborda herramientas como Spark UI y Airflow UI, el uso de logs y m\u00e9tricas, y estrategias pr\u00e1cticas para prevenir y remediar fallos comunes. Desarrollo : La observabilidad de pipelines ETL es un componente cr\u00edtico en cualquier arquitectura de datos moderna. Los ingenieros de datos deben dominar no solo la construcci\u00f3n de flujos, sino tambi\u00e9n su operaci\u00f3n continua. Este tema desarrolla las competencias necesarias para leer y configurar logs, interpretar interfaces de usuario de ejecuci\u00f3n, utilizar m\u00e9tricas relevantes, automatizar alertas y aplicar soluciones efectivas a problemas frecuentes que afectan el desempe\u00f1o de los pipelines.","title":"Tema 3.5. Monitorizaci\u00f3n y Troubleshooting de Pipelines"},{"location":"tema35/#351-configuracion-y-lectura-de-logs-en-spark-y-airflow","text":"La inspecci\u00f3n de logs es crucial para entender el comportamiento interno de tus jobs de Spark y DAGs de Airflow . Al monitorear los logs, puedes identificar r\u00e1pidamente errores, cuellos de botella de rendimiento y problemas de configuraci\u00f3n que podr\u00edan afectar la eficiencia de tus pipelines de datos. Esto te permite mantener la estabilidad y el rendimiento de tus sistemas de Big Data.","title":"3.5.1 Configuraci\u00f3n y lectura de logs en Spark y Airflow"},{"location":"tema35/#configuracion-de-logs-en-apache-spark-y-airflow","text":"Tanto Spark como Airflow ofrecen opciones robustas para personalizar el nivel de logging, lo que te permite ajustar la verbosidad de los logs seg\u00fan tus necesidades espec\u00edficas (por ejemplo, INFO , DEBUG , WARN , ERROR , FATAL ). Una configuraci\u00f3n adecuada asegura que obtengas la trazabilidad necesaria sin sobrecargar tus sistemas de almacenamiento de logs. Configuraci\u00f3n de logs en Apache Spark : El archivo log4j.properties es fundamental para controlar el comportamiento del logging en Spark. Puedes ubicarlo en el directorio conf de tu instalaci\u00f3n de Spark. Dentro de este archivo, puedes definir los niveles de logging para diferentes paquetes de Spark (por ejemplo, org.apache.spark , org.eclipse.jetty ) y configurar los appenders para especificar d\u00f3nde se escribir\u00e1n los logs (por ejemplo, consola, archivos, syslog). Para reducir la verbosidad de los logs de Spark a WARN (solo advertencias y errores) y ver los logs en la consola, a\u00f1adir\u00edas las siguientes l\u00edneas a log4j.properties : log4j.rootCategory=WARN, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n Para habilitar el logging a nivel DEBUG para un paquete espec\u00edfico, por ejemplo, para la configuraci\u00f3n de Spark SQL: log4j.logger.org.apache.spark.sql.execution.SparkSqlParser=DEBUG Configuraci\u00f3n de logs en Apache Airflow : El archivo airflow.cfg es el coraz\u00f3n de la configuraci\u00f3n de Airflow y te permite definir c\u00f3mo y d\u00f3nde se almacenan los logs de tus DAGs y tareas. Dentro de la secci\u00f3n [logging] , puedes especificar el remote_logging para enviar logs a servicios de almacenamiento en la nube, el log_level global y la ruta base_log_folder para los logs locales. Para un control m\u00e1s granular, puedes definir una clase Python personalizada logging_config_class que herede de airflow.utils.log.logging_mixin.LoggingMixin y especifique reglas de logging avanzadas. Esto es especialmente \u00fatil para integrar sistemas de logging personalizados o para configurar loggers de forma din\u00e1mica. En airflow.cfg , puedes establecer el nivel de log global y la ruta local: [logging] log_level = INFO base_log_folder = /opt/airflow/logs Para centralizar los logs en un servicio como Amazon CloudWatch, puedes configurar el remote_logging y especificar la clase de logging: [logging] remote_logging = True remote_base_log_folder = s3://your-airflow-logs-bucket/ remote_log_conn_id = aws_default log_level = INFO logging_config_class = my_project.config.cloud_logging.CloudLoggingConfig Donde CloudLoggingConfig ser\u00eda una clase Python personalizada que manejar\u00eda la integraci\u00f3n con CloudWatch o un servicio similar. Centralizaci\u00f3n de logs en entornos en la nube : En entornos de producci\u00f3n basados en la nube, es una pr\u00e1ctica recomendada centralizar los logs para facilitar su an\u00e1lisis, monitoreo y archivo. Amazon CloudWatch / S3 : Para AWS, puedes enviar logs de Spark directamente a CloudWatch Logs o almacenar los logs de Airflow en S3, lo que permite un acceso unificado y el uso de servicios como CloudWatch Logs Insights para consultas. Google Cloud Logging / Google Cloud Storage : En GCP, los logs pueden ser enviados a Cloud Logging, que ofrece capacidades de b\u00fasqueda y an\u00e1lisis potentes, y los logs a largo plazo pueden archivarse en Google Cloud Storage. Elastic Stack (ELK) : Para una soluci\u00f3n autoalojada o en la nube, Elasticsearch, Logstash y Kibana proporcionan una plataforma robusta para la ingesta, almacenamiento, indexaci\u00f3n y visualizaci\u00f3n de logs de Spark y Airflow.","title":"Configuraci\u00f3n de logs en Apache Spark y Airflow"},{"location":"tema35/#lectura-e-interpretacion-de-logs","text":"Interpretar logs de Spark y Airflow requiere familiaridad con los patrones de mensajes y la estructura jer\u00e1rquica de la ejecuci\u00f3n de trabajos y tareas. Reconocer los errores comunes y sus causas subyacentes te ayudar\u00e1 a diagnosticar problemas de manera m\u00e1s eficiente. Identificaci\u00f3n de errores comunes en Spark : Un error Stage Failed indica que una etapa (Stage) de tu job de Spark no pudo completarse. Esto puede ser debido a fallos en las tareas, problemas de memoria, errores de datos o configuraci\u00f3n incorrecta. Si ves un mensaje como Stage 2 failed in 20.0 s , debes buscar en los logs de las tareas (Tasks) dentro de esa etapa para identificar el error espec\u00edfico, como java.lang.OutOfMemoryError o org.apache.spark.SparkException: Task failed while writing rows . El error* TaskKilled ocurre cuando una tarea es terminada prematuramente. Las razones pueden ser un tiempo de espera excedido ( timeout ), errores de memoria en el executor, o si el propio executor fue terminado. TaskKilled (killed intentionally) o TaskKilled (killed due to memory limit exceeding) indican que la tarea fue eliminada. Debes revisar los logs del executor para entender la causa ra\u00edz, como la configuraci\u00f3n de spark.executor.memory . El error Executor Lost significa que un nodo de trabajo que aloja a un executor de Spark se ha perdido o ha fallado. Esto puede ser causado por problemas de red, fallos de hardware, problemas de memoria en el nodo o configuraciones incorrectas de recursos. ERROR Driver: Lost executor 1 on <hostname> sugiere que el executor se desconect\u00f3. Necesitas investigar los logs del nodo donde se ejecutaba el executor para encontrar la causa, como falta de memoria RAM o problemas de conectividad de red. Identificaci\u00f3n de errores comunes en Airflow : Logs de operadores (tasks) con fallos de conexi\u00f3n : Muchos DAGs interact\u00faan con bases de datos, APIs o sistemas de archivos remotos. Los errores de conexi\u00f3n son comunes y suelen aparecer como excepciones de red o autenticaci\u00f3n. Un log de una tarea de PostgresOperator mostrando psycopg2.OperationalError: could not connect to server: Connection refused indica un problema con las credenciales, la IP o el puerto de la base de datos. Retries fallidos ( retries exhausted ) : Airflow permite configurar reintentos para las tareas. Si una tarea agota todos sus reintentos, su estado final ser\u00e1 failed . Ver Task \"my_task\" failed after 3 retries en los logs del scheduler o worker significa que la tarea no pudo completarse con \u00e9xito despu\u00e9s de m\u00faltiples intentos. Debes analizar los logs de cada intento para identificar el error recurrente. Fallos por timeout : Si una tarea excede el tiempo de ejecuci\u00f3n configurado ( execution_timeout ), Airflow la terminar\u00e1. [2025-06-06 19:13:51,123] {taskinstance.py:1150} ERROR - Task timed out: my_long_running_task indica que la tarea tard\u00f3 m\u00e1s de lo permitido. Es posible que necesites optimizar la tarea o aumentar el execution_timeout si el tiempo de ejecuci\u00f3n es aceptable. B\u00fasqueda de trazas que indiquen problemas con la configuraci\u00f3n del cluster o variables mal definidas : Errores de classpath : Problemas al cargar librer\u00edas o dependencias pueden causar que Spark jobs fallen al inicio. java.lang.ClassNotFoundException: com.example.MyCustomClass en los logs del driver o executor indica que una clase necesaria no est\u00e1 disponible en el classpath del cluster. Variables de entorno incorrectas : Una variable de entorno mal configurada puede afectar tanto a Spark como a Airflow. Un DAG de Airflow que depende de una variable de entorno DB_PASSWORD y falla con un AuthenticationFailed o similar, podr\u00eda indicar que la variable no est\u00e1 definida o es incorrecta. Configuraci\u00f3n de recursos ( memory , cores ) : Valores incorrectos en la configuraci\u00f3n de memoria o CPU para Spark executors pueden llevar a errores de OutOfMemoryError o a un rendimiento deficiente. Mensajes como Container killed by YARN for exceeding memory limits sugieren que los recursos asignados no son suficientes para la carga de trabajo, y necesitas ajustar spark.executor.memory o spark.driver.memory . La clave para una interpretaci\u00f3n efectiva de los logs es la pr\u00e1ctica y la familiaridad con tus propios pipelines y la infraestructura subyacente. Herramientas de visualizaci\u00f3n y agregaci\u00f3n de logs pueden ser de gran ayuda para identificar patrones y correlacionar eventos.","title":"Lectura e interpretaci\u00f3n de logs"},{"location":"tema35/#352-uso-de-interfaces-graficas-airflow-ui-spark-ui-para-diagnostico","text":"Las interfaces visuales son herramientas cruciales para el monitoreo, diagn\u00f3stico y optimizaci\u00f3n de flujos de trabajo de Big Data. Proporcionan una vista estructurada y f\u00e1cilmente navegable del estado de ejecuci\u00f3n, los tiempos de procesamiento, las dependencias entre componentes y la identificaci\u00f3n de posibles errores o cuellos de botella. Estas interfaces transforman datos complejos de logs y m\u00e9tricas en representaciones gr\u00e1ficas intuitivas, lo que facilita la toma de decisiones y la resoluci\u00f3n de problemas.","title":"3.5.2 Uso de interfaces gr\u00e1ficas (Airflow UI, Spark UI) para diagn\u00f3stico"},{"location":"tema35/#airflow-ui","text":"La interfaz de usuario de Apache Airflow es una herramienta esencial para la orquestaci\u00f3n y monitoreo de flujos de trabajo (DAGs). Permite a los desarrolladores y operadores una visibilidad completa sobre el ciclo de vida de las tareas, desde su programaci\u00f3n hasta su finalizaci\u00f3n o fallo. Utilizar el Tree View y Graph View para entender dependencias entre tareas. Tree View : Esta vista ofrece una perspectiva cronol\u00f3gica del estado de cada instancia de tarea a lo largo del tiempo. Es ideal para identificar patrones de fallos recurrentes o para ver el historial de ejecuciones de un DAG. Por ejemplo, puedes observar si una tarea espec\u00edfica siempre falla en la misma hora del d\u00eda o despu\u00e9s de ciertas condiciones, lo que podr\u00eda indicar un problema de recursos o una dependencia externa. Graph View : Proporciona una representaci\u00f3n visual de las relaciones de dependencia entre las tareas dentro de un DAG. Es fundamental para entender el flujo l\u00f3gico del trabajo. Si una tarea est\u00e1 tardando m\u00e1s de lo esperado, puedes ver r\u00e1pidamente qu\u00e9 tareas est\u00e1n esperando su finalizaci\u00f3n y c\u00f3mo esto impacta el flujo completo. Por ejemplo, si la tarea \"transformar_datos\" falla, el Graph View mostrar\u00e1 claramente que las tareas \"cargar_a_dw\" y \"generar_reporte\" no se ejecutar\u00e1n hasta que \"transformar_datos\" se complete exitosamente. Revisar logs por tarea desde la interfaz y aplicar retries manuales. Cada instancia de tarea en Airflow genera logs detallados que son accesibles directamente desde la UI. Estos logs son vitales para depurar errores, ya que contienen la salida est\u00e1ndar y los mensajes de error generados por la tarea. Por ejemplo, si una tarea de carga de datos falla, los logs podr\u00edan indicar un problema de conectividad con la base de datos o un error en el formato de los datos. Cuando una tarea falla debido a un problema transitorio (por ejemplo, un problema de red moment\u00e1neo), Airflow permite realizar retries manuales directamente desde la interfaz. Esto evita tener que esperar a la pr\u00f3xima ejecuci\u00f3n programada o modificar el DAG, agilizando la recuperaci\u00f3n de flujos de trabajo. Verificar el SLA Misses y la duraci\u00f3n de ejecuci\u00f3n por instancia. SLA Misses (Service Level Agreement Misses) : Airflow permite definir SLAs para las tareas, lo que significa que esperas que una tarea se complete dentro de un tiempo determinado. Si una tarea excede este tiempo, Airflow lo marca como un \"SLA Miss\". La UI te permite ver un resumen de estas infracciones, lo que es crucial para identificar tareas que est\u00e1n afectando el rendimiento general o que necesitan optimizaci\u00f3n urgente. Por ejemplo, si la tarea \"procesar_ventas_diarias\" tiene un SLA de 30 minutos y consistentemente tarda 45 minutos, ver\u00e1s un SLA Miss, indicando que el proceso est\u00e1 tardando demasiado. Duraci\u00f3n de ejecuci\u00f3n por instancia : La Airflow UI muestra el tiempo que cada instancia de una tarea tard\u00f3 en completarse. Al analizar estas duraciones a lo largo de varias ejecuciones, puedes identificar tendencias de rendimiento. Si una tarea que sol\u00eda tardar 5 minutos ahora tarda 20, podr\u00eda indicar un aumento en el volumen de datos, una degradaci\u00f3n del rendimiento del sistema subyacente o un problema en el c\u00f3digo de la tarea.","title":"Airflow UI"},{"location":"tema35/#spark-ui","text":"La Spark UI es la herramienta de monitoreo y diagn\u00f3stico por excelencia para aplicaciones Apache Spark. Proporciona una visi\u00f3n granular del progreso de los jobs, las etapas, las tareas y la utilizaci\u00f3n de recursos en el cl\u00faster, lo que es indispensable para la optimizaci\u00f3n del rendimiento y la depuraci\u00f3n de problemas en el procesamiento de Big Data. Analizar el DAG f\u00edsico y l\u00f3gico del job para identificar etapas lentas. DAG L\u00f3gico : La Spark UI muestra el plan l\u00f3gico de ejecuci\u00f3n de tu job, representando las transformaciones de RDDs o DataFrames. Esto te ayuda a entender c\u00f3mo Spark interpreta tu c\u00f3digo. DAG F\u00edsico : Esta es una representaci\u00f3n de las etapas y tareas reales que Spark generar\u00e1 para ejecutar el plan l\u00f3gico. Cada etapa se corresponde con una o m\u00e1s operaciones de \"shuffle\" o con la lectura de datos. La Spark UI permite visualizar el tiempo de ejecuci\u00f3n de cada etapa. Si identificas una etapa que consume una cantidad desproporcionada de tiempo (por ejemplo, una etapa de join o groupByKey ), esto indica un posible cuello de botella. Por ejemplo, una etapa de shuffle excepcionalmente lenta puede indicar un problema de particionamiento de datos o una red saturada. Identificar tareas que consumen m\u00e1s recursos o que est\u00e1n desbalanceadas. Dentro de cada etapa, la Spark UI desglosa el rendimiento a nivel de tarea . Puedes ver m\u00e9tricas como el tiempo de ejecuci\u00f3n de cada tarea, la cantidad de datos le\u00eddos/escritos, y la utilizaci\u00f3n de memoria y CPU. Tareas desbalanceadas (Skew) : Si observas que la mayor\u00eda de las tareas en una etapa se completan r\u00e1pidamente, pero unas pocas tardan significativamente m\u00e1s (por ejemplo, una tarea tarda 5 minutos mientras que el resto tarda 10 segundos), esto es un claro indicio de sesgo de datos (data skew) . Esto significa que algunos ejecutores est\u00e1n procesando una cantidad desproporcionadamente grande de datos debido a valores clave no distribuidos uniformemente. La Spark UI te permite identificar estas tareas \"lentas\" para luego aplicar t\u00e9cnicas de optimizaci\u00f3n como salting o re-particionamiento. Revisar el Event Timeline para ver eventos cr\u00edticos de ejecuci\u00f3n. El Event Timeline de la Spark UI es una representaci\u00f3n visual cronol\u00f3gica de los eventos clave que ocurren durante la ejecuci\u00f3n de un job Spark. Esto incluye el inicio y fin de ejecutores, la asignaci\u00f3n y liberaci\u00f3n de recursos, el inicio y fin de etapas, y los eventos de shuffle. Analizar esta l\u00ednea de tiempo es crucial para detectar problemas a nivel de cl\u00faster. Por ejemplo, si observas un gran n\u00famero de eventos de \"Executor Lost\" (ejecutor perdido), podr\u00eda indicar problemas de memoria en los nodos del cl\u00faster o inestabilidad de la red. Si hay largos periodos de inactividad entre etapas, podr\u00eda sugerir problemas con la asignaci\u00f3n de recursos o con la disponibilidad de datos. Esta vista te ayuda a entender c\u00f3mo los recursos del cl\u00faster est\u00e1n siendo utilizados a lo largo del tiempo y a identificar periodos de inactividad o contenci\u00f3n.","title":"Spark UI"},{"location":"tema35/#353-metricas-clave-tiempo-de-ejecucion-throughput-retries","text":"El seguimiento de m\u00e9tricas clave es esencial para establecer la salud operativa, la eficiencia y la confiabilidad de los pipelines de datos. Estas m\u00e9tricas nos permiten identificar cuellos de botella, predecir fallos y optimizar el uso de recursos.","title":"3.5.3 M\u00e9tricas clave: tiempo de ejecuci\u00f3n, throughput, retries"},{"location":"tema35/#metricas-de-rendimiento","text":"Las m\u00e9tricas de rendimiento nos indican qu\u00e9 tan r\u00e1pido y eficientemente se est\u00e1n ejecutando nuestros pipelines. Tiempo de ejecuci\u00f3n total del DAG/job y por tarea individual : Se refiere al tiempo que tarda un flujo completo de trabajo (DAG o job) en finalizar, as\u00ed como el tiempo que consume cada una de las tareas que lo componen. Monitorear ambos niveles nos permite identificar tareas que est\u00e1n tardando m\u00e1s de lo esperado y que podr\u00edan ser optimizadas. Un pipeline de ETL que carga datos de un sistema transaccional a un data warehouse. Si el tiempo total de ejecuci\u00f3n aumenta de 30 minutos a 2 horas, necesitamos investigar. Al revisar los tiempos individuales de las tareas, podr\u00edamos descubrir que una tarea de JOIN en Spark ha pasado de 5 minutos a 1 hora. En Spark, el Spark UI proporciona detalles sobre el tiempo de ejecuci\u00f3n de cada etapa y tarea. A nivel de c\u00f3digo, puedes registrar el tiempo: from datetime import datetime start_time_job = datetime.now() # ... c\u00f3digo de tu job Spark ... df_transformed = spark.sql(\"SELECT ... FROM large_table JOIN another_table ON ...\") df_transformed.write.parquet(\"s3://data-lake/processed/data.parquet\") # ... end_time_job = datetime.now() print(f\"Tiempo total de ejecuci\u00f3n del job: {end_time_job - start_time_job}\") # Para una tarea espec\u00edfica (ej. una transformaci\u00f3n costosa) start_time_task_join = datetime.now() df_joined = large_df.join(small_df, \"key\") end_time_task_join = datetime.now() print(f\"Tiempo de ejecuci\u00f3n de la tarea JOIN: {end_time_task_join - start_time_task_join}\") Throughput (datos procesados por unidad de tiempo) : Representa la cantidad de datos que el pipeline es capaz de procesar en un determinado per\u00edodo. Es una m\u00e9trica crucial para entender la capacidad y la escalabilidad del sistema. Se puede medir en filas por segundo, megabytes por segundo, etc. Un pipeline de ingesta de logs en tiempo real que procesa 10 GB de datos por hora. Si este throughput disminuye a 2 GB por hora, indica un problema en la ingesta o en el procesamiento. Para una carga de datos: # Suponiendo que 'rows_processed' es el n\u00famero total de filas procesadas # y 'execution_time_seconds' es el tiempo total de ejecuci\u00f3n en segundos rows_processed = 1_000_000 execution_time_seconds = 60 # 1 minuto throughput_rows_per_second = rows_processed / execution_time_seconds print(f\"Throughput: {throughput_rows_per_second:.2f} filas/segundo\") # Para datos en bytes (ej. leyendo un archivo grande) file_size_bytes = 10 * 1024 * 1024 # 10 MB throughput_mb_per_second = (file_size_bytes / (1024 * 1024)) / execution_time_seconds print(f\"Throughput: {throughput_mb_per_second:.2f} MB/segundo\") Latencia entre tareas, importante en flujos encadenados : Se refiere al tiempo de espera entre la finalizaci\u00f3n de una tarea y el inicio de la siguiente. En pipelines donde las tareas dependen secuencialmente unas de otras, una alta latencia puede acumularse y prolongar significativamente el tiempo total de ejecuci\u00f3n del DAG. Esto es especialmente cr\u00edtico en arquitecturas de microservicios o pipelines de streaming. En un pipeline de Airflow, si la Task A finaliza a las 10:00 AM y la Task B (que depende de Task A ) comienza a las 10:15 AM, la latencia entre tareas es de 15 minutos. Un monitoreo de esta m\u00e9trica puede revelar que los recursos del worker o el scheduler est\u00e1n saturados. Airflow registra los tiempos de inicio y fin de cada tarea. La latencia se calcular\u00eda como: start_time(Task B) - end_time(Task A) Puedes obtener estos datos de la base de datos de metadatos de Airflow o a trav\u00e9s de su API. Herramientas de monitoreo como Prometheus y Grafana se integrar\u00edan para visualizar estas brechas.","title":"M\u00e9tricas de rendimiento"},{"location":"tema35/#metricas-de-confiabilidad-y-robustez","text":"Estas m\u00e9tricas nos informan sobre la estabilidad y la capacidad de nuestro sistema para manejar errores y recuperarse de ellos. N\u00famero de reintentos (retries) y su tasa de \u00e9xito : Indica cu\u00e1ntas veces una tarea tuvo que ser reejecutada debido a un fallo transitorio. Un alto n\u00famero de reintentos puede indicar problemas subyacentes (ej. inestabilidad de la red, picos de carga en sistemas externos) que, aunque se recuperen, afectan la eficiencia y la latencia. La tasa de \u00e9xito de los reintentos (cu\u00e1ntos reintentos finalmente lograron que la tarea se completara con \u00e9xito) es crucial para entender la eficacia de nuestra estrategia de reintentos. Si una tarea que escribe a una base de datos externa falla intermitentemente y necesita 3 reintentos para completarse en el 80% de los casos, esto es aceptable. Sin embargo, si en el 50% de los casos agota todos los reintentos y falla definitivamente, indica un problema grave. from airflow import DAG from airflow.operators.dummy import DummyOperator from datetime import datetime, timedelta with DAG( dag_id='example_retries', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 3, # N\u00famero de reintentos 'retry_delay': timedelta(minutes=5), # Retraso entre reintentos } ) as dag: start_task = DummyOperator(task_id='start') failing_task = DummyOperator(task_id='potentially_failing_task') # Esta tarea simular\u00eda un fallo end_task = DummyOperator(task_id='end') start_task >> failing_task >> end_task Las plataformas como Airflow exponen el n\u00famero de reintentos en su interfaz de usuario y en sus logs. Errores por tipo (conexi\u00f3n, permisos, timeouts) : Clasificar los errores por su naturaleza es fundamental para una depuraci\u00f3n efectiva y para identificar patrones. Errores de conexi\u00f3n : Indican problemas de red, bases de datos no disponibles o servicios externos ca\u00eddos. Connection refused al intentar conectar con una API, o Network unreachable al acceder a un bucket S3. Errores de permisos : Apuntan a problemas de autorizaci\u00f3n o configuraci\u00f3n de seguridad. Access Denied al intentar leer de un bucket de S3 sin los permisos adecuados, o Permission denied al escribir en un directorio. Timeouts : Sugieren que una operaci\u00f3n est\u00e1 tardando demasiado en responder, lo que podr\u00eda deberse a cuellos de botella en el sistema llamado, a una alta latencia de red o a una configuraci\u00f3n de timeout demasiado baja. Una consulta a una base de datos que excede el tiempo l\u00edmite de espera configurado ( Query timeout expired ). Estos errores se capturan en los logs de las aplicaciones (Spark, Python scripts, etc.). Utilizar sistemas de log management (ELK Stack, Splunk, Datadog) con an\u00e1lisis de patrones y alertas es crucial. import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) try: # Simulaci\u00f3n de un error de conexi\u00f3n # requests.get(\"http://nonexistent-service.com\", timeout=1) raise ConnectionError(\"Fallo de conexi\u00f3n al servicio externo.\") except ConnectionError as e: logger.error(f\"Error de conexi\u00f3n: {e}\") except PermissionError as e: logger.error(f\"Error de permisos: {e}\") except Exception as e: logger.error(f\"Error inesperado: {type(e).__name__} - {e}\") Disponibilidad y uptime del scheduler (Airflow) : En sistemas orquestadores como Airflow, la disponibilidad del scheduler es cr\u00edtica, ya que es el componente encargado de determinar cu\u00e1ndo deben ejecutarse los DAGs y sus tareas. Un scheduler ca\u00eddo o inestable detendr\u00e1 por completo la ejecuci\u00f3n de todos los pipelines. El uptime es el porcentaje de tiempo que el scheduler ha estado operativo. Si el scheduler de Airflow estuvo ca\u00eddo durante 2 horas en un d\u00eda, su disponibilidad ese d\u00eda fue de (22/24) * 100 = 91.67% , lo cual es inaceptable para la mayor\u00eda de entornos de producci\u00f3n. Monitoreo a nivel de sistema operativo (CPU, memoria, procesos) del servidor donde corre el scheduler. Herramientas de monitoreo de infraestructura (Prometheus, Grafana, Datadog) pueden rastrear el estado del proceso del scheduler y generar alertas si se detiene o si sus m\u00e9tricas de salud (como la cola de tareas pendientes) son inusuales. Airflow tambi\u00e9n tiene m\u00e9tricas internas que pueden exportarse a sistemas de monitoreo.","title":"M\u00e9tricas de confiabilidad y robustez"},{"location":"tema35/#354-alertas-notificaciones-y-automatizacion-de-recuperacion","text":"El monitoreo proactivo es fundamental en cualquier pipeline de datos robusto. Permite no solo detectar problemas de manera temprana, sino tambi\u00e9n reaccionar ante errores de forma automatizada, minimizando la intervenci\u00f3n humana directa y reduciendo el tiempo de inactividad. Esto es crucial para mantener la integridad y disponibilidad de los datos.","title":"3.5.4 Alertas, notificaciones y automatizaci\u00f3n de recuperaci\u00f3n"},{"location":"tema35/#configuracion-de-alertas-y-notificaciones","text":"La configuraci\u00f3n de alertas y notificaciones es el primer paso para un monitoreo proactivo eficaz. Permite que los equipos sean informados de inmediato sobre cualquier anomal\u00eda o fallo en los pipelines de datos. Integraci\u00f3n de notificaciones desde Airflow : Airflow, siendo una orquestador de ETL ampliamente utilizado, ofrece operadores predefinidos para enviar notificaciones a diferentes canales. Notificaciones v\u00eda correo electr\u00f3nico (EmailOperator) : Es una forma est\u00e1ndar y efectiva de alertar a los equipos. Se puede configurar para enviar correos electr\u00f3nicos al fracaso o \u00e9xito de una tarea o DAG. from airflow.operators.email import EmailOperator from airflow.utils.email import send_email # ... dentro de la definici\u00f3n de un DAG o tarea send_email_on_failure = EmailOperator( task_id='email_on_failure', to='equipo_data@example.com', subject='Alerta Airflow: Tarea {{ ti.task_id }} fall\u00f3 en DAG {{ ti.dag_id }}', html_content=\"\"\" <h3>Error en Pipeline de Datos</h3> <p>La tarea <b>{{ ti.task_id }}</b> del DAG <b>{{ ti.dag_id }}</b> ha fallado.</p> <p>Revisar logs para m\u00e1s detalles: {{ ti.log_url }}</p> \"\"\", dag=my_dag, # Asociar al DAG si es una tarea separada, o parte de la configuraci\u00f3n de default_args trigger_rule='one_failed' # Se dispara solo si alguna tarea en el DAG falla ) Alternativamente, se pueden configurar default_args para enviar correos electr\u00f3nicos en caso de fallo: default_args = { 'owner': 'airflow', 'start_date': days_ago(1), 'email': ['equipo_data@example.com'], 'email_on_failure': True, 'email_on_retry': False, 'email_on_success': False, } Notificaciones v\u00eda Slack (SlackWebhookOperator) : Slack es una herramienta de comunicaci\u00f3n muy com\u00fan en equipos de desarrollo y operaciones. Airflow permite enviar mensajes directamente a canales de Slack, lo que facilita una comunicaci\u00f3n m\u00e1s inmediata y contextualizada. from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator # Configurar la conexi\u00f3n de Slack en Airflow (Admin -> Connections) # conn_id: slack_connection, host: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX send_slack_notification = SlackWebhookOperator( task_id='slack_notification', slack_webhook_conn_id='slack_connection', # ID de la conexi\u00f3n configurada en Airflow message=\"\"\" :red_circle: *Alerta en Pipeline de Datos*: La tarea `{{ ti.task_id }}` del DAG `{{ ti.dag_id }}` ha fallado. Por favor, revisa los logs en: {{ ti.log_url }} \"\"\", channel='#alerts_data_pipeline', # Canal de Slack al que se enviar\u00e1 el mensaje dag=my_dag, trigger_rule='one_failed' ) Notificaciones v\u00eda Webhook : Para integraciones con sistemas personalizados o herramientas no directamente soportadas por operadores de Airflow, los webhooks ofrecen una gran flexibilidad. Permiten enviar una petici\u00f3n HTTP (POST) a una URL espec\u00edfica con un payload de datos. from airflow.operators.http import SimpleHttpOperator import json send_custom_alert = SimpleHttpOperator( task_id='send_custom_alert', http_conn_id='custom_alert_api', # Conexi\u00f3n HTTP configurada en Airflow endpoint='/api/v1/alerts', method='POST', headers={\"Content-Type\": \"application/json\"}, data=json.dumps({ \"severity\": \"critical\", \"message\": \"Fallo en el procesamiento de datos del DAG {{ ti.dag_id }}\", \"dag_id\": \"{{ ti.dag_id }}\", \"task_id\": \"{{ ti.task_id }}\", \"log_url\": \"{{ ti.log_url }}\" }), dag=my_dag, trigger_rule='one_failed' ) Configuraci\u00f3n de alertas de m\u00e9tricas an\u00f3malas en herramientas externas (Grafana, Datadog) : M\u00e1s all\u00e1 de las fallas de ejecuci\u00f3n, es vital monitorear el rendimiento y la calidad de los datos. Herramientas de monitoreo como Grafana y Datadog permiten configurar alertas basadas en m\u00e9tricas recolectadas de los pipelines. Grafana : Se pueden configurar paneles en Grafana para visualizar m\u00e9tricas como el tiempo de ejecuci\u00f3n de las tareas, el n\u00famero de registros procesados, la latencia de ingesti\u00f3n, etc. Luego, se definen umbrales y reglas de alerta. Por ejemplo, si el tiempo de ejecuci\u00f3n de un DAG excede un umbral predefinido (indicando un cuello de botella o problema de rendimiento), Grafana puede enviar una notificaci\u00f3n a Slack o correo electr\u00f3nico. M\u00e9tricas a monitorear : airflow.task_duration , airflow.dag_runs_succeeded , airflow.dag_runs_failed , spark.driver.memoryUsage , spark.executor.bytesRead . Regla de Alerta : \"Si el airflow.task_duration promedio para el DAG 'data_ingestion' en los \u00faltimos 15 minutos es > 300 segundos, enviar alerta\". Datadog : Datadog ofrece agentes que recolectan m\u00e9tricas de sistemas, bases de datos y aplicaciones (incluyendo Airflow y Spark). Se pueden crear \"Monitors\" para detectar anomal\u00edas. Monitoreo de latencia : \"Alertar si la latencia de los datos en la tabla 'clientes' (medida por la diferencia entre la marca de tiempo de ingesti\u00f3n y la marca de tiempo actual) es superior a 1 hora durante 5 minutos consecutivos.\" Monitoreo de volumen de datos : \"Alertar si el n\u00famero de registros procesados por el job de Spark 'transform_sales' cae un 20% respecto a la media de la \u00faltima semana.\"","title":"Configuraci\u00f3n de alertas y notificaciones"},{"location":"tema35/#automatizacion-de-recuperacion","text":"La automatizaci\u00f3n de la recuperaci\u00f3n es clave para construir pipelines resilientes que puedan manejar fallos de manera aut\u00f3noma, reduciendo la necesidad de intervenci\u00f3n manual y el impacto en la disponibilidad de los datos. Configurar retries autom\u00e1ticos y escalonados (exponential backoff) : Los reintentos son un mecanismo fundamental para manejar fallos transitorios (ej. problemas de red, saturaci\u00f3n de la base de datos). El \"exponential backoff\" (retroceso exponencial) es una estrategia inteligente para los reintentos. En lugar de reintentar inmediatamente, se espera un per\u00edodo de tiempo que aumenta exponencialmente con cada reintento, lo que da m\u00e1s tiempo al sistema para recuperarse y evita sobrecargar un servicio ya inestable. from airflow.models.dag import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='example_retries_dag', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 5, # N\u00famero de reintentos 'retry_delay': timedelta(minutes=2), # Retraso inicial entre reintentos 'retry_exponential_backoff': True, # Habilitar el retroceso exponencial 'max_retry_delay': timedelta(hours=1) # Retraso m\u00e1ximo entre reintentos } ) as dag: failing_task = BashOperator( task_id='simulate_failing_task', # Este comando fallar\u00e1 intencionadamente para demostrar los reintentos bash_command='exit 1', ) En este ejemplo, la tarea intentar\u00e1 ejecutarse 5 veces. El primer reintento ocurrir\u00e1 despu\u00e9s de 2 minutos, el segundo despu\u00e9s de 4 minutos, el tercero despu\u00e9s de 8 minutos, y as\u00ed sucesivamente (hasta un m\u00e1ximo de 1 hora). Aplicar mecanismos de fallback (ej. cargar datos alternativos) : En situaciones donde un componente cr\u00edtico del pipeline falla y los reintentos no son suficientes, un mecanismo de fallback permite que el pipeline contin\u00fae operando, aunque sea con un conjunto de datos alternativo o con un rendimiento reducido. Esto es particularmente \u00fatil para dashboards o aplicaciones que requieren disponibilidad constante. Si el proceso ETL principal para cargar datos de ventas en tiempo real falla debido a un problema con el API de origen, se podr\u00eda configurar un fallback para cargar los datos de ventas del d\u00eda anterior desde un archivo plano o una base de datos de respaldo ya procesada. Esto asegura que los reportes de ventas no est\u00e9n completamente vac\u00edos, aunque no sean los m\u00e1s actualizados. En Airflow, esto se podr\u00eda lograr con BranchPythonOperator o usando un trigger_rule espec\u00edfico en una tarea de fallback. from airflow.operators.python import PythonOperator, BranchPythonOperator from airflow.utils.trigger_rule import TriggerRule def check_main_etl_status(**kwargs): ti = kwargs['ti'] # Obtener el estado de la tarea principal de ETL main_etl_success = ti.xcom_pull(task_ids='main_sales_etl', key='return_value') if main_etl_success: return 'continue_with_main_data' else: return 'load_fallback_data' with DAG( dag_id='sales_data_pipeline_with_fallback', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(hours=1), catchup=False ) as dag: main_sales_etl = BashOperator( task_id='main_sales_etl', bash_command='python /app/scripts/run_realtime_sales_etl.py', # Simula una tarea que puede fallar retries=3, retry_delay=timedelta(minutes=1), do_xcom_push=True # Para que el estado de \u00e9xito se pueda leer ) check_status = BranchPythonOperator( task_id='check_main_etl_status', python_callable=check_main_etl_status, provide_context=True, ) continue_with_main_data = BashOperator( task_id='continue_with_main_data', bash_command='echo \"Procesando datos de ventas en tiempo real...\"', trigger_rule=TriggerRule.ONE_SUCCESS # Se ejecuta si check_status devuelve este ID ) load_fallback_data = BashOperator( task_id='load_fallback_data', bash_command='python /app/scripts/load_yesterday_sales_data.py', # Script para cargar datos alternativos trigger_rule=TriggerRule.ONE_SUCCESS # Se ejecuta si check_status devuelve este ID ) main_sales_etl >> check_status check_status >> [continue_with_main_data, load_fallback_data] Pausar DAGs autom\u00e1ticamente ante errores cr\u00edticos : En situaciones donde un error es persistente y fundamental (ej. un problema de autenticaci\u00f3n con una base de datos cr\u00edtica que no se resuelve con reintentos), es mejor pausar el DAG autom\u00e1ticamente. Esto previene la ejecuci\u00f3n continua de tareas que inevitablemente fallar\u00e1n, generando ruido en los logs y consumiendo recursos innecesariamente. Airflow : Airflow no tiene un operador nativo para \"pausar DAGs autom\u00e1ticamente\". Sin embargo, se puede implementar esta l\u00f3gica utilizando un PythonOperator que interact\u00fae con la API de Airflow o directamente con la base de datos de Airflow (con precauci\u00f3n). from airflow.operators.python import PythonOperator from airflow.api.client.local_client import Client # Necesitar\u00edas la configuraci\u00f3n adecuada para usar el cliente local import logging log = logging.getLogger(__name__) def pause_dag_on_critical_error(**kwargs): dag_id = kwargs['dag_run'].dag_id ti = kwargs['ti'] # Condici\u00f3n para pausar: Si la tarea fall\u00f3 y ya se agotaron los reintentos (o es un error espec\u00edfico) if ti.current_retries >= ti.max_tries: # O alguna otra l\u00f3gica para determinar un error cr\u00edtico log.info(f\"Error cr\u00edtico detectado en DAG {dag_id}. Pausando DAG...\") # Instanciar el cliente de Airflow para interactuar con la API # Aseg\u00farate de que Airflow est\u00e9 configurado para permitir esta operaci\u00f3n. # En un entorno de producci\u00f3n, es preferible usar la API REST de Airflow con autenticaci\u00f3n. try: client = Client(None, None) # Placeholder. En producci\u00f3n, configurar conexi\u00f3n a Airflow API. client.set_dag_paused(dag_id=dag_id, is_paused=True) log.info(f\"DAG '{dag_id}' pausado exitosamente.\") except Exception as e: log.error(f\"Error al intentar pausar el DAG '{dag_id}': {e}\") else: log.info(f\"Error en DAG {dag_id}, pero no cr\u00edtico para pausar.\") with DAG( dag_id='critical_error_handling_dag', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False, default_args={ 'retries': 3, 'retry_delay': timedelta(minutes=5), } ) as dag: critical_task = BashOperator( task_id='critical_database_access', bash_command='python /app/scripts/access_critical_db.py', # Simula una falla recurrente de DB ) check_and_pause_dag = PythonOperator( task_id='check_and_pause_dag', python_callable=pause_dag_on_critical_error, provide_context=True, trigger_rule='all_failed' # Se ejecuta solo si la tarea anterior falla y no tiene m\u00e1s reintentos ) critical_task >> check_and_pause_dag Es importante destacar que la interacci\u00f3n con la API de Airflow desde dentro de un DAG requiere una configuraci\u00f3n de seguridad adecuada y permisos. Para entornos productivos, es m\u00e1s robusto usar el cliente REST de Airflow con autenticaci\u00f3n, o un sensor externo que monitoree los estados de los DAGs y act\u00fae en consecuencia.","title":"Automatizaci\u00f3n de recuperaci\u00f3n"},{"location":"tema35/#355-troubleshooting-comun","text":"Dominar la resoluci\u00f3n de fallos t\u00edpicos acelera la recuperaci\u00f3n y mejora la resiliencia del sistema, minimizando el impacto en la operaci\u00f3n y los tiempos de inactividad.","title":"3.5.5 Troubleshooting com\u00fan"},{"location":"tema35/#tiempos-largos-de-ejecucion","text":"Cuando los jobs de Spark tardan m\u00e1s de lo esperado, es crucial identificar la causa ra\u00edz. Diagn\u00f3stico de cuellos de botella en etapas espec\u00edficas del DAG (Directed Acyclic Graph) : El DAG de Spark visualiza las transformaciones y acciones del job. Un an\u00e1lisis detallado permite identificar qu\u00e9 etapas est\u00e1n consumiendo m\u00e1s tiempo. Esto se puede lograr a trav\u00e9s de la interfaz de usuario de Spark (Spark UI) o analizando los logs. Si observamos que la etapa de shuffle es la que m\u00e1s tiempo toma, podr\u00eda indicar una gran cantidad de datos siendo transferidos entre los nodos, lo que sugiere una posible necesidad de optimizaci\u00f3n de las operaciones de agregaci\u00f3n o joins. La pesta\u00f1a \"Stages\" en la Spark UI proporciona una visi\u00f3n detallada del tiempo de ejecuci\u00f3n de cada etapa, el tiempo de CPU, los datos le\u00eddos/escritos, etc. Optimizaci\u00f3n del paralelismo y tuning de recursos en Spark : Ajustar el n\u00famero de ejecutores, n\u00facleos por ejecutor y la memoria asignada puede tener un impacto significativo. Un paralelismo insuficiente puede llevar a que los recursos no se utilicen por completo, mientras que un paralelismo excesivo puede generar una sobrecarga de tareas y una gesti\u00f3n ineficiente. Se puede ajustar el spark.sql.shuffle.partitions para operaciones como joins o agregaciones. Tuning de recursos : spark.executor.instances : N\u00famero de ejecutores. spark.executor.cores : N\u00famero de n\u00facleos por ejecutor. spark.executor.memory : Memoria asignada a cada ejecutor. spark.driver.memory : Memoria asignada al driver. from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"LongRunningJobOptimization\") \\ .config(\"spark.executor.instances\", \"10\") \\ .config(\"spark.executor.cores\", \"4\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.sql.shuffle.partitions\", \"200\") \\ .getOrCreate() En este ejemplo, estamos configurando Spark para usar 10 ejecutores, cada uno con 4 n\u00facleos y 8 GB de memoria, y estableciendo el n\u00famero de particiones de shuffle en 200.","title":"Tiempos largos de ejecuci\u00f3n"},{"location":"tema35/#fallos-por-particiones-desbalanceadas","text":"Las particiones desbalanceadas (data skew) pueden causar problemas de rendimiento severos, donde uno o pocos ejecutores terminan haciendo la mayor parte del trabajo, convirti\u00e9ndose en cuellos de botella. Evaluaci\u00f3n del tama\u00f1o y distribuci\u00f3n de particiones : Es fundamental entender c\u00f3mo se distribuyen los datos entre las particiones. Esto se puede hacer inspeccionando los logs de Spark o utilizando herramientas como df.rdd.getNumPartitions() y df.groupBy(spark_partition_id()).count().orderBy(\"count\", ascending=False).show() . Si un count() en una columna espec\u00edfica muestra un valor muy alto para una \u00fanica partici\u00f3n, es un claro indicador de desbalanceo. from pyspark.sql.functions import spark_partition_id # Suponiendo que 'df' es tu DataFrame df.withColumn(\"partitionId\", spark_partition_id()) \\ .groupBy(\"partitionId\") \\ .count() \\ .orderBy(\"count\", ascending=False) \\ .show() Esto mostrar\u00e1 el n\u00famero de registros en cada partici\u00f3n, permitiendo identificar r\u00e1pidamente las particiones con un tama\u00f1o desproporcionado. Uso de repartition() o coalesce() para mejorar el rendimiento : repartition(numPartitions, *cols) : Redistribuye los datos entre un n\u00famero especificado de particiones. Es una operaci\u00f3n de shuffle completa, lo que significa que mueve todos los datos entre los nodos. Es \u00fatil cuando se necesita aumentar o disminuir dr\u00e1sticamente el n\u00famero de particiones o cuando se desea re-distribuir los datos basados en una o m\u00e1s columnas para evitar el desbalanceo. # Suponiendo que 'df' tiene una columna 'customer_id' que est\u00e1 causando skew df_repartitioned = df.repartition(200, \"customer_id\") # Ahora, las operaciones subsiguientes en customer_id deber\u00edan ser m\u00e1s balanceadas coalesce(numPartitions) : Reduce el n\u00famero de particiones de un DataFrame de forma m\u00e1s eficiente que repartition() porque evita un shuffle completo de los datos. Solo puede reducir el n\u00famero de particiones. Es \u00fatil cuando se han realizado muchas transformaciones que han creado un gran n\u00famero de particiones peque\u00f1as. # Si tienes demasiadas particiones peque\u00f1as despu\u00e9s de algunas transformaciones df_coalesced = df.coalesce(50) Consideraciones : Elegir entre repartition y coalesce depende del escenario. Si el desbalanceo es severo y necesitas una redistribuci\u00f3n completa o si necesitas un mayor n\u00famero de particiones, repartition es la elecci\u00f3n. Si solo necesitas reducir el n\u00famero de particiones sin un shuffle costoso, coalesce es m\u00e1s eficiente.","title":"Fallos por particiones desbalanceadas"},{"location":"tema35/#problemas-de-red-o-acceso-a-fuentes","text":"Los problemas de conectividad o acceso a datos externos son comunes en entornos de Big Data y pueden llevar a fallos de jobs aparentemente inexplicables. Verificaci\u00f3n de credenciales, endpoints y latencias : Credenciales : Asegurarse de que las credenciales de acceso (claves API, nombres de usuario/contrase\u00f1as, tokens OAuth) para las fuentes de datos (bases de datos, S3, HDFS, Kafka, etc.) sean correctas y tengan los permisos adecuados. Endpoints : Confirmar que los URLs, IPs o nombres de host de las fuentes de datos sean correctos y accesibles desde los nodos del cluster Spark. Latencias : Las altas latencias de red pueden ralentizar significativamente la lectura de datos o la escritura de resultados. Se pueden usar herramientas como ping , traceroute o herramientas de monitoreo de red para diagnosticar esto. Ejemplo : Un job que falla al intentar leer datos de un bucket S3 puede deberse a que las credenciales IAM no son v\u00e1lidas o el rol asumido no tiene permisos de lectura para ese bucket. Implementaci\u00f3n de retries y validaciones previas de conectividad : Retries (Reintentos) : Configurar el cliente o conector de la fuente de datos para reintentar autom\u00e1ticamente la conexi\u00f3n o la operaci\u00f3n en caso de fallos transitorios (ej., timeouts de red). Muchos conectores de Spark (JDBC, S3) tienen opciones de reintento configurables. Validaciones previas de conectividad : Antes de iniciar operaciones de lectura/escritura a gran escala, realizar una peque\u00f1a prueba de conectividad. Esto puede ser un simple ping a la base de datos o un intento de listar un peque\u00f1o n\u00famero de archivos en un directorio de S3. # Ejemplo de lectura desde una base de datos con reintentos configurados jdbc_url = \"jdbc:postgresql://your_database_host:5432/your_database\" connection_properties = { \"user\": \"your_user\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\", \"reconnect\": \"true\", # Algunas bases de datos soportan esto \"retries\": \"3\", # Configuraci\u00f3n a nivel de conector o driver \"loginTimeout\": \"30\" # Timeout de conexi\u00f3n } try: df_jdbc = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties) df_jdbc.show() except Exception as e: print(f\"Error al conectar o leer desde la base de datos: {e}\") # L\u00f3gica para manejar el fallo, quiz\u00e1s enviar una alerta Validaci\u00f3n de conectividad a S3 (ejemplo conceptual) : import boto3 from botocore.exceptions import ClientError # Suponiendo que ya tienes tus credenciales AWS configuradas s3_client = boto3.client('s3') bucket_name = \"your-s3-bucket\" try: # Intentar listar un prefijo o verificar la existencia del bucket s3_client.head_bucket(Bucket=bucket_name) print(f\"Conexi\u00f3n exitosa al bucket S3: {bucket_name}\") # Proceder con la lectura de datos con Spark df_s3 = spark.read.parquet(f\"s3a://{bucket_name}/path/to/data/\") df_s3.show() except ClientError as e: error_code = e.response['Error']['Code'] if error_code == '404': print(f\"Bucket S3 '{bucket_name}' no encontrado o no accesible.\") elif error_code == '403': print(f\"Permisos insuficientes para acceder al bucket S3 '{bucket_name}'.\") else: print(f\"Error de S3 desconocido: {e}\") # L\u00f3gica para manejar el fallo, evitar la ejecuci\u00f3n del job de Spark except Exception as e: print(f\"Error general al intentar acceder a S3: {e}\")","title":"Problemas de red o acceso a fuentes"},{"location":"tema35/#tarea","text":"Configura un DAG en Airflow con logging personalizado y verifica la salida de logs para una ejecuci\u00f3n exitosa y una fallida. Ejecuta un job en Apache Spark que procese datos particionados de forma desigual. Revisa el Spark UI e identifica cuellos de botella. Simula un error de red al conectar con una fuente de datos externa en un DAG de Airflow. Implementa un retry autom\u00e1tico y registra el comportamiento en logs. Configura alertas por correo para fallos cr\u00edticos en un pipeline ETL usando EmailOperator en Airflow. Analiza m\u00e9tricas de throughput y retries de un flujo ETL hist\u00f3rico y propone ajustes de recursos o paralelismo para mejorar su eficiencia.","title":"Tarea"},{"location":"tema36/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.6. Seguridad en ETL y Protecci\u00f3n de Datos Objetivo : Asegurar la confidencialidad, integridad y acceso controlado a la informaci\u00f3n dentro de los pipelines ETL, mediante t\u00e9cnicas de encriptaci\u00f3n, gesti\u00f3n segura de credenciales, pol\u00edticas de control de acceso, auditor\u00eda de operaciones y cumplimiento normativo. Introducci\u00f3n : Los flujos ETL manejan informaci\u00f3n sensible en distintos puntos del ciclo de vida de los datos: desde su extracci\u00f3n en fuentes externas hasta su carga en almacenes anal\u00edticos o lagos de datos. En este contexto, la seguridad no es opcional. Es imprescindible dise\u00f1ar procesos ETL que protejan los datos en todo momento, mitigando riesgos de fuga, alteraci\u00f3n o uso indebido. La seguridad en pipelines ETL debe considerarse desde el inicio del dise\u00f1o arquitect\u00f3nico y mantenerse en cada etapa del proceso. Desarrollo : Este tema abarca las pr\u00e1cticas y herramientas clave para garantizar la protecci\u00f3n de los datos en los pipelines ETL. Desde la encriptaci\u00f3n en tr\u00e1nsito y en reposo, pasando por el uso adecuado de gestores de secretos, hasta la implementaci\u00f3n de pol\u00edticas de control de acceso y auditor\u00eda, el objetivo es dotar a los estudiantes de una visi\u00f3n integral de seguridad. Adem\u00e1s, se introducen las principales normativas internacionales que regulan el tratamiento de datos, aportando un marco legal y \u00e9tico al dise\u00f1o de soluciones de datos. 3.6.1 Encriptaci\u00f3n en Tr\u00e1nsito (TLS/SSL) y en Reposo (AES, KMS) La encriptaci\u00f3n es una piedra angular en la seguridad de los datos , asegurando su confidencialidad e integridad tanto durante su movimiento entre sistemas (en tr\u00e1nsito) como cuando est\u00e1n almacenados (en reposo) . Este subtema explorar\u00e1 los mecanismos de cifrado modernos y su implementaci\u00f3n pr\u00e1ctica en el contexto de los pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) , vital para cualquier arquitectura de datos robusta. Encriptaci\u00f3n en Tr\u00e1nsito (TLS/SSL) La encriptaci\u00f3n en tr\u00e1nsito se enfoca en proteger los datos mientras viajan a trav\u00e9s de una red . Ya sea que los datos se extraigan de fuentes externas, se consuman a trav\u00e9s de APIs, o se escriban en sistemas de almacenamiento en red, el objetivo es evitar que actores maliciosos los intercepten o modifiquen. El protocolo TLS (Transport Layer Security) , sucesor de SSL, es el est\u00e1ndar de oro para lograr esta protecci\u00f3n. TLS establece un canal de comunicaci\u00f3n seguro entre dos aplicaciones, garantizando la autenticaci\u00f3n de los extremos , la confidencialidad de los datos (mediante cifrado) y la integridad de los datos (asegurando que no han sido alterados). Configuraci\u00f3n de Spark para leer desde una base de datos PostgreSQL usando conexiones TLS : Cuando Spark se conecta a una base de datos externa como PostgreSQL, es crucial que la comunicaci\u00f3n est\u00e9 cifrada. Esto protege credenciales y datos sensibles durante el proceso de extracci\u00f3n. Para habilitar TLS/SSL en una conexi\u00f3n JDBC de Spark a PostgreSQL, se deben configurar propiedades espec\u00edficas en la cadena de conexi\u00f3n. Estas propiedades instruyen al driver JDBC para que establezca una conexi\u00f3n segura, validando el certificado del servidor PostgreSQL. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SecurePostgreSQLRead\").getOrCreate() # Configuraci\u00f3n de la conexi\u00f3n JDBC segura a PostgreSQL jdbc_url = \"jdbc:postgresql://your_db_host:5432/your_database\" # Es crucial incluir 'ssl=true' y, opcionalmente, 'sslmode=require' o 'sslmode=verify-full' # para asegurar que el certificado del servidor sea validado. connection_properties = { \"user\": \"your_username\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\", \"ssl\": \"true\", \"sslmode\": \"require\" # 'require' fuerza SSL, 'verify-full' a\u00f1ade verificaci\u00f3n de certificado } # Leer datos de la tabla de PostgreSQL de forma segura df = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties) df.show() spark.stop() Para sslmode=verify-full , necesitar\u00e1s un TrustStore con el certificado de la CA que emiti\u00f3 el certificado de tu servidor PostgreSQL, o el propio certificado del servidor, y configurar las JVM options de Spark para que lo usen. Consumo de APIs REST que exigen HTTPS con certificados v\u00e1lidos : Muchas fuentes de datos externas se exponen a trav\u00e9s de APIs REST. Si estas APIs manejan informaci\u00f3n sensible, es imperativo que requieran HTTPS (HTTP sobre TLS/SSL) para garantizar la comunicaci\u00f3n segura. Al interactuar con una API HTTPS, el cliente (nuestro pipeline ETL) verifica el certificado SSL/TLS presentado por el servidor para asegurar su autenticidad y la validez de la conexi\u00f3n cifrada. Esto previene ataques \"Man-in-the-Middle\". Bibliotecas HTTP modernas manejan la validaci\u00f3n de certificados de forma predeterminada, pero es importante comprender c\u00f3mo funcionan y c\u00f3mo configurar excepciones o certificados personalizados si es necesario (aunque esto \u00faltimo debe hacerse con precauci\u00f3n). import requests import json api_url = \"https://api.example.com/data\" # URL que exige HTTPS headers = {\"Authorization\": \"Bearer your_api_token\"} try: # requests verifica certificados SSL/TLS por defecto response = requests.get(api_url, headers=headers, timeout=10) response.raise_for_status() # Lanza una excepci\u00f3n para c\u00f3digos de estado HTTP err\u00f3neos (4xx o 5xx) data = response.json() print(\"Datos recibidos de la API de forma segura:\") print(json.dumps(data, indent=2)) except requests.exceptions.SSLError as e: print(f\"Error SSL: Problema con el certificado TLS/SSL. {e}\") except requests.exceptions.ConnectionError as e: print(f\"Error de conexi\u00f3n: No se pudo conectar a la API. {e}\") except requests.exceptions.Timeout as e: print(f\"Error de tiempo de espera: La solicitud a la API tard\u00f3 demasiado. {e}\") except requests.exceptions.RequestException as e: print(f\"Error general de la solicitud: {e}\") Comunicaci\u00f3n segura entre tareas de Airflow mediante redes privadas virtuales y TLS : En entornos de orquestaci\u00f3n como Apache Airflow, las tareas a menudo se ejecutan en diferentes nodos o incluso en servicios en la nube. Proteger la comunicaci\u00f3n entre el programador, los workers y las bases de datos de metadatos es vital. Una estrategia com\u00fan es desplegar Airflow dentro de una red privada virtual (VPN) o una Virtual Private Cloud (VPC) en la nube, lo que segmenta el tr\u00e1fico de red y lo a\u00edsla de la internet p\u00fablica. Dentro de esta red privada, la comunicaci\u00f3n interna (por ejemplo, entre el scheduler de Airflow y los workers, o entre los workers y la base de datos de metadatos de Airflow) a\u00fan puede beneficiarse de TLS para mayor granularidad en la seguridad. Esto asegura que, incluso si un actor malicioso gana acceso a la red privada, los datos en tr\u00e1nsito siguen estando cifrados. Airflow en AWS : Desplegar un cluster de Airflow (usando EC2, ECS o EKS) dentro de una VPC. Configurar Security Groups y Network ACLs para restringir el tr\u00e1fico solo a puertos y or\u00edgenes necesarios. La base de datos de metadatos (RDS PostgreSQL/MySQL) debe configurarse para aceptar solo conexiones SSL/TLS, y el scheduler y los workers de Airflow deben configurarse para usar estas conexiones seguras. Airflow en Azure/GCP : Similarmente, utilizar Virtual Networks (Azure) o VPCs (GCP) con configuraciones de firewall y enrutamiento para aislar el entorno de Airflow. Los servicios de bases de datos gestionadas (Azure Database for PostgreSQL/MySQL, Cloud SQL) ofrecen conexiones SSL/TLS obligatorias o recomendadas. Kubernetes con mTLS : Si Airflow se ejecuta en Kubernetes, se puede implementar mTLS (mutual TLS) entre los diferentes pods (scheduler, workers, webserver) usando una Service Mesh como Istio, lo que proporciona encriptaci\u00f3n de extremo a extremo y autenticaci\u00f3n bidireccional. Encriptaci\u00f3n en Reposo (AES, KMS) La encriptaci\u00f3n en reposo se refiere a la protecci\u00f3n de los datos cuando est\u00e1n almacenados en dispositivos de almacenamiento persistente , como discos duros, bases de datos o servicios de almacenamiento en la nube. El objetivo es prevenir el acceso no autorizado a los datos si el almacenamiento f\u00edsico es comprometido. El cifrado sim\u00e9trico , particularmente el est\u00e1ndar AES (Advanced Encryption Standard) con una longitud de clave de 256 bits (AES-256) , es ampliamente utilizado por su robustez y eficiencia. En entornos de nube, los Servicios de Gesti\u00f3n de Claves (KMS - Key Management Service) son esenciales para gestionar de forma segura las claves de cifrado, rotarlas, auditar su uso y aplicar pol\u00edticas de acceso. Almacenamiento de archivos Parquet cifrados en S3 con claves gestionadas por AWS KMS : Amazon S3 es un almac\u00e9n de objetos altamente escalable y duradero. Almacenar datos en S3 requiere un cifrado en reposo para cumplir con los requisitos de seguridad y cumplimiento. AWS S3 ofrece varias opciones de cifrado en reposo. La m\u00e1s segura y recomendada es el cifrado del lado del servidor con claves administradas por AWS KMS (SSE-KMS) . Con SSE-KMS, S3 cifra los objetos usando una clave de datos \u00fanica para cada objeto, y esa clave de datos se cifra con una clave maestra de cliente (CMK) almacenada en KMS. KMS proporciona una gesti\u00f3n centralizada de las CMK, incluyendo la rotaci\u00f3n autom\u00e1tica, pol\u00edticas de uso detalladas y registros de auditor\u00eda de acceso a las claves. # PySpark y Boto3 para S3/KMS from pyspark.sql import SparkSession import boto3 spark = SparkSession.builder \\ .appName(\"EncryptedParquetS3\") \\ .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\ .getOrCreate() bucket_name = \"your-encrypted-data-bucket\" kms_key_arn = \"arn:aws:kms:your-region:your-account-id:key/your-kms-key-id\" output_path = f\"s3a://{bucket_name}/encrypted_data/my_encrypted_data.parquet\" # Crear un DataFrame de ejemplo data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)] columns = [\"name\", \"id\"] df = spark.createDataFrame(data, columns) # Escribir el DataFrame a S3, especificando la clave KMS para el cifrado # Spark, a trav\u00e9s de S3A, puede ser configurado para usar SSE-KMS df.write \\ .mode(\"overwrite\") \\ .option(\"sse.kms.keyId\", kms_key_arn) \\ .parquet(output_path) print(f\"Datos escritos cifrados en S3 en: {output_path}\") # Para verificar el cifrado en S3 (opcional, usando boto3) s3_client = boto3.client('s3') try: obj_head = s3_client.head_object(Bucket=bucket_name, Key=\"encrypted_data/my_encrypted_data.parquet/_SUCCESS\") # El archivo _SUCCESS indica el \u00e9xito de la escritura encryption_header = obj_head.get('ServerSideEncryption') kms_key_used = obj_head.get('SSEKMSKeyId') print(f\"Cifrado de objeto en S3: {encryption_header}\") print(f\"Clave KMS utilizada: {kms_key_used}\") except Exception as e: print(f\"Error al verificar el objeto en S3: {e}\") spark.stop() El rol de IAM debe tener permisos para S3 (s3:PutObject, s3:GetObject) y KMS (kms:GenerateDataKey, kms:Decrypt). Configuraci\u00f3n de HDFS para cifrar autom\u00e1ticamente bloques de datos con claves rotativas : Para entornos on-premise o h\u00edbridos que utilizan Apache Hadoop HDFS (Hadoop Distributed File System), el cifrado en reposo es igualmente crucial. HDFS soporta zonas de cifrado (Encryption Zones) , que son directorios en HDFS cuyo contenido se cifra autom\u00e1ticamente al ser escrito y se descifra al ser le\u00eddo. Cada zona de cifrado est\u00e1 asociada a una clave de zona (EZ Key) gestionada por un sistema de gesti\u00f3n de claves centralizado como Apache Ranger KMS o HashiCorp Vault . Cuando se escribe un archivo en una zona de cifrado, HDFS genera una clave de cifrado de datos (Data Encryption Key - DEK) para el archivo, cifra la DEK con la EZ Key, y almacena la DEK cifrada junto con el archivo. Los bloques de datos del archivo se cifran con la DEK. Esto permite la rotaci\u00f3n de claves y la revocaci\u00f3n granular. Configuraci\u00f3n (Conceptual en HDFS - No es c\u00f3digo ejecutable) : Configurar un KMS (Key Management Server) para HDFS: Esto implica configurar kms-site.xml en los nodos de HDFS y el KMS, apuntando a una base de datos segura para almacenar las claves. Crear una Clave de Zona (EZ Key) en el KMS: bash hdfs kms createKey my_encryption_zone_key Crear una Zona de Cifrado en HDFS: bash hdfs crypto createZone -path /user/encrypted_data -keyName my_encryption_zone_key Cualquier archivo escrito en /user/encrypted_data (o subdirectorios) se cifrar\u00e1 autom\u00e1ticamente utilizando my_encryption_zone_key . Los usuarios necesitar\u00e1n permisos para acceder a esta clave en el KMS para poder leer los datos. Uso de Google Cloud KMS para cifrar datasets en BigQuery : Google BigQuery es un data warehouse sin servidor altamente escalable. Aunque BigQuery cifra los datos en reposo por defecto (cifrado gestionado por Google), los usuarios pueden proporcionar sus propias claves de cifrado gestionadas por el cliente (CMEK) a trav\u00e9s de Google Cloud KMS para un control adicional. Con CMEK (Customer-Managed Encryption Keys) en BigQuery, t\u00fa controlas la clave de cifrado que se utiliza para proteger tus datos. BigQuery utiliza esta CMEK de Cloud KMS para cifrar la clave de cifrado de datos (DEK) del dataset, y esta DEK a su vez se usa para cifrar los datos reales. Esto te da control sobre la vida \u00fatil de la clave, su rotaci\u00f3n y los permisos de acceso a ella. Es \u00fatil para cumplir con requisitos de cumplimiento normativo espec\u00edficos o para a\u00f1adir una capa extra de seguridad. # usando Google Cloud Client Libraries from google.cloud import bigquery from google.cloud import kms_v1 # Configuraci\u00f3n de tu proyecto y clave KMS project_id = \"your-gcp-project-id\" dataset_id = \"your_encrypted_dataset\" table_id = \"your_encrypted_table\" # La clave KMS debe estar en la misma regi\u00f3n que tu dataset de BigQuery kms_key_name = \"projects/your-gcp-project-id/locations/your-region/keyRings/your-key-ring/cryptoKeys/your-crypto-key\" client = bigquery.Client(project=project_id) # 1. Crear o actualizar un Dataset con CMEK dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\") dataset.location = \"US\" # Aseg\u00farate de que la regi\u00f3n coincida con tu clave KMS # Asignar la clave KMS al dataset dataset.default_kms_key_name = kms_key_name try: dataset = client.create_dataset(dataset, timeout=30) print(f\"Dataset '{dataset_id}' creado/actualizado con CMEK.\") except Exception as e: print(f\"Error al crear/actualizar dataset (ya existe?): {e}\") dataset = client.get_dataset(dataset) # Si ya existe, obt\u00e9nlo para continuar # 2. Crear una Tabla dentro del Dataset con CMEK (hereda del dataset o se puede especificar) # Si el dataset tiene una CMEK, la tabla la heredar\u00e1 autom\u00e1ticamente. # Tambi\u00e9n puedes especificarla directamente para la tabla si no quieres heredarla. table_ref = dataset.table(table_id) schema = [ bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"), bigquery.SchemaField(\"age\", \"INTEGER\", mode=\"NULLABLE\"), ] table = bigquery.Table(table_ref, schema=schema) # Opcional: especificar CMEK directamente para la tabla si es diferente a la del dataset # table.encryption_configuration = bigquery.EncryptionConfiguration(kms_key_name=kms_key_name) try: table = client.create_table(table) print(f\"Tabla '{table_id}' creada con CMEK.\") except Exception as e: print(f\"Error al crear tabla (ya existe?): {e}\") # 3. Insertar datos en la tabla (ser\u00e1n cifrados autom\u00e1ticamente por BigQuery con la CMEK) rows_to_insert = [ {\"name\": \"Juan\", \"age\": 30}, {\"name\": \"Maria\", \"age\": 25}, ] errors = client.insert_rows_json(table, rows_to_insert) if errors: print(f\"Errores al insertar filas: {errors}\") else: print(f\"Filas insertadas en '{table_id}' (cifradas con CMEK).\") Aseg\u00farate de que la cuenta de servicio que ejecuta el c\u00f3digo tenga los roles roles/bigquery.dataEditor y roles/cloudkms.viewer para BigQuery, y roles/cloudkms.cryptoKeyEncrypterDecrypter para la clave KMS. 3.6.2 Gesti\u00f3n de secretos y credenciales La gesti\u00f3n segura de secretos y credenciales es un pilar fundamental en la arquitectura de datos, especialmente en entornos de procesamiento masivo. El uso de credenciales en texto plano o incrustadas directamente en el c\u00f3digo de scripts y aplicaciones presenta un riesgo de seguridad inaceptable. Este subtema aborda en detalle los mecanismos y las mejores pr\u00e1cticas para almacenar, distribuir y acceder de forma segura a informaci\u00f3n sensible, como contrase\u00f1as de bases de datos, tokens de API, claves de cifrado y certificados. El objetivo es asegurar que solo las entidades autorizadas tengan acceso a estos secretos, en el momento y lugar adecuados, minimizando la superficie de ataque y garantizando la integridad de los sistemas de datos. Gestores de secretos y credenciales Los gestores de secretos son herramientas especializadas que proporcionan una soluci\u00f3n centralizada y segura para almacenar, gestionar y distribuir secretos. Estos servicios no solo protegen la informaci\u00f3n sensible, sino que tambi\u00e9n ofrecen funcionalidades avanzadas como la rotaci\u00f3n autom\u00e1tica de credenciales, el control de versiones, la auditor\u00eda de accesos y la integraci\u00f3n con sistemas de gesti\u00f3n de identidades y accesos (IAM). Esto permite a los equipos de desarrollo y operaciones implementar un enfoque de \"confianza cero\" para las credenciales, donde el acceso se otorga solo cuando es estrictamente necesario y por un per\u00edodo limitado. Integraci\u00f3n de Apache Airflow con AWS Secrets Manager para acceder a credenciales de bases de datos. Apache Airflow es una plataforma robusta para orquestar flujos de trabajo de datos. En un entorno de producci\u00f3n, los Directed Acyclic Graphs (DAGs) de Airflow a menudo necesitan conectarse a diversas bases de datos, APIs o servicios externos. Almacenar las credenciales directamente en los archivos de configuraci\u00f3n de Airflow o en variables de entorno es una pr\u00e1ctica insegura. AWS Secrets Manager permite almacenar de forma segura estas credenciales y Airflow puede configurarse para recuperarlas din\u00e1micamente en tiempo de ejecuci\u00f3n, eliminando la necesidad de codificarlas en el DAG. Supongamos que tenemos una base de datos PostgreSQL cuyas credenciales (nombre de usuario y contrase\u00f1a) est\u00e1n almacenadas en AWS Secrets Manager bajo el nombre my_rds_db_credentials . En AWS Secrets Manager: Crear un secreto con un par clave-valor, por ejemplo: { \"username\": \"myuser\", \"password\": \"mYsEcUrEpAsSwOrD\" } Configuraci\u00f3n de Airflow: Se puede configurar Airflow para usar un SecretsBackend que se integre con AWS Secrets Manager. Esto generalmente se hace ajustando el airflow.cfg o mediante variables de entorno: [secrets] backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend backend_kwargs = {\"connections_prefix\": \"airflow/connections\", \"variables_prefix\": \"airflow/variables\"} En un DAG de Airflow: Una vez configurado el backend, Airflow puede resolver las conexiones y variables directamente desde Secrets Manager. Por ejemplo, si tu conexi\u00f3n a la base de datos se llama my_rds_conn en Airflow, y has almacenado la conexi\u00f3n en AWS Secrets Manager como airflow/connections/my_rds_conn , Airflow la resolver\u00e1 autom\u00e1ticamente. from airflow import DAG from airflow.providers.postgres.operators.postgres import PostgresOperator from datetime import datetime with DAG( dag_id='read_from_rds_secrets', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False ) as dag: read_data_task = PostgresOperator( task_id='read_data', postgres_conn_id='my_rds_conn', # Airflow buscar\u00e1 esta conexi\u00f3n en Secrets Manager sql=\"SELECT * FROM my_table LIMIT 10;\" ) Airflow recuperar\u00e1 las credenciales de my_rds_conn desde Secrets Manager, evitando que las credenciales sensibles sean expuestas en el c\u00f3digo del DAG o en la configuraci\u00f3n de Airflow. Uso de HashiCorp Vault para otorgar accesos temporales a procesos Spark en clusters Kubernetes. HashiCorp Vault es una herramienta de gesti\u00f3n de secretos robusta y altamente segura que permite almacenar, acceder y distribuir secretos de manera program\u00e1tica. En un entorno de microservicios o contenedores orquestados por Kubernetes, los trabajos de Spark a menudo necesitan acceder a recursos externos (como sistemas de almacenamiento, bases de datos o servicios de mensajer\u00eda). Vault puede generar credenciales temporales y de corta duraci\u00f3n para estos procesos, lo que reduce dr\u00e1sticamente el riesgo de exposici\u00f3n de credenciales a largo plazo. Esto es crucial en entornos din\u00e1micos donde los pods de Spark pueden ser ef\u00edmeros. Consideremos un trabajo de Spark que necesita acceder a un bucket de S3. En lugar de usar credenciales de S3 est\u00e1ticas, podemos configurar Vault para generar credenciales de AWS temporales que el trabajo de Spark pueda usar. Configuraci\u00f3n de Vault: Vault se configura con un secret engine para AWS que puede generar credenciales IAM temporales. vault secrets enable aws vault write aws/config/root \\ access_key=AKIA... \\ secret_key=... vault write aws/roles/spark-s3-access \\ credential_type=iam_user \\ policy_arns=arn:aws:iam::123456789012:policy/S3ReadOnlyAccess Pod de Spark en Kubernetes: El pod de Spark se configura para autenticarse con Vault (por ejemplo, usando Kubernetes Service Account Token authentication) y obtener las credenciales de AWS temporales. apiVersion: batch/v1 kind: Job metadata: name: spark-s3-job spec: template: spec: serviceAccountName: spark-sa containers: - name: spark-driver image: your-spark-image command: [\"/bin/bash\", \"-c\"] args: - | # Authenticate with Vault and get temporary AWS credentials VAULT_ADDR=\"http://vault.example.com:8200\" VAULT_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) RESPONSE=$(curl -s --request POST \\ --data \"{\\\"jwt\\\": \\\"$VAULT_TOKEN\\\", \\\"role\\\": \\\"spark-s3-access\\\"}\" \\ \"$VAULT_ADDR/v1/auth/kubernetes/login\") VAULT_CLIENT_TOKEN=$(echo $RESPONSE | jq -r .auth.client_token) AWS_CREDS_RESPONSE=$(curl -s --header \"X-Vault-Token: $VAULT_CLIENT_TOKEN\" \\ \"$VAULT_ADDR/v1/aws/creds/spark-s3-access\") export AWS_ACCESS_KEY_ID=$(echo $AWS_CREDS_RESPONSE | jq -r .data.access_key) export AWS_SECRET_ACCESS_KEY=$(echo $AWS_CREDS_RESPONSE | jq -r .data.secret_key) export AWS_SESSION_TOKEN=$(echo $AWS_CREDS_RESPONSE | jq -r .data.security_token) # Now run your Spark job using these temporary credentials /opt/spark/bin/spark-submit \\ --conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID \\ --conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY \\ --conf spark.hadoop.fs.s3a.session.token=$AWS_SESSION_TOKEN \\ your_spark_app.py volumeMounts: - name: vault-token mountPath: /var/run/secrets/kubernetes.io/serviceaccount volumes: - name: vault-token projected: sources: - serviceAccountToken: path: token restartPolicy: OnFailure Este enfoque asegura que las credenciales solo existen en la memoria del pod de Spark durante su ejecuci\u00f3n y son autom\u00e1ticamente revocadas por Vault despu\u00e9s de un per\u00edodo de tiempo definido. Enmascaramiento din\u00e1mico de variables sensibles en Databricks notebooks mediante Azure Key Vault. Databricks es una plataforma unificada de datos y AI que se utiliza ampliamente para el procesamiento de Big Data y el machine learning. Los notebooks de Databricks, escritos en lenguajes como Python, Scala o SQL, a menudo necesitan acceder a bases de datos, APIs o servicios de almacenamiento que requieren credenciales. Azure Key Vault es el servicio de gesti\u00f3n de secretos de Azure, que ofrece una forma segura de almacenar y acceder a secretos, claves y certificados. La integraci\u00f3n de Databricks con Azure Key Vault permite a los usuarios acceder a secretos de forma segura desde sus notebooks sin exponerlos directamente en el c\u00f3digo o en la interfaz del notebook. Imaginemos que necesitamos acceder a una base de datos SQL Server desde un notebook de Databricks y las credenciales est\u00e1n en Azure Key Vault. En Azure Key Vault: Crear un secreto llamado sql-server-password con la contrase\u00f1a de la base de datos. Configuraci\u00f3n de Databricks: En Databricks, se crea un Secret Scope respaldado por Azure Key Vault. Esto vincula un \u00e1mbito de secretos de Databricks con un Key Vault espec\u00edfico. # Usando la CLI de Databricks para crear un secret scope databricks secrets create-scope \\ --scope my-keyvault-scope \\ --scope-backend-type AZURE_KEYVAULT \\ --resource-id /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.KeyVault/vaults/<key-vault-name> \\ --dns-name https://<key-vault-name>.vault.azure.net/ En un Notebook de Databricks (Python): Una vez configurado el secret scope , puedes acceder a los secretos de forma segura desde tu notebook utilizando la utilidad dbutils.secrets . # Acceder a la contrase\u00f1a de SQL Server desde Azure Key Vault db_password = dbutils.secrets.get(scope=\"my-keyvault-scope\", key=\"sql-server-password\") db_user = \"your_sql_user\" # Este podr\u00eda venir de otro secreto o ser fijo db_host = \"your_sql_server.database.windows.net\" db_name = \"your_database_name\" # Construir la cadena de conexi\u00f3n de forma segura jdbc_url = f\"jdbc:sqlserver://{db_host}:1433;database={db_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\" # Leer datos de la base de datos usando Spark df = spark.read \\ .format(\"jdbc\") \\ .option(\"url\", jdbc_url) \\ .option(\"dbtable\", \"your_table\") \\ .option(\"user\", db_user) \\ .option(\"password\", db_password) \\ .load() df.display() dbutils.secrets.get() enmascara autom\u00e1ticamente el valor del secreto cuando se muestra en la salida del notebook, garantizando que la contrase\u00f1a nunca sea visible. Buenas pr\u00e1cticas en el uso de secretos La implementaci\u00f3n de gestores de secretos es solo la primera parte de una estrategia de seguridad integral. Es igualmente crucial adoptar buenas pr\u00e1cticas operativas que refuercen la postura de seguridad y mitiguen los riesgos asociados con el uso de credenciales. Estas pr\u00e1cticas se centran en el principio de \"menor privilegio\" y en la visibilidad completa sobre el ciclo de vida de los secretos. Configuraci\u00f3n de pol\u00edticas para que los secretos tengan vigencia limitada. Los secretos no deber\u00edan tener una vida \u00fatil ilimitada. Al implementar pol\u00edticas de rotaci\u00f3n peri\u00f3dica y vigencia limitada , se reduce significativamente la ventana de oportunidad para que un secreto comprometido sea explotado. La rotaci\u00f3n autom\u00e1tica es una caracter\u00edstica clave ofrecida por la mayor\u00eda de los gestores de secretos, que deber\u00eda ser aprovechada al m\u00e1ximo. AWS Secrets Manager : Puedes configurar una rotaci\u00f3n autom\u00e1tica para los secretos. Para una credencial de base de datos RDS, AWS Secrets Manager puede integrarse directamente con RDS para rotar la contrase\u00f1a cada X d\u00edas. # Fragmento de configuraci\u00f3n de rotaci\u00f3n en AWS Secrets Manager (v\u00eda SDK/CLI) { \"RotationRules\": { \"AutomaticallyRotateAfterDays\": 30, # Rotar cada 30 d\u00edas \"Duration\": \"30m\", # La rotaci\u00f3n debe completarse en 30 minutos \"ScheduleExpression\": \"cron(0 0 ? * MON *)\" # Rotar los lunes a medianoche }, \"RotationLambdaARN\": \"arn:aws:lambda:us-east-1:123456789012:function:SecretsManagerRDSMySQLRotationLambda\" } Esto asegura que, incluso si una credencial se ve comprometida, su validez es temporal, limitando el da\u00f1o potencial. HashiCorp Vault : Define leases o duraciones de vida para las credenciales generadas. vault write aws/roles/spark-s3-access \\ credential_type=iam_user \\ policy_arns=arn:aws:iam::123456789012:policy/S3ReadOnlyAccess \\ default_lease_ttl=\"1h\" \\ max_lease_ttl=\"24h\" Aqu\u00ed, las credenciales generadas por este rol expirar\u00e1n despu\u00e9s de 1 hora por defecto, y no podr\u00e1n ser usadas por m\u00e1s de 24 horas, forzando su re-obtenci\u00f3n. Restricci\u00f3n de acceso a claves solo a servicios autenticados. El principio de menor privilegio es fundamental. El acceso a los secretos debe ser granular, basado en roles y solo otorgado a las entidades (usuarios, servicios, aplicaciones) que lo necesitan expl\u00edcitamente para realizar sus funciones. Esto implica el uso de mecanismos de autenticaci\u00f3n robustos y pol\u00edticas de autorizaci\u00f3n detalladas en el gestor de secretos. Azure Key Vault : Uso de pol\u00edticas de acceso o Azure Role-Based Access Control (RBAC). # Pol\u00edtica de acceso en Azure Key Vault para una aplicaci\u00f3n/Service Principal { \"id\": \"/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.KeyVault/vaults/<key-vault-name>/accessPolicies/<policy-id>\", \"properties\": { \"objectId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", # Object ID de un Service Principal de Azure AD \"tenantId\": \"yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy\", # Tenant ID de Azure AD \"permissions\": { \"secrets\": [\"get\", \"list\"] # Solo permisos para obtener y listar secretos } } } Esta pol\u00edtica asegura que solo un Service Principal espec\u00edfico (que representa una aplicaci\u00f3n o servicio) puede recuperar y listar los secretos del Key Vault, y no tiene permisos para modificarlos o eliminarlos. AWS Secrets Manager : Integraci\u00f3n con AWS IAM. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:us-east-1:123456789012:secret:my_rds_db_credentials-*\" } ] } Esta pol\u00edtica IAM puede ser adjuntada a un rol que asuma una instancia EC2 o un pod de EKS, permitiendo que solo esa entidad espec\u00edfica acceda a un secreto particular en Secrets Manager. Log de accesos a secretos y alertas ante uso no autorizado. La auditor\u00eda es vital para la seguridad. Cada acceso a un secreto, ya sea exitoso o fallido, debe ser registrado. Estos registros, o logs de auditor\u00eda, son esenciales para el monitoreo de seguridad, la detecci\u00f3n de anomal\u00edas y la respuesta a incidentes. La integraci\u00f3n con sistemas de monitoreo y alerta permite una reacci\u00f3n r\u00e1pida ante cualquier intento de acceso no autorizado o un patr\u00f3n de uso sospechoso. AWS Secrets Manager con CloudTrail y CloudWatch : AWS CloudTrail registra todas las llamadas a la API de Secrets Manager. Estos eventos se pueden enviar a CloudWatch Logs, donde se pueden crear m\u00e9tricas y alarmas. # Ejemplo de filtro de m\u00e9trica en CloudWatch Logs para intentos fallidos de acceso a secretos { \"filterPattern\": \"{ ($.eventName = GetSecretValue || $.eventName = DescribeSecret) && $.errorCode = AccessDenied }\", \"metricTransformations\": [ { \"metricName\": \"SecretAccessDeniedCount\", \"metricNamespace\": \"SecretsManagerMetrics\", \"metricValue\": \"1\" } ] } Se puede configurar una alarma de CloudWatch que se active cuando la m\u00e9trica SecretAccessDeniedCount supere un umbral determinado, notificando al equipo de seguridad. HashiCorp Vault con Audit Devices : Vault tiene \"audit devices\" que registran todas las solicitudes y respuestas. vault audit enable file file_path=/var/log/vault_audit.log vault audit enable syslog Estos logs se pueden enviar a un sistema de gesti\u00f3n de eventos e informaci\u00f3n de seguridad (SIEM) como Splunk o ELK Stack para su an\u00e1lisis y correlaci\u00f3n con otros eventos de seguridad. 3.6.3 Control de Acceso Basado en Roles (RBAC) y Pol\u00edticas El control de acceso es fundamental para garantizar que solo las entidades autorizadas (usuarios o servicios) puedan interactuar con los pipelines ETL y sus datos subyacentes. Esto es crucial para la seguridad, la integridad y la confidencialidad de la informaci\u00f3n en entornos de Big Data. Implementaci\u00f3n de RBAC (Role-Based Access Control) El RBAC (Control de Acceso Basado en Roles) es un m\u00e9todo eficiente para gestionar permisos al agruparlos y asignarlos a roles espec\u00edficos (por ejemplo, Analista de Datos, Ingeniero de Datos, Administrador de Infraestructura). Esto simplifica la administraci\u00f3n de la seguridad y reduce el riesgo de errores. Definici\u00f3n de roles en Apache Airflow para limitar la ejecuci\u00f3n de DAGs : En Airflow, los roles se utilizan para controlar qu\u00e9 usuarios pueden ver, modificar o ejecutar ciertos DAGs (Directed Acyclic Graphs). Esto es vital para asegurar que solo el personal calificado pueda desplegar y operar pipelines de producci\u00f3n. Airflow proporciona una interfaz de usuario y una API para definir roles y asignar permisos a los usuarios. Los permisos pueden ser muy granulares, permitiendo controlar el acceso a DAGs espec\u00edficos, a vistas de la interfaz de usuario, o a acciones como pausar, reanudar o activar runs de DAGs. Al definir un rol, se especifican las acciones que los usuarios con ese rol pueden realizar sobre los recursos de Airflow. Rol Ingeniero de Datos : Tiene permisos para crear, modificar y ejecutar DAGs en entornos de desarrollo y pruebas. Puede ver el estado de todos los DAGs de producci\u00f3n, pero no puede modificarlos ni activarlos manualmente en producci\u00f3n. Rol Operaciones de Datos : Tiene permisos para monitorear y activar runs de DAGs en producci\u00f3n, as\u00ed como para gestionar el estado de los tasks . No puede modificar la definici\u00f3n de los DAGs. Rol Analista de Datos : Solo tiene permisos de visualizaci\u00f3n sobre el estado de los DAGs de producci\u00f3n relevantes para sus informes, sin capacidad de ejecuci\u00f3n o modificaci\u00f3n. Para configurar esto en Airflow, se usa la interfaz de administraci\u00f3n o, program\u00e1ticamente, se pueden usar las APIs de Airflow. Por ejemplo, para un DAG llamado data_ingestion_prod , se puede configurar que solo el rol \"Operaciones de Datos\" tenga permiso de can_dag_run sobre \u00e9l. Aplicaci\u00f3n de RBAC en sistemas de archivos distribuidos (como HDFS) para restringir lectura/escritura : En sistemas de almacenamiento distribuido como HDFS (Hadoop Distributed File System), la gesti\u00f3n de permisos es crucial para proteger los datos en reposo. El RBAC permite controlar qui\u00e9n puede leer, escribir o ejecutar archivos y directorios. HDFS utiliza un modelo de permisos similar al de sistemas de archivos POSIX, que incluye permisos de propietario, grupo y otros. Sin embargo, para entornos de Big Data, se suelen usar ACLs (Access Control Lists) para una gesti\u00f3n m\u00e1s granular. Las ACLs permiten asignar permisos espec\u00edficos a usuarios y grupos individuales m\u00e1s all\u00e1 de los permisos b\u00e1sicos de propietario y grupo. Supongamos un directorio en HDFS donde se almacenan datos de clientes sensibles: /user/raw_data/customer_info . Asignaci\u00f3n de permisos b\u00e1sicos: hdfs dfs -chown data_engineer:data_team /user/raw_data/customer_info hdfs dfs -chmod 750 /user/raw_data/customer_info Esto da al usuario data_engineer (propietario) permisos de lectura, escritura y ejecuci\u00f3n (7), al grupo data_team permisos de lectura y ejecuci\u00f3n (5), y a otros ning\u00fan permiso (0). Uso de ACLs para RBAC m\u00e1s granular: Si queremos que un usuario espec\u00edfico, audit_user , tenga solo permisos de lectura en ese directorio, sin ser parte del grupo data_team , podemos usar ACLs: hdfs dfs -setfacl -m user:audit_user:r-x /user/raw_data/customer_info Esto permite al audit_user leer y listar el contenido del directorio, pero no modificarlo. Por otro lado, si un rol \"Cient\u00edfico de Datos\" solo necesita leer datos agregados en /user/processed_data/agg_metrics , se le otorgar\u00eda acceso de lectura solo a ese directorio. Pol\u00edticas de Acceso y Segregaci\u00f3n de Entornos La seguridad se robustece significativamente al segregar los entornos (desarrollo, pruebas, producci\u00f3n) y establecer pol\u00edticas de acceso claras y estrictas para cada uno, controlando c\u00f3mo los datos y los pipelines fluyen entre ellos. Esto previene que cambios accidentales o maliciosos en desarrollo afecten la producci\u00f3n. Airflow con DAGs de producci\u00f3n ejecutables solo por usuarios con rol \"operaciones\" : Esta pol\u00edtica garantiza que el despliegue y la ejecuci\u00f3n de pipelines cr\u00edticos en producci\u00f3n est\u00e9n altamente controlados, reduciendo el riesgo de interrupciones o errores. En un entorno de CI/CD para Airflow, los DAGs de producci\u00f3n suelen ser desplegados autom\u00e1ticamente desde un repositorio de c\u00f3digo fuente (como Git) a un ambiente de producci\u00f3n de Airflow. Sin embargo, la activaci\u00f3n manual o la modificaci\u00f3n de estos DAGs en producci\u00f3n debe ser un privilegio restringido. Esto se logra configurando Airflow con roles espec\u00edficos que tienen permisos expl\u00edcitos para interactuar con DAGs de producci\u00f3n. Se define un rol produccion_airflow_operator que tiene permisos para can_dag_run , can_dag_edit , y can_dag_pause_unpause solo para DAGs que residen en el folder de producci\u00f3n de Airflow (por ejemplo, /opt/airflow/dags/prod_dags ). Los ingenieros de datos solo tienen un rol desarrollo_airflow_engineer que les permite operar en /opt/airflow/dags/dev_dags y /opt/airflow/dags/test_dags . Cuando se despliega un nuevo DAG a producci\u00f3n, solo los usuarios con el rol produccion_airflow_operator pueden activar sus runs iniciales o reiniciar runs fallidos, siguiendo un protocolo estricto de cambios controlados . Configuraci\u00f3n de entornos de Spark aislados por cluster o namespace : El aislamiento de entornos en Spark previene la interferencia entre cargas de trabajo de desarrollo, pruebas y producci\u00f3n, y asegura que los recursos computacionales y los datos se utilicen de manera segura y eficiente. En plataformas en la nube o en cl\u00fasteres locales, es una buena pr\u00e1ctica configurar cl\u00fasteres Spark o namespaces separados para cada entorno. Esto significa que un trabajo de desarrollo no puede acceder accidentalmente a los datos de producci\u00f3n o consumir sus recursos computacionales. Los permisos de acceso a los datos subyacentes (en S3, ADLS Gen2, GCS, HDFS, Snowflake, etc.) tambi\u00e9n se configuran para cada entorno. Databricks Workspaces : Se crean workspaces separados para desarrollo, staging y producci\u00f3n. Cada workspace tiene sus propios cl\u00fasteres Spark, cuadernos, tablas y configuraciones de seguridad. Los usuarios solo tienen acceso a su workspace asignado. Un ingeniero de datos que desarrolla en el workspace de \"Desarrollo\" no puede ver ni acceder a los datos o jobs en el workspace de \"Producci\u00f3n\". AWS EMR : Se lanzan cl\u00fasteres EMR dedicados para cada entorno. Por ejemplo, un cl\u00faster EMR \"dev-spark-cluster\" y un cl\u00faster \"prod-spark-cluster\". Los roles de IAM de AWS se utilizan para controlar qu\u00e9 cl\u00fasteres pueden lanzar los usuarios y qu\u00e9 buckets de S3 pueden leer o escribir cada cl\u00faster. # Pol\u00edtica IAM para el rol de producci\u00f3n de EMR { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::prod-data-lake/*\", \"arn:aws:s3:::prod-spark-logs/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"glue:GetTable\", \"glue:GetTables\", \"glue:GetDatabase\", \"glue:GetDatabases\" ], \"Resource\": \"*\" # Limitar esto a bases de datos de producci\u00f3n si es posible } ] } Esta pol\u00edtica permitir\u00eda a un cl\u00faster Spark de producci\u00f3n acceder solo a los buckets S3 designados para producci\u00f3n y al cat\u00e1logo de Glue Data Catalog relevante para producci\u00f3n. 3.6.4 Pr\u00e1cticas de auditor\u00eda y trazabilidad de operaciones Una arquitectura de datos robusta y segura no solo se enfoca en la protecci\u00f3n perimetral o el control de acceso; tambi\u00e9n requiere mecanismos sofisticados para rastrear qu\u00e9 sucedi\u00f3, qui\u00e9n lo hizo, cu\u00e1ndo y c\u00f3mo los datos se transformaron a lo largo de su ciclo de vida . Estas pr\u00e1cticas son cruciales no solo para la seguridad y la detecci\u00f3n de incidentes , sino tambi\u00e9n para el cumplimiento normativo, la depuraci\u00f3n de errores y la gobernanza de datos . Auditor\u00eda de ejecuciones y cambios La auditor\u00eda de ejecuciones y cambios implica el registro sistem\u00e1tico y centralizado de todas las actividades operacionales, modificaciones de configuraci\u00f3n y accesos a datos dentro de la plataforma Big Data . Este registro detallado es fundamental para la identificaci\u00f3n temprana de comportamientos an\u00f3malos, la investigaci\u00f3n forense de posibles brechas de seguridad, y la demostraci\u00f3n de cumplimiento frente a regulaciones como GDPR, HIPAA o SOX . Uso de Airflow Logs centralizados con integraci\u00f3n a herramientas como ELK o CloudWatch. Apache Airflow es una herramienta ampliamente utilizada para orquestar flujos de trabajo de datos. Cada ejecuci\u00f3n de una tarea (DAG Run) genera logs detallados que contienen informaci\u00f3n sobre el inicio, finalizaci\u00f3n, errores, salidas de los scripts y recursos utilizados. Para una auditor\u00eda efectiva, estos logs deben ser centralizados y persistidos fuera del entorno ef\u00edmero de Airflow workers . Herramientas como Elastic Stack (ELK: Elasticsearch, Logstash, Kibana) o Amazon CloudWatch (en AWS) permiten recolectar, indexar, buscar y visualizar estos logs de manera eficiente. Esto facilita la creaci\u00f3n de dashboards de monitoreo , la configuraci\u00f3n de alertas para fallos o accesos inusuales, y la generaci\u00f3n de reportes de auditor\u00eda . Imagina que tienes un DAG de Airflow ( data_ingestion_dag ) que ingesta datos sensibles. Si un usuario no autorizado intenta modificar este DAG o si una ejecuci\u00f3n falla repetidamente, necesitas ser notificado y tener un registro de estos eventos. Configuraci\u00f3n en Airflow : Aseg\u00farate de que Airflow est\u00e9 configurado para enviar sus logs a un sistema de log centralizado. Para AWS, esto podr\u00eda ser S3 y CloudWatch: # airflow.cfg o variables de entorno para Airflow # [logging] # remote_logging = True # remote_base_log_folder = s3://your-airflow-logs-bucket/ # remote_log_conn_id = aws_default_conn # S3_REMOTE_HANDLES = s3 Luego, puedes configurar CloudWatch Logs para ingerir logs desde S3 o directamente desde tus instancias EC2 donde corre Airflow. Consulta en Kibana (ejemplo para ELK) : Una vez que los logs est\u00e9n en Elasticsearch, puedes buscar eventos espec\u00edficos, por ejemplo, todas las ejecuciones fallidas de un DAG en particular o intentos de acceso a la interfaz de Airflow por usuarios espec\u00edficos. # Ejemplo de consulta en Kibana \"dag_id\": \"data_ingestion_dag\" AND \"status\": \"failed\" AND \"@timestamp\": [now-24h TO now] Alerta en CloudWatch (ejemplo para AWS) : Puedes crear una m\u00e9trica de log en CloudWatch que cuente los errores de Airflow y configure una alarma para notificarte. { \"logGroupNames\": [\"/aws/containerinsights/your-cluster-name/application\"], \"metricFilterPatterns\": [ \"{ $.logStreamName = \\\"/ecs/airflow-worker*\\\" && $.message like \\\"ERROR\\\" }\" ], \"metricTransformations\": [ { \"metricName\": \"AirflowErrors\", \"metricNamespace\": \"AirflowCustomMetrics\", \"metricValue\": \"1\" } ] } Esta configuraci\u00f3n te permitir\u00eda ver un aumento en los errores de Airflow y actuar proactivamente. Activaci\u00f3n de audit logs en Snowflake para rastrear consultas, cambios de roles o ingestas. Snowflake, como un Data Warehouse en la nube, ofrece capacidades de auditor\u00eda robustas a trav\u00e9s de sus vistas de Account Usage y la funci\u00f3n QUERY_HISTORY . Estas herramientas permiten a los administradores rastrear cada consulta ejecutada, los usuarios que las realizaron, el tiempo de ejecuci\u00f3n, la cantidad de datos procesados, los roles utilizados , y tambi\u00e9n cambios en la configuraci\u00f3n de seguridad (como la creaci\u00f3n o modificaci\u00f3n de usuarios y roles) o actividades de ingesta de datos (uso de COPY INTO) . La auditor\u00eda en Snowflake es fundamental para garantizar la seguridad de los datos, identificar patrones de uso inusuales, optimizar el rendimiento y cumplir con las pol\u00edticas de gobernanza de datos . Supongamos que necesitas saber qui\u00e9n accedi\u00f3 a una tabla cr\u00edtica de clientes en los \u00faltimos 7 d\u00edas o si alguien intent\u00f3 modificar los permisos de un usuario espec\u00edfico. Rastreo de consultas a una tabla espec\u00edfica : SELECT query_id, query_text, user_name, role_name, start_time, end_time, error_message FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE query_text ILIKE '%SELECT%FROM%customer_data%' -- Busca consultas que accedan a 'customer_data' AND start_time >= DATEADD(day, -7, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Auditor\u00eda de cambios de rol o usuario : Snowflake registra eventos de seguridad en la vista LOGIN_HISTORY y USERS para logins fallidos. Para cambios en roles y permisos, aunque no hay una vista ROLE_HISTORY directa, estos eventos se registran en QUERY_HISTORY si se ejecutan comandos DDL. SELECT query_id, query_text, user_name, start_time FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE query_type IN ('CREATE_ROLE', 'ALTER_ROLE', 'GRANT_ROLE', 'REVOKE_ROLE', 'CREATE_USER', 'ALTER_USER', 'DROP_USER') AND start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Monitoreo de ingestas de datos (COPY INTO) : SELECT query_id, query_text, user_name, start_time, rows_inserted, files_scanned FROM SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY WHERE table_name = 'YOUR_TARGET_TABLE' -- Reemplaza con tu tabla AND start_time >= DATEADD(day, -1, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Estas consultas permiten a los equipos de seguridad y operaciones mantener un registro inmutable de las actividades cruciales en Snowflake. Lineage y trazabilidad de datos El linaje de datos (data lineage) es la capacidad de mapear y comprender el recorrido de los datos desde su origen inicial hasta su destino final , incluyendo todas las transformaciones y movimientos intermedios. Es fundamental para la gobernanza de datos, la calidad de los datos, la depuraci\u00f3n, el an\u00e1lisis de impacto de cambios y el cumplimiento normativo . Saber de d\u00f3nde provienen los datos y c\u00f3mo se transformaron es clave para confiar en los resultados anal\u00edticos y asegurar la integridad de la informaci\u00f3n . Integraci\u00f3n de Apache Atlas con Spark para visualizar el lineage completo de columnas. Apache Atlas es una plataforma de gobernanza de datos y metadatos que proporciona un cat\u00e1logo de datos centralizado y capacidades de linaje . Cuando se integra con motores de procesamiento de datos como Apache Spark, Atlas puede interceptar y registrar las transformaciones de datos a nivel de columna . Esto significa que puedes visualizar no solo c\u00f3mo un conjunto de datos se deriva de otro, sino tambi\u00e9n c\u00f3mo cada columna individual en un conjunto de datos de salida se construye a partir de columnas espec\u00edficas en los conjuntos de datos de entrada. Esta granularidad es invaluable para rastrear el origen de un valor, entender el impacto de un cambio en una columna fuente, o identificar la causa ra\u00edz de un problema de calidad de datos. Imagina un proceso Spark que lee datos de ventas de una tabla, los limpia, agrega informaci\u00f3n de clientes de otra tabla y luego los escribe en una tabla de reportes. Proceso Spark (pseudoc\u00f3digo) : from pyspark.sql import SparkSession from pyspark.sql.functions import col, concat_ws spark = SparkSession.builder \\ .appName(\"SalesDataProcessing\") \\ .enableHiveSupport() \\ .getOrCreate() # Lectura de datos de ventas (origen 1) sales_df = spark.read.table(\"raw_data.sales\") # Lectura de datos de clientes (origen 2) customers_df = spark.read.table(\"raw_data.customers\") # Limpieza y transformaci\u00f3n de datos de ventas cleaned_sales_df = sales_df.filter(col(\"quantity\") > 0) \\ .withColumn(\"total_price\", col(\"quantity\") * col(\"unit_price\")) # Unir con datos de clientes enriched_sales_df = cleaned_sales_df.join(customers_df, \"customer_id\", \"inner\") \\ .select( col(\"sale_id\"), col(\"product_id\"), col(\"total_price\"), concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"customer_full_name\"), col(\"region\") ) # Escribir el resultado en una tabla de reportes enriched_sales_df.write.mode(\"overwrite\").saveAsTable(\"reporting.daily_sales_summary\") spark.stop() Visualizaci\u00f3n en Apache Atlas : Una vez que este trabajo Spark se ejecuta y Apache Atlas est\u00e1 configurado para escuchar eventos de Spark (a trav\u00e9s de hooks de Spark o un plugin), Atlas registrar\u00e1 autom\u00e1ticamente el linaje. En la interfaz de usuario de Atlas, podr\u00edas ver un gr\u00e1fico de linaje como este: raw_data.sales (tabla) --> cleaned_sales_df (dataframe Spark) --(Transformaci\u00f3n: total_price de quantity y unit_price )--> enriched_sales_df (dataframe Spark) --(Uni\u00f3n con raw_data.customers )--> reporting.daily_sales_summary (tabla) Y al profundizar, podr\u00edas ver el linaje a nivel de columna, por ejemplo: reporting.daily_sales_summary.total_price se deriva de raw_data.sales.quantity y raw_data.sales.unit_price . reporting.daily_sales_summary.customer_full_name se deriva de raw_data.customers.first_name y raw_data.customers.last_name . Aplicaci\u00f3n de etiquetas (tags) a datasets sensibles en Data Catalogs para trazabilidad y control. Un Cat\u00e1logo de Datos (Data Catalog) es una herramienta esencial para la gobernanza de datos que act\u00faa como un inventario centralizado de todos los activos de datos de una organizaci\u00f3n . Permite descubrir, comprender y gestionar los datos. Una funcionalidad clave es la capacidad de aplicar etiquetas (tags) o clasificaciones a los datasets, tablas, o incluso columnas, especialmente aquellos que contienen informaci\u00f3n sensible (PII, datos financieros, etc.) o que son cr\u00edticos para el negocio. Estas etiquetas no solo mejoran la trazabilidad al indicar la naturaleza y sensibilidad de los datos , sino que tambi\u00e9n pueden automatizar la aplicaci\u00f3n de pol\u00edticas de seguridad y cumplimiento . Por ejemplo, una etiqueta \"PII\" podr\u00eda desencadenar autom\u00e1ticamente reglas de enmascaramiento o acceso restringido. Supongamos que tienes un conjunto de datos en Google BigQuery que contiene informaci\u00f3n personal identificable de clientes. Identificaci\u00f3n y Etiquetado : Usar\u00edas tu Data Catalog (por ejemplo, Google Data Catalog, Collibra, Amundsen, etc.) para buscar la tabla project.dataset.customers . Una vez localizada, podr\u00edas aplicar etiquetas (tags) a la tabla completa o a columnas espec\u00edficas. Ejemplo de tags a nivel de tabla : Sensibilidad: Alto Clasificaci\u00f3n: PII Regulaci\u00f3n: GDPR Propietario: Equipo de Datos de Clientes Ejemplo de tags a nivel de columna : Columna email : Tipo: Contacto , Sensibilidad: PII-Directo Columna credit_card_number : Tipo: Financiero , Sensibilidad: Muy Alto , Regulaci\u00f3n: PCI-DSS Beneficios de la Trazabilidad con Tags : Descubrimiento : Cualquier analista que busque datos de clientes ver\u00e1 inmediatamente las etiquetas de sensibilidad, lo que le ayudar\u00e1 a decidir si puede usar esos datos y bajo qu\u00e9 condiciones. Cumplimiento : Las herramientas de gobernanza pueden usar estas etiquetas para generar reportes sobre la ubicaci\u00f3n de datos PII y demostrar el cumplimiento de GDPR. Automatizaci\u00f3n de Pol\u00edticas : En algunos cat\u00e1logos, estas etiquetas pueden integrarse con sistemas de seguridad para que, por ejemplo, el acceso a datos etiquetados como \"PII\" requiera una aprobaci\u00f3n adicional o se apliquen autom\u00e1ticamente funciones de enmascaramiento para usuarios no autorizados. An\u00e1lisis de Impacto : Si una columna etiquetada como \"PII\" se modifica o se elimina, el cat\u00e1logo puede mostrar qu\u00e9 otros conjuntos de datos o reportes dependen de ella, ayudando a predecir el impacto antes de realizar el cambio. Estas pr\u00e1cticas de auditor\u00eda y trazabilidad son los pilares para construir una arquitectura de datos no solo eficiente y escalable, sino tambi\u00e9n segura, transparente y conforme a las regulaciones . 3.6.5 Requerimientos normativos comunes: GDPR, HIPAA, etc. Los pipelines ETL (Extract, Transform, Load) no son solo herramientas t\u00e9cnicas; tambi\u00e9n deben dise\u00f1arse y operarse en estricto cumplimiento con una serie de regulaciones que protegen la privacidad y los derechos de los usuarios sobre sus datos personales . La omisi\u00f3n o el manejo inadecuado de estas normativas puede acarrear multas significativas, da\u00f1o reputacional y la p\u00e9rdida de confianza de los clientes. Es crucial que los arquitectos de datos y desarrolladores de pipelines entiendan la importancia de la privacidad desde el dise\u00f1o ( Privacy by Design ) y la seguridad desde el dise\u00f1o ( Security by Design ). Regulaciones aplicables al tratamiento de datos La naturaleza global del Big Data significa que las organizaciones a menudo deben cumplir con un mosaico de regulaciones dependiendo de la ubicaci\u00f3n de los datos, la residencia de los usuarios y el sector industrial. Algunas de las normativas m\u00e1s prominentes incluyen: Aplicaci\u00f3n de anonimizaci\u00f3n en pipelines que tratan datos personales de ciudadanos europeos (GDPR). El Reglamento General de Protecci\u00f3n de Datos (GDPR) de la Uni\u00f3n Europea es una de las normativas de privacidad de datos m\u00e1s estrictas a nivel mundial. Exige que los datos personales de los ciudadanos de la UE sean procesados de forma l\u00edcita, leal y transparente. Esto a menudo implica la anonimizaci\u00f3n o seudonimizaci\u00f3n de los datos para reducir el riesgo de identificaci\u00f3n directa de los individuos. Un pipeline que procesa datos de compras en l\u00ednea de clientes europeos debe anonimizar campos como nombre , direcci\u00f3n y email antes de almacenarlos en un data lake anal\u00edtico. Esto podr\u00eda implicar el uso de t\u00e9cnicas como el hashing irreversible para email o la generalizaci\u00f3n para c\u00f3digo_postal . # Ejemplo de seudonimizaci\u00f3n con hashing (Python + PySpark) from pyspark.sql.functions import sha2, concat_ws, lit # Suponiendo un DataFrame 'df_clientes_europeos' # con columnas 'nombre', 'apellido', 'email', 'direccion' df_anonimizado = df_clientes_europeos.withColumn( \"email_hash\", sha2(df_clientes_europeos[\"email\"], 256) # Hashing SHA-256 del email ).withColumn( \"nombre_seudonimo\", concat_ws(\"_\", lit(\"cliente\"), sha2(df_clientes_europeos[\"nombre\"], 256)) ).drop(\"email\", \"nombre\", \"apellido\", \"direccion\") # Eliminar columnas de identificaci\u00f3n directa df_anonimizado.show() Encriptaci\u00f3n obligatoria de historiales cl\u00ednicos en pipelines hospitalarios (HIPAA). La Ley de Portabilidad y Responsabilidad del Seguro M\u00e9dico (HIPAA) en Estados Unidos establece est\u00e1ndares nacionales para proteger la informaci\u00f3n de salud protegida ( PHI ). Cualquier pipeline que maneje datos de salud, como historiales m\u00e9dicos, resultados de laboratorio o informaci\u00f3n de seguros, debe asegurar que esta informaci\u00f3n est\u00e9 encriptada tanto en tr\u00e1nsito como en reposo . Un pipeline ETL que ingiere datos de sistemas EHR (Electronic Health Records) a un almac\u00e9n de datos para an\u00e1lisis de investigaci\u00f3n debe garantizar que los archivos de datos se encripten antes de ser subidos a un bucket de S3 o a Azure Blob Storage, y que la comunicaci\u00f3n con la base de datos de origen utilice SSL/TLS. # Ejemplo de encriptaci\u00f3n de archivos en reposo (AWS S3) # Al subir un archivo a S3, especificar encriptaci\u00f3n del lado del servidor aws s3 cp /ruta/a/historial_clinico.csv s3://mi-bucket-salud/data/ --sse AES256 # O si se usa un cliente de Python (boto3) import boto3 s3 = boto3.client('s3') bucket_name = 'mi-bucket-salud' file_path = '/ruta/a/historial_clinico.csv' object_name = 'data/historial_clinico.csv' s3.upload_file( file_path, bucket_name, object_name, ExtraArgs={ 'ServerSideEncryption': 'AES256' # Encriptaci\u00f3n del lado del servidor con AES-256 } ) Inclusi\u00f3n de consentimiento informado como metadato procesado en flujos de datos personales. Muchas regulaciones, como GDPR y LGPD (Brasil), requieren que el consentimiento del usuario para el procesamiento de sus datos personales sea expl\u00edcito, informado y revocable. Esto significa que los pipelines no solo deben procesar los datos, sino tambi\u00e9n la informaci\u00f3n sobre el consentimiento asociada a esos datos, lo que a menudo implica almacenar metadatos relacionados con la fecha de consentimiento, el tipo de consentimiento y la versi\u00f3n de la pol\u00edtica de privacidad aceptada. Un pipeline que recopila datos de comportamiento de usuario de una aplicaci\u00f3n m\u00f3vil debe adjuntar un ID_consentimiento y fecha_consentimiento a cada evento de datos, permitiendo as\u00ed filtrar y borrar datos en caso de que un usuario revoque su consentimiento. # Ejemplo de un registro de evento de datos con metadatos de consentimiento { \"user_id\": \"usuario_abc123\", \"event_type\": \"pagina_vista\", \"page_url\": \"/productos/item123\", \"timestamp\": \"2025-06-10T10:30:00Z\", \"consent_metadata\": { \"consent_id\": \"cons_xyz789\", \"consent_date\": \"2025-01-15T09:00:00Z\", \"policy_version\": \"2.1\", \"data_processing_purposes\": [\"analitica\", \"marketing_personalizado\"] } } Dise\u00f1o de pipelines \"compliance-ready\" Para cumplir con las normativas de forma proactiva y evitar problemas, es fundamental adoptar un enfoque de dise\u00f1o de pipelines \"compliance-ready\" o \"privacy-by-design\". Esto implica integrar consideraciones de cumplimiento normativo en cada etapa del ciclo de vida del desarrollo del pipeline, minimizando as\u00ed los riesgos legales y operativos. Creaci\u00f3n de m\u00f3dulos reutilizables para anonimizar, enmascarar o eliminar datos bajo demanda. La estandarizaci\u00f3n es clave para la consistencia y la eficiencia en el cumplimiento. Desarrollar un conjunto de funciones o microservicios reusables para las operaciones comunes de protecci\u00f3n de datos (anonimizaci\u00f3n, seudonimizaci\u00f3n, enmascaramiento, tokenizaci\u00f3n y borrado seguro) permite aplicarlas de manera uniforme en diferentes pipelines y fuentes de datos. Un equipo de datos podr\u00eda construir una librer\u00eda interna en Spark para ofuscar PII (Informaci\u00f3n de Identificaci\u00f3n Personal). # spark_data_masking_lib.py from pyspark.sql.functions import udf from pyspark.sql.types import StringType import hashlib @udf(returnType=StringType()) def hash_email(email): if email: return hashlib.sha256(email.lower().encode('utf-8')).hexdigest() return None @udf(returnType=StringType()) def mask_phone_number(phone): if phone and len(phone) > 4: return '*' * (len(phone) - 4) + phone[-4:] return phone # En un pipeline de Spark: # from spark_data_masking_lib import hash_email, mask_phone_number # df_raw.withColumn(\"hashed_email\", hash_email(\"email\")) \\ # .withColumn(\"masked_phone\", mask_phone_number(\"telefono\")) Implementaci\u00f3n de pol\u00edticas de retenci\u00f3n y borrado automatizado de datos personales. Las regulaciones suelen establecer l\u00edmites sobre cu\u00e1nto tiempo se pueden retener los datos personales. Los pipelines deben ser capaces de aplicar pol\u00edticas de retenci\u00f3n de datos que automaticen el borrado o la anonimizaci\u00f3n de datos que ya no son necesarios o para los que el consentimiento ha sido revocado. Esto podr\u00eda implicar el uso de funcionalidades de gesti\u00f3n del ciclo de vida (lifecycle management) en sistemas de almacenamiento en la nube o tareas programadas. Configurar reglas de ciclo de vida en un bucket de AWS S3 para que los objetos de m\u00e1s de 7 a\u00f1os sean eliminados autom\u00e1ticamente, o un trabajo de Airflow que ejecute un script de borrado l\u00f3gico en una tabla de Snowflake para registros con antig\u00fcedad superior a la pol\u00edtica de retenci\u00f3n. # Ejemplo de tarea de Airflow para borrado de datos antiguos (pseudoc\u00f3digo) from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='data_retention_policy_cleaner', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(days=7), # Ejecutar semanalmente catchup=False ) as dag: clean_old_customer_data = BashOperator( task_id='clean_customer_data_in_snowflake', bash_command=\"\"\" snowflake_conn_string=\"user={{ var.value.snowflake_user }} password={{ var.value.snowflake_password }} account={{ var.value.snowflake_account }}\" SNOWFLAKE_QUERY=\"DELETE FROM analytics_db.public.customer_data WHERE created_at < DATEADD(year, -7, CURRENT_DATE());\" snowsql -c \"$snowflake_conn_string\" -q \"$SNOWFLAKE_QUERY\" \"\"\" ) Inclusi\u00f3n de revisiones legales en el ciclo de vida del pipeline (Data Governance Boards). El cumplimiento normativo no es solo una preocupaci\u00f3n t\u00e9cnica, sino tambi\u00e9n legal y de gobernanza. Es fundamental establecer un marco de gobernanza de datos que incluya revisiones legales y de cumplimiento en las etapas clave del dise\u00f1o, desarrollo y despliegue de los pipelines. Un Data Governance Board (Junta de Gobernanza de Datos) o un Privacy Officer debe aprobar c\u00f3mo se procesan los datos personales, asegurando que las decisiones t\u00e9cnicas est\u00e9n alineadas con las pol\u00edticas corporativas y los requisitos legales. Antes de lanzar un nuevo pipeline de ingesta de datos de clientes para un nuevo servicio, el dise\u00f1o arquitect\u00f3nico y el plan de procesamiento de datos son revisados por el equipo legal y el oficial de privacidad de datos para asegurar el cumplimiento con GDPR, CCPA y cualquier otra regulaci\u00f3n relevante. Esto podr\u00eda implicar una reuni\u00f3n formal donde los ingenieros de datos presenten el flujo de datos, las medidas de seguridad y anonimizaci\u00f3n implementadas, y las pol\u00edticas de retenci\u00f3n, recibiendo la aprobaci\u00f3n formal del comit\u00e9. Tarea Desarrolla los siguientes ejercicios pr\u00e1cticos en tu entorno de laboratorio: Configura una conexi\u00f3n segura TLS entre Apache Spark y una base de datos PostgreSQL y documenta los pasos seguidos. Implementa el acceso a secretos mediante AWS Secrets Manager desde un DAG de Apache Airflow que conecta con un bucket de S3. Dise\u00f1a una pol\u00edtica de control de acceso por roles (RBAC) para un pipeline ETL en Databricks, diferenciando roles de analista, ingeniero y auditor. Audita un pipeline ejecutado en Airflow e identifica los registros clave de su trazabilidad y posibles fallas. Redise\u00f1a un pipeline para que cumpla con GDPR incorporando enmascaramiento de datos personales y l\u00f3gica de eliminaci\u00f3n bajo solicitud del usuario.","title":"Seguridad en ETL y Protecci\u00f3n de Datos"},{"location":"tema36/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema36/#tema-36-seguridad-en-etl-y-proteccion-de-datos","text":"Objetivo : Asegurar la confidencialidad, integridad y acceso controlado a la informaci\u00f3n dentro de los pipelines ETL, mediante t\u00e9cnicas de encriptaci\u00f3n, gesti\u00f3n segura de credenciales, pol\u00edticas de control de acceso, auditor\u00eda de operaciones y cumplimiento normativo. Introducci\u00f3n : Los flujos ETL manejan informaci\u00f3n sensible en distintos puntos del ciclo de vida de los datos: desde su extracci\u00f3n en fuentes externas hasta su carga en almacenes anal\u00edticos o lagos de datos. En este contexto, la seguridad no es opcional. Es imprescindible dise\u00f1ar procesos ETL que protejan los datos en todo momento, mitigando riesgos de fuga, alteraci\u00f3n o uso indebido. La seguridad en pipelines ETL debe considerarse desde el inicio del dise\u00f1o arquitect\u00f3nico y mantenerse en cada etapa del proceso. Desarrollo : Este tema abarca las pr\u00e1cticas y herramientas clave para garantizar la protecci\u00f3n de los datos en los pipelines ETL. Desde la encriptaci\u00f3n en tr\u00e1nsito y en reposo, pasando por el uso adecuado de gestores de secretos, hasta la implementaci\u00f3n de pol\u00edticas de control de acceso y auditor\u00eda, el objetivo es dotar a los estudiantes de una visi\u00f3n integral de seguridad. Adem\u00e1s, se introducen las principales normativas internacionales que regulan el tratamiento de datos, aportando un marco legal y \u00e9tico al dise\u00f1o de soluciones de datos.","title":"Tema 3.6. Seguridad en ETL y Protecci\u00f3n de Datos"},{"location":"tema36/#361-encriptacion-en-transito-tlsssl-y-en-reposo-aes-kms","text":"La encriptaci\u00f3n es una piedra angular en la seguridad de los datos , asegurando su confidencialidad e integridad tanto durante su movimiento entre sistemas (en tr\u00e1nsito) como cuando est\u00e1n almacenados (en reposo) . Este subtema explorar\u00e1 los mecanismos de cifrado modernos y su implementaci\u00f3n pr\u00e1ctica en el contexto de los pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) , vital para cualquier arquitectura de datos robusta.","title":"3.6.1 Encriptaci\u00f3n en Tr\u00e1nsito (TLS/SSL) y en Reposo (AES, KMS)"},{"location":"tema36/#encriptacion-en-transito-tlsssl","text":"La encriptaci\u00f3n en tr\u00e1nsito se enfoca en proteger los datos mientras viajan a trav\u00e9s de una red . Ya sea que los datos se extraigan de fuentes externas, se consuman a trav\u00e9s de APIs, o se escriban en sistemas de almacenamiento en red, el objetivo es evitar que actores maliciosos los intercepten o modifiquen. El protocolo TLS (Transport Layer Security) , sucesor de SSL, es el est\u00e1ndar de oro para lograr esta protecci\u00f3n. TLS establece un canal de comunicaci\u00f3n seguro entre dos aplicaciones, garantizando la autenticaci\u00f3n de los extremos , la confidencialidad de los datos (mediante cifrado) y la integridad de los datos (asegurando que no han sido alterados). Configuraci\u00f3n de Spark para leer desde una base de datos PostgreSQL usando conexiones TLS : Cuando Spark se conecta a una base de datos externa como PostgreSQL, es crucial que la comunicaci\u00f3n est\u00e9 cifrada. Esto protege credenciales y datos sensibles durante el proceso de extracci\u00f3n. Para habilitar TLS/SSL en una conexi\u00f3n JDBC de Spark a PostgreSQL, se deben configurar propiedades espec\u00edficas en la cadena de conexi\u00f3n. Estas propiedades instruyen al driver JDBC para que establezca una conexi\u00f3n segura, validando el certificado del servidor PostgreSQL. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SecurePostgreSQLRead\").getOrCreate() # Configuraci\u00f3n de la conexi\u00f3n JDBC segura a PostgreSQL jdbc_url = \"jdbc:postgresql://your_db_host:5432/your_database\" # Es crucial incluir 'ssl=true' y, opcionalmente, 'sslmode=require' o 'sslmode=verify-full' # para asegurar que el certificado del servidor sea validado. connection_properties = { \"user\": \"your_username\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\", \"ssl\": \"true\", \"sslmode\": \"require\" # 'require' fuerza SSL, 'verify-full' a\u00f1ade verificaci\u00f3n de certificado } # Leer datos de la tabla de PostgreSQL de forma segura df = spark.read.jdbc(url=jdbc_url, table=\"your_table\", properties=connection_properties) df.show() spark.stop() Para sslmode=verify-full , necesitar\u00e1s un TrustStore con el certificado de la CA que emiti\u00f3 el certificado de tu servidor PostgreSQL, o el propio certificado del servidor, y configurar las JVM options de Spark para que lo usen. Consumo de APIs REST que exigen HTTPS con certificados v\u00e1lidos : Muchas fuentes de datos externas se exponen a trav\u00e9s de APIs REST. Si estas APIs manejan informaci\u00f3n sensible, es imperativo que requieran HTTPS (HTTP sobre TLS/SSL) para garantizar la comunicaci\u00f3n segura. Al interactuar con una API HTTPS, el cliente (nuestro pipeline ETL) verifica el certificado SSL/TLS presentado por el servidor para asegurar su autenticidad y la validez de la conexi\u00f3n cifrada. Esto previene ataques \"Man-in-the-Middle\". Bibliotecas HTTP modernas manejan la validaci\u00f3n de certificados de forma predeterminada, pero es importante comprender c\u00f3mo funcionan y c\u00f3mo configurar excepciones o certificados personalizados si es necesario (aunque esto \u00faltimo debe hacerse con precauci\u00f3n). import requests import json api_url = \"https://api.example.com/data\" # URL que exige HTTPS headers = {\"Authorization\": \"Bearer your_api_token\"} try: # requests verifica certificados SSL/TLS por defecto response = requests.get(api_url, headers=headers, timeout=10) response.raise_for_status() # Lanza una excepci\u00f3n para c\u00f3digos de estado HTTP err\u00f3neos (4xx o 5xx) data = response.json() print(\"Datos recibidos de la API de forma segura:\") print(json.dumps(data, indent=2)) except requests.exceptions.SSLError as e: print(f\"Error SSL: Problema con el certificado TLS/SSL. {e}\") except requests.exceptions.ConnectionError as e: print(f\"Error de conexi\u00f3n: No se pudo conectar a la API. {e}\") except requests.exceptions.Timeout as e: print(f\"Error de tiempo de espera: La solicitud a la API tard\u00f3 demasiado. {e}\") except requests.exceptions.RequestException as e: print(f\"Error general de la solicitud: {e}\") Comunicaci\u00f3n segura entre tareas de Airflow mediante redes privadas virtuales y TLS : En entornos de orquestaci\u00f3n como Apache Airflow, las tareas a menudo se ejecutan en diferentes nodos o incluso en servicios en la nube. Proteger la comunicaci\u00f3n entre el programador, los workers y las bases de datos de metadatos es vital. Una estrategia com\u00fan es desplegar Airflow dentro de una red privada virtual (VPN) o una Virtual Private Cloud (VPC) en la nube, lo que segmenta el tr\u00e1fico de red y lo a\u00edsla de la internet p\u00fablica. Dentro de esta red privada, la comunicaci\u00f3n interna (por ejemplo, entre el scheduler de Airflow y los workers, o entre los workers y la base de datos de metadatos de Airflow) a\u00fan puede beneficiarse de TLS para mayor granularidad en la seguridad. Esto asegura que, incluso si un actor malicioso gana acceso a la red privada, los datos en tr\u00e1nsito siguen estando cifrados. Airflow en AWS : Desplegar un cluster de Airflow (usando EC2, ECS o EKS) dentro de una VPC. Configurar Security Groups y Network ACLs para restringir el tr\u00e1fico solo a puertos y or\u00edgenes necesarios. La base de datos de metadatos (RDS PostgreSQL/MySQL) debe configurarse para aceptar solo conexiones SSL/TLS, y el scheduler y los workers de Airflow deben configurarse para usar estas conexiones seguras. Airflow en Azure/GCP : Similarmente, utilizar Virtual Networks (Azure) o VPCs (GCP) con configuraciones de firewall y enrutamiento para aislar el entorno de Airflow. Los servicios de bases de datos gestionadas (Azure Database for PostgreSQL/MySQL, Cloud SQL) ofrecen conexiones SSL/TLS obligatorias o recomendadas. Kubernetes con mTLS : Si Airflow se ejecuta en Kubernetes, se puede implementar mTLS (mutual TLS) entre los diferentes pods (scheduler, workers, webserver) usando una Service Mesh como Istio, lo que proporciona encriptaci\u00f3n de extremo a extremo y autenticaci\u00f3n bidireccional.","title":"Encriptaci\u00f3n en Tr\u00e1nsito (TLS/SSL)"},{"location":"tema36/#encriptacion-en-reposo-aes-kms","text":"La encriptaci\u00f3n en reposo se refiere a la protecci\u00f3n de los datos cuando est\u00e1n almacenados en dispositivos de almacenamiento persistente , como discos duros, bases de datos o servicios de almacenamiento en la nube. El objetivo es prevenir el acceso no autorizado a los datos si el almacenamiento f\u00edsico es comprometido. El cifrado sim\u00e9trico , particularmente el est\u00e1ndar AES (Advanced Encryption Standard) con una longitud de clave de 256 bits (AES-256) , es ampliamente utilizado por su robustez y eficiencia. En entornos de nube, los Servicios de Gesti\u00f3n de Claves (KMS - Key Management Service) son esenciales para gestionar de forma segura las claves de cifrado, rotarlas, auditar su uso y aplicar pol\u00edticas de acceso. Almacenamiento de archivos Parquet cifrados en S3 con claves gestionadas por AWS KMS : Amazon S3 es un almac\u00e9n de objetos altamente escalable y duradero. Almacenar datos en S3 requiere un cifrado en reposo para cumplir con los requisitos de seguridad y cumplimiento. AWS S3 ofrece varias opciones de cifrado en reposo. La m\u00e1s segura y recomendada es el cifrado del lado del servidor con claves administradas por AWS KMS (SSE-KMS) . Con SSE-KMS, S3 cifra los objetos usando una clave de datos \u00fanica para cada objeto, y esa clave de datos se cifra con una clave maestra de cliente (CMK) almacenada en KMS. KMS proporciona una gesti\u00f3n centralizada de las CMK, incluyendo la rotaci\u00f3n autom\u00e1tica, pol\u00edticas de uso detalladas y registros de auditor\u00eda de acceso a las claves. # PySpark y Boto3 para S3/KMS from pyspark.sql import SparkSession import boto3 spark = SparkSession.builder \\ .appName(\"EncryptedParquetS3\") \\ .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\ .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\ .getOrCreate() bucket_name = \"your-encrypted-data-bucket\" kms_key_arn = \"arn:aws:kms:your-region:your-account-id:key/your-kms-key-id\" output_path = f\"s3a://{bucket_name}/encrypted_data/my_encrypted_data.parquet\" # Crear un DataFrame de ejemplo data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)] columns = [\"name\", \"id\"] df = spark.createDataFrame(data, columns) # Escribir el DataFrame a S3, especificando la clave KMS para el cifrado # Spark, a trav\u00e9s de S3A, puede ser configurado para usar SSE-KMS df.write \\ .mode(\"overwrite\") \\ .option(\"sse.kms.keyId\", kms_key_arn) \\ .parquet(output_path) print(f\"Datos escritos cifrados en S3 en: {output_path}\") # Para verificar el cifrado en S3 (opcional, usando boto3) s3_client = boto3.client('s3') try: obj_head = s3_client.head_object(Bucket=bucket_name, Key=\"encrypted_data/my_encrypted_data.parquet/_SUCCESS\") # El archivo _SUCCESS indica el \u00e9xito de la escritura encryption_header = obj_head.get('ServerSideEncryption') kms_key_used = obj_head.get('SSEKMSKeyId') print(f\"Cifrado de objeto en S3: {encryption_header}\") print(f\"Clave KMS utilizada: {kms_key_used}\") except Exception as e: print(f\"Error al verificar el objeto en S3: {e}\") spark.stop() El rol de IAM debe tener permisos para S3 (s3:PutObject, s3:GetObject) y KMS (kms:GenerateDataKey, kms:Decrypt). Configuraci\u00f3n de HDFS para cifrar autom\u00e1ticamente bloques de datos con claves rotativas : Para entornos on-premise o h\u00edbridos que utilizan Apache Hadoop HDFS (Hadoop Distributed File System), el cifrado en reposo es igualmente crucial. HDFS soporta zonas de cifrado (Encryption Zones) , que son directorios en HDFS cuyo contenido se cifra autom\u00e1ticamente al ser escrito y se descifra al ser le\u00eddo. Cada zona de cifrado est\u00e1 asociada a una clave de zona (EZ Key) gestionada por un sistema de gesti\u00f3n de claves centralizado como Apache Ranger KMS o HashiCorp Vault . Cuando se escribe un archivo en una zona de cifrado, HDFS genera una clave de cifrado de datos (Data Encryption Key - DEK) para el archivo, cifra la DEK con la EZ Key, y almacena la DEK cifrada junto con el archivo. Los bloques de datos del archivo se cifran con la DEK. Esto permite la rotaci\u00f3n de claves y la revocaci\u00f3n granular. Configuraci\u00f3n (Conceptual en HDFS - No es c\u00f3digo ejecutable) : Configurar un KMS (Key Management Server) para HDFS: Esto implica configurar kms-site.xml en los nodos de HDFS y el KMS, apuntando a una base de datos segura para almacenar las claves. Crear una Clave de Zona (EZ Key) en el KMS: bash hdfs kms createKey my_encryption_zone_key Crear una Zona de Cifrado en HDFS: bash hdfs crypto createZone -path /user/encrypted_data -keyName my_encryption_zone_key Cualquier archivo escrito en /user/encrypted_data (o subdirectorios) se cifrar\u00e1 autom\u00e1ticamente utilizando my_encryption_zone_key . Los usuarios necesitar\u00e1n permisos para acceder a esta clave en el KMS para poder leer los datos. Uso de Google Cloud KMS para cifrar datasets en BigQuery : Google BigQuery es un data warehouse sin servidor altamente escalable. Aunque BigQuery cifra los datos en reposo por defecto (cifrado gestionado por Google), los usuarios pueden proporcionar sus propias claves de cifrado gestionadas por el cliente (CMEK) a trav\u00e9s de Google Cloud KMS para un control adicional. Con CMEK (Customer-Managed Encryption Keys) en BigQuery, t\u00fa controlas la clave de cifrado que se utiliza para proteger tus datos. BigQuery utiliza esta CMEK de Cloud KMS para cifrar la clave de cifrado de datos (DEK) del dataset, y esta DEK a su vez se usa para cifrar los datos reales. Esto te da control sobre la vida \u00fatil de la clave, su rotaci\u00f3n y los permisos de acceso a ella. Es \u00fatil para cumplir con requisitos de cumplimiento normativo espec\u00edficos o para a\u00f1adir una capa extra de seguridad. # usando Google Cloud Client Libraries from google.cloud import bigquery from google.cloud import kms_v1 # Configuraci\u00f3n de tu proyecto y clave KMS project_id = \"your-gcp-project-id\" dataset_id = \"your_encrypted_dataset\" table_id = \"your_encrypted_table\" # La clave KMS debe estar en la misma regi\u00f3n que tu dataset de BigQuery kms_key_name = \"projects/your-gcp-project-id/locations/your-region/keyRings/your-key-ring/cryptoKeys/your-crypto-key\" client = bigquery.Client(project=project_id) # 1. Crear o actualizar un Dataset con CMEK dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\") dataset.location = \"US\" # Aseg\u00farate de que la regi\u00f3n coincida con tu clave KMS # Asignar la clave KMS al dataset dataset.default_kms_key_name = kms_key_name try: dataset = client.create_dataset(dataset, timeout=30) print(f\"Dataset '{dataset_id}' creado/actualizado con CMEK.\") except Exception as e: print(f\"Error al crear/actualizar dataset (ya existe?): {e}\") dataset = client.get_dataset(dataset) # Si ya existe, obt\u00e9nlo para continuar # 2. Crear una Tabla dentro del Dataset con CMEK (hereda del dataset o se puede especificar) # Si el dataset tiene una CMEK, la tabla la heredar\u00e1 autom\u00e1ticamente. # Tambi\u00e9n puedes especificarla directamente para la tabla si no quieres heredarla. table_ref = dataset.table(table_id) schema = [ bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"), bigquery.SchemaField(\"age\", \"INTEGER\", mode=\"NULLABLE\"), ] table = bigquery.Table(table_ref, schema=schema) # Opcional: especificar CMEK directamente para la tabla si es diferente a la del dataset # table.encryption_configuration = bigquery.EncryptionConfiguration(kms_key_name=kms_key_name) try: table = client.create_table(table) print(f\"Tabla '{table_id}' creada con CMEK.\") except Exception as e: print(f\"Error al crear tabla (ya existe?): {e}\") # 3. Insertar datos en la tabla (ser\u00e1n cifrados autom\u00e1ticamente por BigQuery con la CMEK) rows_to_insert = [ {\"name\": \"Juan\", \"age\": 30}, {\"name\": \"Maria\", \"age\": 25}, ] errors = client.insert_rows_json(table, rows_to_insert) if errors: print(f\"Errores al insertar filas: {errors}\") else: print(f\"Filas insertadas en '{table_id}' (cifradas con CMEK).\") Aseg\u00farate de que la cuenta de servicio que ejecuta el c\u00f3digo tenga los roles roles/bigquery.dataEditor y roles/cloudkms.viewer para BigQuery, y roles/cloudkms.cryptoKeyEncrypterDecrypter para la clave KMS.","title":"Encriptaci\u00f3n en Reposo (AES, KMS)"},{"location":"tema36/#362-gestion-de-secretos-y-credenciales","text":"La gesti\u00f3n segura de secretos y credenciales es un pilar fundamental en la arquitectura de datos, especialmente en entornos de procesamiento masivo. El uso de credenciales en texto plano o incrustadas directamente en el c\u00f3digo de scripts y aplicaciones presenta un riesgo de seguridad inaceptable. Este subtema aborda en detalle los mecanismos y las mejores pr\u00e1cticas para almacenar, distribuir y acceder de forma segura a informaci\u00f3n sensible, como contrase\u00f1as de bases de datos, tokens de API, claves de cifrado y certificados. El objetivo es asegurar que solo las entidades autorizadas tengan acceso a estos secretos, en el momento y lugar adecuados, minimizando la superficie de ataque y garantizando la integridad de los sistemas de datos.","title":"3.6.2 Gesti\u00f3n de secretos y credenciales"},{"location":"tema36/#gestores-de-secretos-y-credenciales","text":"Los gestores de secretos son herramientas especializadas que proporcionan una soluci\u00f3n centralizada y segura para almacenar, gestionar y distribuir secretos. Estos servicios no solo protegen la informaci\u00f3n sensible, sino que tambi\u00e9n ofrecen funcionalidades avanzadas como la rotaci\u00f3n autom\u00e1tica de credenciales, el control de versiones, la auditor\u00eda de accesos y la integraci\u00f3n con sistemas de gesti\u00f3n de identidades y accesos (IAM). Esto permite a los equipos de desarrollo y operaciones implementar un enfoque de \"confianza cero\" para las credenciales, donde el acceso se otorga solo cuando es estrictamente necesario y por un per\u00edodo limitado. Integraci\u00f3n de Apache Airflow con AWS Secrets Manager para acceder a credenciales de bases de datos. Apache Airflow es una plataforma robusta para orquestar flujos de trabajo de datos. En un entorno de producci\u00f3n, los Directed Acyclic Graphs (DAGs) de Airflow a menudo necesitan conectarse a diversas bases de datos, APIs o servicios externos. Almacenar las credenciales directamente en los archivos de configuraci\u00f3n de Airflow o en variables de entorno es una pr\u00e1ctica insegura. AWS Secrets Manager permite almacenar de forma segura estas credenciales y Airflow puede configurarse para recuperarlas din\u00e1micamente en tiempo de ejecuci\u00f3n, eliminando la necesidad de codificarlas en el DAG. Supongamos que tenemos una base de datos PostgreSQL cuyas credenciales (nombre de usuario y contrase\u00f1a) est\u00e1n almacenadas en AWS Secrets Manager bajo el nombre my_rds_db_credentials . En AWS Secrets Manager: Crear un secreto con un par clave-valor, por ejemplo: { \"username\": \"myuser\", \"password\": \"mYsEcUrEpAsSwOrD\" } Configuraci\u00f3n de Airflow: Se puede configurar Airflow para usar un SecretsBackend que se integre con AWS Secrets Manager. Esto generalmente se hace ajustando el airflow.cfg o mediante variables de entorno: [secrets] backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend backend_kwargs = {\"connections_prefix\": \"airflow/connections\", \"variables_prefix\": \"airflow/variables\"} En un DAG de Airflow: Una vez configurado el backend, Airflow puede resolver las conexiones y variables directamente desde Secrets Manager. Por ejemplo, si tu conexi\u00f3n a la base de datos se llama my_rds_conn en Airflow, y has almacenado la conexi\u00f3n en AWS Secrets Manager como airflow/connections/my_rds_conn , Airflow la resolver\u00e1 autom\u00e1ticamente. from airflow import DAG from airflow.providers.postgres.operators.postgres import PostgresOperator from datetime import datetime with DAG( dag_id='read_from_rds_secrets', start_date=datetime(2023, 1, 1), schedule_interval=None, catchup=False ) as dag: read_data_task = PostgresOperator( task_id='read_data', postgres_conn_id='my_rds_conn', # Airflow buscar\u00e1 esta conexi\u00f3n en Secrets Manager sql=\"SELECT * FROM my_table LIMIT 10;\" ) Airflow recuperar\u00e1 las credenciales de my_rds_conn desde Secrets Manager, evitando que las credenciales sensibles sean expuestas en el c\u00f3digo del DAG o en la configuraci\u00f3n de Airflow. Uso de HashiCorp Vault para otorgar accesos temporales a procesos Spark en clusters Kubernetes. HashiCorp Vault es una herramienta de gesti\u00f3n de secretos robusta y altamente segura que permite almacenar, acceder y distribuir secretos de manera program\u00e1tica. En un entorno de microservicios o contenedores orquestados por Kubernetes, los trabajos de Spark a menudo necesitan acceder a recursos externos (como sistemas de almacenamiento, bases de datos o servicios de mensajer\u00eda). Vault puede generar credenciales temporales y de corta duraci\u00f3n para estos procesos, lo que reduce dr\u00e1sticamente el riesgo de exposici\u00f3n de credenciales a largo plazo. Esto es crucial en entornos din\u00e1micos donde los pods de Spark pueden ser ef\u00edmeros. Consideremos un trabajo de Spark que necesita acceder a un bucket de S3. En lugar de usar credenciales de S3 est\u00e1ticas, podemos configurar Vault para generar credenciales de AWS temporales que el trabajo de Spark pueda usar. Configuraci\u00f3n de Vault: Vault se configura con un secret engine para AWS que puede generar credenciales IAM temporales. vault secrets enable aws vault write aws/config/root \\ access_key=AKIA... \\ secret_key=... vault write aws/roles/spark-s3-access \\ credential_type=iam_user \\ policy_arns=arn:aws:iam::123456789012:policy/S3ReadOnlyAccess Pod de Spark en Kubernetes: El pod de Spark se configura para autenticarse con Vault (por ejemplo, usando Kubernetes Service Account Token authentication) y obtener las credenciales de AWS temporales. apiVersion: batch/v1 kind: Job metadata: name: spark-s3-job spec: template: spec: serviceAccountName: spark-sa containers: - name: spark-driver image: your-spark-image command: [\"/bin/bash\", \"-c\"] args: - | # Authenticate with Vault and get temporary AWS credentials VAULT_ADDR=\"http://vault.example.com:8200\" VAULT_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) RESPONSE=$(curl -s --request POST \\ --data \"{\\\"jwt\\\": \\\"$VAULT_TOKEN\\\", \\\"role\\\": \\\"spark-s3-access\\\"}\" \\ \"$VAULT_ADDR/v1/auth/kubernetes/login\") VAULT_CLIENT_TOKEN=$(echo $RESPONSE | jq -r .auth.client_token) AWS_CREDS_RESPONSE=$(curl -s --header \"X-Vault-Token: $VAULT_CLIENT_TOKEN\" \\ \"$VAULT_ADDR/v1/aws/creds/spark-s3-access\") export AWS_ACCESS_KEY_ID=$(echo $AWS_CREDS_RESPONSE | jq -r .data.access_key) export AWS_SECRET_ACCESS_KEY=$(echo $AWS_CREDS_RESPONSE | jq -r .data.secret_key) export AWS_SESSION_TOKEN=$(echo $AWS_CREDS_RESPONSE | jq -r .data.security_token) # Now run your Spark job using these temporary credentials /opt/spark/bin/spark-submit \\ --conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID \\ --conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY \\ --conf spark.hadoop.fs.s3a.session.token=$AWS_SESSION_TOKEN \\ your_spark_app.py volumeMounts: - name: vault-token mountPath: /var/run/secrets/kubernetes.io/serviceaccount volumes: - name: vault-token projected: sources: - serviceAccountToken: path: token restartPolicy: OnFailure Este enfoque asegura que las credenciales solo existen en la memoria del pod de Spark durante su ejecuci\u00f3n y son autom\u00e1ticamente revocadas por Vault despu\u00e9s de un per\u00edodo de tiempo definido. Enmascaramiento din\u00e1mico de variables sensibles en Databricks notebooks mediante Azure Key Vault. Databricks es una plataforma unificada de datos y AI que se utiliza ampliamente para el procesamiento de Big Data y el machine learning. Los notebooks de Databricks, escritos en lenguajes como Python, Scala o SQL, a menudo necesitan acceder a bases de datos, APIs o servicios de almacenamiento que requieren credenciales. Azure Key Vault es el servicio de gesti\u00f3n de secretos de Azure, que ofrece una forma segura de almacenar y acceder a secretos, claves y certificados. La integraci\u00f3n de Databricks con Azure Key Vault permite a los usuarios acceder a secretos de forma segura desde sus notebooks sin exponerlos directamente en el c\u00f3digo o en la interfaz del notebook. Imaginemos que necesitamos acceder a una base de datos SQL Server desde un notebook de Databricks y las credenciales est\u00e1n en Azure Key Vault. En Azure Key Vault: Crear un secreto llamado sql-server-password con la contrase\u00f1a de la base de datos. Configuraci\u00f3n de Databricks: En Databricks, se crea un Secret Scope respaldado por Azure Key Vault. Esto vincula un \u00e1mbito de secretos de Databricks con un Key Vault espec\u00edfico. # Usando la CLI de Databricks para crear un secret scope databricks secrets create-scope \\ --scope my-keyvault-scope \\ --scope-backend-type AZURE_KEYVAULT \\ --resource-id /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.KeyVault/vaults/<key-vault-name> \\ --dns-name https://<key-vault-name>.vault.azure.net/ En un Notebook de Databricks (Python): Una vez configurado el secret scope , puedes acceder a los secretos de forma segura desde tu notebook utilizando la utilidad dbutils.secrets . # Acceder a la contrase\u00f1a de SQL Server desde Azure Key Vault db_password = dbutils.secrets.get(scope=\"my-keyvault-scope\", key=\"sql-server-password\") db_user = \"your_sql_user\" # Este podr\u00eda venir de otro secreto o ser fijo db_host = \"your_sql_server.database.windows.net\" db_name = \"your_database_name\" # Construir la cadena de conexi\u00f3n de forma segura jdbc_url = f\"jdbc:sqlserver://{db_host}:1433;database={db_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\" # Leer datos de la base de datos usando Spark df = spark.read \\ .format(\"jdbc\") \\ .option(\"url\", jdbc_url) \\ .option(\"dbtable\", \"your_table\") \\ .option(\"user\", db_user) \\ .option(\"password\", db_password) \\ .load() df.display() dbutils.secrets.get() enmascara autom\u00e1ticamente el valor del secreto cuando se muestra en la salida del notebook, garantizando que la contrase\u00f1a nunca sea visible.","title":"Gestores de secretos y credenciales"},{"location":"tema36/#buenas-practicas-en-el-uso-de-secretos","text":"La implementaci\u00f3n de gestores de secretos es solo la primera parte de una estrategia de seguridad integral. Es igualmente crucial adoptar buenas pr\u00e1cticas operativas que refuercen la postura de seguridad y mitiguen los riesgos asociados con el uso de credenciales. Estas pr\u00e1cticas se centran en el principio de \"menor privilegio\" y en la visibilidad completa sobre el ciclo de vida de los secretos. Configuraci\u00f3n de pol\u00edticas para que los secretos tengan vigencia limitada. Los secretos no deber\u00edan tener una vida \u00fatil ilimitada. Al implementar pol\u00edticas de rotaci\u00f3n peri\u00f3dica y vigencia limitada , se reduce significativamente la ventana de oportunidad para que un secreto comprometido sea explotado. La rotaci\u00f3n autom\u00e1tica es una caracter\u00edstica clave ofrecida por la mayor\u00eda de los gestores de secretos, que deber\u00eda ser aprovechada al m\u00e1ximo. AWS Secrets Manager : Puedes configurar una rotaci\u00f3n autom\u00e1tica para los secretos. Para una credencial de base de datos RDS, AWS Secrets Manager puede integrarse directamente con RDS para rotar la contrase\u00f1a cada X d\u00edas. # Fragmento de configuraci\u00f3n de rotaci\u00f3n en AWS Secrets Manager (v\u00eda SDK/CLI) { \"RotationRules\": { \"AutomaticallyRotateAfterDays\": 30, # Rotar cada 30 d\u00edas \"Duration\": \"30m\", # La rotaci\u00f3n debe completarse en 30 minutos \"ScheduleExpression\": \"cron(0 0 ? * MON *)\" # Rotar los lunes a medianoche }, \"RotationLambdaARN\": \"arn:aws:lambda:us-east-1:123456789012:function:SecretsManagerRDSMySQLRotationLambda\" } Esto asegura que, incluso si una credencial se ve comprometida, su validez es temporal, limitando el da\u00f1o potencial. HashiCorp Vault : Define leases o duraciones de vida para las credenciales generadas. vault write aws/roles/spark-s3-access \\ credential_type=iam_user \\ policy_arns=arn:aws:iam::123456789012:policy/S3ReadOnlyAccess \\ default_lease_ttl=\"1h\" \\ max_lease_ttl=\"24h\" Aqu\u00ed, las credenciales generadas por este rol expirar\u00e1n despu\u00e9s de 1 hora por defecto, y no podr\u00e1n ser usadas por m\u00e1s de 24 horas, forzando su re-obtenci\u00f3n. Restricci\u00f3n de acceso a claves solo a servicios autenticados. El principio de menor privilegio es fundamental. El acceso a los secretos debe ser granular, basado en roles y solo otorgado a las entidades (usuarios, servicios, aplicaciones) que lo necesitan expl\u00edcitamente para realizar sus funciones. Esto implica el uso de mecanismos de autenticaci\u00f3n robustos y pol\u00edticas de autorizaci\u00f3n detalladas en el gestor de secretos. Azure Key Vault : Uso de pol\u00edticas de acceso o Azure Role-Based Access Control (RBAC). # Pol\u00edtica de acceso en Azure Key Vault para una aplicaci\u00f3n/Service Principal { \"id\": \"/subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.KeyVault/vaults/<key-vault-name>/accessPolicies/<policy-id>\", \"properties\": { \"objectId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\", # Object ID de un Service Principal de Azure AD \"tenantId\": \"yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy\", # Tenant ID de Azure AD \"permissions\": { \"secrets\": [\"get\", \"list\"] # Solo permisos para obtener y listar secretos } } } Esta pol\u00edtica asegura que solo un Service Principal espec\u00edfico (que representa una aplicaci\u00f3n o servicio) puede recuperar y listar los secretos del Key Vault, y no tiene permisos para modificarlos o eliminarlos. AWS Secrets Manager : Integraci\u00f3n con AWS IAM. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:us-east-1:123456789012:secret:my_rds_db_credentials-*\" } ] } Esta pol\u00edtica IAM puede ser adjuntada a un rol que asuma una instancia EC2 o un pod de EKS, permitiendo que solo esa entidad espec\u00edfica acceda a un secreto particular en Secrets Manager. Log de accesos a secretos y alertas ante uso no autorizado. La auditor\u00eda es vital para la seguridad. Cada acceso a un secreto, ya sea exitoso o fallido, debe ser registrado. Estos registros, o logs de auditor\u00eda, son esenciales para el monitoreo de seguridad, la detecci\u00f3n de anomal\u00edas y la respuesta a incidentes. La integraci\u00f3n con sistemas de monitoreo y alerta permite una reacci\u00f3n r\u00e1pida ante cualquier intento de acceso no autorizado o un patr\u00f3n de uso sospechoso. AWS Secrets Manager con CloudTrail y CloudWatch : AWS CloudTrail registra todas las llamadas a la API de Secrets Manager. Estos eventos se pueden enviar a CloudWatch Logs, donde se pueden crear m\u00e9tricas y alarmas. # Ejemplo de filtro de m\u00e9trica en CloudWatch Logs para intentos fallidos de acceso a secretos { \"filterPattern\": \"{ ($.eventName = GetSecretValue || $.eventName = DescribeSecret) && $.errorCode = AccessDenied }\", \"metricTransformations\": [ { \"metricName\": \"SecretAccessDeniedCount\", \"metricNamespace\": \"SecretsManagerMetrics\", \"metricValue\": \"1\" } ] } Se puede configurar una alarma de CloudWatch que se active cuando la m\u00e9trica SecretAccessDeniedCount supere un umbral determinado, notificando al equipo de seguridad. HashiCorp Vault con Audit Devices : Vault tiene \"audit devices\" que registran todas las solicitudes y respuestas. vault audit enable file file_path=/var/log/vault_audit.log vault audit enable syslog Estos logs se pueden enviar a un sistema de gesti\u00f3n de eventos e informaci\u00f3n de seguridad (SIEM) como Splunk o ELK Stack para su an\u00e1lisis y correlaci\u00f3n con otros eventos de seguridad.","title":"Buenas pr\u00e1cticas en el uso de secretos"},{"location":"tema36/#363-control-de-acceso-basado-en-roles-rbac-y-politicas","text":"El control de acceso es fundamental para garantizar que solo las entidades autorizadas (usuarios o servicios) puedan interactuar con los pipelines ETL y sus datos subyacentes. Esto es crucial para la seguridad, la integridad y la confidencialidad de la informaci\u00f3n en entornos de Big Data.","title":"3.6.3 Control de Acceso Basado en Roles (RBAC) y Pol\u00edticas"},{"location":"tema36/#implementacion-de-rbac-role-based-access-control","text":"El RBAC (Control de Acceso Basado en Roles) es un m\u00e9todo eficiente para gestionar permisos al agruparlos y asignarlos a roles espec\u00edficos (por ejemplo, Analista de Datos, Ingeniero de Datos, Administrador de Infraestructura). Esto simplifica la administraci\u00f3n de la seguridad y reduce el riesgo de errores. Definici\u00f3n de roles en Apache Airflow para limitar la ejecuci\u00f3n de DAGs : En Airflow, los roles se utilizan para controlar qu\u00e9 usuarios pueden ver, modificar o ejecutar ciertos DAGs (Directed Acyclic Graphs). Esto es vital para asegurar que solo el personal calificado pueda desplegar y operar pipelines de producci\u00f3n. Airflow proporciona una interfaz de usuario y una API para definir roles y asignar permisos a los usuarios. Los permisos pueden ser muy granulares, permitiendo controlar el acceso a DAGs espec\u00edficos, a vistas de la interfaz de usuario, o a acciones como pausar, reanudar o activar runs de DAGs. Al definir un rol, se especifican las acciones que los usuarios con ese rol pueden realizar sobre los recursos de Airflow. Rol Ingeniero de Datos : Tiene permisos para crear, modificar y ejecutar DAGs en entornos de desarrollo y pruebas. Puede ver el estado de todos los DAGs de producci\u00f3n, pero no puede modificarlos ni activarlos manualmente en producci\u00f3n. Rol Operaciones de Datos : Tiene permisos para monitorear y activar runs de DAGs en producci\u00f3n, as\u00ed como para gestionar el estado de los tasks . No puede modificar la definici\u00f3n de los DAGs. Rol Analista de Datos : Solo tiene permisos de visualizaci\u00f3n sobre el estado de los DAGs de producci\u00f3n relevantes para sus informes, sin capacidad de ejecuci\u00f3n o modificaci\u00f3n. Para configurar esto en Airflow, se usa la interfaz de administraci\u00f3n o, program\u00e1ticamente, se pueden usar las APIs de Airflow. Por ejemplo, para un DAG llamado data_ingestion_prod , se puede configurar que solo el rol \"Operaciones de Datos\" tenga permiso de can_dag_run sobre \u00e9l. Aplicaci\u00f3n de RBAC en sistemas de archivos distribuidos (como HDFS) para restringir lectura/escritura : En sistemas de almacenamiento distribuido como HDFS (Hadoop Distributed File System), la gesti\u00f3n de permisos es crucial para proteger los datos en reposo. El RBAC permite controlar qui\u00e9n puede leer, escribir o ejecutar archivos y directorios. HDFS utiliza un modelo de permisos similar al de sistemas de archivos POSIX, que incluye permisos de propietario, grupo y otros. Sin embargo, para entornos de Big Data, se suelen usar ACLs (Access Control Lists) para una gesti\u00f3n m\u00e1s granular. Las ACLs permiten asignar permisos espec\u00edficos a usuarios y grupos individuales m\u00e1s all\u00e1 de los permisos b\u00e1sicos de propietario y grupo. Supongamos un directorio en HDFS donde se almacenan datos de clientes sensibles: /user/raw_data/customer_info . Asignaci\u00f3n de permisos b\u00e1sicos: hdfs dfs -chown data_engineer:data_team /user/raw_data/customer_info hdfs dfs -chmod 750 /user/raw_data/customer_info Esto da al usuario data_engineer (propietario) permisos de lectura, escritura y ejecuci\u00f3n (7), al grupo data_team permisos de lectura y ejecuci\u00f3n (5), y a otros ning\u00fan permiso (0). Uso de ACLs para RBAC m\u00e1s granular: Si queremos que un usuario espec\u00edfico, audit_user , tenga solo permisos de lectura en ese directorio, sin ser parte del grupo data_team , podemos usar ACLs: hdfs dfs -setfacl -m user:audit_user:r-x /user/raw_data/customer_info Esto permite al audit_user leer y listar el contenido del directorio, pero no modificarlo. Por otro lado, si un rol \"Cient\u00edfico de Datos\" solo necesita leer datos agregados en /user/processed_data/agg_metrics , se le otorgar\u00eda acceso de lectura solo a ese directorio.","title":"Implementaci\u00f3n de RBAC (Role-Based Access Control)"},{"location":"tema36/#politicas-de-acceso-y-segregacion-de-entornos","text":"La seguridad se robustece significativamente al segregar los entornos (desarrollo, pruebas, producci\u00f3n) y establecer pol\u00edticas de acceso claras y estrictas para cada uno, controlando c\u00f3mo los datos y los pipelines fluyen entre ellos. Esto previene que cambios accidentales o maliciosos en desarrollo afecten la producci\u00f3n. Airflow con DAGs de producci\u00f3n ejecutables solo por usuarios con rol \"operaciones\" : Esta pol\u00edtica garantiza que el despliegue y la ejecuci\u00f3n de pipelines cr\u00edticos en producci\u00f3n est\u00e9n altamente controlados, reduciendo el riesgo de interrupciones o errores. En un entorno de CI/CD para Airflow, los DAGs de producci\u00f3n suelen ser desplegados autom\u00e1ticamente desde un repositorio de c\u00f3digo fuente (como Git) a un ambiente de producci\u00f3n de Airflow. Sin embargo, la activaci\u00f3n manual o la modificaci\u00f3n de estos DAGs en producci\u00f3n debe ser un privilegio restringido. Esto se logra configurando Airflow con roles espec\u00edficos que tienen permisos expl\u00edcitos para interactuar con DAGs de producci\u00f3n. Se define un rol produccion_airflow_operator que tiene permisos para can_dag_run , can_dag_edit , y can_dag_pause_unpause solo para DAGs que residen en el folder de producci\u00f3n de Airflow (por ejemplo, /opt/airflow/dags/prod_dags ). Los ingenieros de datos solo tienen un rol desarrollo_airflow_engineer que les permite operar en /opt/airflow/dags/dev_dags y /opt/airflow/dags/test_dags . Cuando se despliega un nuevo DAG a producci\u00f3n, solo los usuarios con el rol produccion_airflow_operator pueden activar sus runs iniciales o reiniciar runs fallidos, siguiendo un protocolo estricto de cambios controlados . Configuraci\u00f3n de entornos de Spark aislados por cluster o namespace : El aislamiento de entornos en Spark previene la interferencia entre cargas de trabajo de desarrollo, pruebas y producci\u00f3n, y asegura que los recursos computacionales y los datos se utilicen de manera segura y eficiente. En plataformas en la nube o en cl\u00fasteres locales, es una buena pr\u00e1ctica configurar cl\u00fasteres Spark o namespaces separados para cada entorno. Esto significa que un trabajo de desarrollo no puede acceder accidentalmente a los datos de producci\u00f3n o consumir sus recursos computacionales. Los permisos de acceso a los datos subyacentes (en S3, ADLS Gen2, GCS, HDFS, Snowflake, etc.) tambi\u00e9n se configuran para cada entorno. Databricks Workspaces : Se crean workspaces separados para desarrollo, staging y producci\u00f3n. Cada workspace tiene sus propios cl\u00fasteres Spark, cuadernos, tablas y configuraciones de seguridad. Los usuarios solo tienen acceso a su workspace asignado. Un ingeniero de datos que desarrolla en el workspace de \"Desarrollo\" no puede ver ni acceder a los datos o jobs en el workspace de \"Producci\u00f3n\". AWS EMR : Se lanzan cl\u00fasteres EMR dedicados para cada entorno. Por ejemplo, un cl\u00faster EMR \"dev-spark-cluster\" y un cl\u00faster \"prod-spark-cluster\". Los roles de IAM de AWS se utilizan para controlar qu\u00e9 cl\u00fasteres pueden lanzar los usuarios y qu\u00e9 buckets de S3 pueden leer o escribir cada cl\u00faster. # Pol\u00edtica IAM para el rol de producci\u00f3n de EMR { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::prod-data-lake/*\", \"arn:aws:s3:::prod-spark-logs/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"glue:GetTable\", \"glue:GetTables\", \"glue:GetDatabase\", \"glue:GetDatabases\" ], \"Resource\": \"*\" # Limitar esto a bases de datos de producci\u00f3n si es posible } ] } Esta pol\u00edtica permitir\u00eda a un cl\u00faster Spark de producci\u00f3n acceder solo a los buckets S3 designados para producci\u00f3n y al cat\u00e1logo de Glue Data Catalog relevante para producci\u00f3n.","title":"Pol\u00edticas de Acceso y Segregaci\u00f3n de Entornos"},{"location":"tema36/#364-practicas-de-auditoria-y-trazabilidad-de-operaciones","text":"Una arquitectura de datos robusta y segura no solo se enfoca en la protecci\u00f3n perimetral o el control de acceso; tambi\u00e9n requiere mecanismos sofisticados para rastrear qu\u00e9 sucedi\u00f3, qui\u00e9n lo hizo, cu\u00e1ndo y c\u00f3mo los datos se transformaron a lo largo de su ciclo de vida . Estas pr\u00e1cticas son cruciales no solo para la seguridad y la detecci\u00f3n de incidentes , sino tambi\u00e9n para el cumplimiento normativo, la depuraci\u00f3n de errores y la gobernanza de datos .","title":"3.6.4 Pr\u00e1cticas de auditor\u00eda y trazabilidad de operaciones"},{"location":"tema36/#auditoria-de-ejecuciones-y-cambios","text":"La auditor\u00eda de ejecuciones y cambios implica el registro sistem\u00e1tico y centralizado de todas las actividades operacionales, modificaciones de configuraci\u00f3n y accesos a datos dentro de la plataforma Big Data . Este registro detallado es fundamental para la identificaci\u00f3n temprana de comportamientos an\u00f3malos, la investigaci\u00f3n forense de posibles brechas de seguridad, y la demostraci\u00f3n de cumplimiento frente a regulaciones como GDPR, HIPAA o SOX . Uso de Airflow Logs centralizados con integraci\u00f3n a herramientas como ELK o CloudWatch. Apache Airflow es una herramienta ampliamente utilizada para orquestar flujos de trabajo de datos. Cada ejecuci\u00f3n de una tarea (DAG Run) genera logs detallados que contienen informaci\u00f3n sobre el inicio, finalizaci\u00f3n, errores, salidas de los scripts y recursos utilizados. Para una auditor\u00eda efectiva, estos logs deben ser centralizados y persistidos fuera del entorno ef\u00edmero de Airflow workers . Herramientas como Elastic Stack (ELK: Elasticsearch, Logstash, Kibana) o Amazon CloudWatch (en AWS) permiten recolectar, indexar, buscar y visualizar estos logs de manera eficiente. Esto facilita la creaci\u00f3n de dashboards de monitoreo , la configuraci\u00f3n de alertas para fallos o accesos inusuales, y la generaci\u00f3n de reportes de auditor\u00eda . Imagina que tienes un DAG de Airflow ( data_ingestion_dag ) que ingesta datos sensibles. Si un usuario no autorizado intenta modificar este DAG o si una ejecuci\u00f3n falla repetidamente, necesitas ser notificado y tener un registro de estos eventos. Configuraci\u00f3n en Airflow : Aseg\u00farate de que Airflow est\u00e9 configurado para enviar sus logs a un sistema de log centralizado. Para AWS, esto podr\u00eda ser S3 y CloudWatch: # airflow.cfg o variables de entorno para Airflow # [logging] # remote_logging = True # remote_base_log_folder = s3://your-airflow-logs-bucket/ # remote_log_conn_id = aws_default_conn # S3_REMOTE_HANDLES = s3 Luego, puedes configurar CloudWatch Logs para ingerir logs desde S3 o directamente desde tus instancias EC2 donde corre Airflow. Consulta en Kibana (ejemplo para ELK) : Una vez que los logs est\u00e9n en Elasticsearch, puedes buscar eventos espec\u00edficos, por ejemplo, todas las ejecuciones fallidas de un DAG en particular o intentos de acceso a la interfaz de Airflow por usuarios espec\u00edficos. # Ejemplo de consulta en Kibana \"dag_id\": \"data_ingestion_dag\" AND \"status\": \"failed\" AND \"@timestamp\": [now-24h TO now] Alerta en CloudWatch (ejemplo para AWS) : Puedes crear una m\u00e9trica de log en CloudWatch que cuente los errores de Airflow y configure una alarma para notificarte. { \"logGroupNames\": [\"/aws/containerinsights/your-cluster-name/application\"], \"metricFilterPatterns\": [ \"{ $.logStreamName = \\\"/ecs/airflow-worker*\\\" && $.message like \\\"ERROR\\\" }\" ], \"metricTransformations\": [ { \"metricName\": \"AirflowErrors\", \"metricNamespace\": \"AirflowCustomMetrics\", \"metricValue\": \"1\" } ] } Esta configuraci\u00f3n te permitir\u00eda ver un aumento en los errores de Airflow y actuar proactivamente. Activaci\u00f3n de audit logs en Snowflake para rastrear consultas, cambios de roles o ingestas. Snowflake, como un Data Warehouse en la nube, ofrece capacidades de auditor\u00eda robustas a trav\u00e9s de sus vistas de Account Usage y la funci\u00f3n QUERY_HISTORY . Estas herramientas permiten a los administradores rastrear cada consulta ejecutada, los usuarios que las realizaron, el tiempo de ejecuci\u00f3n, la cantidad de datos procesados, los roles utilizados , y tambi\u00e9n cambios en la configuraci\u00f3n de seguridad (como la creaci\u00f3n o modificaci\u00f3n de usuarios y roles) o actividades de ingesta de datos (uso de COPY INTO) . La auditor\u00eda en Snowflake es fundamental para garantizar la seguridad de los datos, identificar patrones de uso inusuales, optimizar el rendimiento y cumplir con las pol\u00edticas de gobernanza de datos . Supongamos que necesitas saber qui\u00e9n accedi\u00f3 a una tabla cr\u00edtica de clientes en los \u00faltimos 7 d\u00edas o si alguien intent\u00f3 modificar los permisos de un usuario espec\u00edfico. Rastreo de consultas a una tabla espec\u00edfica : SELECT query_id, query_text, user_name, role_name, start_time, end_time, error_message FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE query_text ILIKE '%SELECT%FROM%customer_data%' -- Busca consultas que accedan a 'customer_data' AND start_time >= DATEADD(day, -7, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Auditor\u00eda de cambios de rol o usuario : Snowflake registra eventos de seguridad en la vista LOGIN_HISTORY y USERS para logins fallidos. Para cambios en roles y permisos, aunque no hay una vista ROLE_HISTORY directa, estos eventos se registran en QUERY_HISTORY si se ejecutan comandos DDL. SELECT query_id, query_text, user_name, start_time FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE query_type IN ('CREATE_ROLE', 'ALTER_ROLE', 'GRANT_ROLE', 'REVOKE_ROLE', 'CREATE_USER', 'ALTER_USER', 'DROP_USER') AND start_time >= DATEADD(day, -30, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Monitoreo de ingestas de datos (COPY INTO) : SELECT query_id, query_text, user_name, start_time, rows_inserted, files_scanned FROM SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY WHERE table_name = 'YOUR_TARGET_TABLE' -- Reemplaza con tu tabla AND start_time >= DATEADD(day, -1, CURRENT_TIMESTAMP()) ORDER BY start_time DESC; Estas consultas permiten a los equipos de seguridad y operaciones mantener un registro inmutable de las actividades cruciales en Snowflake.","title":"Auditor\u00eda de ejecuciones y cambios"},{"location":"tema36/#lineage-y-trazabilidad-de-datos","text":"El linaje de datos (data lineage) es la capacidad de mapear y comprender el recorrido de los datos desde su origen inicial hasta su destino final , incluyendo todas las transformaciones y movimientos intermedios. Es fundamental para la gobernanza de datos, la calidad de los datos, la depuraci\u00f3n, el an\u00e1lisis de impacto de cambios y el cumplimiento normativo . Saber de d\u00f3nde provienen los datos y c\u00f3mo se transformaron es clave para confiar en los resultados anal\u00edticos y asegurar la integridad de la informaci\u00f3n . Integraci\u00f3n de Apache Atlas con Spark para visualizar el lineage completo de columnas. Apache Atlas es una plataforma de gobernanza de datos y metadatos que proporciona un cat\u00e1logo de datos centralizado y capacidades de linaje . Cuando se integra con motores de procesamiento de datos como Apache Spark, Atlas puede interceptar y registrar las transformaciones de datos a nivel de columna . Esto significa que puedes visualizar no solo c\u00f3mo un conjunto de datos se deriva de otro, sino tambi\u00e9n c\u00f3mo cada columna individual en un conjunto de datos de salida se construye a partir de columnas espec\u00edficas en los conjuntos de datos de entrada. Esta granularidad es invaluable para rastrear el origen de un valor, entender el impacto de un cambio en una columna fuente, o identificar la causa ra\u00edz de un problema de calidad de datos. Imagina un proceso Spark que lee datos de ventas de una tabla, los limpia, agrega informaci\u00f3n de clientes de otra tabla y luego los escribe en una tabla de reportes. Proceso Spark (pseudoc\u00f3digo) : from pyspark.sql import SparkSession from pyspark.sql.functions import col, concat_ws spark = SparkSession.builder \\ .appName(\"SalesDataProcessing\") \\ .enableHiveSupport() \\ .getOrCreate() # Lectura de datos de ventas (origen 1) sales_df = spark.read.table(\"raw_data.sales\") # Lectura de datos de clientes (origen 2) customers_df = spark.read.table(\"raw_data.customers\") # Limpieza y transformaci\u00f3n de datos de ventas cleaned_sales_df = sales_df.filter(col(\"quantity\") > 0) \\ .withColumn(\"total_price\", col(\"quantity\") * col(\"unit_price\")) # Unir con datos de clientes enriched_sales_df = cleaned_sales_df.join(customers_df, \"customer_id\", \"inner\") \\ .select( col(\"sale_id\"), col(\"product_id\"), col(\"total_price\"), concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"customer_full_name\"), col(\"region\") ) # Escribir el resultado en una tabla de reportes enriched_sales_df.write.mode(\"overwrite\").saveAsTable(\"reporting.daily_sales_summary\") spark.stop() Visualizaci\u00f3n en Apache Atlas : Una vez que este trabajo Spark se ejecuta y Apache Atlas est\u00e1 configurado para escuchar eventos de Spark (a trav\u00e9s de hooks de Spark o un plugin), Atlas registrar\u00e1 autom\u00e1ticamente el linaje. En la interfaz de usuario de Atlas, podr\u00edas ver un gr\u00e1fico de linaje como este: raw_data.sales (tabla) --> cleaned_sales_df (dataframe Spark) --(Transformaci\u00f3n: total_price de quantity y unit_price )--> enriched_sales_df (dataframe Spark) --(Uni\u00f3n con raw_data.customers )--> reporting.daily_sales_summary (tabla) Y al profundizar, podr\u00edas ver el linaje a nivel de columna, por ejemplo: reporting.daily_sales_summary.total_price se deriva de raw_data.sales.quantity y raw_data.sales.unit_price . reporting.daily_sales_summary.customer_full_name se deriva de raw_data.customers.first_name y raw_data.customers.last_name . Aplicaci\u00f3n de etiquetas (tags) a datasets sensibles en Data Catalogs para trazabilidad y control. Un Cat\u00e1logo de Datos (Data Catalog) es una herramienta esencial para la gobernanza de datos que act\u00faa como un inventario centralizado de todos los activos de datos de una organizaci\u00f3n . Permite descubrir, comprender y gestionar los datos. Una funcionalidad clave es la capacidad de aplicar etiquetas (tags) o clasificaciones a los datasets, tablas, o incluso columnas, especialmente aquellos que contienen informaci\u00f3n sensible (PII, datos financieros, etc.) o que son cr\u00edticos para el negocio. Estas etiquetas no solo mejoran la trazabilidad al indicar la naturaleza y sensibilidad de los datos , sino que tambi\u00e9n pueden automatizar la aplicaci\u00f3n de pol\u00edticas de seguridad y cumplimiento . Por ejemplo, una etiqueta \"PII\" podr\u00eda desencadenar autom\u00e1ticamente reglas de enmascaramiento o acceso restringido. Supongamos que tienes un conjunto de datos en Google BigQuery que contiene informaci\u00f3n personal identificable de clientes. Identificaci\u00f3n y Etiquetado : Usar\u00edas tu Data Catalog (por ejemplo, Google Data Catalog, Collibra, Amundsen, etc.) para buscar la tabla project.dataset.customers . Una vez localizada, podr\u00edas aplicar etiquetas (tags) a la tabla completa o a columnas espec\u00edficas. Ejemplo de tags a nivel de tabla : Sensibilidad: Alto Clasificaci\u00f3n: PII Regulaci\u00f3n: GDPR Propietario: Equipo de Datos de Clientes Ejemplo de tags a nivel de columna : Columna email : Tipo: Contacto , Sensibilidad: PII-Directo Columna credit_card_number : Tipo: Financiero , Sensibilidad: Muy Alto , Regulaci\u00f3n: PCI-DSS Beneficios de la Trazabilidad con Tags : Descubrimiento : Cualquier analista que busque datos de clientes ver\u00e1 inmediatamente las etiquetas de sensibilidad, lo que le ayudar\u00e1 a decidir si puede usar esos datos y bajo qu\u00e9 condiciones. Cumplimiento : Las herramientas de gobernanza pueden usar estas etiquetas para generar reportes sobre la ubicaci\u00f3n de datos PII y demostrar el cumplimiento de GDPR. Automatizaci\u00f3n de Pol\u00edticas : En algunos cat\u00e1logos, estas etiquetas pueden integrarse con sistemas de seguridad para que, por ejemplo, el acceso a datos etiquetados como \"PII\" requiera una aprobaci\u00f3n adicional o se apliquen autom\u00e1ticamente funciones de enmascaramiento para usuarios no autorizados. An\u00e1lisis de Impacto : Si una columna etiquetada como \"PII\" se modifica o se elimina, el cat\u00e1logo puede mostrar qu\u00e9 otros conjuntos de datos o reportes dependen de ella, ayudando a predecir el impacto antes de realizar el cambio. Estas pr\u00e1cticas de auditor\u00eda y trazabilidad son los pilares para construir una arquitectura de datos no solo eficiente y escalable, sino tambi\u00e9n segura, transparente y conforme a las regulaciones .","title":"Lineage y trazabilidad de datos"},{"location":"tema36/#365-requerimientos-normativos-comunes-gdpr-hipaa-etc","text":"Los pipelines ETL (Extract, Transform, Load) no son solo herramientas t\u00e9cnicas; tambi\u00e9n deben dise\u00f1arse y operarse en estricto cumplimiento con una serie de regulaciones que protegen la privacidad y los derechos de los usuarios sobre sus datos personales . La omisi\u00f3n o el manejo inadecuado de estas normativas puede acarrear multas significativas, da\u00f1o reputacional y la p\u00e9rdida de confianza de los clientes. Es crucial que los arquitectos de datos y desarrolladores de pipelines entiendan la importancia de la privacidad desde el dise\u00f1o ( Privacy by Design ) y la seguridad desde el dise\u00f1o ( Security by Design ).","title":"3.6.5 Requerimientos normativos comunes: GDPR, HIPAA, etc."},{"location":"tema36/#regulaciones-aplicables-al-tratamiento-de-datos","text":"La naturaleza global del Big Data significa que las organizaciones a menudo deben cumplir con un mosaico de regulaciones dependiendo de la ubicaci\u00f3n de los datos, la residencia de los usuarios y el sector industrial. Algunas de las normativas m\u00e1s prominentes incluyen: Aplicaci\u00f3n de anonimizaci\u00f3n en pipelines que tratan datos personales de ciudadanos europeos (GDPR). El Reglamento General de Protecci\u00f3n de Datos (GDPR) de la Uni\u00f3n Europea es una de las normativas de privacidad de datos m\u00e1s estrictas a nivel mundial. Exige que los datos personales de los ciudadanos de la UE sean procesados de forma l\u00edcita, leal y transparente. Esto a menudo implica la anonimizaci\u00f3n o seudonimizaci\u00f3n de los datos para reducir el riesgo de identificaci\u00f3n directa de los individuos. Un pipeline que procesa datos de compras en l\u00ednea de clientes europeos debe anonimizar campos como nombre , direcci\u00f3n y email antes de almacenarlos en un data lake anal\u00edtico. Esto podr\u00eda implicar el uso de t\u00e9cnicas como el hashing irreversible para email o la generalizaci\u00f3n para c\u00f3digo_postal . # Ejemplo de seudonimizaci\u00f3n con hashing (Python + PySpark) from pyspark.sql.functions import sha2, concat_ws, lit # Suponiendo un DataFrame 'df_clientes_europeos' # con columnas 'nombre', 'apellido', 'email', 'direccion' df_anonimizado = df_clientes_europeos.withColumn( \"email_hash\", sha2(df_clientes_europeos[\"email\"], 256) # Hashing SHA-256 del email ).withColumn( \"nombre_seudonimo\", concat_ws(\"_\", lit(\"cliente\"), sha2(df_clientes_europeos[\"nombre\"], 256)) ).drop(\"email\", \"nombre\", \"apellido\", \"direccion\") # Eliminar columnas de identificaci\u00f3n directa df_anonimizado.show() Encriptaci\u00f3n obligatoria de historiales cl\u00ednicos en pipelines hospitalarios (HIPAA). La Ley de Portabilidad y Responsabilidad del Seguro M\u00e9dico (HIPAA) en Estados Unidos establece est\u00e1ndares nacionales para proteger la informaci\u00f3n de salud protegida ( PHI ). Cualquier pipeline que maneje datos de salud, como historiales m\u00e9dicos, resultados de laboratorio o informaci\u00f3n de seguros, debe asegurar que esta informaci\u00f3n est\u00e9 encriptada tanto en tr\u00e1nsito como en reposo . Un pipeline ETL que ingiere datos de sistemas EHR (Electronic Health Records) a un almac\u00e9n de datos para an\u00e1lisis de investigaci\u00f3n debe garantizar que los archivos de datos se encripten antes de ser subidos a un bucket de S3 o a Azure Blob Storage, y que la comunicaci\u00f3n con la base de datos de origen utilice SSL/TLS. # Ejemplo de encriptaci\u00f3n de archivos en reposo (AWS S3) # Al subir un archivo a S3, especificar encriptaci\u00f3n del lado del servidor aws s3 cp /ruta/a/historial_clinico.csv s3://mi-bucket-salud/data/ --sse AES256 # O si se usa un cliente de Python (boto3) import boto3 s3 = boto3.client('s3') bucket_name = 'mi-bucket-salud' file_path = '/ruta/a/historial_clinico.csv' object_name = 'data/historial_clinico.csv' s3.upload_file( file_path, bucket_name, object_name, ExtraArgs={ 'ServerSideEncryption': 'AES256' # Encriptaci\u00f3n del lado del servidor con AES-256 } ) Inclusi\u00f3n de consentimiento informado como metadato procesado en flujos de datos personales. Muchas regulaciones, como GDPR y LGPD (Brasil), requieren que el consentimiento del usuario para el procesamiento de sus datos personales sea expl\u00edcito, informado y revocable. Esto significa que los pipelines no solo deben procesar los datos, sino tambi\u00e9n la informaci\u00f3n sobre el consentimiento asociada a esos datos, lo que a menudo implica almacenar metadatos relacionados con la fecha de consentimiento, el tipo de consentimiento y la versi\u00f3n de la pol\u00edtica de privacidad aceptada. Un pipeline que recopila datos de comportamiento de usuario de una aplicaci\u00f3n m\u00f3vil debe adjuntar un ID_consentimiento y fecha_consentimiento a cada evento de datos, permitiendo as\u00ed filtrar y borrar datos en caso de que un usuario revoque su consentimiento. # Ejemplo de un registro de evento de datos con metadatos de consentimiento { \"user_id\": \"usuario_abc123\", \"event_type\": \"pagina_vista\", \"page_url\": \"/productos/item123\", \"timestamp\": \"2025-06-10T10:30:00Z\", \"consent_metadata\": { \"consent_id\": \"cons_xyz789\", \"consent_date\": \"2025-01-15T09:00:00Z\", \"policy_version\": \"2.1\", \"data_processing_purposes\": [\"analitica\", \"marketing_personalizado\"] } }","title":"Regulaciones aplicables al tratamiento de datos"},{"location":"tema36/#diseno-de-pipelines-compliance-ready","text":"Para cumplir con las normativas de forma proactiva y evitar problemas, es fundamental adoptar un enfoque de dise\u00f1o de pipelines \"compliance-ready\" o \"privacy-by-design\". Esto implica integrar consideraciones de cumplimiento normativo en cada etapa del ciclo de vida del desarrollo del pipeline, minimizando as\u00ed los riesgos legales y operativos. Creaci\u00f3n de m\u00f3dulos reutilizables para anonimizar, enmascarar o eliminar datos bajo demanda. La estandarizaci\u00f3n es clave para la consistencia y la eficiencia en el cumplimiento. Desarrollar un conjunto de funciones o microservicios reusables para las operaciones comunes de protecci\u00f3n de datos (anonimizaci\u00f3n, seudonimizaci\u00f3n, enmascaramiento, tokenizaci\u00f3n y borrado seguro) permite aplicarlas de manera uniforme en diferentes pipelines y fuentes de datos. Un equipo de datos podr\u00eda construir una librer\u00eda interna en Spark para ofuscar PII (Informaci\u00f3n de Identificaci\u00f3n Personal). # spark_data_masking_lib.py from pyspark.sql.functions import udf from pyspark.sql.types import StringType import hashlib @udf(returnType=StringType()) def hash_email(email): if email: return hashlib.sha256(email.lower().encode('utf-8')).hexdigest() return None @udf(returnType=StringType()) def mask_phone_number(phone): if phone and len(phone) > 4: return '*' * (len(phone) - 4) + phone[-4:] return phone # En un pipeline de Spark: # from spark_data_masking_lib import hash_email, mask_phone_number # df_raw.withColumn(\"hashed_email\", hash_email(\"email\")) \\ # .withColumn(\"masked_phone\", mask_phone_number(\"telefono\")) Implementaci\u00f3n de pol\u00edticas de retenci\u00f3n y borrado automatizado de datos personales. Las regulaciones suelen establecer l\u00edmites sobre cu\u00e1nto tiempo se pueden retener los datos personales. Los pipelines deben ser capaces de aplicar pol\u00edticas de retenci\u00f3n de datos que automaticen el borrado o la anonimizaci\u00f3n de datos que ya no son necesarios o para los que el consentimiento ha sido revocado. Esto podr\u00eda implicar el uso de funcionalidades de gesti\u00f3n del ciclo de vida (lifecycle management) en sistemas de almacenamiento en la nube o tareas programadas. Configurar reglas de ciclo de vida en un bucket de AWS S3 para que los objetos de m\u00e1s de 7 a\u00f1os sean eliminados autom\u00e1ticamente, o un trabajo de Airflow que ejecute un script de borrado l\u00f3gico en una tabla de Snowflake para registros con antig\u00fcedad superior a la pol\u00edtica de retenci\u00f3n. # Ejemplo de tarea de Airflow para borrado de datos antiguos (pseudoc\u00f3digo) from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime, timedelta with DAG( dag_id='data_retention_policy_cleaner', start_date=datetime(2023, 1, 1), schedule_interval=timedelta(days=7), # Ejecutar semanalmente catchup=False ) as dag: clean_old_customer_data = BashOperator( task_id='clean_customer_data_in_snowflake', bash_command=\"\"\" snowflake_conn_string=\"user={{ var.value.snowflake_user }} password={{ var.value.snowflake_password }} account={{ var.value.snowflake_account }}\" SNOWFLAKE_QUERY=\"DELETE FROM analytics_db.public.customer_data WHERE created_at < DATEADD(year, -7, CURRENT_DATE());\" snowsql -c \"$snowflake_conn_string\" -q \"$SNOWFLAKE_QUERY\" \"\"\" ) Inclusi\u00f3n de revisiones legales en el ciclo de vida del pipeline (Data Governance Boards). El cumplimiento normativo no es solo una preocupaci\u00f3n t\u00e9cnica, sino tambi\u00e9n legal y de gobernanza. Es fundamental establecer un marco de gobernanza de datos que incluya revisiones legales y de cumplimiento en las etapas clave del dise\u00f1o, desarrollo y despliegue de los pipelines. Un Data Governance Board (Junta de Gobernanza de Datos) o un Privacy Officer debe aprobar c\u00f3mo se procesan los datos personales, asegurando que las decisiones t\u00e9cnicas est\u00e9n alineadas con las pol\u00edticas corporativas y los requisitos legales. Antes de lanzar un nuevo pipeline de ingesta de datos de clientes para un nuevo servicio, el dise\u00f1o arquitect\u00f3nico y el plan de procesamiento de datos son revisados por el equipo legal y el oficial de privacidad de datos para asegurar el cumplimiento con GDPR, CCPA y cualquier otra regulaci\u00f3n relevante. Esto podr\u00eda implicar una reuni\u00f3n formal donde los ingenieros de datos presenten el flujo de datos, las medidas de seguridad y anonimizaci\u00f3n implementadas, y las pol\u00edticas de retenci\u00f3n, recibiendo la aprobaci\u00f3n formal del comit\u00e9.","title":"Dise\u00f1o de pipelines \"compliance-ready\""},{"location":"tema36/#tarea","text":"Desarrolla los siguientes ejercicios pr\u00e1cticos en tu entorno de laboratorio: Configura una conexi\u00f3n segura TLS entre Apache Spark y una base de datos PostgreSQL y documenta los pasos seguidos. Implementa el acceso a secretos mediante AWS Secrets Manager desde un DAG de Apache Airflow que conecta con un bucket de S3. Dise\u00f1a una pol\u00edtica de control de acceso por roles (RBAC) para un pipeline ETL en Databricks, diferenciando roles de analista, ingeniero y auditor. Audita un pipeline ejecutado en Airflow e identifica los registros clave de su trazabilidad y posibles fallas. Redise\u00f1a un pipeline para que cumpla con GDPR incorporando enmascaramiento de datos personales y l\u00f3gica de eliminaci\u00f3n bajo solicitud del usuario.","title":"Tarea"},{"location":"tema37/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.7. Patrones de Dise\u00f1o y Optimizaci\u00f3n en la Nube Objetivo : Consolidar las buenas pr\u00e1cticas de dise\u00f1o y optimizaci\u00f3n para el desarrollo de soluciones de Big Data en entornos de nube, enfoc\u00e1ndose en la eficiencia operativa, la robustez del sistema y el control de costos. Introducci\u00f3n : El dise\u00f1o y la implementaci\u00f3n de soluciones de Big Data en la nube presentan oportunidades sin precedentes para la escalabilidad, flexibilidad y velocidad de procesamiento. Sin embargo, para capitalizar estas ventajas y evitar trampas comunes como los costos descontrolados o la ineficiencia operativa, es crucial comprender y aplicar patrones de dise\u00f1o y estrategias de optimizaci\u00f3n probadas. Este tema profundiza en las mejores pr\u00e1cticas para la extracci\u00f3n, transformaci\u00f3n, carga (ETL), el control y monitoreo de datos, as\u00ed como en patrones arquitect\u00f3nicos y t\u00e9cnicas espec\u00edficas para minimizar los costos en infraestructuras cloud. Desarrollo : La adopci\u00f3n de patrones de dise\u00f1o en el \u00e1mbito de Big Data y la nube no solo estandariza la forma en que se construyen los pipelines de datos, sino que tambi\u00e9n fomenta la reutilizaci\u00f3n, mejora la mantenibilidad y asegura la resiliencia de los sistemas. Al integrar estas pr\u00e1cticas desde la fase de dise\u00f1o, se pueden anticipar y mitigar problemas como la inconsistencia de datos, los cuellos de botella en el rendimiento y los gastos excesivos. Adem\u00e1s, una comprensi\u00f3n profunda de las estrategias de optimizaci\u00f3n de costos en la nube es vital para garantizar la viabilidad a largo plazo de las soluciones implementadas, permitiendo a las organizaciones escalar sus operaciones de datos de manera sostenible. 3.7.1. Patrones de Extracci\u00f3n La extracci\u00f3n de datos es la primera fase cr\u00edtica en cualquier pipeline ETL, donde la eficiencia y la selecci\u00f3n adecuada del patr\u00f3n pueden impactar significativamente el rendimiento y los costos. Estos patrones se enfocan en c\u00f3mo los datos son le\u00eddos desde las fuentes de origen. Extracci\u00f3n Incremental La extracci\u00f3n incremental es una t\u00e9cnica fundamental que busca reducir la carga de procesamiento y el volumen de datos transferidos al obtener \u00fanicamente los registros que han sido modificados o a\u00f1adidos desde la \u00faltima ejecuci\u00f3n del proceso de extracci\u00f3n. Esto se logra t\u00edpicamente mediante el uso de marcas de tiempo ( timestamps ), n\u00fameros de secuencia o banderas de modificaci\u00f3n ( flags de modificaci\u00f3n ) en las tablas de origen. Su implementaci\u00f3n es vital en entornos de Big Data para manejar grandes vol\u00famenes de informaci\u00f3n que cambian constantemente, ya que evita el reprocesamiento innecesario de datos ya existentes y estables. Imaginemos una base de datos transaccional de ventas que se actualiza continuamente. En lugar de extraer toda la tabla de ventas cada hora, podemos extraer solo las transacciones que ocurrieron desde la \u00faltima extracci\u00f3n. -- \u00daltimo timestamp de extracci\u00f3n guardado en una tabla de metadatos SET @last_extracted_timestamp = (SELECT max_timestamp FROM etl_metadata WHERE job_name = 'sales_extraction'); -- Extracci\u00f3n incremental de nuevas ventas SELECT * FROM sales_transactions WHERE last_updated_at > @last_extracted_timestamp; -- Actualizar el timestamp de la \u00faltima extracci\u00f3n despu\u00e9s de una extracci\u00f3n exitosa UPDATE etl_metadata SET max_timestamp = CURRENT_TIMESTAMP WHERE job_name = 'sales_extraction'; Extracci\u00f3n por Lotes vs Streaming La elecci\u00f3n entre extracci\u00f3n por lotes y streaming depende directamente de los requisitos de latencia y el volumen de datos. Extracci\u00f3n por Lotes (Batch Processing) : Procesa grandes vol\u00famenes de datos en intervalos programados (por ejemplo, cada hora, diariamente, semanalmente). Es ideal para datos que no requieren inmediatez, como informes anal\u00edticos hist\u00f3ricos o cargas de data warehouses nocturnas. Este enfoque es generalmente m\u00e1s eficiente en t\u00e9rminos de recursos para grandes vol\u00famenes de datos est\u00e1ticos o semi-est\u00e1ticos. Extracci\u00f3n por Streaming (Stream Processing) : Procesa datos en tiempo real conforme llegan a la fuente. Es crucial para aplicaciones que requieren baja latencia, como detecci\u00f3n de fraude, monitoreo de sistemas, personalizaci\u00f3n en tiempo real o IoT. Este patr\u00f3n implica el uso de tecnolog\u00edas que puedan manejar flujos continuos de datos, como Apache Kafka o Amazon Kinesis. Para la generaci\u00f3n de un informe de ventas mensual consolidado, la extracci\u00f3n por lotes es adecuada. Para monitorear transacciones bancarias en busca de actividad fraudulenta, el streaming es indispensable. # Productor de Kafka (simula un sistema de punto de venta enviando transacciones) from kafka import KafkaProducer import json import time producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8')) for i in range(100): transaction = {'id': i, 'amount': i * 10.5, 'timestamp': time.time()} producer.send('transactions_topic', transaction) print(f\"Sent: {transaction}\") time.sleep(0.1) # Simula llegada de transacciones en tiempo real # Consumidor de Kafka (simula un procesador de fraude) from kafka import KafkaConsumer import json consumer = KafkaConsumer('transactions_topic', bootstrap_servers='localhost:9092', auto_offset_reset='earliest', enable_auto_commit=True, group_id='fraud_detector_group', value_deserializer=lambda x: json.loads(x.decode('utf-8'))) for message in consumer: transaction = message.value if transaction['amount'] > 500: print(f\"ALERTA DE FRAUDE POTENCIAL: {transaction}\") else: print(f\"Procesando transacci\u00f3n normal: {transaction}\") Change Data Capture (CDC) Change Data Capture (CDC) es una t\u00e9cnica que identifica y captura los cambios (inserciones, actualizaciones, eliminaciones) realizados en una base de datos fuente. A diferencia de la extracci\u00f3n incremental basada en timestamps , CDC opera a un nivel m\u00e1s granular, t\u00edpicamente leyendo los logs de transacciones de la base de datos (como el binlog de MySQL o el WAL de PostgreSQL) o utilizando triggers. Esto permite replicar los cambios de forma as\u00edncrona y con una latencia m\u00ednima, sin imponer una carga significativa en el sistema operacional de origen. Es ideal para mantener r\u00e9plicas de bases de datos, alimentar data warehouses en tiempo real o sincronizar sistemas. Mantener un data lake o data warehouse actualizado con los \u00faltimos movimientos de una base de datos OLTP sin afectar su rendimiento. Debezium es un popular framework de CDC. # Configuraci\u00f3n de un conector Debezium para MySQL en Kafka Connect # Este es un ejemplo de c\u00f3mo se configura en un archivo properties/JSON name: mysql-connector connector.class: io.debezium.connector.mysql.MySqlConnector database.hostname: 192.168.99.100 database.port: 3306 database.user: debezium database.password: dbz database.server.id: 12345 database.server.name: my-app-connector database.whitelist: inventory,customers database.history.kafka.bootstrap.servers: kafka:9092 database.history.kafka.topic: dbhistory.inventory # Una vez configurado, Debezium publicar\u00e1 los cambios en Kafka topics # Por ejemplo, para la tabla 'customers', los cambios ir\u00e1n a 'my-app-connector.inventory.customers' Posteriormente, un consumidor de Kafka (como un procesador Spark Streaming) puede leer estos cambios y aplicarlos a un destino: # Pseudo-c\u00f3digo para un consumidor Spark Streaming leyendo cambios de CDC from pyspark.sql import SparkSession from pyspark.sql.functions import from_json, col from pyspark.sql.types import StructType, StringType, TimestampType, LongType, IntegerType spark = SparkSession.builder.appName(\"CDCProcessor\").getOrCreate() # Esquema para los mensajes de Debezium (simplificado) # Un mensaje de Debezium t\u00edpico incluye 'before' y 'after' estados del registro, y el tipo de operaci\u00f3n 'op' schema = StructType([ StructField(\"before\", StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"email\", StringType(), True) ]), True), StructField(\"after\", StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"email\", StringType(), True) ]), True), StructField(\"op\", StringType(), True) # 'c' for create, 'u' for update, 'd' for delete ]) # Leer el stream de Kafka df = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\ .option(\"subscribe\", \"my-app-connector.inventory.customers\") \\ .load() # Parsear el valor JSON del mensaje de Kafka parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\") # Aplicar l\u00f3gica basada en el tipo de operaci\u00f3n query = parsed_df \\ .writeStream \\ .foreachBatch(lambda df, epoch_id: process_cdc_batch(df)) \\ .start() def process_cdc_batch(df): # L\u00f3gica para aplicar los cambios a un Data Warehouse o Data Lake df_inserts = df.filter(col(\"op\") == \"c\").select(col(\"after.*\")) df_updates = df.filter(col(\"op\") == \"u\").select(col(\"after.*\")) df_deletes = df.filter(col(\"op\") == \"d\").select(col(\"before.*\")) # Aqu\u00ed ir\u00eda la l\u00f3gica para escribir en el destino, por ejemplo, en una tabla Delta Lake # df_inserts.write.format(\"delta\").mode(\"append\").save(\"/delta/customers\") # df_updates.write.format(\"delta\").mode(\"merge\").option(\"mergeSchema\", \"true\").save(\"/delta/customers\") # Asumiendo Merge Into # df_deletes.write.format(\"delta\").mode(\"merge\").where(\"id = <deleted_id>\").delete(\"/delta/customers\") # Pseudo-c\u00f3digo para delete print(f\"Processed batch {epoch_id}: Inserts={df_inserts.count()}, Updates={df_updates.count()}, Deletes={df_deletes.count()}\") query.awaitTermination() 3.7.2. Patrones de Transformaci\u00f3n La fase de transformaci\u00f3n es donde los datos brutos se limpian, enriquecen, validan y se preparan para su consumo final. Los patrones aqu\u00ed se centran en la eficiencia y la modularidad del procesamiento. Pipeline de Transformaci\u00f3n Un pipeline de transformaci\u00f3n organiza las operaciones de procesamiento de datos en una secuencia l\u00f3gica de pasos, donde la salida de una etapa se convierte en la entrada de la siguiente. Esta modularidad facilita el mantenimiento, la depuraci\u00f3n y la reutilizaci\u00f3n de componentes. Cada paso del pipeline puede ser una funci\u00f3n o un microservicio independiente que realiza una tarea espec\u00edfica, como la limpieza de datos nulos, la normalizaci\u00f3n de formatos o la agregaci\u00f3n de informaci\u00f3n. Este patr\u00f3n es fundamental para construir flujos de trabajo ETL complejos y robustos. Un pipeline que primero limpia los datos de clientes (elimina duplicados, corrige errores de formato), luego los enriquece con informaci\u00f3n geogr\u00e1fica y finalmente los agrega por regi\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import col, upper, trim, count spark = SparkSession.builder.appName(\"DataTransformationPipeline\").getOrCreate() # Paso 1: Cargar datos brutos (simulamos un DataFrame) raw_data = [ (\"john doe\", \"john.doe@example.com \", \"New York\", 1), (\"Jane Smith\", \"jane.smith@example.com\", \"Los Angeles \", 2), (\" JOHN DOE \", \"john.doe@example.com\", \"new york\", 3), # Duplicado, nombre en may\u00fasculas (\"Peter Jones\", \"peter.jones@example.com\", \"Chicago\", 4) ] raw_df = spark.createDataFrame(raw_data, [\"name\", \"email\", \"city\", \"id\"]) # Paso 2: Limpieza de datos def clean_data(df): return df.withColumn(\"name\", trim(upper(col(\"name\")))) \\ .withColumn(\"city\", trim(col(\"city\"))) \\ .dropDuplicates([\"email\"]) cleaned_df = clean_data(raw_df) cleaned_df.show() # +----------+--------------------+----------+---+ # | name| email| city| id| # +----------+--------------------+----------+---+ # | PETER JONES|peter.jones@example.com| Chicago| 4| # | JOHN DOE|john.doe@example.com| New York| 1| # |JANE SMITH|jane.smith@example.com|Los Angeles| 2| # +----------+--------------------+----------+---+ # Paso 3: Enriquecimiento (simulamos la adici\u00f3n de una columna \"region\") def enrich_data(df): # En un caso real, esto podr\u00eda ser una b\u00fasqueda en una tabla de mapeo de ciudades a regiones return df.withColumn(\"region\", when(col(\"city\") == \"New York\", \"East\") .when(col(\"city\") == \"Los Angeles\", \"West\") .otherwise(\"Central\")) enriched_df = enrich_data(cleaned_df) enriched_df.show() # +----------+--------------------+----------+---+-------+ # | name| email| city| id| region| # +----------+--------------------+----------+---+-------+ # | PETER JONES|peter.jones@example.com| Chicago| 4|Central| # | JOHN DOE|john.doe@example.com| New York| 1| East| # |JANE SMITH|jane.smith@example.com|Los Angeles| 2| West| # +----------+--------------------+----------+---+-------+ # Paso 4: Agregaci\u00f3n def aggregate_data(df): return df.groupBy(\"region\").agg(count(\"*\").alias(\"customer_count\")) aggregated_df = aggregate_data(enriched_df) aggregated_df.show() # +-------+--------------+ # | region|customer_count| # +-------+--------------+ # |Central| 1| # | East| 1| # | West| 1| # +-------+--------------+ spark.stop() Transformaci\u00f3n en Paralelo La transformaci\u00f3n en paralelo es una t\u00e9cnica que divide un gran conjunto de datos en subconjuntos m\u00e1s peque\u00f1os (chunks) y procesa estos subconjuntos simult\u00e1neamente utilizando m\u00faltiples hilos, procesos o nodos de un cl\u00faster distribuido. Este patr\u00f3n es fundamental en entornos de Big Data para aprovechar al m\u00e1ximo los recursos disponibles, ya sea en un solo servidor con m\u00faltiples n\u00facleos o en un cl\u00faster distribuido (como Spark o Hadoop). Al procesar datos en paralelo, se reduce significativamente el tiempo total de ejecuci\u00f3n y se mejora la escalabilidad del pipeline de transformaci\u00f3n. Procesar millones de registros de logs para extraer informaci\u00f3n relevante, donde cada archivo de log o un bloque de registros puede ser procesado independientemente. Con c\u00f3digo (ejemplo conceptual con Dask, una librer\u00eda Python para computaci\u00f3n paralela) : import dask.dataframe as dd import pandas as pd # Crear un DataFrame de Pandas grande data = {'value': range(100_000_000)} df = pd.DataFrame(data) # Convertir a Dask DataFrame (lo divide en particiones autom\u00e1ticamente) ddf = dd.from_pandas(df, npartitions=8) # Procesar\u00e1 en 8 particiones/tareas # Definir una funci\u00f3n de transformaci\u00f3n (ejemplo: elevar al cuadrado) def square_value(x): return x * x # Aplicar la transformaci\u00f3n en paralelo result_ddf = ddf['value'].apply(square_value, meta=('value', 'int64')) # Computar el resultado (esto dispara la ejecuci\u00f3n paralela) # Utiliza un scheduler local por defecto, que usa m\u00faltiples n\u00facleos final_result = result_ddf.compute() print(f\"Processed {len(final_result)} records.\") print(f\"First 5 results: {final_result.head()}\") # Otro ejemplo con PySpark (Spark ya es distribuido por defecto) from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"ParallelTransformation\").getOrCreate() # Cargar un dataset grande (ejemplo: CSV de 100 millones de registros) # spark.read.csv(\"s3://your-bucket/large_dataset.csv\", header=True, inferSchema=True).repartition(100) # Para este ejemplo, crearemos un DataFrame grande en memoria data = [(i,) for i in range(10_000_000)] df = spark.createDataFrame(data, [\"value\"]) # La transformaci\u00f3n se aplica autom\u00e1ticamente en paralelo por Spark transformed_df = df.withColumn(\"squared_value\", col(\"value\") * col(\"value\")) # Mostrar algunos resultados (esto activa la computaci\u00f3n distribuida) transformed_df.show(5) # +--------+-------------+ # | value|squared_value| # +--------+-------------+ # | 0| 0| # | 1| 1| # | 2| 4| # | 3| 9| # | 4| 16| # +--------+-------------+ # Escribir el resultado a un destino distribuido (ej. Parquet) # transformed_df.write.mode(\"overwrite\").parquet(\"s3://your-bucket/output/squared_values.parquet\") spark.stop() Lookup y Enriquecimiento Este patr\u00f3n implica combinar datos de una fuente principal con informaci\u00f3n adicional de una o varias fuentes secundarias para enriquecer los registros. El enriquecimiento puede lograrse mediante operaciones de join en bases de datos relacionales, b\u00fasquedas en tablas de referencia (dimensiones en un data warehouse), o llamadas a APIs externas para obtener informaci\u00f3n en tiempo real. Es crucial para a\u00f1adir contexto y valor a los datos brutos, permitiendo an\u00e1lisis m\u00e1s profundos. Enriquecer registros de transacciones de ventas con detalles del producto (nombre, categor\u00eda, precio unitario) y del cliente (datos demogr\u00e1ficos, historial de compras) que se encuentran en tablas separadas. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"LookupAndEnrichment\").getOrCreate() # Datos de transacciones (Hechos) transactions_data = [ (1, \"prodA\", 100, 50.0), (2, \"prodB\", 101, 75.0), (3, \"prodA\", 102, 50.0), (4, \"prodC\", 100, 120.0) ] transactions_df = spark.createDataFrame(transactions_data, [\"transaction_id\", \"product_id\", \"customer_id\", \"amount\"]) # Datos de productos (Dimensi\u00f3n) products_data = [ (\"prodA\", \"Laptop\", \"Electronics\"), (\"prodB\", \"Mouse\", \"Electronics\"), (\"prodC\", \"Keyboard\", \"Peripherals\") ] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"category\"]) # Datos de clientes (Dimensi\u00f3n) customers_data = [ (100, \"Alice\", \"USA\"), (101, \"Bob\", \"Canada\"), (102, \"Charlie\", \"Mexico\") ] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\", \"country\"]) # Enriquecimiento de transacciones con datos de productos enriched_transactions_df = transactions_df.join(products_df, \"product_id\", \"inner\") enriched_transactions_df.show() # +----------+--------------+-----------+------+------------+-----------+ # |product_id|transaction_id|customer_id|amount|product_name| category| # +----------+--------------+-----------+------+------------+-----------+ # | prodA| 1| 100| 50.0| Laptop|Electronics| # | prodA| 3| 102| 50.0| Laptop|Electronics| # | prodB| 2| 101| 75.0| Mouse|Electronics| # | prodC| 4| 100| 120.0| Keyboard|Peripherals| # +----------+--------------+-----------+------+------------+-----------+ # Enriquecimiento adicional con datos de clientes final_enriched_df = enriched_transactions_df.join(customers_df, \"customer_id\", \"inner\") final_enriched_df.show() # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ # |product_id|transaction_id|customer_id|amount|product_name| category|customer_name|country| # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ # | prodA| 1| 100| 50.0| Laptop|Electronics| Alice| USA| # | prodC| 4| 100| 120.0| Keyboard|Peripherals| Alice| USA| # | prodB| 2| 101| 75.0| Mouse|Electronics| Bob| Canada| # | prodA| 3| 102| 50.0| Laptop|Electronics| Charlie| Mexico| # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ spark.stop() 3.7.3. Patrones de Carga La fase de carga es la etapa final del proceso ETL, donde los datos transformados se mueven al destino final (data warehouse, data lake, base de datos anal\u00edtica). La eficiencia y la estrategia de carga son cruciales para el rendimiento y la integridad de los datos en el destino. Upsert (Insert/Update) El patr\u00f3n Upsert es una operaci\u00f3n que intenta insertar un registro en una tabla; si el registro ya existe (basado en una clave \u00fanica), se actualiza en lugar de insertarse un nuevo registro. Esto es particularmente \u00fatil en escenarios donde los datos de origen pueden contener actualizaciones para registros ya existentes o nuevos registros que se a\u00f1aden con el tiempo. El Upsert garantiza que el destino refleje el estado m\u00e1s actual de los datos sin crear duplicados ni requerir l\u00f3gica de DELETE e INSERT separadas. Mantener una tabla de clientes o productos donde se reciben actualizaciones espor\u00e1dicas y tambi\u00e9n nuevos registros. Delta Lake es un formato de tabla de almacenamiento de c\u00f3digo abierto que soporta operaciones MERGE INTO (Upsert). from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"UpsertPattern\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").getOrCreate() # Ruta para la tabla Delta delta_table_path = \"/tmp/delta/customer_profiles\" # Crear una tabla Delta inicial (si no existe) try: spark.read.format(\"delta\").load(delta_table_path).show() except Exception: initial_data = [ (1, \"Alice\", \"alice@example.com\"), (2, \"Bob\", \"bob@example.com\") ] initial_df = spark.createDataFrame(initial_data, [\"id\", \"name\", \"email\"]) initial_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path) print(\"Initial Delta table created.\") # Cargar la tabla Delta existente customer_profiles_df = spark.read.format(\"delta\").load(delta_table_path) print(\"Current customer profiles:\") customer_profiles_df.show() # Nuevos datos a upsert: un nuevo registro (id 3) y una actualizaci\u00f3n (id 1) new_data = [ (1, \"Alice Smith\", \"alice.smith@example.com\"), # Update (3, \"Charlie\", \"charlie@example.com\") # New ] updates_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"email\"]) print(\"Updates to apply:\") updates_df.show() # Realizar la operaci\u00f3n MERGE INTO (Upsert) from delta.tables import DeltaTable deltaTable = DeltaTable.forPath(spark, delta_table_path) deltaTable.alias(\"target\") \\ .merge( updates_df.alias(\"source\"), \"target.id = source.id\" ) \\ .whenMatchedUpdate(set = { \"name\" : col(\"source.name\"), \"email\" : col(\"source.email\") }) \\ .whenNotMatchedInsert(values = { \"id\" : col(\"source.id\"), \"name\" : col(\"source.name\"), \"email\" : col(\"source.email\") }) \\ .execute() print(\"Customer profiles after upsert:\") spark.read.format(\"delta\").load(delta_table_path).show() spark.stop() Slowly Changing Dimensions (SCD) Las Dimensiones de Cambio Lento (SCD) son un concepto crucial en el modelado de data warehouses para manejar cambios en los datos de dimensiones a lo largo del tiempo. A diferencia de las tablas de hechos que registran eventos transaccionales, las tablas de dimensiones describen entidades (como clientes, productos, ubicaciones) que pueden cambiar sus atributos. Existen varios tipos de SCD: SCD Tipo 1 (Sobreescritura) : El valor anterior se sobrescribe con el nuevo valor. Esto es simple de implementar, pero no conserva el historial de cambios. Es adecuado para correcciones o cuando el historial no es relevante. SCD Tipo 2 (Historial Completo) : Se crea una nueva fila en la tabla de dimensiones para cada cambio en un atributo clave. La fila anterior se \"cierra\" (por ejemplo, con una fecha de fin o una bandera de activo/inactivo), y la nueva fila se \"abre\". Esto permite analizar los datos de hechos con el estado de la dimensi\u00f3n en un punto espec\u00edfico del tiempo. Es el tipo m\u00e1s com\u00fan para la anal\u00edtica hist\u00f3rica. SCD Tipo 3 (Historial Limitado) : Se a\u00f1ade una nueva columna a la tabla de dimensiones para almacenar el valor anterior de un atributo espec\u00edfico. Solo mantiene un historial limitado (t\u00edpicamente el valor actual y el valor anterior). Es \u00fatil cuando solo se necesita rastrear un cambio reciente y no un historial completo. El cambio de direcci\u00f3n de un cliente en una empresa; con SCD Tipo 2, se podr\u00eda rastrear d\u00f3nde viv\u00eda el cliente en cada momento. Para SCD Tipo 2 en un data warehouse, supongamos que tenemos una tabla dim_customers y un nuevo lote de datos de clientes ( staging_customers ). -- Estructura de la tabla dim_customers para SCD Tipo 2 CREATE TABLE dim_customers ( customer_key INT IDENTITY(1,1) PRIMARY KEY, customer_id VARCHAR(50) NOT NULL, customer_name VARCHAR(255), address VARCHAR(255), start_date DATE, end_date DATE, is_current BOOLEAN ); -- Suponiendo que staging_customers tiene los nuevos datos (ej. id, name, address) -- L\u00f3gica para insertar nuevos clientes o actualizar existentes (SCD Tipo 2) INSERT INTO dim_customers (customer_id, customer_name, address, start_date, end_date, is_current) SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE, '9999-12-31', TRUE FROM staging_customers s LEFT JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = TRUE WHERE d.customer_id IS NULL; -- Solo insertar clientes nuevos -- Actualizar registros existentes (cerrar la versi\u00f3n anterior y abrir una nueva) MERGE INTO dim_customers AS target USING ( SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE AS new_start_date FROM staging_customers s JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = TRUE WHERE s.customer_name <> d.customer_name OR s.address <> d.address -- Solo si hay cambios en atributos relevantes ) AS source ON target.customer_id = source.customer_id AND target.is_current = TRUE WHEN MATCHED THEN UPDATE SET target.end_date = DATEADD(day, -1, source.new_start_date), target.is_current = FALSE; -- Insertar las nuevas versiones de los registros actualizados INSERT INTO dim_customers (customer_id, customer_name, address, start_date, end_date, is_current) SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE, '9999-12-31', TRUE FROM staging_customers s JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = FALSE AND d.end_date = DATEADD(day, -1, CURRENT_DATE) WHERE s.customer_name <> d.customer_name OR s.address <> d.address; Bulk Loading El Bulk Loading, o carga masiva, es una t\u00e9cnica optimizada para cargar grandes vol\u00famenes de datos en una base de datos o sistema de almacenamiento. En lugar de procesar cada registro individualmente con sentencias INSERT una por una (lo cual es ineficiente para grandes conjuntos de datos), las operaciones de bulk loading agrupan los datos y los cargan de manera m\u00e1s eficiente, minimizando la sobrecarga transaccional. Esto a menudo implica deshabilitar temporalmente \u00edndices, restricciones ( constraints ) y triggers durante la carga, y luego reconstruirlos una vez que la carga ha finalizado para maximizar la velocidad. Es el m\u00e9todo preferido para las cargas iniciales o para a\u00f1adir grandes conjuntos de datos peri\u00f3dicamente. Cargar archivos Parquet o CSV directamente en una tabla de un data warehouse en la nube como Snowflake, Amazon Redshift o Google BigQuery, utilizando sus comandos nativos de carga masiva. Supongamos que en Snowflake, tenemos un archivo CSV llamado sales_data.csv en un bucket S3. -- Crear un formato de archivo para CSV CREATE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1 NULL_IF = ('\\\\N', 'NULL') EMPTY_FIELD_AS_NULL = TRUE; -- Crear un stage externo que apunte a un bucket S3 CREATE STAGE my_s3_stage URL = 's3://your-s3-bucket/data/sales/' CREDENTIALS = (AWS_KEY_ID = 'YOUR_AWS_ACCESS_KEY_ID', AWS_SECRET_KEY = 'YOUR_AWS_SECRET_ACCESS_KEY'); -- Crear la tabla donde se cargar\u00e1n los datos CREATE TABLE sales_transactions ( transaction_id INT, product_id VARCHAR(50), customer_id INT, amount DECIMAL(10, 2), transaction_date DATE ); -- Usar el comando COPY INTO para realizar la carga masiva desde S3 a la tabla COPY INTO sales_transactions FROM @my_s3_stage/sales_data.csv FILE_FORMAT = (FORMAT_NAME = my_csv_format) ON_ERROR = 'CONTINUE'; -- Contin\u00faa la carga incluso si hay errores en algunas filas -- Ejemplo de carga masiva en Google BigQuery desde un archivo CSV en GCS -- Asumiendo que el esquema est\u00e1 en un archivo JSON o se detecta autom\u00e1ticamente bq load --source_format=CSV --skip_leading_rows=1 \\ your_dataset.your_table gs://your-gcs-bucket/data/sales/sales_data.csv \\ ./schema.json # O BigQuery detecta el esquema autom\u00e1ticamente 3.7.4. Patrones de Control y Monitoreo La resiliencia y la observabilidad son cruciales en los pipelines de Big Data. Estos patrones aseguran que los procesos puedan recuperarse de fallas y que se puedan identificar y resolver problemas de manera eficiente. Checkpoint y Restart El patr\u00f3n Checkpoint y Restart implica guardar peri\u00f3dicamente el estado de un proceso de larga duraci\u00f3n en puntos de control ( checkpoints ). En caso de una falla (por ejemplo, interrupci\u00f3n de la red, error de software, fallo de un nodo), el proceso puede reanudarse desde el \u00faltimo checkpoint exitoso en lugar de tener que comenzar desde el principio. Esto no solo ahorra tiempo y recursos al evitar el reprocesamiento de datos ya completados, sino que tambi\u00e9n garantiza la integridad de los datos en entornos distribuidos donde las fallas son comunes. Es fundamental para la robustez de los pipelines de datos distribuidos y en tiempo real. Un proceso de Spark Streaming que procesa datos de Kafka. Si el cl\u00faster de Spark falla, puede reiniciar desde el \u00faltimo offset de Kafka procesado con \u00e9xito, sin perder datos ni procesar duplicados. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_timestamp import os spark = SparkSession.builder.appName(\"SparkStreamingCheckpoint\") \\ .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark_checkpoint\") \\ .getOrCreate() # Limpiar el directorio de checkpoint para una nueva ejecuci\u00f3n if os.path.exists(\"/tmp/spark_checkpoint\"): import shutil shutil.rmtree(\"/tmp/spark_checkpoint\") print(\"Cleaned existing checkpoint directory.\") # Simular un stream de datos desde una fuente de datos (ej. Kafka) # Para este ejemplo, usaremos una fuente de datos en memoria para simplicidad # En un caso real, esto ser\u00eda: spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"input_topic\").load() data = [(\"A\", 1), (\"B\", 2), (\"C\", 3), (\"D\", 4)] df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load() # Transformaci\u00f3n de ejemplo transformed_df = df_stream.withColumn(\"processed_time\", current_timestamp()) \\ .withColumn(\"value_squared\", col(\"value\") * col(\"value\")) # Escribir el stream a la consola con un checkpoint query = transformed_df \\ .writeStream \\ .outputMode(\"append\") \\ .format(\"console\") \\ .option(\"truncate\", False) \\ .trigger(processingTime=\"5 seconds\") \\ .start() # Para simular una falla, puedes detener el script manualmente despu\u00e9s de unos segundos # y luego reiniciarlo. Spark deber\u00eda continuar desde donde se qued\u00f3. query.awaitTermination() Dead Letter Queue (DLQ) Una Dead Letter Queue (DLQ), o cola de mensajes no procesables, es un mecanismo donde se enrutan los mensajes o registros que fallaron en su procesamiento. En un pipeline de datos, si un registro no cumple con las validaciones, causa un error de procesamiento o no puede ser entregado al destino despu\u00e9s de varios reintentos, en lugar de detener todo el pipeline, se mueve a la DLQ. Esto permite que el flujo de datos principal contin\u00fae ininterrumpido mientras los registros problem\u00e1ticos pueden ser analizados, depurados y, si es posible, reprocesados manualmente o a trav\u00e9s de un proceso separado. Mejora la robustez y la resiliencia del sistema. En un sistema de procesamiento de eventos en tiempo real con Apache Kafka y Kafka Streams, los mensajes que fallan la deserializaci\u00f3n o validaci\u00f3n pueden ser enviados a una DLQ. Procesamiento de mensajes fallidos en AWS SQS y Lambda, supongamos que una funci\u00f3n Lambda procesa mensajes de SQS. Si falla, el mensaje se env\u00eda a una DLQ. # C\u00f3digo Python para una funci\u00f3n AWS Lambda (ejemplo conceptual) import json import boto3 def lambda_handler(event, context): sqs_client = boto3.client('sqs') dlq_url = '[https://sqs.us-east-1.amazonaws.com/123456789012/MyDeadLetterQueue](https://sqs.us-east-1.amazonaws.com/123456789012/MyDeadLetterQueue)' for record in event['Records']: message_body = json.loads(record['body']) try: # L\u00f3gica de procesamiento de datos if message_body.get('error_flag'): raise ValueError(\"Simulating a processing error for this message.\") print(f\"Successfully processed message: {message_body}\") # Si se procesa con \u00e9xito, el mensaje se elimina de la cola principal autom\u00e1ticamente except Exception as e: print(f\"Error processing message: {message_body}, Error: {e}\") # Enviar el mensaje a la DLQ # En AWS SQS, esto se configura directamente en la cola principal con una Redrive Policy # La Lambda no necesita enviar expl\u00edcitamente a la DLQ si la Redrive Policy est\u00e1 configurada # pero este pseudo-c\u00f3digo muestra la intenci\u00f3n de manejo de errores sqs_client.send_message( QueueUrl=dlq_url, MessageBody=json.dumps({\"original_message\": message_body, \"error\": str(e)}) ) return { 'statusCode': 200, 'body': json.dumps('Processing complete') } Configuraci\u00f3n de SQS (Redrive Policy) : En la consola de AWS SQS, para la cola principal, se configura una \"Redrive policy\" que especifica la DLQ a la que se deben enviar los mensajes despu\u00e9s de un n\u00famero determinado de intentos fallidos. Idempotencia La idempotencia es una propiedad de una operaci\u00f3n que, cuando se ejecuta m\u00faltiples veces con los mismos par\u00e1metros de entrada, produce el mismo resultado y efecto secundario que si se hubiera ejecutado una sola vez. En el contexto de los pipelines ETL, dise\u00f1ar procesos idempotentes es crucial para la recuperaci\u00f3n de errores y el reprocesamiento seguro. Si un paso del ETL falla y se reintenta, la idempotencia asegura que los datos no se dupliquen o corrompan en el destino. Esto es especialmente importante en sistemas distribuidos donde las operaciones pueden fallar parcialmente o los mensajes pueden entregarse varias veces. Un proceso de carga de datos que utiliza una clave primaria para UPSERT registros en lugar de solo INSERT , garantizando que si se ejecuta dos veces, el registro solo se actualice o se inserte una vez. -- Suponiendo una tabla de destino `processed_transactions` con una clave primaria `transaction_id` -- y una tabla de staging `new_transactions` -- Enfoque no idempotente (podr\u00eda insertar duplicados si se re-ejecuta) -- INSERT INTO processed_transactions (transaction_id, amount, status) -- SELECT transaction_id, amount, status FROM new_transactions; -- Enfoque idempotente usando UPSERT (para PostgreSQL, por ejemplo) INSERT INTO processed_transactions (transaction_id, amount, status, last_processed_at) SELECT transaction_id, amount, status, NOW() FROM new_transactions ON CONFLICT (transaction_id) DO UPDATE SET amount = EXCLUDED.amount, status = EXCLUDED.status, last_processed_at = EXCLUDED.last_processed_at; -- Otro ejemplo conceptual: procesar archivos y marcarlos como procesados -- Si el proceso lee archivos de un bucket, y los mueve o renombra despu\u00e9s de procesarlos -- Esto es idempotente porque no procesar\u00e1 el mismo archivo dos veces si ya fue movido/renombrado. # Pseudo-c\u00f3digo Python para un procesamiento de archivos idempotente def process_file_idempotently(file_path, processed_path): if not file_exists(file_path): print(f\"File {file_path} does not exist, likely already processed.\") return try: # L\u00f3gica de procesamiento del archivo print(f\"Processing file: {file_path}\") # ... hacer algo con el archivo ... # Mover el archivo a una carpeta de \"procesados\" move_file(file_path, processed_path) print(f\"File {file_path} moved to {processed_path}\") except Exception as e: print(f\"Error processing {file_path}: {e}\") # En caso de error, el archivo permanece en la ubicaci\u00f3n original para reintento 3.7.5. Patrones Arquitect\u00f3nicos Estos patrones definen la estructura general de los sistemas de Big Data, abordando c\u00f3mo los componentes interact\u00faan para manejar el flujo de datos. Staging Area Una Staging Area, o \u00e1rea de preparaci\u00f3n, es un espacio de almacenamiento temporal donde los datos extra\u00eddos de las fuentes se almacenan antes de ser transformados y cargados al destino final (generalmente un data warehouse o data lake). Esta \u00e1rea sirve como un \"amortiguador\" entre los sistemas fuente y el destino. Ofrece m\u00faltiples beneficios: Punto de Recuperaci\u00f3n : Si una transformaci\u00f3n falla, los datos brutos a\u00fan est\u00e1n disponibles en el \u00e1rea de staging para ser reprocesados sin necesidad de re-extraer desde la fuente. Aislamiento : Protege los sistemas fuente de las cargas de procesamiento de las transformaciones y permite que las validaciones y limpieza iniciales se realicen en un entorno separado. Consistencia : Permite consolidar datos de m\u00faltiples fuentes heterog\u00e9neas en un formato com\u00fan antes de la transformaci\u00f3n. Validaci\u00f3n : Es un lugar ideal para realizar comprobaciones de calidad de datos iniciales. Cargar archivos CSV desde un sistema de punto de venta a un bucket de AWS S3 (staging area) antes de que un trabajo de Spark los procese y los cargue en un data warehouse en Redshift. # Pseudo-c\u00f3digo de un script de AWS Glue (PySpark) import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) # 1. Extracci\u00f3n de datos brutos a la Staging Area (ej. S3) # Este paso generalmente se hace fuera de Glue, por ejemplo, v\u00eda AWS DataSync, Kinesis Firehose o FTP. # Asumimos que los archivos ya est\u00e1n en s3://your-bucket/raw_data/ # 2. Leer datos desde la Staging Area datasource0 = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", connection_options={\"paths\": [\"s3://your-bucket/raw_data/sales_data/\"], \"recurse\": True}, format=\"csv\", format_options={\"withHeader\": True, \"separator\": \",\"} ) # 3. Aplicar transformaciones applymapping1 = ApplyMapping.apply(frame=datasource0, mappings=[ (\"transaction_id\", \"string\", \"transaction_id\", \"long\"), (\"product_name\", \"string\", \"product_name\", \"string\"), (\"amount\", \"string\", \"amount\", \"double\") ]) # 4. Cargar a la tabla final (ej. Redshift) datasink2 = glueContext.write_dynamic_frame.from_jdbc_conf( catalog_connection=\"redshift_connection\", connection_options={\"dbtable\": \"public.processed_sales\", \"database\": \"dev\"}, redshift_tmp_dir=args[\"TempDir\"], frame=applymapping1 ) job.commit() Hub and Spoke El patr\u00f3n Hub and Spoke, o \"Centro y Radios\", es una arquitectura que centraliza la extracci\u00f3n de datos de m\u00faltiples fuentes en un punto central (el \"Hub\") antes de distribuir esos datos a varios destinos o consumidores (los \"Spokes\"). En lugar de tener conexiones punto a punto entre cada fuente y cada destino (lo que puede volverse inmanejable con muchas integraciones), los datos fluyen a trav\u00e9s de un nodo centralizado. Este patr\u00f3n simplifica la gesti\u00f3n de integraciones, mejora la consistencia de los datos, facilita el monitoreo centralizado y reduce la complejidad al escalar. Una empresa con m\u00faltiples sistemas de origen (CRM, ERP, bases de datos de clientes) que env\u00edan datos a un Data Lake central (usando Apache Kafka como Hub), desde donde los datos se distribuyen a data warehouses, herramientas de BI, o modelos de Machine Learning (los Spokes). # Pseudo-c\u00f3digo: M\u00faltiples productores enviando a Kafka (el Hub) # Productor 1 (CRM) from kafka import KafkaProducer import json producer_crm = KafkaProducer(bootstrap_servers='localhost:9099') producer_crm.send('raw_data_hub_topic', json.dumps({\"source\": \"CRM\", \"data\": {\"customer_id\": \"C123\", \"name\": \"Alice\"}}).encode('utf-8')) # Productor 2 (ERP) producer_erp = KafkaProducer(bootstrap_servers='localhost:9099') producer_erp.send('raw_data_hub_topic', json.dumps({\"source\": \"ERP\", \"data\": {\"order_id\": \"O456\", \"item\": \"Laptop\"}}).encode('utf-8')) # Un consumidor centralizado (el Hub) que ingiere y clasifica/transforma datos # Este podr\u00eda ser un proceso Spark Streaming, un Kafka Streams application, etc. # Su funci\u00f3n es tomar los datos brutos del topic 'raw_data_hub_topic' # y luego, despu\u00e9s de una limpieza o estandarizaci\u00f3n m\u00ednima, # publicarlos en topics m\u00e1s espec\u00edficos para los spokes. # Ejemplo: 'customers_topic', 'orders_topic' # Pseudo-c\u00f3digo de un consumidor/procesador central (parte del Hub) from kafka import KafkaConsumer import json consumer_hub = KafkaConsumer('raw_data_hub_topic', bootstrap_servers='localhost:9099', value_deserializer=lambda x: json.loads(x.decode('utf-8'))) producer_processed = KafkaProducer(bootstrap_servers='localhost:9099') for message in consumer_hub: data = message.value if data['source'] == 'CRM': # Simple transformaci\u00f3n: agregar un timestamp data['data']['processed_at'] = time.time() producer_processed.send('processed_customers_topic', json.dumps(data['data']).encode('utf-8')) elif data['source'] == 'ERP': data['data']['processed_at'] = time.time() producer_processed.send('processed_orders_topic', json.dumps(data['data']).encode('utf-8')) # M\u00faltiples consumidores (Spokes) leyendo de topics espec\u00edficos # Consumidor Spoke 1 (Data Warehouse para clientes) consumer_dw_customers = KafkaConsumer('processed_customers_topic', bootstrap_servers='localhost:9099') # L\u00f3gica para cargar en Data Warehouse # Consumidor Spoke 2 (Sistema de anal\u00edtica de pedidos) consumer_analytics_orders = KafkaConsumer('processed_orders_topic', bootstrap_servers='localhost:9099') # L\u00f3gica para alimentar sistema de anal\u00edtica Event-Driven ETL El patr\u00f3n Event-Driven ETL (ETL impulsado por eventos) transforma los procesos ETL de operaciones programadas (batch) a operaciones reactivas que se disparan en respuesta a eventos espec\u00edficos. Estos eventos pueden ser la llegada de un nuevo archivo a un bucket de almacenamiento, un cambio en una base de datos (usando CDC), la publicaci\u00f3n de un mensaje en una cola de mensajes, o una se\u00f1al de un sistema externo. Este enfoque reduce la latencia de los datos, optimiza el uso de recursos al procesar solo cuando hay datos nuevos, y permite una mayor agilidad en el flujo de informaci\u00f3n. Es fundamental para arquitecturas de datos en tiempo real y microservicios. Cuando se carga un archivo de log a un bucket AWS S3, un evento de S3 activa una funci\u00f3n AWS Lambda, que a su vez invoca un trabajo de AWS Glue para procesar el log y cargarlo en un data lake. Configuraci\u00f3n de S3 para enviar eventos a Lambda: Configurar una notificaci\u00f3n de evento en un bucket S3 para s3:ObjectCreated:* que apunte a una funci\u00f3n Lambda. C\u00f3digo de la funci\u00f3n AWS Lambda (Python): import json import boto3 def lambda_handler(event, context): print(f\"Received event: {json.dumps(event)}\") glue_client = boto3.client('glue') for record in event['Records']: bucket_name = record['s3']['bucket']['name'] object_key = record['s3']['object']['key'] print(f\"New object created in bucket: {bucket_name}, key: {object_key}\") # Determinar qu\u00e9 trabajo de Glue ejecutar basado en el prefijo del objeto, etc. # Aqu\u00ed, un ejemplo simple que siempre dispara el mismo trabajo job_name = \"MyGlueETLJob\" try: # Disparar el trabajo de AWS Glue response = glue_client.start_job_run( JobName=job_name, Arguments={ '--input_bucket': bucket_name, '--input_key': object_key } ) print(f\"Started Glue job {job_name} with RunId: {response['JobRunId']}\") except Exception as e: print(f\"Error starting Glue job {job_name}: {e}\") raise e # Re-lanzar la excepci\u00f3n para que Lambda la maneje si es necesario return { 'statusCode': 200, 'body': json.dumps('Glue job triggered successfully!') } C\u00f3digo de AWS Glue Job (PySpark): Este trabajo de Glue leer\u00eda el archivo especificado por --input_key y lo procesar\u00eda. # Pseudo-c\u00f3digo para el trabajo de AWS Glue (PySpark) import sys from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_bucket', 'input_key']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) input_path = f\"s3://{args['input_bucket']}/{args['input_key']}\" print(f\"Processing file: {input_path}\") # Leer el archivo que activ\u00f3 el evento df = spark.read.csv(input_path, header=True, inferSchema=True) # Realizar transformaciones (ejemplo: contar filas) row_count = df.count() print(f\"File {args['input_key']} has {row_count} rows.\") # Escribir el resultado a otro destino (ej. una tabla procesada en S3 o un Data Warehouse) output_path = f\"s3://your-processed-bucket/processed_data/{args['input_key'].replace('.csv', '')}_processed.parquet\" df.write.mode(\"overwrite\").parquet(output_path) print(f\"Processed data written to: {output_path}\") job.commit() 3.7.6. Estrategias de optimizaci\u00f3n de costos La optimizaci\u00f3n de costos en la nube es tan crucial como el rendimiento. Permite a las organizaciones escalar sin incurrir en gastos excesivos, garantizando la sostenibilidad financiera de las operaciones de Big Data. Autoscaling El Autoscaling (autoescalado) es la capacidad de un sistema en la nube para ajustar autom\u00e1ticamente la cantidad de recursos de computaci\u00f3n utilizados en funci\u00f3n de la demanda actual. Esto significa que la infraestructura puede aumentar (escalar hacia arriba) durante los picos de carga y disminuir (escalar hacia abajo) durante los per\u00edodos de baja actividad. El autoscaling es fundamental para la optimizaci\u00f3n de costos porque se paga solo por los recursos que realmente se utilizan, evitando el sobreaprovisionamiento y el desperdicio de capacidad. Aplica tanto a cl\u00fasteres de computaci\u00f3n (ej. instancias EC2, nodos de Spark) como a servicios de bases de datos o colas. Un cl\u00faster de Apache Spark en Amazon EMR que autom\u00e1ticamente a\u00f1ade o quita nodos de worker seg\u00fan la carga de trabajo de los jobs. # Configuraci\u00f3n de Auto Scaling en un Cluster de EMR (ejemplo simplificado) { \"InstanceGroups\": [ { \"InstanceRole\": \"MASTER\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 1 }, { \"InstanceRole\": \"CORE\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"AutoScalingPolicy\": { \"Constraints\": { \"MinCapacity\": 2, \"MaxCapacity\": 10 }, \"Rules\": [ { \"Name\": \"ScaleOut\", \"Description\": \"Scale out when YARN memory utilization is high\", \"Action\": { \"Market\": \"ON_DEMAND\", \"SimpleScalingPolicyConfiguration\": { \"AdjustmentType\": \"CHANGE_IN_CAPACITY\", \"ScalingAdjustment\": 1, \"CoolDown\": 300 } }, \"Trigger\": { \"CloudWatchAlarmDefinition\": { \"ComparisonOperator\": \"GREATER_THAN_OR_EQUAL\", \"EvaluationPeriods\": 5, \"MetricName\": \"YARNMemoryAvailablePercentage\", \"Namespace\": \"AWS/ElasticMapReduce\", \"Period\": 300, \"Statistic\": \"Average\", \"Threshold\": 15, # Trigger if available memory drops below 15% \"Unit\": \"Percent\", \"Dimensions\": [ {\"Name\": \"ClusterId\", \"Value\": \"${emr.cluster.id}\"} ] } } }, { \"Name\": \"ScaleIn\", \"Description\": \"Scale in when YARN memory utilization is low\", \"Action\": { \"Market\": \"ON_DEMAND\", \"SimpleScalingPolicyConfiguration\": { \"AdjustmentType\": \"CHANGE_IN_CAPACITY\", \"ScalingAdjustment\": -1, \"CoolDown\": 300 } }, \"Trigger\": { \"CloudWatchAlarmDefinition\": { \"ComparisonOperator\": \"LESS_THAN_OR_EQUAL\", \"EvaluationPeriods\": 5, \"MetricName\": \"YARNMemoryAvailablePercentage\", \"Namespace\": \"AWS/ElasticMapReduce\", \"Period\": 300, \"Statistic\": \"Average\", \"Threshold\": 80, # Trigger if available memory is above 80% (low utilization) \"Unit\": \"Percent\", \"Dimensions\": [ {\"Name\": \"ClusterId\", \"Value\": \"${emr.cluster.id}\"} ] } } } ] } } ] } Uso de spot instances Las spot instances (instancias spot) son instancias de computaci\u00f3n en la nube que se ofrecen a un precio significativamente reducido (hasta un 90% de descuento en comparaci\u00f3n con las instancias bajo demanda) a cambio de la posibilidad de que el proveedor de la nube las recupere (interrumpa) con poca antelaci\u00f3n si necesita la capacidad. Son ideales para cargas de trabajo tolerantes a fallos, no cr\u00edticas, o aquellas que pueden resumirse f\u00e1cilmente, como trabajos de procesamiento de Big Data por lotes que pueden reintentar tareas o donde los datos pueden ser reprocesados. Su uso puede generar ahorros masivos en la infraestructura de computaci\u00f3n. Ejecutar un gran trabajo de Spark para un an\u00e1lisis de datos que no es de misi\u00f3n cr\u00edtica y que se ejecuta durante la noche. Si algunas instancias spot son interrumpidas, Spark puede redistribuir el trabajo a otras instancias o reintentar las tareas fallidas. Al crear un cl\u00faster EMR, se puede especificar el tipo de instancias para los grupos de instancias Core y Task. # Configuraci\u00f3n de un cluster EMR usando instancias Spot para los grupos de Core y Task { \"Name\": \"MySpotEMRCluster\", \"ReleaseLabel\": \"emr-6.x.0\", \"Applications\": [ {\"Name\": \"Spark\"}, {\"Name\": \"Hadoop\"} ], \"Instances\": { \"InstanceGroups\": [ { \"Name\": \"Master Instance Group\", \"InstanceRole\": \"MASTER\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 1 }, { \"Name\": \"Core Instance Group\", \"InstanceRole\": \"CORE\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"Market\": \"SPOT\", # Usar Spot Instances para Core \"BidPrice\": \"0.50\" # Por ejemplo, ofertar hasta $0.50 por hora }, { \"Name\": \"Task Instance Group\", \"InstanceRole\": \"TASK\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"Market\": \"SPOT\", # Usar Spot Instances para Task \"BidPriceAsPercentageOfOnDemandPrice\": 100 # Ofertar el 100% del precio bajo demanda, a\u00fan as\u00ed es m\u00e1s barato } ] }, \"JobFlowRole\": \"EMR_EC2_DefaultRole\", \"ServiceRole\": \"EMR_DefaultRole\" } Dise\u00f1o para uso eficiente del almacenamiento y ejecuci\u00f3n La optimizaci\u00f3n del almacenamiento y la ejecuci\u00f3n son pilares de la reducci\u00f3n de costos en la nube, ya que ambos recursos son facturados por uso. Un dise\u00f1o eficiente implica: Formatos de Almacenamiento : Usar formatos de archivo optimizados para Big Data como Parquet u ORC. Estos formatos son columnares, lo que permite una mayor compresi\u00f3n y una lectura m\u00e1s eficiente al seleccionar solo las columnas necesarias. Tambi\u00e9n soportan esquemas y metadatos, mejorando la interoperabilidad. Compresi\u00f3n de Datos : Aplicar algoritmos de compresi\u00f3n (Snappy, Gzip, Zstd) a los datos almacenados. Esto reduce el espacio de almacenamiento y, consecuentemente, el costo. Adem\u00e1s, al haber menos datos que transferir, se mejora el rendimiento de la lectura. Particionamiento y Agrupaci\u00f3n (Bucketing) : Organizar los datos en el almacenamiento (ej. S3, HDFS) en directorios l\u00f3gicos (particiones) basados en columnas de uso frecuente (ej. fecha, regi\u00f3n). Esto permite que las consultas escaneen solo un subconjunto de los datos, reduciendo el volumen de I/O y los costos de c\u00f3mputo. El bucketing organiza datos dentro de las particiones para optimizar joins y agregaciones. Selecci\u00f3n de Instancias y Tipo de Computaci\u00f3n : Elegir el tipo y tama\u00f1o de instancia adecuados para la carga de trabajo. No sobre-aprovisionar CPU o memoria. Considerar el uso de servicios sin servidor (serverless) como AWS Lambda o Google Cloud Functions para tareas de corta duraci\u00f3n, ya que solo se paga por el tiempo de ejecuci\u00f3n. Optimizaci\u00f3n de Consultas : Escribir consultas SQL o scripts de procesamiento que minimicen el escaneo de datos, aprovechen los \u00edndices o las particiones, y eviten operaciones costosas como FULL SCAN o CROSS JOIN innecesarios. Almacenar datos en un Data Lake en S3 en formato Parquet, particionados por a\u00f1o/mes/d\u00eda , y usar Spark para procesar solo las particiones relevantes para una consulta. # PySpark y almacenamiento en Parquet particionado from pyspark.sql import SparkSession from pyspark.sql.functions import col, year, month, dayofmonth spark = SparkSession.builder.appName(\"EfficientStorageAndExecution\").getOrCreate() # Simular la creaci\u00f3n de un DataFrame de logs log_data = [ (\"user1\", \"login\", \"2023-01-01 10:00:00\"), (\"user2\", \"logout\", \"2023-01-01 10:05:00\"), (\"user1\", \"purchase\", \"2023-01-02 11:15:00\"), (\"user3\", \"login\", \"2023-01-02 12:00:00\"), (\"user4\", \"view_product\", \"2023-02-15 09:30:00\") ] logs_df = spark.createDataFrame(log_data, [\"user_id\", \"event_type\", \"event_timestamp\"]) # Convertir el timestamp a tipo de dato de fecha y extraer componentes para particionamiento logs_df = logs_df.withColumn(\"event_date\", col(\"event_timestamp\").cast(\"date\")) \\ .withColumn(\"year\", year(col(\"event_date\"))) \\ .withColumn(\"month\", month(col(\"event_date\"))) \\ .withColumn(\"day\", dayofmonth(col(\"event_date\"))) # Escribir el DataFrame en formato Parquet, particionado por a\u00f1o, mes, y d\u00eda # Esto crear\u00e1 una estructura de carpetas como s3://your-bucket/logs/year=2023/month=1/day=1/ output_path = \"/tmp/logs_partitioned_parquet\" # En un entorno cloud, ser\u00eda s3://your-bucket/logs/ logs_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"day\").parquet(output_path) print(f\"Data written to {output_path} with partitioning.\") # Leer solo los datos de una partici\u00f3n espec\u00edfica para una consulta (eficiente) # Por ejemplo, para obtener logs del 1 de enero de 2023 query_df = spark.read.parquet(f\"{output_path}/year=2023/month=1/day=1\") print(\"\\nLogs for 2023-01-01:\") query_df.show() # +-------+----------+-------------------+----------+-----+-----+---+ # |user_id|event_type| event_timestamp|event_date| year|month|day| # +-------+----------+-------------------+----------+-----+-----+---+ # | user1| login|2023-01-01 10:00:00|2023-01-01| 2023| 1| 1| # | user2| logout|2023-01-01 10:05:00|2023-01-01| 2023| 1| 1| # +-------+----------+-------------------+----------+-----+-----+---+ # Al consultar, Spark solo leer\u00e1 los archivos dentro de esa partici\u00f3n espec\u00edfica, # ahorrando I/O y c\u00f3mputo. spark.stop() Tarea Dise\u00f1o de un Pipeline ETL Event-Driven (Te\u00f3rico) : Imagina que trabajas para una empresa de e-commerce. Cuando se carga un nuevo archivo CSV de \"devoluciones de productos\" a un bucket de S3, necesitas que se active autom\u00e1ticamente un proceso ETL para: Cargar el CSV. Validar que las columnas clave (ID de producto, cantidad, fecha de devoluci\u00f3n) no est\u00e9n vac\u00edas. Enriquecer los datos con el nombre del producto y la categor\u00eda, consultando una tabla de productos existente. Actualizar una tabla de m\u00e9tricas de devoluciones, realizando un UPSERT para evitar duplicados y manteniendo un registro \u00fanico por producto y fecha de devoluci\u00f3n. Describe paso a paso c\u00f3mo dise\u00f1ar\u00edas esta arquitectura en la nube usando servicios de AWS (S3, Lambda, Glue, Redshift/DynamoDB para la tabla de productos y la de m\u00e9tricas). Optimizaci\u00f3n de Costos en un Cluster Spark (An\u00e1lisis de Caso) : Tu equipo ha notado que el costo de su cl\u00faster de Spark (ejecutado en EMR o Databricks) para los trabajos ETL diarios se ha disparado. Los trabajos se ejecutan una vez al d\u00eda y tardan aproximadamente 3 horas. Actualmente, utilizan instancias bajo demanda grandes. Prop\u00f3n al menos tres estrategias espec\u00edficas, basadas en lo aprendido, para reducir significativamente estos costos, justificando cada una con sus pros y contras. Implementaci\u00f3n de SCD Tipo 2 (SQL) : Dise\u00f1a una tabla de dimensiones para dim_productos que soporte SCD Tipo 2. La tabla debe incluir al menos product_id , product_name , category , price , start_date , end_date , y is_current . Luego, escribe las sentencias SQL (simulando una base de datos relacional) para manejar un escenario donde: Llega un nuevo producto. El precio de un producto existente cambia. El nombre de un producto existente cambia. Aseg\u00farate de que la l\u00f3gica de actualizaci\u00f3n cierre la versi\u00f3n anterior y cree una nueva. Desarrollo de un Patr\u00f3n de Extracci\u00f3n Incremental (Python/Pandas o PySpark) : Escribe un script en Python (puedes usar Pandas para simular DataFrames peque\u00f1os o PySpark si tienes un entorno) que demuestre la extracci\u00f3n incremental. Simula una tabla de origen con una columna last_updated_at . Crea una l\u00f3gica para: Mantener un \"\u00faltimo timestamp procesado\" en una variable o archivo. Extraer solo los registros donde last_updated_at es mayor que el \u00faltimo timestamp procesado. Actualizar el \"\u00faltimo timestamp procesado\" despu\u00e9s de una extracci\u00f3n exitosa. Proporciona un ejemplo de datos de entrada y salida para dos ejecuciones sucesivas del script. Dise\u00f1o Idempotente para Carga de Archivos (Conceptos) : Explica c\u00f3mo har\u00edas que un proceso de carga de archivos (que lee archivos de un directorio de entrada y los mueve a un directorio de \"procesados\") sea idempotente. \u00bfQu\u00e9 suceder\u00eda si el proceso fallara justo despu\u00e9s de leer un archivo pero antes de moverlo? \u00bfC\u00f3mo asegurar\u00edas que al reintentar el proceso, ese archivo no sea procesado dos veces, o que su estado final sea el mismo que si se hubiera procesado una sola vez correctamente?","title":"Patrones de Dise\u00f1o y Optimizaci\u00f3n en la Nube"},{"location":"tema37/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema37/#tema-37-patrones-de-diseno-y-optimizacion-en-la-nube","text":"Objetivo : Consolidar las buenas pr\u00e1cticas de dise\u00f1o y optimizaci\u00f3n para el desarrollo de soluciones de Big Data en entornos de nube, enfoc\u00e1ndose en la eficiencia operativa, la robustez del sistema y el control de costos. Introducci\u00f3n : El dise\u00f1o y la implementaci\u00f3n de soluciones de Big Data en la nube presentan oportunidades sin precedentes para la escalabilidad, flexibilidad y velocidad de procesamiento. Sin embargo, para capitalizar estas ventajas y evitar trampas comunes como los costos descontrolados o la ineficiencia operativa, es crucial comprender y aplicar patrones de dise\u00f1o y estrategias de optimizaci\u00f3n probadas. Este tema profundiza en las mejores pr\u00e1cticas para la extracci\u00f3n, transformaci\u00f3n, carga (ETL), el control y monitoreo de datos, as\u00ed como en patrones arquitect\u00f3nicos y t\u00e9cnicas espec\u00edficas para minimizar los costos en infraestructuras cloud. Desarrollo : La adopci\u00f3n de patrones de dise\u00f1o en el \u00e1mbito de Big Data y la nube no solo estandariza la forma en que se construyen los pipelines de datos, sino que tambi\u00e9n fomenta la reutilizaci\u00f3n, mejora la mantenibilidad y asegura la resiliencia de los sistemas. Al integrar estas pr\u00e1cticas desde la fase de dise\u00f1o, se pueden anticipar y mitigar problemas como la inconsistencia de datos, los cuellos de botella en el rendimiento y los gastos excesivos. Adem\u00e1s, una comprensi\u00f3n profunda de las estrategias de optimizaci\u00f3n de costos en la nube es vital para garantizar la viabilidad a largo plazo de las soluciones implementadas, permitiendo a las organizaciones escalar sus operaciones de datos de manera sostenible.","title":"Tema 3.7. Patrones de Dise\u00f1o y Optimizaci\u00f3n en la Nube"},{"location":"tema37/#371-patrones-de-extraccion","text":"La extracci\u00f3n de datos es la primera fase cr\u00edtica en cualquier pipeline ETL, donde la eficiencia y la selecci\u00f3n adecuada del patr\u00f3n pueden impactar significativamente el rendimiento y los costos. Estos patrones se enfocan en c\u00f3mo los datos son le\u00eddos desde las fuentes de origen.","title":"3.7.1. Patrones de Extracci\u00f3n"},{"location":"tema37/#extraccion-incremental","text":"La extracci\u00f3n incremental es una t\u00e9cnica fundamental que busca reducir la carga de procesamiento y el volumen de datos transferidos al obtener \u00fanicamente los registros que han sido modificados o a\u00f1adidos desde la \u00faltima ejecuci\u00f3n del proceso de extracci\u00f3n. Esto se logra t\u00edpicamente mediante el uso de marcas de tiempo ( timestamps ), n\u00fameros de secuencia o banderas de modificaci\u00f3n ( flags de modificaci\u00f3n ) en las tablas de origen. Su implementaci\u00f3n es vital en entornos de Big Data para manejar grandes vol\u00famenes de informaci\u00f3n que cambian constantemente, ya que evita el reprocesamiento innecesario de datos ya existentes y estables. Imaginemos una base de datos transaccional de ventas que se actualiza continuamente. En lugar de extraer toda la tabla de ventas cada hora, podemos extraer solo las transacciones que ocurrieron desde la \u00faltima extracci\u00f3n. -- \u00daltimo timestamp de extracci\u00f3n guardado en una tabla de metadatos SET @last_extracted_timestamp = (SELECT max_timestamp FROM etl_metadata WHERE job_name = 'sales_extraction'); -- Extracci\u00f3n incremental de nuevas ventas SELECT * FROM sales_transactions WHERE last_updated_at > @last_extracted_timestamp; -- Actualizar el timestamp de la \u00faltima extracci\u00f3n despu\u00e9s de una extracci\u00f3n exitosa UPDATE etl_metadata SET max_timestamp = CURRENT_TIMESTAMP WHERE job_name = 'sales_extraction';","title":"Extracci\u00f3n Incremental"},{"location":"tema37/#extraccion-por-lotes-vs-streaming","text":"La elecci\u00f3n entre extracci\u00f3n por lotes y streaming depende directamente de los requisitos de latencia y el volumen de datos. Extracci\u00f3n por Lotes (Batch Processing) : Procesa grandes vol\u00famenes de datos en intervalos programados (por ejemplo, cada hora, diariamente, semanalmente). Es ideal para datos que no requieren inmediatez, como informes anal\u00edticos hist\u00f3ricos o cargas de data warehouses nocturnas. Este enfoque es generalmente m\u00e1s eficiente en t\u00e9rminos de recursos para grandes vol\u00famenes de datos est\u00e1ticos o semi-est\u00e1ticos. Extracci\u00f3n por Streaming (Stream Processing) : Procesa datos en tiempo real conforme llegan a la fuente. Es crucial para aplicaciones que requieren baja latencia, como detecci\u00f3n de fraude, monitoreo de sistemas, personalizaci\u00f3n en tiempo real o IoT. Este patr\u00f3n implica el uso de tecnolog\u00edas que puedan manejar flujos continuos de datos, como Apache Kafka o Amazon Kinesis. Para la generaci\u00f3n de un informe de ventas mensual consolidado, la extracci\u00f3n por lotes es adecuada. Para monitorear transacciones bancarias en busca de actividad fraudulenta, el streaming es indispensable. # Productor de Kafka (simula un sistema de punto de venta enviando transacciones) from kafka import KafkaProducer import json import time producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8')) for i in range(100): transaction = {'id': i, 'amount': i * 10.5, 'timestamp': time.time()} producer.send('transactions_topic', transaction) print(f\"Sent: {transaction}\") time.sleep(0.1) # Simula llegada de transacciones en tiempo real # Consumidor de Kafka (simula un procesador de fraude) from kafka import KafkaConsumer import json consumer = KafkaConsumer('transactions_topic', bootstrap_servers='localhost:9092', auto_offset_reset='earliest', enable_auto_commit=True, group_id='fraud_detector_group', value_deserializer=lambda x: json.loads(x.decode('utf-8'))) for message in consumer: transaction = message.value if transaction['amount'] > 500: print(f\"ALERTA DE FRAUDE POTENCIAL: {transaction}\") else: print(f\"Procesando transacci\u00f3n normal: {transaction}\")","title":"Extracci\u00f3n por Lotes vs Streaming"},{"location":"tema37/#change-data-capture-cdc","text":"Change Data Capture (CDC) es una t\u00e9cnica que identifica y captura los cambios (inserciones, actualizaciones, eliminaciones) realizados en una base de datos fuente. A diferencia de la extracci\u00f3n incremental basada en timestamps , CDC opera a un nivel m\u00e1s granular, t\u00edpicamente leyendo los logs de transacciones de la base de datos (como el binlog de MySQL o el WAL de PostgreSQL) o utilizando triggers. Esto permite replicar los cambios de forma as\u00edncrona y con una latencia m\u00ednima, sin imponer una carga significativa en el sistema operacional de origen. Es ideal para mantener r\u00e9plicas de bases de datos, alimentar data warehouses en tiempo real o sincronizar sistemas. Mantener un data lake o data warehouse actualizado con los \u00faltimos movimientos de una base de datos OLTP sin afectar su rendimiento. Debezium es un popular framework de CDC. # Configuraci\u00f3n de un conector Debezium para MySQL en Kafka Connect # Este es un ejemplo de c\u00f3mo se configura en un archivo properties/JSON name: mysql-connector connector.class: io.debezium.connector.mysql.MySqlConnector database.hostname: 192.168.99.100 database.port: 3306 database.user: debezium database.password: dbz database.server.id: 12345 database.server.name: my-app-connector database.whitelist: inventory,customers database.history.kafka.bootstrap.servers: kafka:9092 database.history.kafka.topic: dbhistory.inventory # Una vez configurado, Debezium publicar\u00e1 los cambios en Kafka topics # Por ejemplo, para la tabla 'customers', los cambios ir\u00e1n a 'my-app-connector.inventory.customers' Posteriormente, un consumidor de Kafka (como un procesador Spark Streaming) puede leer estos cambios y aplicarlos a un destino: # Pseudo-c\u00f3digo para un consumidor Spark Streaming leyendo cambios de CDC from pyspark.sql import SparkSession from pyspark.sql.functions import from_json, col from pyspark.sql.types import StructType, StringType, TimestampType, LongType, IntegerType spark = SparkSession.builder.appName(\"CDCProcessor\").getOrCreate() # Esquema para los mensajes de Debezium (simplificado) # Un mensaje de Debezium t\u00edpico incluye 'before' y 'after' estados del registro, y el tipo de operaci\u00f3n 'op' schema = StructType([ StructField(\"before\", StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"email\", StringType(), True) ]), True), StructField(\"after\", StructType([ StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"email\", StringType(), True) ]), True), StructField(\"op\", StringType(), True) # 'c' for create, 'u' for update, 'd' for delete ]) # Leer el stream de Kafka df = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\ .option(\"subscribe\", \"my-app-connector.inventory.customers\") \\ .load() # Parsear el valor JSON del mensaje de Kafka parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\") # Aplicar l\u00f3gica basada en el tipo de operaci\u00f3n query = parsed_df \\ .writeStream \\ .foreachBatch(lambda df, epoch_id: process_cdc_batch(df)) \\ .start() def process_cdc_batch(df): # L\u00f3gica para aplicar los cambios a un Data Warehouse o Data Lake df_inserts = df.filter(col(\"op\") == \"c\").select(col(\"after.*\")) df_updates = df.filter(col(\"op\") == \"u\").select(col(\"after.*\")) df_deletes = df.filter(col(\"op\") == \"d\").select(col(\"before.*\")) # Aqu\u00ed ir\u00eda la l\u00f3gica para escribir en el destino, por ejemplo, en una tabla Delta Lake # df_inserts.write.format(\"delta\").mode(\"append\").save(\"/delta/customers\") # df_updates.write.format(\"delta\").mode(\"merge\").option(\"mergeSchema\", \"true\").save(\"/delta/customers\") # Asumiendo Merge Into # df_deletes.write.format(\"delta\").mode(\"merge\").where(\"id = <deleted_id>\").delete(\"/delta/customers\") # Pseudo-c\u00f3digo para delete print(f\"Processed batch {epoch_id}: Inserts={df_inserts.count()}, Updates={df_updates.count()}, Deletes={df_deletes.count()}\") query.awaitTermination()","title":"Change Data Capture (CDC)"},{"location":"tema37/#372-patrones-de-transformacion","text":"La fase de transformaci\u00f3n es donde los datos brutos se limpian, enriquecen, validan y se preparan para su consumo final. Los patrones aqu\u00ed se centran en la eficiencia y la modularidad del procesamiento.","title":"3.7.2. Patrones de Transformaci\u00f3n"},{"location":"tema37/#pipeline-de-transformacion","text":"Un pipeline de transformaci\u00f3n organiza las operaciones de procesamiento de datos en una secuencia l\u00f3gica de pasos, donde la salida de una etapa se convierte en la entrada de la siguiente. Esta modularidad facilita el mantenimiento, la depuraci\u00f3n y la reutilizaci\u00f3n de componentes. Cada paso del pipeline puede ser una funci\u00f3n o un microservicio independiente que realiza una tarea espec\u00edfica, como la limpieza de datos nulos, la normalizaci\u00f3n de formatos o la agregaci\u00f3n de informaci\u00f3n. Este patr\u00f3n es fundamental para construir flujos de trabajo ETL complejos y robustos. Un pipeline que primero limpia los datos de clientes (elimina duplicados, corrige errores de formato), luego los enriquece con informaci\u00f3n geogr\u00e1fica y finalmente los agrega por regi\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import col, upper, trim, count spark = SparkSession.builder.appName(\"DataTransformationPipeline\").getOrCreate() # Paso 1: Cargar datos brutos (simulamos un DataFrame) raw_data = [ (\"john doe\", \"john.doe@example.com \", \"New York\", 1), (\"Jane Smith\", \"jane.smith@example.com\", \"Los Angeles \", 2), (\" JOHN DOE \", \"john.doe@example.com\", \"new york\", 3), # Duplicado, nombre en may\u00fasculas (\"Peter Jones\", \"peter.jones@example.com\", \"Chicago\", 4) ] raw_df = spark.createDataFrame(raw_data, [\"name\", \"email\", \"city\", \"id\"]) # Paso 2: Limpieza de datos def clean_data(df): return df.withColumn(\"name\", trim(upper(col(\"name\")))) \\ .withColumn(\"city\", trim(col(\"city\"))) \\ .dropDuplicates([\"email\"]) cleaned_df = clean_data(raw_df) cleaned_df.show() # +----------+--------------------+----------+---+ # | name| email| city| id| # +----------+--------------------+----------+---+ # | PETER JONES|peter.jones@example.com| Chicago| 4| # | JOHN DOE|john.doe@example.com| New York| 1| # |JANE SMITH|jane.smith@example.com|Los Angeles| 2| # +----------+--------------------+----------+---+ # Paso 3: Enriquecimiento (simulamos la adici\u00f3n de una columna \"region\") def enrich_data(df): # En un caso real, esto podr\u00eda ser una b\u00fasqueda en una tabla de mapeo de ciudades a regiones return df.withColumn(\"region\", when(col(\"city\") == \"New York\", \"East\") .when(col(\"city\") == \"Los Angeles\", \"West\") .otherwise(\"Central\")) enriched_df = enrich_data(cleaned_df) enriched_df.show() # +----------+--------------------+----------+---+-------+ # | name| email| city| id| region| # +----------+--------------------+----------+---+-------+ # | PETER JONES|peter.jones@example.com| Chicago| 4|Central| # | JOHN DOE|john.doe@example.com| New York| 1| East| # |JANE SMITH|jane.smith@example.com|Los Angeles| 2| West| # +----------+--------------------+----------+---+-------+ # Paso 4: Agregaci\u00f3n def aggregate_data(df): return df.groupBy(\"region\").agg(count(\"*\").alias(\"customer_count\")) aggregated_df = aggregate_data(enriched_df) aggregated_df.show() # +-------+--------------+ # | region|customer_count| # +-------+--------------+ # |Central| 1| # | East| 1| # | West| 1| # +-------+--------------+ spark.stop()","title":"Pipeline de Transformaci\u00f3n"},{"location":"tema37/#transformacion-en-paralelo","text":"La transformaci\u00f3n en paralelo es una t\u00e9cnica que divide un gran conjunto de datos en subconjuntos m\u00e1s peque\u00f1os (chunks) y procesa estos subconjuntos simult\u00e1neamente utilizando m\u00faltiples hilos, procesos o nodos de un cl\u00faster distribuido. Este patr\u00f3n es fundamental en entornos de Big Data para aprovechar al m\u00e1ximo los recursos disponibles, ya sea en un solo servidor con m\u00faltiples n\u00facleos o en un cl\u00faster distribuido (como Spark o Hadoop). Al procesar datos en paralelo, se reduce significativamente el tiempo total de ejecuci\u00f3n y se mejora la escalabilidad del pipeline de transformaci\u00f3n. Procesar millones de registros de logs para extraer informaci\u00f3n relevante, donde cada archivo de log o un bloque de registros puede ser procesado independientemente. Con c\u00f3digo (ejemplo conceptual con Dask, una librer\u00eda Python para computaci\u00f3n paralela) : import dask.dataframe as dd import pandas as pd # Crear un DataFrame de Pandas grande data = {'value': range(100_000_000)} df = pd.DataFrame(data) # Convertir a Dask DataFrame (lo divide en particiones autom\u00e1ticamente) ddf = dd.from_pandas(df, npartitions=8) # Procesar\u00e1 en 8 particiones/tareas # Definir una funci\u00f3n de transformaci\u00f3n (ejemplo: elevar al cuadrado) def square_value(x): return x * x # Aplicar la transformaci\u00f3n en paralelo result_ddf = ddf['value'].apply(square_value, meta=('value', 'int64')) # Computar el resultado (esto dispara la ejecuci\u00f3n paralela) # Utiliza un scheduler local por defecto, que usa m\u00faltiples n\u00facleos final_result = result_ddf.compute() print(f\"Processed {len(final_result)} records.\") print(f\"First 5 results: {final_result.head()}\") # Otro ejemplo con PySpark (Spark ya es distribuido por defecto) from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"ParallelTransformation\").getOrCreate() # Cargar un dataset grande (ejemplo: CSV de 100 millones de registros) # spark.read.csv(\"s3://your-bucket/large_dataset.csv\", header=True, inferSchema=True).repartition(100) # Para este ejemplo, crearemos un DataFrame grande en memoria data = [(i,) for i in range(10_000_000)] df = spark.createDataFrame(data, [\"value\"]) # La transformaci\u00f3n se aplica autom\u00e1ticamente en paralelo por Spark transformed_df = df.withColumn(\"squared_value\", col(\"value\") * col(\"value\")) # Mostrar algunos resultados (esto activa la computaci\u00f3n distribuida) transformed_df.show(5) # +--------+-------------+ # | value|squared_value| # +--------+-------------+ # | 0| 0| # | 1| 1| # | 2| 4| # | 3| 9| # | 4| 16| # +--------+-------------+ # Escribir el resultado a un destino distribuido (ej. Parquet) # transformed_df.write.mode(\"overwrite\").parquet(\"s3://your-bucket/output/squared_values.parquet\") spark.stop()","title":"Transformaci\u00f3n en Paralelo"},{"location":"tema37/#lookup-y-enriquecimiento","text":"Este patr\u00f3n implica combinar datos de una fuente principal con informaci\u00f3n adicional de una o varias fuentes secundarias para enriquecer los registros. El enriquecimiento puede lograrse mediante operaciones de join en bases de datos relacionales, b\u00fasquedas en tablas de referencia (dimensiones en un data warehouse), o llamadas a APIs externas para obtener informaci\u00f3n en tiempo real. Es crucial para a\u00f1adir contexto y valor a los datos brutos, permitiendo an\u00e1lisis m\u00e1s profundos. Enriquecer registros de transacciones de ventas con detalles del producto (nombre, categor\u00eda, precio unitario) y del cliente (datos demogr\u00e1ficos, historial de compras) que se encuentran en tablas separadas. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"LookupAndEnrichment\").getOrCreate() # Datos de transacciones (Hechos) transactions_data = [ (1, \"prodA\", 100, 50.0), (2, \"prodB\", 101, 75.0), (3, \"prodA\", 102, 50.0), (4, \"prodC\", 100, 120.0) ] transactions_df = spark.createDataFrame(transactions_data, [\"transaction_id\", \"product_id\", \"customer_id\", \"amount\"]) # Datos de productos (Dimensi\u00f3n) products_data = [ (\"prodA\", \"Laptop\", \"Electronics\"), (\"prodB\", \"Mouse\", \"Electronics\"), (\"prodC\", \"Keyboard\", \"Peripherals\") ] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"category\"]) # Datos de clientes (Dimensi\u00f3n) customers_data = [ (100, \"Alice\", \"USA\"), (101, \"Bob\", \"Canada\"), (102, \"Charlie\", \"Mexico\") ] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\", \"country\"]) # Enriquecimiento de transacciones con datos de productos enriched_transactions_df = transactions_df.join(products_df, \"product_id\", \"inner\") enriched_transactions_df.show() # +----------+--------------+-----------+------+------------+-----------+ # |product_id|transaction_id|customer_id|amount|product_name| category| # +----------+--------------+-----------+------+------------+-----------+ # | prodA| 1| 100| 50.0| Laptop|Electronics| # | prodA| 3| 102| 50.0| Laptop|Electronics| # | prodB| 2| 101| 75.0| Mouse|Electronics| # | prodC| 4| 100| 120.0| Keyboard|Peripherals| # +----------+--------------+-----------+------+------------+-----------+ # Enriquecimiento adicional con datos de clientes final_enriched_df = enriched_transactions_df.join(customers_df, \"customer_id\", \"inner\") final_enriched_df.show() # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ # |product_id|transaction_id|customer_id|amount|product_name| category|customer_name|country| # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ # | prodA| 1| 100| 50.0| Laptop|Electronics| Alice| USA| # | prodC| 4| 100| 120.0| Keyboard|Peripherals| Alice| USA| # | prodB| 2| 101| 75.0| Mouse|Electronics| Bob| Canada| # | prodA| 3| 102| 50.0| Laptop|Electronics| Charlie| Mexico| # +----------+--------------+-----------+------+------------+-----------+-------------+-------+ spark.stop()","title":"Lookup y Enriquecimiento"},{"location":"tema37/#373-patrones-de-carga","text":"La fase de carga es la etapa final del proceso ETL, donde los datos transformados se mueven al destino final (data warehouse, data lake, base de datos anal\u00edtica). La eficiencia y la estrategia de carga son cruciales para el rendimiento y la integridad de los datos en el destino.","title":"3.7.3. Patrones de Carga"},{"location":"tema37/#upsert-insertupdate","text":"El patr\u00f3n Upsert es una operaci\u00f3n que intenta insertar un registro en una tabla; si el registro ya existe (basado en una clave \u00fanica), se actualiza en lugar de insertarse un nuevo registro. Esto es particularmente \u00fatil en escenarios donde los datos de origen pueden contener actualizaciones para registros ya existentes o nuevos registros que se a\u00f1aden con el tiempo. El Upsert garantiza que el destino refleje el estado m\u00e1s actual de los datos sin crear duplicados ni requerir l\u00f3gica de DELETE e INSERT separadas. Mantener una tabla de clientes o productos donde se reciben actualizaciones espor\u00e1dicas y tambi\u00e9n nuevos registros. Delta Lake es un formato de tabla de almacenamiento de c\u00f3digo abierto que soporta operaciones MERGE INTO (Upsert). from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"UpsertPattern\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").getOrCreate() # Ruta para la tabla Delta delta_table_path = \"/tmp/delta/customer_profiles\" # Crear una tabla Delta inicial (si no existe) try: spark.read.format(\"delta\").load(delta_table_path).show() except Exception: initial_data = [ (1, \"Alice\", \"alice@example.com\"), (2, \"Bob\", \"bob@example.com\") ] initial_df = spark.createDataFrame(initial_data, [\"id\", \"name\", \"email\"]) initial_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path) print(\"Initial Delta table created.\") # Cargar la tabla Delta existente customer_profiles_df = spark.read.format(\"delta\").load(delta_table_path) print(\"Current customer profiles:\") customer_profiles_df.show() # Nuevos datos a upsert: un nuevo registro (id 3) y una actualizaci\u00f3n (id 1) new_data = [ (1, \"Alice Smith\", \"alice.smith@example.com\"), # Update (3, \"Charlie\", \"charlie@example.com\") # New ] updates_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"email\"]) print(\"Updates to apply:\") updates_df.show() # Realizar la operaci\u00f3n MERGE INTO (Upsert) from delta.tables import DeltaTable deltaTable = DeltaTable.forPath(spark, delta_table_path) deltaTable.alias(\"target\") \\ .merge( updates_df.alias(\"source\"), \"target.id = source.id\" ) \\ .whenMatchedUpdate(set = { \"name\" : col(\"source.name\"), \"email\" : col(\"source.email\") }) \\ .whenNotMatchedInsert(values = { \"id\" : col(\"source.id\"), \"name\" : col(\"source.name\"), \"email\" : col(\"source.email\") }) \\ .execute() print(\"Customer profiles after upsert:\") spark.read.format(\"delta\").load(delta_table_path).show() spark.stop()","title":"Upsert (Insert/Update)"},{"location":"tema37/#slowly-changing-dimensions-scd","text":"Las Dimensiones de Cambio Lento (SCD) son un concepto crucial en el modelado de data warehouses para manejar cambios en los datos de dimensiones a lo largo del tiempo. A diferencia de las tablas de hechos que registran eventos transaccionales, las tablas de dimensiones describen entidades (como clientes, productos, ubicaciones) que pueden cambiar sus atributos. Existen varios tipos de SCD: SCD Tipo 1 (Sobreescritura) : El valor anterior se sobrescribe con el nuevo valor. Esto es simple de implementar, pero no conserva el historial de cambios. Es adecuado para correcciones o cuando el historial no es relevante. SCD Tipo 2 (Historial Completo) : Se crea una nueva fila en la tabla de dimensiones para cada cambio en un atributo clave. La fila anterior se \"cierra\" (por ejemplo, con una fecha de fin o una bandera de activo/inactivo), y la nueva fila se \"abre\". Esto permite analizar los datos de hechos con el estado de la dimensi\u00f3n en un punto espec\u00edfico del tiempo. Es el tipo m\u00e1s com\u00fan para la anal\u00edtica hist\u00f3rica. SCD Tipo 3 (Historial Limitado) : Se a\u00f1ade una nueva columna a la tabla de dimensiones para almacenar el valor anterior de un atributo espec\u00edfico. Solo mantiene un historial limitado (t\u00edpicamente el valor actual y el valor anterior). Es \u00fatil cuando solo se necesita rastrear un cambio reciente y no un historial completo. El cambio de direcci\u00f3n de un cliente en una empresa; con SCD Tipo 2, se podr\u00eda rastrear d\u00f3nde viv\u00eda el cliente en cada momento. Para SCD Tipo 2 en un data warehouse, supongamos que tenemos una tabla dim_customers y un nuevo lote de datos de clientes ( staging_customers ). -- Estructura de la tabla dim_customers para SCD Tipo 2 CREATE TABLE dim_customers ( customer_key INT IDENTITY(1,1) PRIMARY KEY, customer_id VARCHAR(50) NOT NULL, customer_name VARCHAR(255), address VARCHAR(255), start_date DATE, end_date DATE, is_current BOOLEAN ); -- Suponiendo que staging_customers tiene los nuevos datos (ej. id, name, address) -- L\u00f3gica para insertar nuevos clientes o actualizar existentes (SCD Tipo 2) INSERT INTO dim_customers (customer_id, customer_name, address, start_date, end_date, is_current) SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE, '9999-12-31', TRUE FROM staging_customers s LEFT JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = TRUE WHERE d.customer_id IS NULL; -- Solo insertar clientes nuevos -- Actualizar registros existentes (cerrar la versi\u00f3n anterior y abrir una nueva) MERGE INTO dim_customers AS target USING ( SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE AS new_start_date FROM staging_customers s JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = TRUE WHERE s.customer_name <> d.customer_name OR s.address <> d.address -- Solo si hay cambios en atributos relevantes ) AS source ON target.customer_id = source.customer_id AND target.is_current = TRUE WHEN MATCHED THEN UPDATE SET target.end_date = DATEADD(day, -1, source.new_start_date), target.is_current = FALSE; -- Insertar las nuevas versiones de los registros actualizados INSERT INTO dim_customers (customer_id, customer_name, address, start_date, end_date, is_current) SELECT s.customer_id, s.customer_name, s.address, CURRENT_DATE, '9999-12-31', TRUE FROM staging_customers s JOIN dim_customers d ON s.customer_id = d.customer_id AND d.is_current = FALSE AND d.end_date = DATEADD(day, -1, CURRENT_DATE) WHERE s.customer_name <> d.customer_name OR s.address <> d.address;","title":"Slowly Changing Dimensions (SCD)"},{"location":"tema37/#bulk-loading","text":"El Bulk Loading, o carga masiva, es una t\u00e9cnica optimizada para cargar grandes vol\u00famenes de datos en una base de datos o sistema de almacenamiento. En lugar de procesar cada registro individualmente con sentencias INSERT una por una (lo cual es ineficiente para grandes conjuntos de datos), las operaciones de bulk loading agrupan los datos y los cargan de manera m\u00e1s eficiente, minimizando la sobrecarga transaccional. Esto a menudo implica deshabilitar temporalmente \u00edndices, restricciones ( constraints ) y triggers durante la carga, y luego reconstruirlos una vez que la carga ha finalizado para maximizar la velocidad. Es el m\u00e9todo preferido para las cargas iniciales o para a\u00f1adir grandes conjuntos de datos peri\u00f3dicamente. Cargar archivos Parquet o CSV directamente en una tabla de un data warehouse en la nube como Snowflake, Amazon Redshift o Google BigQuery, utilizando sus comandos nativos de carga masiva. Supongamos que en Snowflake, tenemos un archivo CSV llamado sales_data.csv en un bucket S3. -- Crear un formato de archivo para CSV CREATE FILE FORMAT my_csv_format TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1 NULL_IF = ('\\\\N', 'NULL') EMPTY_FIELD_AS_NULL = TRUE; -- Crear un stage externo que apunte a un bucket S3 CREATE STAGE my_s3_stage URL = 's3://your-s3-bucket/data/sales/' CREDENTIALS = (AWS_KEY_ID = 'YOUR_AWS_ACCESS_KEY_ID', AWS_SECRET_KEY = 'YOUR_AWS_SECRET_ACCESS_KEY'); -- Crear la tabla donde se cargar\u00e1n los datos CREATE TABLE sales_transactions ( transaction_id INT, product_id VARCHAR(50), customer_id INT, amount DECIMAL(10, 2), transaction_date DATE ); -- Usar el comando COPY INTO para realizar la carga masiva desde S3 a la tabla COPY INTO sales_transactions FROM @my_s3_stage/sales_data.csv FILE_FORMAT = (FORMAT_NAME = my_csv_format) ON_ERROR = 'CONTINUE'; -- Contin\u00faa la carga incluso si hay errores en algunas filas -- Ejemplo de carga masiva en Google BigQuery desde un archivo CSV en GCS -- Asumiendo que el esquema est\u00e1 en un archivo JSON o se detecta autom\u00e1ticamente bq load --source_format=CSV --skip_leading_rows=1 \\ your_dataset.your_table gs://your-gcs-bucket/data/sales/sales_data.csv \\ ./schema.json # O BigQuery detecta el esquema autom\u00e1ticamente","title":"Bulk Loading"},{"location":"tema37/#374-patrones-de-control-y-monitoreo","text":"La resiliencia y la observabilidad son cruciales en los pipelines de Big Data. Estos patrones aseguran que los procesos puedan recuperarse de fallas y que se puedan identificar y resolver problemas de manera eficiente.","title":"3.7.4. Patrones de Control y Monitoreo"},{"location":"tema37/#checkpoint-y-restart","text":"El patr\u00f3n Checkpoint y Restart implica guardar peri\u00f3dicamente el estado de un proceso de larga duraci\u00f3n en puntos de control ( checkpoints ). En caso de una falla (por ejemplo, interrupci\u00f3n de la red, error de software, fallo de un nodo), el proceso puede reanudarse desde el \u00faltimo checkpoint exitoso en lugar de tener que comenzar desde el principio. Esto no solo ahorra tiempo y recursos al evitar el reprocesamiento de datos ya completados, sino que tambi\u00e9n garantiza la integridad de los datos en entornos distribuidos donde las fallas son comunes. Es fundamental para la robustez de los pipelines de datos distribuidos y en tiempo real. Un proceso de Spark Streaming que procesa datos de Kafka. Si el cl\u00faster de Spark falla, puede reiniciar desde el \u00faltimo offset de Kafka procesado con \u00e9xito, sin perder datos ni procesar duplicados. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_timestamp import os spark = SparkSession.builder.appName(\"SparkStreamingCheckpoint\") \\ .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark_checkpoint\") \\ .getOrCreate() # Limpiar el directorio de checkpoint para una nueva ejecuci\u00f3n if os.path.exists(\"/tmp/spark_checkpoint\"): import shutil shutil.rmtree(\"/tmp/spark_checkpoint\") print(\"Cleaned existing checkpoint directory.\") # Simular un stream de datos desde una fuente de datos (ej. Kafka) # Para este ejemplo, usaremos una fuente de datos en memoria para simplicidad # En un caso real, esto ser\u00eda: spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"input_topic\").load() data = [(\"A\", 1), (\"B\", 2), (\"C\", 3), (\"D\", 4)] df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load() # Transformaci\u00f3n de ejemplo transformed_df = df_stream.withColumn(\"processed_time\", current_timestamp()) \\ .withColumn(\"value_squared\", col(\"value\") * col(\"value\")) # Escribir el stream a la consola con un checkpoint query = transformed_df \\ .writeStream \\ .outputMode(\"append\") \\ .format(\"console\") \\ .option(\"truncate\", False) \\ .trigger(processingTime=\"5 seconds\") \\ .start() # Para simular una falla, puedes detener el script manualmente despu\u00e9s de unos segundos # y luego reiniciarlo. Spark deber\u00eda continuar desde donde se qued\u00f3. query.awaitTermination()","title":"Checkpoint y Restart"},{"location":"tema37/#dead-letter-queue-dlq","text":"Una Dead Letter Queue (DLQ), o cola de mensajes no procesables, es un mecanismo donde se enrutan los mensajes o registros que fallaron en su procesamiento. En un pipeline de datos, si un registro no cumple con las validaciones, causa un error de procesamiento o no puede ser entregado al destino despu\u00e9s de varios reintentos, en lugar de detener todo el pipeline, se mueve a la DLQ. Esto permite que el flujo de datos principal contin\u00fae ininterrumpido mientras los registros problem\u00e1ticos pueden ser analizados, depurados y, si es posible, reprocesados manualmente o a trav\u00e9s de un proceso separado. Mejora la robustez y la resiliencia del sistema. En un sistema de procesamiento de eventos en tiempo real con Apache Kafka y Kafka Streams, los mensajes que fallan la deserializaci\u00f3n o validaci\u00f3n pueden ser enviados a una DLQ. Procesamiento de mensajes fallidos en AWS SQS y Lambda, supongamos que una funci\u00f3n Lambda procesa mensajes de SQS. Si falla, el mensaje se env\u00eda a una DLQ. # C\u00f3digo Python para una funci\u00f3n AWS Lambda (ejemplo conceptual) import json import boto3 def lambda_handler(event, context): sqs_client = boto3.client('sqs') dlq_url = '[https://sqs.us-east-1.amazonaws.com/123456789012/MyDeadLetterQueue](https://sqs.us-east-1.amazonaws.com/123456789012/MyDeadLetterQueue)' for record in event['Records']: message_body = json.loads(record['body']) try: # L\u00f3gica de procesamiento de datos if message_body.get('error_flag'): raise ValueError(\"Simulating a processing error for this message.\") print(f\"Successfully processed message: {message_body}\") # Si se procesa con \u00e9xito, el mensaje se elimina de la cola principal autom\u00e1ticamente except Exception as e: print(f\"Error processing message: {message_body}, Error: {e}\") # Enviar el mensaje a la DLQ # En AWS SQS, esto se configura directamente en la cola principal con una Redrive Policy # La Lambda no necesita enviar expl\u00edcitamente a la DLQ si la Redrive Policy est\u00e1 configurada # pero este pseudo-c\u00f3digo muestra la intenci\u00f3n de manejo de errores sqs_client.send_message( QueueUrl=dlq_url, MessageBody=json.dumps({\"original_message\": message_body, \"error\": str(e)}) ) return { 'statusCode': 200, 'body': json.dumps('Processing complete') } Configuraci\u00f3n de SQS (Redrive Policy) : En la consola de AWS SQS, para la cola principal, se configura una \"Redrive policy\" que especifica la DLQ a la que se deben enviar los mensajes despu\u00e9s de un n\u00famero determinado de intentos fallidos.","title":"Dead Letter Queue (DLQ)"},{"location":"tema37/#idempotencia","text":"La idempotencia es una propiedad de una operaci\u00f3n que, cuando se ejecuta m\u00faltiples veces con los mismos par\u00e1metros de entrada, produce el mismo resultado y efecto secundario que si se hubiera ejecutado una sola vez. En el contexto de los pipelines ETL, dise\u00f1ar procesos idempotentes es crucial para la recuperaci\u00f3n de errores y el reprocesamiento seguro. Si un paso del ETL falla y se reintenta, la idempotencia asegura que los datos no se dupliquen o corrompan en el destino. Esto es especialmente importante en sistemas distribuidos donde las operaciones pueden fallar parcialmente o los mensajes pueden entregarse varias veces. Un proceso de carga de datos que utiliza una clave primaria para UPSERT registros en lugar de solo INSERT , garantizando que si se ejecuta dos veces, el registro solo se actualice o se inserte una vez. -- Suponiendo una tabla de destino `processed_transactions` con una clave primaria `transaction_id` -- y una tabla de staging `new_transactions` -- Enfoque no idempotente (podr\u00eda insertar duplicados si se re-ejecuta) -- INSERT INTO processed_transactions (transaction_id, amount, status) -- SELECT transaction_id, amount, status FROM new_transactions; -- Enfoque idempotente usando UPSERT (para PostgreSQL, por ejemplo) INSERT INTO processed_transactions (transaction_id, amount, status, last_processed_at) SELECT transaction_id, amount, status, NOW() FROM new_transactions ON CONFLICT (transaction_id) DO UPDATE SET amount = EXCLUDED.amount, status = EXCLUDED.status, last_processed_at = EXCLUDED.last_processed_at; -- Otro ejemplo conceptual: procesar archivos y marcarlos como procesados -- Si el proceso lee archivos de un bucket, y los mueve o renombra despu\u00e9s de procesarlos -- Esto es idempotente porque no procesar\u00e1 el mismo archivo dos veces si ya fue movido/renombrado. # Pseudo-c\u00f3digo Python para un procesamiento de archivos idempotente def process_file_idempotently(file_path, processed_path): if not file_exists(file_path): print(f\"File {file_path} does not exist, likely already processed.\") return try: # L\u00f3gica de procesamiento del archivo print(f\"Processing file: {file_path}\") # ... hacer algo con el archivo ... # Mover el archivo a una carpeta de \"procesados\" move_file(file_path, processed_path) print(f\"File {file_path} moved to {processed_path}\") except Exception as e: print(f\"Error processing {file_path}: {e}\") # En caso de error, el archivo permanece en la ubicaci\u00f3n original para reintento","title":"Idempotencia"},{"location":"tema37/#375-patrones-arquitectonicos","text":"Estos patrones definen la estructura general de los sistemas de Big Data, abordando c\u00f3mo los componentes interact\u00faan para manejar el flujo de datos.","title":"3.7.5. Patrones Arquitect\u00f3nicos"},{"location":"tema37/#staging-area","text":"Una Staging Area, o \u00e1rea de preparaci\u00f3n, es un espacio de almacenamiento temporal donde los datos extra\u00eddos de las fuentes se almacenan antes de ser transformados y cargados al destino final (generalmente un data warehouse o data lake). Esta \u00e1rea sirve como un \"amortiguador\" entre los sistemas fuente y el destino. Ofrece m\u00faltiples beneficios: Punto de Recuperaci\u00f3n : Si una transformaci\u00f3n falla, los datos brutos a\u00fan est\u00e1n disponibles en el \u00e1rea de staging para ser reprocesados sin necesidad de re-extraer desde la fuente. Aislamiento : Protege los sistemas fuente de las cargas de procesamiento de las transformaciones y permite que las validaciones y limpieza iniciales se realicen en un entorno separado. Consistencia : Permite consolidar datos de m\u00faltiples fuentes heterog\u00e9neas en un formato com\u00fan antes de la transformaci\u00f3n. Validaci\u00f3n : Es un lugar ideal para realizar comprobaciones de calidad de datos iniciales. Cargar archivos CSV desde un sistema de punto de venta a un bucket de AWS S3 (staging area) antes de que un trabajo de Spark los procese y los cargue en un data warehouse en Redshift. # Pseudo-c\u00f3digo de un script de AWS Glue (PySpark) import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) # 1. Extracci\u00f3n de datos brutos a la Staging Area (ej. S3) # Este paso generalmente se hace fuera de Glue, por ejemplo, v\u00eda AWS DataSync, Kinesis Firehose o FTP. # Asumimos que los archivos ya est\u00e1n en s3://your-bucket/raw_data/ # 2. Leer datos desde la Staging Area datasource0 = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", connection_options={\"paths\": [\"s3://your-bucket/raw_data/sales_data/\"], \"recurse\": True}, format=\"csv\", format_options={\"withHeader\": True, \"separator\": \",\"} ) # 3. Aplicar transformaciones applymapping1 = ApplyMapping.apply(frame=datasource0, mappings=[ (\"transaction_id\", \"string\", \"transaction_id\", \"long\"), (\"product_name\", \"string\", \"product_name\", \"string\"), (\"amount\", \"string\", \"amount\", \"double\") ]) # 4. Cargar a la tabla final (ej. Redshift) datasink2 = glueContext.write_dynamic_frame.from_jdbc_conf( catalog_connection=\"redshift_connection\", connection_options={\"dbtable\": \"public.processed_sales\", \"database\": \"dev\"}, redshift_tmp_dir=args[\"TempDir\"], frame=applymapping1 ) job.commit()","title":"Staging Area"},{"location":"tema37/#hub-and-spoke","text":"El patr\u00f3n Hub and Spoke, o \"Centro y Radios\", es una arquitectura que centraliza la extracci\u00f3n de datos de m\u00faltiples fuentes en un punto central (el \"Hub\") antes de distribuir esos datos a varios destinos o consumidores (los \"Spokes\"). En lugar de tener conexiones punto a punto entre cada fuente y cada destino (lo que puede volverse inmanejable con muchas integraciones), los datos fluyen a trav\u00e9s de un nodo centralizado. Este patr\u00f3n simplifica la gesti\u00f3n de integraciones, mejora la consistencia de los datos, facilita el monitoreo centralizado y reduce la complejidad al escalar. Una empresa con m\u00faltiples sistemas de origen (CRM, ERP, bases de datos de clientes) que env\u00edan datos a un Data Lake central (usando Apache Kafka como Hub), desde donde los datos se distribuyen a data warehouses, herramientas de BI, o modelos de Machine Learning (los Spokes). # Pseudo-c\u00f3digo: M\u00faltiples productores enviando a Kafka (el Hub) # Productor 1 (CRM) from kafka import KafkaProducer import json producer_crm = KafkaProducer(bootstrap_servers='localhost:9099') producer_crm.send('raw_data_hub_topic', json.dumps({\"source\": \"CRM\", \"data\": {\"customer_id\": \"C123\", \"name\": \"Alice\"}}).encode('utf-8')) # Productor 2 (ERP) producer_erp = KafkaProducer(bootstrap_servers='localhost:9099') producer_erp.send('raw_data_hub_topic', json.dumps({\"source\": \"ERP\", \"data\": {\"order_id\": \"O456\", \"item\": \"Laptop\"}}).encode('utf-8')) # Un consumidor centralizado (el Hub) que ingiere y clasifica/transforma datos # Este podr\u00eda ser un proceso Spark Streaming, un Kafka Streams application, etc. # Su funci\u00f3n es tomar los datos brutos del topic 'raw_data_hub_topic' # y luego, despu\u00e9s de una limpieza o estandarizaci\u00f3n m\u00ednima, # publicarlos en topics m\u00e1s espec\u00edficos para los spokes. # Ejemplo: 'customers_topic', 'orders_topic' # Pseudo-c\u00f3digo de un consumidor/procesador central (parte del Hub) from kafka import KafkaConsumer import json consumer_hub = KafkaConsumer('raw_data_hub_topic', bootstrap_servers='localhost:9099', value_deserializer=lambda x: json.loads(x.decode('utf-8'))) producer_processed = KafkaProducer(bootstrap_servers='localhost:9099') for message in consumer_hub: data = message.value if data['source'] == 'CRM': # Simple transformaci\u00f3n: agregar un timestamp data['data']['processed_at'] = time.time() producer_processed.send('processed_customers_topic', json.dumps(data['data']).encode('utf-8')) elif data['source'] == 'ERP': data['data']['processed_at'] = time.time() producer_processed.send('processed_orders_topic', json.dumps(data['data']).encode('utf-8')) # M\u00faltiples consumidores (Spokes) leyendo de topics espec\u00edficos # Consumidor Spoke 1 (Data Warehouse para clientes) consumer_dw_customers = KafkaConsumer('processed_customers_topic', bootstrap_servers='localhost:9099') # L\u00f3gica para cargar en Data Warehouse # Consumidor Spoke 2 (Sistema de anal\u00edtica de pedidos) consumer_analytics_orders = KafkaConsumer('processed_orders_topic', bootstrap_servers='localhost:9099') # L\u00f3gica para alimentar sistema de anal\u00edtica","title":"Hub and Spoke"},{"location":"tema37/#event-driven-etl","text":"El patr\u00f3n Event-Driven ETL (ETL impulsado por eventos) transforma los procesos ETL de operaciones programadas (batch) a operaciones reactivas que se disparan en respuesta a eventos espec\u00edficos. Estos eventos pueden ser la llegada de un nuevo archivo a un bucket de almacenamiento, un cambio en una base de datos (usando CDC), la publicaci\u00f3n de un mensaje en una cola de mensajes, o una se\u00f1al de un sistema externo. Este enfoque reduce la latencia de los datos, optimiza el uso de recursos al procesar solo cuando hay datos nuevos, y permite una mayor agilidad en el flujo de informaci\u00f3n. Es fundamental para arquitecturas de datos en tiempo real y microservicios. Cuando se carga un archivo de log a un bucket AWS S3, un evento de S3 activa una funci\u00f3n AWS Lambda, que a su vez invoca un trabajo de AWS Glue para procesar el log y cargarlo en un data lake. Configuraci\u00f3n de S3 para enviar eventos a Lambda: Configurar una notificaci\u00f3n de evento en un bucket S3 para s3:ObjectCreated:* que apunte a una funci\u00f3n Lambda. C\u00f3digo de la funci\u00f3n AWS Lambda (Python): import json import boto3 def lambda_handler(event, context): print(f\"Received event: {json.dumps(event)}\") glue_client = boto3.client('glue') for record in event['Records']: bucket_name = record['s3']['bucket']['name'] object_key = record['s3']['object']['key'] print(f\"New object created in bucket: {bucket_name}, key: {object_key}\") # Determinar qu\u00e9 trabajo de Glue ejecutar basado en el prefijo del objeto, etc. # Aqu\u00ed, un ejemplo simple que siempre dispara el mismo trabajo job_name = \"MyGlueETLJob\" try: # Disparar el trabajo de AWS Glue response = glue_client.start_job_run( JobName=job_name, Arguments={ '--input_bucket': bucket_name, '--input_key': object_key } ) print(f\"Started Glue job {job_name} with RunId: {response['JobRunId']}\") except Exception as e: print(f\"Error starting Glue job {job_name}: {e}\") raise e # Re-lanzar la excepci\u00f3n para que Lambda la maneje si es necesario return { 'statusCode': 200, 'body': json.dumps('Glue job triggered successfully!') } C\u00f3digo de AWS Glue Job (PySpark): Este trabajo de Glue leer\u00eda el archivo especificado por --input_key y lo procesar\u00eda. # Pseudo-c\u00f3digo para el trabajo de AWS Glue (PySpark) import sys from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_bucket', 'input_key']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) input_path = f\"s3://{args['input_bucket']}/{args['input_key']}\" print(f\"Processing file: {input_path}\") # Leer el archivo que activ\u00f3 el evento df = spark.read.csv(input_path, header=True, inferSchema=True) # Realizar transformaciones (ejemplo: contar filas) row_count = df.count() print(f\"File {args['input_key']} has {row_count} rows.\") # Escribir el resultado a otro destino (ej. una tabla procesada en S3 o un Data Warehouse) output_path = f\"s3://your-processed-bucket/processed_data/{args['input_key'].replace('.csv', '')}_processed.parquet\" df.write.mode(\"overwrite\").parquet(output_path) print(f\"Processed data written to: {output_path}\") job.commit()","title":"Event-Driven ETL"},{"location":"tema37/#376-estrategias-de-optimizacion-de-costos","text":"La optimizaci\u00f3n de costos en la nube es tan crucial como el rendimiento. Permite a las organizaciones escalar sin incurrir en gastos excesivos, garantizando la sostenibilidad financiera de las operaciones de Big Data.","title":"3.7.6. Estrategias de optimizaci\u00f3n de costos"},{"location":"tema37/#autoscaling","text":"El Autoscaling (autoescalado) es la capacidad de un sistema en la nube para ajustar autom\u00e1ticamente la cantidad de recursos de computaci\u00f3n utilizados en funci\u00f3n de la demanda actual. Esto significa que la infraestructura puede aumentar (escalar hacia arriba) durante los picos de carga y disminuir (escalar hacia abajo) durante los per\u00edodos de baja actividad. El autoscaling es fundamental para la optimizaci\u00f3n de costos porque se paga solo por los recursos que realmente se utilizan, evitando el sobreaprovisionamiento y el desperdicio de capacidad. Aplica tanto a cl\u00fasteres de computaci\u00f3n (ej. instancias EC2, nodos de Spark) como a servicios de bases de datos o colas. Un cl\u00faster de Apache Spark en Amazon EMR que autom\u00e1ticamente a\u00f1ade o quita nodos de worker seg\u00fan la carga de trabajo de los jobs. # Configuraci\u00f3n de Auto Scaling en un Cluster de EMR (ejemplo simplificado) { \"InstanceGroups\": [ { \"InstanceRole\": \"MASTER\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 1 }, { \"InstanceRole\": \"CORE\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"AutoScalingPolicy\": { \"Constraints\": { \"MinCapacity\": 2, \"MaxCapacity\": 10 }, \"Rules\": [ { \"Name\": \"ScaleOut\", \"Description\": \"Scale out when YARN memory utilization is high\", \"Action\": { \"Market\": \"ON_DEMAND\", \"SimpleScalingPolicyConfiguration\": { \"AdjustmentType\": \"CHANGE_IN_CAPACITY\", \"ScalingAdjustment\": 1, \"CoolDown\": 300 } }, \"Trigger\": { \"CloudWatchAlarmDefinition\": { \"ComparisonOperator\": \"GREATER_THAN_OR_EQUAL\", \"EvaluationPeriods\": 5, \"MetricName\": \"YARNMemoryAvailablePercentage\", \"Namespace\": \"AWS/ElasticMapReduce\", \"Period\": 300, \"Statistic\": \"Average\", \"Threshold\": 15, # Trigger if available memory drops below 15% \"Unit\": \"Percent\", \"Dimensions\": [ {\"Name\": \"ClusterId\", \"Value\": \"${emr.cluster.id}\"} ] } } }, { \"Name\": \"ScaleIn\", \"Description\": \"Scale in when YARN memory utilization is low\", \"Action\": { \"Market\": \"ON_DEMAND\", \"SimpleScalingPolicyConfiguration\": { \"AdjustmentType\": \"CHANGE_IN_CAPACITY\", \"ScalingAdjustment\": -1, \"CoolDown\": 300 } }, \"Trigger\": { \"CloudWatchAlarmDefinition\": { \"ComparisonOperator\": \"LESS_THAN_OR_EQUAL\", \"EvaluationPeriods\": 5, \"MetricName\": \"YARNMemoryAvailablePercentage\", \"Namespace\": \"AWS/ElasticMapReduce\", \"Period\": 300, \"Statistic\": \"Average\", \"Threshold\": 80, # Trigger if available memory is above 80% (low utilization) \"Unit\": \"Percent\", \"Dimensions\": [ {\"Name\": \"ClusterId\", \"Value\": \"${emr.cluster.id}\"} ] } } } ] } } ] }","title":"Autoscaling"},{"location":"tema37/#uso-de-spot-instances","text":"Las spot instances (instancias spot) son instancias de computaci\u00f3n en la nube que se ofrecen a un precio significativamente reducido (hasta un 90% de descuento en comparaci\u00f3n con las instancias bajo demanda) a cambio de la posibilidad de que el proveedor de la nube las recupere (interrumpa) con poca antelaci\u00f3n si necesita la capacidad. Son ideales para cargas de trabajo tolerantes a fallos, no cr\u00edticas, o aquellas que pueden resumirse f\u00e1cilmente, como trabajos de procesamiento de Big Data por lotes que pueden reintentar tareas o donde los datos pueden ser reprocesados. Su uso puede generar ahorros masivos en la infraestructura de computaci\u00f3n. Ejecutar un gran trabajo de Spark para un an\u00e1lisis de datos que no es de misi\u00f3n cr\u00edtica y que se ejecuta durante la noche. Si algunas instancias spot son interrumpidas, Spark puede redistribuir el trabajo a otras instancias o reintentar las tareas fallidas. Al crear un cl\u00faster EMR, se puede especificar el tipo de instancias para los grupos de instancias Core y Task. # Configuraci\u00f3n de un cluster EMR usando instancias Spot para los grupos de Core y Task { \"Name\": \"MySpotEMRCluster\", \"ReleaseLabel\": \"emr-6.x.0\", \"Applications\": [ {\"Name\": \"Spark\"}, {\"Name\": \"Hadoop\"} ], \"Instances\": { \"InstanceGroups\": [ { \"Name\": \"Master Instance Group\", \"InstanceRole\": \"MASTER\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 1 }, { \"Name\": \"Core Instance Group\", \"InstanceRole\": \"CORE\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"Market\": \"SPOT\", # Usar Spot Instances para Core \"BidPrice\": \"0.50\" # Por ejemplo, ofertar hasta $0.50 por hora }, { \"Name\": \"Task Instance Group\", \"InstanceRole\": \"TASK\", \"InstanceType\": \"m5.xlarge\", \"InstanceCount\": 2, \"Market\": \"SPOT\", # Usar Spot Instances para Task \"BidPriceAsPercentageOfOnDemandPrice\": 100 # Ofertar el 100% del precio bajo demanda, a\u00fan as\u00ed es m\u00e1s barato } ] }, \"JobFlowRole\": \"EMR_EC2_DefaultRole\", \"ServiceRole\": \"EMR_DefaultRole\" }","title":"Uso de spot instances"},{"location":"tema37/#diseno-para-uso-eficiente-del-almacenamiento-y-ejecucion","text":"La optimizaci\u00f3n del almacenamiento y la ejecuci\u00f3n son pilares de la reducci\u00f3n de costos en la nube, ya que ambos recursos son facturados por uso. Un dise\u00f1o eficiente implica: Formatos de Almacenamiento : Usar formatos de archivo optimizados para Big Data como Parquet u ORC. Estos formatos son columnares, lo que permite una mayor compresi\u00f3n y una lectura m\u00e1s eficiente al seleccionar solo las columnas necesarias. Tambi\u00e9n soportan esquemas y metadatos, mejorando la interoperabilidad. Compresi\u00f3n de Datos : Aplicar algoritmos de compresi\u00f3n (Snappy, Gzip, Zstd) a los datos almacenados. Esto reduce el espacio de almacenamiento y, consecuentemente, el costo. Adem\u00e1s, al haber menos datos que transferir, se mejora el rendimiento de la lectura. Particionamiento y Agrupaci\u00f3n (Bucketing) : Organizar los datos en el almacenamiento (ej. S3, HDFS) en directorios l\u00f3gicos (particiones) basados en columnas de uso frecuente (ej. fecha, regi\u00f3n). Esto permite que las consultas escaneen solo un subconjunto de los datos, reduciendo el volumen de I/O y los costos de c\u00f3mputo. El bucketing organiza datos dentro de las particiones para optimizar joins y agregaciones. Selecci\u00f3n de Instancias y Tipo de Computaci\u00f3n : Elegir el tipo y tama\u00f1o de instancia adecuados para la carga de trabajo. No sobre-aprovisionar CPU o memoria. Considerar el uso de servicios sin servidor (serverless) como AWS Lambda o Google Cloud Functions para tareas de corta duraci\u00f3n, ya que solo se paga por el tiempo de ejecuci\u00f3n. Optimizaci\u00f3n de Consultas : Escribir consultas SQL o scripts de procesamiento que minimicen el escaneo de datos, aprovechen los \u00edndices o las particiones, y eviten operaciones costosas como FULL SCAN o CROSS JOIN innecesarios. Almacenar datos en un Data Lake en S3 en formato Parquet, particionados por a\u00f1o/mes/d\u00eda , y usar Spark para procesar solo las particiones relevantes para una consulta. # PySpark y almacenamiento en Parquet particionado from pyspark.sql import SparkSession from pyspark.sql.functions import col, year, month, dayofmonth spark = SparkSession.builder.appName(\"EfficientStorageAndExecution\").getOrCreate() # Simular la creaci\u00f3n de un DataFrame de logs log_data = [ (\"user1\", \"login\", \"2023-01-01 10:00:00\"), (\"user2\", \"logout\", \"2023-01-01 10:05:00\"), (\"user1\", \"purchase\", \"2023-01-02 11:15:00\"), (\"user3\", \"login\", \"2023-01-02 12:00:00\"), (\"user4\", \"view_product\", \"2023-02-15 09:30:00\") ] logs_df = spark.createDataFrame(log_data, [\"user_id\", \"event_type\", \"event_timestamp\"]) # Convertir el timestamp a tipo de dato de fecha y extraer componentes para particionamiento logs_df = logs_df.withColumn(\"event_date\", col(\"event_timestamp\").cast(\"date\")) \\ .withColumn(\"year\", year(col(\"event_date\"))) \\ .withColumn(\"month\", month(col(\"event_date\"))) \\ .withColumn(\"day\", dayofmonth(col(\"event_date\"))) # Escribir el DataFrame en formato Parquet, particionado por a\u00f1o, mes, y d\u00eda # Esto crear\u00e1 una estructura de carpetas como s3://your-bucket/logs/year=2023/month=1/day=1/ output_path = \"/tmp/logs_partitioned_parquet\" # En un entorno cloud, ser\u00eda s3://your-bucket/logs/ logs_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"day\").parquet(output_path) print(f\"Data written to {output_path} with partitioning.\") # Leer solo los datos de una partici\u00f3n espec\u00edfica para una consulta (eficiente) # Por ejemplo, para obtener logs del 1 de enero de 2023 query_df = spark.read.parquet(f\"{output_path}/year=2023/month=1/day=1\") print(\"\\nLogs for 2023-01-01:\") query_df.show() # +-------+----------+-------------------+----------+-----+-----+---+ # |user_id|event_type| event_timestamp|event_date| year|month|day| # +-------+----------+-------------------+----------+-----+-----+---+ # | user1| login|2023-01-01 10:00:00|2023-01-01| 2023| 1| 1| # | user2| logout|2023-01-01 10:05:00|2023-01-01| 2023| 1| 1| # +-------+----------+-------------------+----------+-----+-----+---+ # Al consultar, Spark solo leer\u00e1 los archivos dentro de esa partici\u00f3n espec\u00edfica, # ahorrando I/O y c\u00f3mputo. spark.stop()","title":"Dise\u00f1o para uso eficiente del almacenamiento y ejecuci\u00f3n"},{"location":"tema37/#tarea","text":"Dise\u00f1o de un Pipeline ETL Event-Driven (Te\u00f3rico) : Imagina que trabajas para una empresa de e-commerce. Cuando se carga un nuevo archivo CSV de \"devoluciones de productos\" a un bucket de S3, necesitas que se active autom\u00e1ticamente un proceso ETL para: Cargar el CSV. Validar que las columnas clave (ID de producto, cantidad, fecha de devoluci\u00f3n) no est\u00e9n vac\u00edas. Enriquecer los datos con el nombre del producto y la categor\u00eda, consultando una tabla de productos existente. Actualizar una tabla de m\u00e9tricas de devoluciones, realizando un UPSERT para evitar duplicados y manteniendo un registro \u00fanico por producto y fecha de devoluci\u00f3n. Describe paso a paso c\u00f3mo dise\u00f1ar\u00edas esta arquitectura en la nube usando servicios de AWS (S3, Lambda, Glue, Redshift/DynamoDB para la tabla de productos y la de m\u00e9tricas). Optimizaci\u00f3n de Costos en un Cluster Spark (An\u00e1lisis de Caso) : Tu equipo ha notado que el costo de su cl\u00faster de Spark (ejecutado en EMR o Databricks) para los trabajos ETL diarios se ha disparado. Los trabajos se ejecutan una vez al d\u00eda y tardan aproximadamente 3 horas. Actualmente, utilizan instancias bajo demanda grandes. Prop\u00f3n al menos tres estrategias espec\u00edficas, basadas en lo aprendido, para reducir significativamente estos costos, justificando cada una con sus pros y contras. Implementaci\u00f3n de SCD Tipo 2 (SQL) : Dise\u00f1a una tabla de dimensiones para dim_productos que soporte SCD Tipo 2. La tabla debe incluir al menos product_id , product_name , category , price , start_date , end_date , y is_current . Luego, escribe las sentencias SQL (simulando una base de datos relacional) para manejar un escenario donde: Llega un nuevo producto. El precio de un producto existente cambia. El nombre de un producto existente cambia. Aseg\u00farate de que la l\u00f3gica de actualizaci\u00f3n cierre la versi\u00f3n anterior y cree una nueva. Desarrollo de un Patr\u00f3n de Extracci\u00f3n Incremental (Python/Pandas o PySpark) : Escribe un script en Python (puedes usar Pandas para simular DataFrames peque\u00f1os o PySpark si tienes un entorno) que demuestre la extracci\u00f3n incremental. Simula una tabla de origen con una columna last_updated_at . Crea una l\u00f3gica para: Mantener un \"\u00faltimo timestamp procesado\" en una variable o archivo. Extraer solo los registros donde last_updated_at es mayor que el \u00faltimo timestamp procesado. Actualizar el \"\u00faltimo timestamp procesado\" despu\u00e9s de una extracci\u00f3n exitosa. Proporciona un ejemplo de datos de entrada y salida para dos ejecuciones sucesivas del script. Dise\u00f1o Idempotente para Carga de Archivos (Conceptos) : Explica c\u00f3mo har\u00edas que un proceso de carga de archivos (que lee archivos de un directorio de entrada y los mueve a un directorio de \"procesados\") sea idempotente. \u00bfQu\u00e9 suceder\u00eda si el proceso fallara justo despu\u00e9s de leer un archivo pero antes de moverlo? \u00bfC\u00f3mo asegurar\u00edas que al reintentar el proceso, ese archivo no sea procesado dos veces, o que su estado final sea el mismo que si se hubiera procesado una sola vez correctamente?","title":"Tarea"},{"location":"tema41/","text":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow Tema 4.1. Arquitectura y componentes de Airflow Objetivo : Comprender y analizar los componentes fundamentales de Apache Airflow, su arquitectura interna y las configuraciones necesarias para su implementaci\u00f3n, con el fin de dise\u00f1ar flujos de trabajo robustos, escalables y mantenibles en entornos de Big Data. Introducci\u00f3n : Apache Airflow es una plataforma open-source para la orquestaci\u00f3n de flujos de trabajo basada en programaci\u00f3n. Su arquitectura modular y escalable permite coordinar tareas distribuidas, administrar dependencias entre procesos y monitorear su ejecuci\u00f3n en tiempo real. Para aprovechar todo su potencial, es necesario conocer a fondo sus componentes principales y c\u00f3mo se interrelacionan, as\u00ed como las opciones de configuraci\u00f3n y despliegue disponibles. Desarrollo : El n\u00facleo de Apache Airflow se basa en una arquitectura distribuida compuesta por m\u00faltiples servicios que trabajan de manera coordinada: un Webserver para la interfaz de usuario, un Scheduler para planificar y lanzar tareas, un Executor para ejecutar las tareas, una base de datos de metadatos que registra el estado de las ejecuciones y, en algunos casos, Workers que procesan las tareas. Esta arquitectura flexible permite escalar desde una instalaci\u00f3n local simple hasta implementaciones en cl\u00fasteres de Kubernetes o en la nube. La comprensi\u00f3n profunda de estos componentes, junto con el manejo adecuado de configuraciones y variables de entorno, permite adaptar Airflow a una amplia variedad de contextos y necesidades. 4.1.1 Webserver, Scheduler, Executor, Metadata Database Apache Airflow est\u00e1 compuesto por varios servicios fundamentales que interact\u00faan para programar, ejecutar y monitorear flujos de trabajo (DAGs). Cada componente cumple un rol espec\u00edfico y puede escalarse de manera independiente. Webserver El Webserver es el componente que proporciona la interfaz gr\u00e1fica de usuario (GUI) de Airflow. A trav\u00e9s de esta interfaz, los usuarios pueden visualizar, ejecutar manualmente y monitorear DAGs, revisar logs, configurar conexiones y m\u00e1s. Corre t\u00edpicamente en un proceso separado y puede desplegarse detr\u00e1s de un balanceador de carga para escalar horizontalmente. Ofrece paneles para: DAGs, tareas, logs, variables, pools, conexiones y m\u00e1s. Admite autenticaci\u00f3n (LDAP, OAuth, Kerberos, etc.) Corre como un servicio Flask, basado en Gunicorn para producci\u00f3n. Scheduler El Scheduler es el componente responsable de analizar los DAGs y programar las tareas seg\u00fan su configuraci\u00f3n ( schedule_interval , dependencias, etc.). Se encarga de colocar las tareas listas en la cola de ejecuci\u00f3n correspondiente seg\u00fan el Executor en uso. Revisa constantemente la base de metadatos para detectar DAGs nuevos o actualizados. Eval\u00faa dependencias y lanza tareas seg\u00fan su planificaci\u00f3n y estado. Requiere alta disponibilidad en entornos de producci\u00f3n (puede ejecutarse en m\u00faltiples instancias). Executor El Executor es el componente que decide c\u00f3mo y d\u00f3nde se ejecutan las tareas. Define el modelo de ejecuci\u00f3n, ya sea en el mismo proceso (LocalExecutor), en Workers (CeleryExecutor), o en pods (KubernetesExecutor). Es una de las piezas clave para definir la escalabilidad del sistema. Determina si las tareas se ejecutan de forma secuencial, paralela o distribuida. Metadata Database Airflow utiliza una base de datos relacional (PostgreSQL o MySQL) para almacenar metadatos de la ejecuci\u00f3n: estado de DAGs y tareas, logs, registros de ejecuci\u00f3n, variables, conexiones, etc. Es el estado centralizado del sistema. Debe estar siempre disponible y respaldada. Permite trazabilidad y auditor\u00eda completa de los flujos de trabajo. 4.1.2 Workers y tipos de executors (Local, Celery, Kubernetes, CeleryKubernetes) El tipo de Executor elegido determina si es necesario usar Workers y define c\u00f3mo se distribuyen y ejecutan las tareas de los DAGs. Airflow soporta varios modelos de ejecuci\u00f3n, cada uno adecuado para diferentes escenarios. LocalExecutor Ejecuta las tareas en procesos paralelos dentro del mismo nodo donde se ejecuta el Scheduler. \u00datil para entornos de desarrollo o producci\u00f3n ligera. No requiere Workers externos. Ventajas Limitaciones Simple de configurar No se distribuye entre nodos Sin dependencias adicionales (como colas o cl\u00fasteres) Escalabilidad limitada a los recursos del host CeleryExecutor Usa Celery como backend de procesamiento distribuido y RabbitMQ o Redis como broker de mensajes. Permite escalar horizontalmente mediante Workers externos que procesan tareas desde una cola. Ventajas Limitaciones Altamente escalable Mayor complejidad operativa Separaci\u00f3n clara entre Scheduler y ejecuci\u00f3n de tareas Necesita configuraci\u00f3n de broker y backend de resultados KubernetesExecutor Cada tarea se ejecuta como un pod independiente en un cl\u00faster de Kubernetes. Ideal para entornos cloud-native y escalabilidad extrema. Ventajas Limitaciones Escalabilidad autom\u00e1tica Requiere experiencia en Kubernetes Aislamiento total de tareas Inicializaci\u00f3n de pods puede a\u00f1adir latencia Integraci\u00f3n nativa con servicios de nube CeleryKubernetesExecutor H\u00edbrido entre Celery y Kubernetes. Algunas tareas se ejecutan en Workers Celery, y otras en pods Kubernetes. Ventajas Limitaciones Permite flexibilidad en entornos h\u00edbridos Complejidad elevada de configuraci\u00f3n y monitoreo Equilibrio entre tareas persistentes y din\u00e1micas 4.1.3 Configuraci\u00f3n y variables de entorno Airflow permite configurar su comportamiento mediante archivos, variables de entorno y par\u00e1metros internos. Esto otorga flexibilidad para adaptarse a m\u00faltiples entornos: local, CI/CD, nube, cl\u00fasteres distribuidos, etc. Archivo airflow.cfg Es el archivo principal de configuraci\u00f3n de Airflow. Contiene m\u00e1s de 70 par\u00e1metros organizados por secciones: core , scheduler , webserver , logging , executor , entre otros. Define la conexi\u00f3n a la base de datos ( sql_alchemy_conn ). Determina el tipo de executor. Configura el path de los DAGs. Controla la frecuencia de chequeo del scheduler, la pol\u00edtica de retries, etc. Este archivo puede ser sobrescrito o templado en entornos Dockerizados o de nube. Variables de entorno Airflow permite sobrescribir cualquier configuraci\u00f3n de airflow.cfg mediante variables de entorno, utilizando el prefijo AIRFLOW__ . Por ejemplo: AIRFLOW__CORE__SQL_ALCHEMY_CONN : define la URL de la base de datos. AIRFLOW__CORE__EXECUTOR : define el tipo de executor. Esto es especialmente \u00fatil en entornos de contenedores, pipelines de CI/CD o despliegues cloud. Variables y conexiones en la interfaz Adem\u00e1s de variables de entorno del sistema, Airflow permite definir variables internas ( Variables ) y conexiones ( Connections ) desde la UI o CLI: Variables : par\u00e1metros reutilizables dentro de los DAGs. Connections : credenciales y URIs de servicios externos (bases de datos, APIs, buckets, etc.) Estas son accesibles desde los DAGs mediante la API interna de Airflow ( Variable.get() , BaseHook.get_connection() ). Tarea \u00bfQu\u00e9 componente de Airflow proporciona la interfaz gr\u00e1fica de usuario? \u00bfCu\u00e1l es el rol principal del Scheduler en la arquitectura de Airflow? \u00bfQu\u00e9 tipo de executor se recomienda para ambientes con Kubernetes? \u00bfCu\u00e1l es la funci\u00f3n del archivo airflow.cfg ? \u00bfQu\u00e9 base de datos se utiliza para almacenar metadatos en Airflow? Menciona dos ventajas del CeleryExecutor frente al LocalExecutor. \u00bfQu\u00e9 significa AIRFLOW__CORE__EXECUTOR ? \u00bfQu\u00e9 componente almacena el historial de ejecuciones y tareas? \u00bfCu\u00e1l es la diferencia entre una Variable y una Connection en Airflow? \u00bfPara qu\u00e9 sirve la configuraci\u00f3n sql_alchemy_conn ?","title":"Arquitectura y componentes de Airflow"},{"location":"tema41/#4-automatizacion-y-orquestacion-con-apache-airflow","text":"","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"tema41/#tema-41-arquitectura-y-componentes-de-airflow","text":"Objetivo : Comprender y analizar los componentes fundamentales de Apache Airflow, su arquitectura interna y las configuraciones necesarias para su implementaci\u00f3n, con el fin de dise\u00f1ar flujos de trabajo robustos, escalables y mantenibles en entornos de Big Data. Introducci\u00f3n : Apache Airflow es una plataforma open-source para la orquestaci\u00f3n de flujos de trabajo basada en programaci\u00f3n. Su arquitectura modular y escalable permite coordinar tareas distribuidas, administrar dependencias entre procesos y monitorear su ejecuci\u00f3n en tiempo real. Para aprovechar todo su potencial, es necesario conocer a fondo sus componentes principales y c\u00f3mo se interrelacionan, as\u00ed como las opciones de configuraci\u00f3n y despliegue disponibles. Desarrollo : El n\u00facleo de Apache Airflow se basa en una arquitectura distribuida compuesta por m\u00faltiples servicios que trabajan de manera coordinada: un Webserver para la interfaz de usuario, un Scheduler para planificar y lanzar tareas, un Executor para ejecutar las tareas, una base de datos de metadatos que registra el estado de las ejecuciones y, en algunos casos, Workers que procesan las tareas. Esta arquitectura flexible permite escalar desde una instalaci\u00f3n local simple hasta implementaciones en cl\u00fasteres de Kubernetes o en la nube. La comprensi\u00f3n profunda de estos componentes, junto con el manejo adecuado de configuraciones y variables de entorno, permite adaptar Airflow a una amplia variedad de contextos y necesidades.","title":"Tema 4.1. Arquitectura y componentes de Airflow"},{"location":"tema41/#411-webserver-scheduler-executor-metadata-database","text":"Apache Airflow est\u00e1 compuesto por varios servicios fundamentales que interact\u00faan para programar, ejecutar y monitorear flujos de trabajo (DAGs). Cada componente cumple un rol espec\u00edfico y puede escalarse de manera independiente.","title":"4.1.1 Webserver, Scheduler, Executor, Metadata Database"},{"location":"tema41/#webserver","text":"El Webserver es el componente que proporciona la interfaz gr\u00e1fica de usuario (GUI) de Airflow. A trav\u00e9s de esta interfaz, los usuarios pueden visualizar, ejecutar manualmente y monitorear DAGs, revisar logs, configurar conexiones y m\u00e1s. Corre t\u00edpicamente en un proceso separado y puede desplegarse detr\u00e1s de un balanceador de carga para escalar horizontalmente. Ofrece paneles para: DAGs, tareas, logs, variables, pools, conexiones y m\u00e1s. Admite autenticaci\u00f3n (LDAP, OAuth, Kerberos, etc.) Corre como un servicio Flask, basado en Gunicorn para producci\u00f3n.","title":"Webserver"},{"location":"tema41/#scheduler","text":"El Scheduler es el componente responsable de analizar los DAGs y programar las tareas seg\u00fan su configuraci\u00f3n ( schedule_interval , dependencias, etc.). Se encarga de colocar las tareas listas en la cola de ejecuci\u00f3n correspondiente seg\u00fan el Executor en uso. Revisa constantemente la base de metadatos para detectar DAGs nuevos o actualizados. Eval\u00faa dependencias y lanza tareas seg\u00fan su planificaci\u00f3n y estado. Requiere alta disponibilidad en entornos de producci\u00f3n (puede ejecutarse en m\u00faltiples instancias).","title":"Scheduler"},{"location":"tema41/#executor","text":"El Executor es el componente que decide c\u00f3mo y d\u00f3nde se ejecutan las tareas. Define el modelo de ejecuci\u00f3n, ya sea en el mismo proceso (LocalExecutor), en Workers (CeleryExecutor), o en pods (KubernetesExecutor). Es una de las piezas clave para definir la escalabilidad del sistema. Determina si las tareas se ejecutan de forma secuencial, paralela o distribuida.","title":"Executor"},{"location":"tema41/#metadata-database","text":"Airflow utiliza una base de datos relacional (PostgreSQL o MySQL) para almacenar metadatos de la ejecuci\u00f3n: estado de DAGs y tareas, logs, registros de ejecuci\u00f3n, variables, conexiones, etc. Es el estado centralizado del sistema. Debe estar siempre disponible y respaldada. Permite trazabilidad y auditor\u00eda completa de los flujos de trabajo.","title":"Metadata Database"},{"location":"tema41/#412-workers-y-tipos-de-executors-local-celery-kubernetes-celerykubernetes","text":"El tipo de Executor elegido determina si es necesario usar Workers y define c\u00f3mo se distribuyen y ejecutan las tareas de los DAGs. Airflow soporta varios modelos de ejecuci\u00f3n, cada uno adecuado para diferentes escenarios.","title":"4.1.2 Workers y tipos de executors (Local, Celery, Kubernetes, CeleryKubernetes)"},{"location":"tema41/#localexecutor","text":"Ejecuta las tareas en procesos paralelos dentro del mismo nodo donde se ejecuta el Scheduler. \u00datil para entornos de desarrollo o producci\u00f3n ligera. No requiere Workers externos. Ventajas Limitaciones Simple de configurar No se distribuye entre nodos Sin dependencias adicionales (como colas o cl\u00fasteres) Escalabilidad limitada a los recursos del host","title":"LocalExecutor"},{"location":"tema41/#celeryexecutor","text":"Usa Celery como backend de procesamiento distribuido y RabbitMQ o Redis como broker de mensajes. Permite escalar horizontalmente mediante Workers externos que procesan tareas desde una cola. Ventajas Limitaciones Altamente escalable Mayor complejidad operativa Separaci\u00f3n clara entre Scheduler y ejecuci\u00f3n de tareas Necesita configuraci\u00f3n de broker y backend de resultados","title":"CeleryExecutor"},{"location":"tema41/#kubernetesexecutor","text":"Cada tarea se ejecuta como un pod independiente en un cl\u00faster de Kubernetes. Ideal para entornos cloud-native y escalabilidad extrema. Ventajas Limitaciones Escalabilidad autom\u00e1tica Requiere experiencia en Kubernetes Aislamiento total de tareas Inicializaci\u00f3n de pods puede a\u00f1adir latencia Integraci\u00f3n nativa con servicios de nube","title":"KubernetesExecutor"},{"location":"tema41/#celerykubernetesexecutor","text":"H\u00edbrido entre Celery y Kubernetes. Algunas tareas se ejecutan en Workers Celery, y otras en pods Kubernetes. Ventajas Limitaciones Permite flexibilidad en entornos h\u00edbridos Complejidad elevada de configuraci\u00f3n y monitoreo Equilibrio entre tareas persistentes y din\u00e1micas","title":"CeleryKubernetesExecutor"},{"location":"tema41/#413-configuracion-y-variables-de-entorno","text":"Airflow permite configurar su comportamiento mediante archivos, variables de entorno y par\u00e1metros internos. Esto otorga flexibilidad para adaptarse a m\u00faltiples entornos: local, CI/CD, nube, cl\u00fasteres distribuidos, etc.","title":"4.1.3 Configuraci\u00f3n y variables de entorno"},{"location":"tema41/#archivo-airflowcfg","text":"Es el archivo principal de configuraci\u00f3n de Airflow. Contiene m\u00e1s de 70 par\u00e1metros organizados por secciones: core , scheduler , webserver , logging , executor , entre otros. Define la conexi\u00f3n a la base de datos ( sql_alchemy_conn ). Determina el tipo de executor. Configura el path de los DAGs. Controla la frecuencia de chequeo del scheduler, la pol\u00edtica de retries, etc. Este archivo puede ser sobrescrito o templado en entornos Dockerizados o de nube.","title":"Archivo airflow.cfg"},{"location":"tema41/#variables-de-entorno","text":"Airflow permite sobrescribir cualquier configuraci\u00f3n de airflow.cfg mediante variables de entorno, utilizando el prefijo AIRFLOW__ . Por ejemplo: AIRFLOW__CORE__SQL_ALCHEMY_CONN : define la URL de la base de datos. AIRFLOW__CORE__EXECUTOR : define el tipo de executor. Esto es especialmente \u00fatil en entornos de contenedores, pipelines de CI/CD o despliegues cloud.","title":"Variables de entorno"},{"location":"tema41/#variables-y-conexiones-en-la-interfaz","text":"Adem\u00e1s de variables de entorno del sistema, Airflow permite definir variables internas ( Variables ) y conexiones ( Connections ) desde la UI o CLI: Variables : par\u00e1metros reutilizables dentro de los DAGs. Connections : credenciales y URIs de servicios externos (bases de datos, APIs, buckets, etc.) Estas son accesibles desde los DAGs mediante la API interna de Airflow ( Variable.get() , BaseHook.get_connection() ).","title":"Variables y conexiones en la interfaz"},{"location":"tema41/#tarea","text":"\u00bfQu\u00e9 componente de Airflow proporciona la interfaz gr\u00e1fica de usuario? \u00bfCu\u00e1l es el rol principal del Scheduler en la arquitectura de Airflow? \u00bfQu\u00e9 tipo de executor se recomienda para ambientes con Kubernetes? \u00bfCu\u00e1l es la funci\u00f3n del archivo airflow.cfg ? \u00bfQu\u00e9 base de datos se utiliza para almacenar metadatos en Airflow? Menciona dos ventajas del CeleryExecutor frente al LocalExecutor. \u00bfQu\u00e9 significa AIRFLOW__CORE__EXECUTOR ? \u00bfQu\u00e9 componente almacena el historial de ejecuciones y tareas? \u00bfCu\u00e1l es la diferencia entre una Variable y una Connection en Airflow? \u00bfPara qu\u00e9 sirve la configuraci\u00f3n sql_alchemy_conn ?","title":"Tarea"},{"location":"tema42/","text":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow 4.2. DAGs, operadores y tareas Objetivo : Comprender, dise\u00f1ar y construir flujos de trabajo programables en Apache Airflow utilizando DAGs, operadores y tareas, incluyendo su agrupaci\u00f3n, parametrizaci\u00f3n y personalizaci\u00f3n para gestionar eficientemente flujos de datos complejos en entornos Big Data. Introducci\u00f3n : Apache Airflow permite definir y gestionar flujos de trabajo como c\u00f3digo, a trav\u00e9s de DAGs (Directed Acyclic Graphs), que representan secuencias de tareas con dependencias expl\u00edcitas. Este enfoque permite un control detallado del orden de ejecuci\u00f3n, la l\u00f3gica condicional, la integraci\u00f3n con diversas herramientas y servicios, y la reutilizaci\u00f3n de c\u00f3digo para tareas repetitivas. En este tema se profundiza en c\u00f3mo construir DAGs robustos, emplear operadores integrados y personalizados, utilizar estructuras avanzadas como TaskGroups y SubDAGs, y aplicar templating din\u00e1mico con Jinja2. Desarrollo : La definici\u00f3n de flujos de trabajo (pipelines) en Apache Airflow se basa en DAGs que agrupan tareas individuales y definen la l\u00f3gica de ejecuci\u00f3n. Cada tarea puede realizar operaciones diversas como ejecutar comandos del sistema, procesar datos con Spark, mover datos entre servicios cloud, o enviar notificaciones. Airflow proporciona una gran variedad de operadores listos para usar y permite crear operadores personalizados. Tambi\u00e9n ofrece herramientas para organizar tareas y aplicar plantillas din\u00e1micas, lo cual facilita el mantenimiento y escalabilidad de flujos complejos. 4.2.1 Definici\u00f3n y estructura de DAGs Un DAG (Directed Acyclic Graph) es la pieza fundamental de Apache Airflow que representa un flujo de trabajo completo como un grafo dirigido sin ciclos. Cada DAG puede ser: Dirigido : Las tareas tienen un orden espec\u00edfico de ejecuci\u00f3n (flujo direccional) Ac\u00edclico : No puede haber bucles infinitos o dependencias circulares Grafo : Conjunto de nodos (tareas) conectados por aristas (dependencias) Par\u00e1metros Esenciales Son los par\u00e1metros fundamentales que definen el comportamiento b\u00e1sico de un DAG en Apache Airflow. # Par\u00e1metros obligatorios dag_id='nombre_unico_del_dag' # identificador \u00fanico en todo Airflow start_date=datetime(2024, 1, 1) # fecha desde cuando puede ejecutarse schedule_interval='@daily' # frecuencia de ejecuci\u00f3n # Par\u00e1metros importantes catchup=False # si ejecutar ejecuciones pasadas max_active_runs=1 # n\u00famero m\u00e1ximo de ejecuciones simult\u00e1neas default_retries=1 # reintentos por defecto para todas las tareas retry_delay=timedelta(minutes=5) # tiempo entre reintentos default_args = { # par\u00e1metros que heredan todas las tareas del DAG 'owner': 'team_data', 'retries': 2, 'retry_delay': timedelta(minutes=5), 'email_on_failure': True, 'email': ['alerts@empresa.com'] } Planificaci\u00f3n de la Ejecuci\u00f3n (Scheduling) Hay varias formas de configurar la frecuencias de ejecuci\u00f3n: # Opciones de schedule_interval '@once' # Ejecutar solo una vez '@hourly' # Cada hora (0 * * * *) '@daily' # Diariamente a medianoche (0 0 * * *) '@weekly' # Semanalmente los domingos (0 0 * * 0) '@monthly' # Primer d\u00eda de cada mes (0 0 1 * *) '@yearly' # 1 de enero cada a\u00f1o (0 0 1 1 *) # Cron personalizado '30 6 * * 1-5' # 6:30 AM, lunes a viernes '0 */4 * * *' # Cada 4 horas # Program\u00e1tico timedelta(hours=2) # Cada 2 horas timedelta(days=1, hours=12) # Cada d\u00eda y medio Estructura b\u00e1sica de un DAG from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime with DAG( dag_id='mi_primer_dag', start_date=datetime(2024, 1, 1), schedule_interval='@daily', catchup=False, tags=['ejemplo'] ) as dag: tarea_1 = BashOperator( task_id='imprimir_fecha', bash_command='date' ) tarea_2 = BashOperator( task_id='mostrar_hola_mundo', bash_command='echo \"Hola mundo desde Airflow!\"' ) tarea_1 >> tarea_2 # Define la dependencia DAG Completo y Detallado Diagrama : flowchart TD %% Estilos para diferentes tipos de tareas classDef verificacion fill:#e1f5fe,stroke:#01579b,stroke-width:2px classDef extraccion fill:#f3e5f5,stroke:#4a148c,stroke-width:2px classDef procesamiento fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px classDef validacion fill:#fff3e0,stroke:#e65100,stroke-width:2px classDef reporte fill:#fce4ec,stroke:#880e4f,stroke-width:2px classDef carga fill:#e0f2f1,stroke:#004d40,stroke-width:2px classDef notificacion fill:#f1f8e9,stroke:#33691e,stroke-width:2px classDef limpieza fill:#fafafa,stroke:#424242,stroke-width:2px %% Definici\u00f3n de nodos A[verificar_sistema<br/>\ud83d\udd0d Verificar recursos del sistema]:::verificacion B[extraer_datos<br/>\ud83d\udce5 Extraer datos de PostgreSQL]:::extraccion C[procesar_datos<br/>\u2699\ufe0f Transformar y limpiar datos]:::procesamiento D[validar_calidad<br/>\u2705 Validar calidad de datos]:::validacion E[generar_reporte_task<br/>\ud83d\udcca Generar reporte de m\u00e9tricas]:::reporte F[cargar_datos_warehouse<br/>\ud83c\udfd7\ufe0f Cargar al data warehouse]:::carga G[enviar_notificacion<br/>\ud83d\udce7 Enviar email de confirmaci\u00f3n]:::notificacion H[limpiar_archivos_temporales<br/>\ud83e\uddf9 Limpiar archivos temp]:::limpieza %% Flujo de dependencias A --> B B --> C %% Ramificaci\u00f3n paralela despu\u00e9s del procesamiento C --> D C --> E %% Convergencia para la carga (requiere procesamiento Y validaci\u00f3n) C --> F D --> F %% Reporte va a notificaci\u00f3n E --> G %% Limpieza al final (despu\u00e9s de carga Y notificaci\u00f3n) F --> H G --> H %% Anotaciones explicativas subgraph \"Fase 1: Preparaci\u00f3n\" A B end subgraph \"Fase 2: Procesamiento\" C end subgraph \"Fase 3: Validaci\u00f3n y Reportes (Paralelo)\" D E end subgraph \"Fase 4: Finalizaci\u00f3n\" F G H end Implementaci\u00f3n : from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator from airflow.operators.email import EmailOperator from datetime import datetime, timedelta import pandas as pd import logging # Estos argumentos se aplicar\u00e1n a todas las tareas del DAG # a menos que se sobrescriban espec\u00edficamente default_args = { 'owner': 'equipo_data_engineering', # Responsable del DAG 'depends_on_past': False, # No depende de ejecuciones anteriores 'start_date': datetime(2024, 1, 1), # Fecha de inicio del DAG 'email_on_failure': True, # Enviar email si falla 'email_on_retry': False, # No enviar email en reintento 'email': ['team@empresa.com'], # Lista de emails para notificaciones 'retries': 2, # N\u00famero de reintentos por tarea 'retry_delay': timedelta(minutes=5), # Tiempo entre reintentos 'execution_timeout': timedelta(hours=2), # Timeout m\u00e1ximo por tarea } # DEFINICI\u00d3N DEL DAG with DAG( # Identificador \u00fanico del DAG en toda la instancia de Airflow dag_id='pipeline_procesamiento_ventas_v2', # Hereda la configuraci\u00f3n por defecto definida arriba default_args=default_args, # Descripci\u00f3n que aparece en la UI de Airflow description='Pipeline completo para procesar datos de ventas diarias', # Frecuencia de ejecuci\u00f3n - se ejecuta diariamente a las 2:00 AM schedule_interval='0 2 * * *', # No ejecutar DAGs para fechas pasadas (evita backfill autom\u00e1tico) catchup=False, # Solo una instancia del DAG puede ejecutarse simult\u00e1neamente max_active_runs=1, # Tags para organizar DAGs en la UI tags=['ventas', 'etl', 'produccion', 'diario'], # Documentaci\u00f3n en formato Markdown doc_md=\"\"\" ## Pipeline de Procesamiento de Ventas Este DAG procesa los datos de ventas diarias: 1. Extrae datos de la base de datos transaccional 2. Aplica transformaciones y limpieza 3. Carga los datos al data warehouse 4. Env\u00eda reporte de resumen por email **Dependencias externas**: Base de datos PostgreSQL, S3 bucket **Tiempo estimado**: 45 minutos **Criticidad**: Alta - bloquea reportes ejecutivos \"\"\", ) as dag: # FUNCI\u00d3N PYTHON PERSONALIZADA def procesar_datos_ventas(**context): \"\"\" Funci\u00f3n que procesa los datos de ventas. **context contiene informaci\u00f3n del contexto de ejecuci\u00f3n de Airflow \"\"\" # Obtener la fecha de ejecuci\u00f3n del contexto execution_date = context['execution_date'] # Simular procesamiento de datos logging.info(f\"Procesando datos de ventas para {execution_date}\") # En un caso real, aqu\u00ed har\u00edas: # - Conexi\u00f3n a base de datos # - Extracci\u00f3n de datos # - Transformaciones con pandas/spark # - Validaciones de calidad # Retornar m\u00e9tricas para usar en tareas posteriores return { 'registros_procesados': 15420, 'ventas_totales': 89750.50, 'fecha_proceso': execution_date.strftime('%Y-%m-%d') } def generar_reporte(**context): \"\"\"Genera un reporte basado en los datos procesados\"\"\" # Obtener datos de la tarea anterior usando XCom datos_ventas = context['task_instance'].xcom_pull(task_ids='procesar_datos') reporte = f\"\"\" Reporte de Ventas - {datos_ventas['fecha_proceso']} ================================================ Registros procesados: {datos_ventas['registros_procesados']:,} Ventas totales: ${datos_ventas['ventas_totales']:,.2f} \"\"\" logging.info(reporte) return reporte # DEFINICI\u00d3N DE TAREAS # TAREA 1: Verificaci\u00f3n del sistema verificar_sistema = BashOperator( task_id='verificar_sistema', # Comando bash que verifica conectividad y recursos bash_command=\"\"\" echo \"Verificando sistema...\" df -h | grep -v tmpfs # Verificar espacio en disco echo \"Sistema verificado correctamente\" \"\"\", # Documentaci\u00f3n espec\u00edfica de la tarea doc_md=\"\"\" ### Verificaci\u00f3n del Sistema Valida que el sistema tenga los recursos necesarios: - Espacio en disco suficiente - Conectividad de red - Servicios requeridos activos \"\"\" ) # TAREA 2: Extracci\u00f3n de datos extraer_datos = BashOperator( task_id='extraer_datos', bash_command=\"\"\" echo \"Iniciando extracci\u00f3n de datos...\" # En producci\u00f3n, esto ser\u00eda algo como: # psql -h $DB_HOST -d ventas -c \"COPY (...) TO STDOUT\" > /tmp/ventas_{{ ds }}.csv echo \"Simulando extracci\u00f3n de 15,420 registros\" echo \"Datos extra\u00eddos exitosamente\" \"\"\", # Esta tarea puede fallar ocasionalmente, aumentamos reintentos retries=3, retry_delay=timedelta(minutes=2) ) # TAREA 3: Procesamiento con Python procesar_datos = PythonOperator( task_id='procesar_datos', python_callable=procesar_datos_ventas, # Proporcionar el contexto de Airflow a la funci\u00f3n provide_context=True ) # TAREA 4: Validaci\u00f3n de calidad validar_calidad = BashOperator( task_id='validar_calidad', bash_command=\"\"\" echo \"Ejecutando validaciones de calidad de datos...\" # Simular validaciones echo \"\u2713 Sin valores nulos en campos cr\u00edticos\" echo \"\u2713 Rangos de fechas v\u00e1lidos\" echo \"\u2713 Integridad referencial correcta\" echo \"Validaci\u00f3n completada exitosamente\" \"\"\" ) # TAREA 5: Carga al data warehouse cargar_datos = BashOperator( task_id='cargar_datos_warehouse', bash_command=\"\"\" echo \"Cargando datos al data warehouse...\" # En producci\u00f3n ser\u00eda algo como: # aws s3 cp /tmp/ventas_processed_{{ ds }}.parquet s3://warehouse/ventas/ echo \"Datos cargados exitosamente al data warehouse\" \"\"\" ) # TAREA 6: Generaci\u00f3n de reporte generar_reporte_task = PythonOperator( task_id='generar_reporte', python_callable=generar_reporte, provide_context=True ) # TAREA 7: Env\u00edo de notificaci\u00f3n por email enviar_notificacion = EmailOperator( task_id='enviar_notificacion', to=['gerencia@empresa.com', 'ventas@empresa.com'], subject='Pipeline de Ventas Completado - {{ ds }}', html_content=\"\"\" <h3>Pipeline de Procesamiento de Ventas</h3> <p><strong>Fecha:</strong> {{ ds }}</p> <p><strong>Estado:</strong> Completado exitosamente \u2705</p> <p><strong>Tiempo de ejecuci\u00f3n:</strong> {{ macros.datetime.now() }}</p> <p>Los datos de ventas han sido procesados y est\u00e1n disponibles en el data warehouse.</p> <hr> <small>Este es un mensaje autom\u00e1tico del sistema de orquestaci\u00f3n de datos.</small> \"\"\" ) # TAREA 8: Limpieza de archivos temporales limpiar_archivos = BashOperator( task_id='limpiar_archivos_temporales', bash_command=\"\"\" echo \"Limpiando archivos temporales...\" # rm -f /tmp/ventas_{{ ds }}.* echo \"Limpieza completada\" \"\"\", # Esta tarea no es cr\u00edtica, si falla no debe afectar el pipeline trigger_rule='all_done' # Se ejecuta sin importar si las anteriores fallan ) # DEFINICI\u00d3N DE DEPENDENCIAS # Secuencia lineal b\u00e1sica verificar_sistema >> extraer_datos >> procesar_datos # Despu\u00e9s del procesamiento, dos ramas paralelas procesar_datos >> [validar_calidad, generar_reporte_task] # La carga depende de que tanto el procesamiento como la validaci\u00f3n sean exitosos [procesar_datos, validar_calidad] >> cargar_datos # El reporte se puede generar en paralelo con la carga generar_reporte_task >> enviar_notificacion # La limpieza se ejecuta al final, despu\u00e9s de todo [cargar_datos, enviar_notificacion] >> limpiar_archivos 4.2.2 Operadores built-in y custom operators Los operadores son las unidades de ejecuci\u00f3n en Airflow. Representan tareas concretas que se ejecutan en el DAG. Airflow incluye m\u00faltiples operadores built-in y permite crear operadores personalizados para extender su funcionalidad. Operadores de Ejecuci\u00f3n B\u00e1sica Estos operadores permiten ejecutar comandos del sistema, scripts, o funciones personalizadas. BashOperator : Fundamental para ejecutar comandos del sistema, scripts shell y herramientas CLI de big data como Hadoop, Spark-submit, etc. PythonOperator : Cr\u00edtico para ejecutar funciones Python personalizadas, transformaciones de datos y l\u00f3gica de negocio EmailOperator : Esencial para notificaciones y alertas en pipelines de producci\u00f3n from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator def saludar(): print(\"Hola desde PythonOperator\") tarea_bash = BashOperator( task_id='tarea_bash', bash_command='echo \"Ejecutando comando Bash\"', dag=dag ) tarea_python = PythonOperator( task_id='tarea_python', python_callable=saludar, dag=dag ) Operadores de Bases de Datos Estos operadores permiten ejecutar consultas SQL directamente desde Airflow hacia motores relacionales o embebidos. PostgresOperator / MySqlOperator : Para ejecutar consultas SQL en bases de datos relacionales SqliteOperator : \u00datil para pruebas y desarrollo local from airflow.providers.postgres.operators.postgres import PostgresOperator tarea_sql = PostgresOperator( task_id='crear_tabla', postgres_conn_id='mi_conexion_postgres', sql='CREATE TABLE IF NOT EXISTS tabla_ejemplo (id SERIAL PRIMARY KEY);', dag=dag ) Operadores de Transferencia Son usados para mover datos entre servicios, especialmente en la nube. S3ToRedshiftOperator : Transferir datos desde S3 a Redshift (muy com\u00fan en AWS) BigQueryOperator : Para operaciones en Google BigQuery RedshiftToS3Operator : Exportar datos desde Redshift from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator tarea_transferencia = S3ToRedshiftOperator( task_id='s3_a_redshift', s3_bucket='mi-bucket', s3_key='datos.csv', schema='public', table='mi_tabla', copy_options=['csv'], aws_conn_id='aws_default', redshift_conn_id='redshift_default', dag=dag ) Operadores de Control de Flujo Controlan la l\u00f3gica de ejecuci\u00f3n dentro de un DAG. BranchPythonOperator : Para l\u00f3gica condicional en el pipeline EmptyOperator (antes DummyOperator ): Para puntos de sincronizaci\u00f3n TriggerDagRunOperator : Para activar otros DAGs from airflow.operators.empty import EmptyOperator from airflow.operators.branch import BranchPythonOperator def decidir(): return 'tarea_a' if datetime.now().hour < 12 else 'tarea_b' rama = BranchPythonOperator( task_id='evaluar_ruta', python_callable=decidir, dag=dag ) tarea_a = EmptyOperator(task_id='tarea_a', dag=dag) tarea_b = EmptyOperator(task_id='tarea_b', dag=dag) rama >> [tarea_a, tarea_b] Operadores para Spark y Hadoop Permiten ejecutar aplicaciones en motores de procesamiento distribuido. SparkSubmitOperator : Ejecutar aplicaciones Spark (batch processing) DataprocSubmitJobOperator : Para trabajos en Google Cloud Dataproc EMRStepOperator : Para clusters Amazon EMR from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator spark_job = SparkSubmitOperator( task_id='ejecutar_spark', application='/ruta/a/mi_app_spark.py', conn_id='spark_default', dag=dag ) Operadores para Streaming y HDFS Permiten flujos en tiempo real y acceso a HDFS KafkaOperator : Interactuar con Apache Kafka para datos en tiempo real HDFSOperator : Operaciones en Hadoop Distributed File System from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor esperar_archivo = HdfsSensor( task_id='esperar_archivo_hdfs', filepath='/data/input/archivo.csv', hdfs_conn_id='hdfs_default', dag=dag ) Operadores para Validaci\u00f3n de Datos Permiten realizar validaciones sobre los datos GreatExpectationsOperator : Validaci\u00f3n de calidad de datos Custom Data Validation Operators : Para reglas espec\u00edficas del negocio from airflow.providers.great_expectations.operators.great_expectations import GreatExpectationsOperator validacion = GreatExpectationsOperator( task_id='validar_datos', data_context_root_dir='/opt/ge/', checkpoint_name='check_ventas', dag=dag ) Operadores para MLOps Flujos relacionados con procesos de Machine Learning MLflowOperator : Para experimentos y modelos de machine learning KubernetesOperator : Para ejecutar contenedores en Kubernetes from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator ml_entrenamiento = KubernetesPodOperator( task_id='entrenar_modelo', name='ml-train', image='ml/entrenamiento:latest', dag=dag ) 4.2.3 TaskGroups Cuando los DAGs crecen en complejidad (m\u00e1s de 10-15 tareas), surgen varios problemas: Legibilidad: Dif\u00edcil entender el flujo en la UI de Airflow Mantenimiento: C\u00f3digo desordenado y dif\u00edcil de modificar Reutilizaci\u00f3n: Patrones repetitivos sin forma de encapsular Colaboraci\u00f3n: Equipos diferentes trabajando en el mismo DAG Debugging: Dif\u00edcil identificar grupos de tareas relacionadas La soluci\u00f3n es agrupar tareas l\u00f3gicamente relacionadas usando TaskGroups. Nota: Existe un mecanismo adicional llamado SubDAGs, pero ya es considerado obsoleto. Uso de TaskGroups Los TaskGroups son una forma de organizar tareas visualmente en la UI de Airflow sin crear DAGs separados. Introducidos en Airflow 2.0, son la mejor pr\u00e1ctica actual para estructurar DAGs complejos. Ventajas de TaskGroups Organizaci\u00f3n visual: Agrupa tareas en la UI de Airflow Reutilizaci\u00f3n de c\u00f3digo: Encapsula l\u00f3gica repetitiva Mejor rendimiento: No crean procesos separados como SubDAGs Simplicidad: F\u00e1cil de usar y entender Debugging mejorado: Logs y estados agrupados Escalabilidad: Maneja DAGs con cientos de tareas Los TaskGroup permiten agrupar visual y l\u00f3gicamente tareas relacionadas dentro de un DAG. Ejemplo de un TaskGroup : from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator from airflow.utils.task_group import TaskGroup from datetime import datetime, timedelta # Configuraci\u00f3n del DAG with DAG( dag_id='pipeline_con_taskgroups_v1', start_date=datetime(2024, 1, 1), schedule_interval='@daily', catchup=False, tags=['taskgroups', 'ejemplo'] ) as dag: # TASKGROUP 1: PREPARACI\u00d3N DE DATOS with TaskGroup( group_id='preparacion_datos', tooltip='Descarga, validaci\u00f3n y limpieza de datos raw' ) as grupo_preparacion: # Tarea 1: Descargar datos de m\u00faltiples fuentes descargar_datos = BashOperator( task_id='descargar_datos', bash_command=''' echo \"Descargando datos de APIs y bases de datos...\" # Simular descarga de m\u00faltiples fuentes echo \"\u2713 API de ventas: 15,420 registros\" echo \"\u2713 API de clientes: 8,932 registros\" echo \"\u2713 DB transaccional: 45,123 registros\" ''' ) # Tarea 2: Validar integridad de los datos validar_integridad = BashOperator( task_id='validar_integridad', bash_command=''' echo \"Validando integridad de datos descargados...\" echo \"\u2713 Verificando checksums\" echo \"\u2713 Validando esquemas JSON/CSV\" echo \"\u2713 Contando registros esperados\" ''' ) # Tarea 3: Limpiar y estandarizar datos limpiar_datos = PythonOperator( task_id='limpiar_datos', python_callable=lambda: print(\"\"\" Limpiando datos: \u2713 Removiendo duplicados \u2713 Estandarizando formatos de fecha \u2713 Normalizando nombres y direcciones \u2713 Aplicando reglas de negocio \"\"\") ) # Dependencias dentro del TaskGroup descargar_datos >> validar_integridad >> limpiar_datos # TASKGROUP 2: TRANSFORMACIONES DE NEGOCIO with TaskGroup( group_id='transformaciones_negocio', tooltip='Aplicar reglas de negocio y c\u00e1lculos complejos' ) as grupo_transformaciones: def calcular_metricas_ventas(): print(\"Calculando m\u00e9tricas de ventas:\") print(\"\u2713 Revenue por regi\u00f3n\") print(\"\u2713 Customer Lifetime Value\") print(\"\u2713 Churn rate\") print(\"\u2713 Forecasting pr\u00f3ximo trimestre\") return \"M\u00e9tricas calculadas exitosamente\" def aplicar_segmentacion(): print(\"Aplicando segmentaci\u00f3n de clientes:\") print(\"\u2713 RFM Analysis (Recency, Frequency, Monetary)\") print(\"\u2713 Clustering de comportamiento\") print(\"\u2713 Propensi\u00f3n de compra\") return \"Segmentaci\u00f3n completada\" calcular_metricas = PythonOperator( task_id='calcular_metricas_ventas', python_callable=calcular_metricas_ventas ) segmentar_clientes = PythonOperator( task_id='segmentar_clientes', python_callable=aplicar_segmentacion ) # Estas tareas pueden ejecutarse en paralelo # No hay dependencia entre calcular m\u00e9tricas y segmentar clientes pass # Sin dependencias internas = ejecuci\u00f3n paralela # TASKGROUP 3: CARGA DE DATOS with TaskGroup( group_id='carga_datos', tooltip='Carga a diferentes sistemas de destino' ) as grupo_carga: cargar_datawarehouse = BashOperator( task_id='cargar_datawarehouse', bash_command=''' echo \"Cargando datos al data warehouse...\" echo \"\u2713 Upserting tabla dimensiones\" echo \"\u2713 Insertando hechos de ventas\" echo \"\u2713 Actualizando \u00edndices\" ''' ) cargar_data_lake = BashOperator( task_id='cargar_data_lake', bash_command=''' echo \"Cargando datos al data lake...\" echo \"\u2713 Particionando por fecha\" echo \"\u2713 Comprimiendo en formato Parquet\" echo \"\u2713 Actualizando cat\u00e1logo de metadatos\" ''' ) actualizar_cache = BashOperator( task_id='actualizar_cache', bash_command=''' echo \"Actualizando sistemas de cache...\" echo \"\u2713 Redis para m\u00e9tricas en tiempo real\" echo \"\u2713 Elasticsearch para b\u00fasquedas\" echo \"\u2713 CDN para reportes est\u00e1ticos\" ''' ) # Todas las cargas pueden ejecutarse en paralelo pass # Sin dependencias = m\u00e1ximo paralelismo # TAREA INDIVIDUAL: NOTIFICACIONES enviar_notificaciones = BashOperator( task_id='enviar_notificaciones', bash_command=''' echo \"Enviando notificaciones finales...\" echo \"\u2713 Email a stakeholders\" echo \"\u2713 Slack al equipo de datos\" echo \"\u2713 Dashboard actualizado\" ''' ) # DEPENDENCIAS ENTRE TASKGROUPS # Flujo secuencial entre grupos principales grupo_preparacion >> grupo_transformaciones >> grupo_carga # La notificaci\u00f3n final depende de que toda la carga est\u00e9 completa grupo_carga >> enviar_notificaciones 4.2.4 Templating con Jinja2 Apache Airflow aprovecha la potencia de Jinja2 , un motor de plantillas moderno y de alto rendimiento para Python, para permitir la generaci\u00f3n din\u00e1mica de comandos, configuraciones y otros valores dentro de tus DAGs (Directed Acyclic Graphs) y operadores. Esta capacidad de templating es fundamental para crear flujos de trabajo flexibles y reutilizables, ya que te permite adaptar el comportamiento de tus tareas en funci\u00f3n de variables de tiempo de ejecuci\u00f3n, configuraciones del entorno o datos espec\u00edficos. Uso de variables y contextos El templating con Jinja2 te permite incrustar variables y acceder al contexto de ejecuci\u00f3n de Airflow directamente dentro de las propiedades de tus operadores que soportan plantillas (identificadas por tener un sufijo _template o ser parte de una lista definida por template_fields en el operador). Considera el siguiente ejemplo con un BashOperator : tarea_con_template = BashOperator( task_id='mostrar_fecha', bash_command='echo \"La fecha de ejecuci\u00f3n es {{ ds }}\"', dag=dag ) En este caso, la cadena bash_command es una plantilla Jinja2. Durante la ejecuci\u00f3n de la tarea {{ ds }} ser\u00e1 reemplazado autom\u00e1ticamente por la fecha de ejecuci\u00f3n actual del DAG en formato YYYY-MM-DD . Esto es \u00fatil para tareas que dependen de la fecha, como el procesamiento de datos diarios o la generaci\u00f3n de informes con nombres de archivo basados en la fecha. Acceso a macros y funciones de utilidad Airflow expone un conjunto de macros y funciones de utilidad a las plantillas Jinja2, lo que te permite realizar operaciones comunes sin necesidad de escribir l\u00f3gica Python adicional. Estas macros proporcionan acceso conveniente a informaci\u00f3n clave del contexto de ejecuci\u00f3n y a funciones de manipulaci\u00f3n de fechas, entre otras cosas. Algunas de las macros m\u00e1s utilizadas incluyen: {{ ds }} : Representa la fecha de ejecuci\u00f3n del DAG en formato de cadena YYYY-MM-DD . Es ideal para operaciones de carga de datos diarias. {{ ds_nodash }} : Similar a ds , pero la fecha se presenta sin guiones, por ejemplo, YYYYMMDD . \u00datil para nombres de archivos o rutas sin caracteres especiales. {{ execution_date }} : Es un objeto datetime completo que representa el momento exacto en que la instancia del DAG fue programada para ejecutarse. Este objeto te permite realizar manipulaciones de tiempo m\u00e1s complejas. {{ prev_ds }} : La fecha de ejecuci\u00f3n anterior, si existe, en formato YYYY-MM-DD . {{ next_ds }} : La siguiente fecha de ejecuci\u00f3n programada, si existe, en formato YYYY-MM-DD . {{ macros.ds_add(ds, 7) }} : Esta es una funci\u00f3n de macro que te permite sumar o restar d\u00edas a una fecha dada. En este ejemplo, suma 7 d\u00edas a la fecha de ejecuci\u00f3n ( ds ). Puedes usarla para definir ventanas de tiempo relativas o para procesar datos con un desfase. Por ejemplo, macros.ds_format(ds, \"%Y-%m-%d\", \"%Y/%m/%d\") para cambiar el formato. {{ dag_run }} : Acceso al objeto DagRun completo, lo que te permite obtener informaci\u00f3n m\u00e1s detallada sobre la ejecuci\u00f3n actual del DAG. {{ ti }} : Acceso al objeto TaskInstance actual, \u00fatil para obtener informaci\u00f3n espec\u00edfica de la instancia de la tarea. Templating avanzado en operadores Python El templating no se limita solo a operadores de Bash. Muchos operadores de Airflow, incluido el PythonOperator , permiten que los valores sean puestos en un template . Para los PythonOperator que necesitan acceder al contexto de Airflow, puedes usar el argumento provide_context=True . Cuando esta opci\u00f3n est\u00e1 activada, Airflow inyecta un diccionario con el contexto de ejecuci\u00f3n (incluyendo las variables y macros Jinja2 disponibles) como argumentos kwargs al python_callable definido. from airflow.operators.python import PythonOperator def mostrar_contexto(**kwargs): # 'ds' est\u00e1 disponible en kwargs cuando provide_context=True print(f\"La tarea se ejecuta para la fecha: {kwargs['ds']}\") print(f\"Fecha de ejecuci\u00f3n completa: {kwargs['execution_date']}\") # Tambi\u00e9n puedes acceder a otras variables del contexto print(f\"ID de la tarea: {kwargs['task_instance'].task_id}\") tarea_contexto = PythonOperator( task_id='ver_contexto', python_callable=mostrar_contexto, provide_context=True, # Esto inyecta el contexto como kwargs dag=dag ) En este PythonOperator , la funci\u00f3n mostrar_contexto recibe **kwargs , que contendr\u00e1 el diccionario de contexto de Airflow. Dentro de esta funci\u00f3n, puedes acceder a kwargs['ds'] , kwargs['execution_date'] , kwargs['task_instance'] , y cualquier otra variable o macro que normalmente estar\u00eda disponible en las plantillas Jinja2. Esto te brinda un control program\u00e1tico completo sobre c\u00f3mo tus funciones Python interact\u00faan con el entorno de ejecuci\u00f3n de Airflow. Tarea Realiza los siguientes ejercicios para afianzar tu dominio sobre DAGs, operadores y tareas en Airflow: Define un DAG diario que ejecute dos tareas: una que imprima la fecha del sistema y otra que salude al usuario. Usa un BranchPythonOperator para decidir entre dos caminos: ejecutar un script Python o un comando Bash, seg\u00fan el d\u00eda de la semana. Agrupa tres tareas de transformaci\u00f3n (descargar, limpiar y cargar datos) usando TaskGroup . Crea un operador personalizado que verifique si existe un archivo en un bucket de S3 y registre el resultado. Implementa templating en un BashOperator para imprimir din\u00e1micamente la fecha de ejecuci\u00f3n y el nombre del DAG.","title":"DAGs, operadores y tareas"},{"location":"tema42/#4-automatizacion-y-orquestacion-con-apache-airflow","text":"","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"tema42/#42-dags-operadores-y-tareas","text":"Objetivo : Comprender, dise\u00f1ar y construir flujos de trabajo programables en Apache Airflow utilizando DAGs, operadores y tareas, incluyendo su agrupaci\u00f3n, parametrizaci\u00f3n y personalizaci\u00f3n para gestionar eficientemente flujos de datos complejos en entornos Big Data. Introducci\u00f3n : Apache Airflow permite definir y gestionar flujos de trabajo como c\u00f3digo, a trav\u00e9s de DAGs (Directed Acyclic Graphs), que representan secuencias de tareas con dependencias expl\u00edcitas. Este enfoque permite un control detallado del orden de ejecuci\u00f3n, la l\u00f3gica condicional, la integraci\u00f3n con diversas herramientas y servicios, y la reutilizaci\u00f3n de c\u00f3digo para tareas repetitivas. En este tema se profundiza en c\u00f3mo construir DAGs robustos, emplear operadores integrados y personalizados, utilizar estructuras avanzadas como TaskGroups y SubDAGs, y aplicar templating din\u00e1mico con Jinja2. Desarrollo : La definici\u00f3n de flujos de trabajo (pipelines) en Apache Airflow se basa en DAGs que agrupan tareas individuales y definen la l\u00f3gica de ejecuci\u00f3n. Cada tarea puede realizar operaciones diversas como ejecutar comandos del sistema, procesar datos con Spark, mover datos entre servicios cloud, o enviar notificaciones. Airflow proporciona una gran variedad de operadores listos para usar y permite crear operadores personalizados. Tambi\u00e9n ofrece herramientas para organizar tareas y aplicar plantillas din\u00e1micas, lo cual facilita el mantenimiento y escalabilidad de flujos complejos.","title":"4.2. DAGs, operadores y tareas"},{"location":"tema42/#421-definicion-y-estructura-de-dags","text":"Un DAG (Directed Acyclic Graph) es la pieza fundamental de Apache Airflow que representa un flujo de trabajo completo como un grafo dirigido sin ciclos. Cada DAG puede ser: Dirigido : Las tareas tienen un orden espec\u00edfico de ejecuci\u00f3n (flujo direccional) Ac\u00edclico : No puede haber bucles infinitos o dependencias circulares Grafo : Conjunto de nodos (tareas) conectados por aristas (dependencias)","title":"4.2.1 Definici\u00f3n y estructura de DAGs"},{"location":"tema42/#parametros-esenciales","text":"Son los par\u00e1metros fundamentales que definen el comportamiento b\u00e1sico de un DAG en Apache Airflow. # Par\u00e1metros obligatorios dag_id='nombre_unico_del_dag' # identificador \u00fanico en todo Airflow start_date=datetime(2024, 1, 1) # fecha desde cuando puede ejecutarse schedule_interval='@daily' # frecuencia de ejecuci\u00f3n # Par\u00e1metros importantes catchup=False # si ejecutar ejecuciones pasadas max_active_runs=1 # n\u00famero m\u00e1ximo de ejecuciones simult\u00e1neas default_retries=1 # reintentos por defecto para todas las tareas retry_delay=timedelta(minutes=5) # tiempo entre reintentos default_args = { # par\u00e1metros que heredan todas las tareas del DAG 'owner': 'team_data', 'retries': 2, 'retry_delay': timedelta(minutes=5), 'email_on_failure': True, 'email': ['alerts@empresa.com'] } Planificaci\u00f3n de la Ejecuci\u00f3n (Scheduling) Hay varias formas de configurar la frecuencias de ejecuci\u00f3n: # Opciones de schedule_interval '@once' # Ejecutar solo una vez '@hourly' # Cada hora (0 * * * *) '@daily' # Diariamente a medianoche (0 0 * * *) '@weekly' # Semanalmente los domingos (0 0 * * 0) '@monthly' # Primer d\u00eda de cada mes (0 0 1 * *) '@yearly' # 1 de enero cada a\u00f1o (0 0 1 1 *) # Cron personalizado '30 6 * * 1-5' # 6:30 AM, lunes a viernes '0 */4 * * *' # Cada 4 horas # Program\u00e1tico timedelta(hours=2) # Cada 2 horas timedelta(days=1, hours=12) # Cada d\u00eda y medio","title":"Par\u00e1metros Esenciales"},{"location":"tema42/#estructura-basica-de-un-dag","text":"from airflow import DAG from airflow.operators.bash import BashOperator from datetime import datetime with DAG( dag_id='mi_primer_dag', start_date=datetime(2024, 1, 1), schedule_interval='@daily', catchup=False, tags=['ejemplo'] ) as dag: tarea_1 = BashOperator( task_id='imprimir_fecha', bash_command='date' ) tarea_2 = BashOperator( task_id='mostrar_hola_mundo', bash_command='echo \"Hola mundo desde Airflow!\"' ) tarea_1 >> tarea_2 # Define la dependencia","title":"Estructura b\u00e1sica de un DAG"},{"location":"tema42/#dag-completo-y-detallado","text":"Diagrama : flowchart TD %% Estilos para diferentes tipos de tareas classDef verificacion fill:#e1f5fe,stroke:#01579b,stroke-width:2px classDef extraccion fill:#f3e5f5,stroke:#4a148c,stroke-width:2px classDef procesamiento fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px classDef validacion fill:#fff3e0,stroke:#e65100,stroke-width:2px classDef reporte fill:#fce4ec,stroke:#880e4f,stroke-width:2px classDef carga fill:#e0f2f1,stroke:#004d40,stroke-width:2px classDef notificacion fill:#f1f8e9,stroke:#33691e,stroke-width:2px classDef limpieza fill:#fafafa,stroke:#424242,stroke-width:2px %% Definici\u00f3n de nodos A[verificar_sistema<br/>\ud83d\udd0d Verificar recursos del sistema]:::verificacion B[extraer_datos<br/>\ud83d\udce5 Extraer datos de PostgreSQL]:::extraccion C[procesar_datos<br/>\u2699\ufe0f Transformar y limpiar datos]:::procesamiento D[validar_calidad<br/>\u2705 Validar calidad de datos]:::validacion E[generar_reporte_task<br/>\ud83d\udcca Generar reporte de m\u00e9tricas]:::reporte F[cargar_datos_warehouse<br/>\ud83c\udfd7\ufe0f Cargar al data warehouse]:::carga G[enviar_notificacion<br/>\ud83d\udce7 Enviar email de confirmaci\u00f3n]:::notificacion H[limpiar_archivos_temporales<br/>\ud83e\uddf9 Limpiar archivos temp]:::limpieza %% Flujo de dependencias A --> B B --> C %% Ramificaci\u00f3n paralela despu\u00e9s del procesamiento C --> D C --> E %% Convergencia para la carga (requiere procesamiento Y validaci\u00f3n) C --> F D --> F %% Reporte va a notificaci\u00f3n E --> G %% Limpieza al final (despu\u00e9s de carga Y notificaci\u00f3n) F --> H G --> H %% Anotaciones explicativas subgraph \"Fase 1: Preparaci\u00f3n\" A B end subgraph \"Fase 2: Procesamiento\" C end subgraph \"Fase 3: Validaci\u00f3n y Reportes (Paralelo)\" D E end subgraph \"Fase 4: Finalizaci\u00f3n\" F G H end Implementaci\u00f3n : from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator from airflow.operators.email import EmailOperator from datetime import datetime, timedelta import pandas as pd import logging # Estos argumentos se aplicar\u00e1n a todas las tareas del DAG # a menos que se sobrescriban espec\u00edficamente default_args = { 'owner': 'equipo_data_engineering', # Responsable del DAG 'depends_on_past': False, # No depende de ejecuciones anteriores 'start_date': datetime(2024, 1, 1), # Fecha de inicio del DAG 'email_on_failure': True, # Enviar email si falla 'email_on_retry': False, # No enviar email en reintento 'email': ['team@empresa.com'], # Lista de emails para notificaciones 'retries': 2, # N\u00famero de reintentos por tarea 'retry_delay': timedelta(minutes=5), # Tiempo entre reintentos 'execution_timeout': timedelta(hours=2), # Timeout m\u00e1ximo por tarea } # DEFINICI\u00d3N DEL DAG with DAG( # Identificador \u00fanico del DAG en toda la instancia de Airflow dag_id='pipeline_procesamiento_ventas_v2', # Hereda la configuraci\u00f3n por defecto definida arriba default_args=default_args, # Descripci\u00f3n que aparece en la UI de Airflow description='Pipeline completo para procesar datos de ventas diarias', # Frecuencia de ejecuci\u00f3n - se ejecuta diariamente a las 2:00 AM schedule_interval='0 2 * * *', # No ejecutar DAGs para fechas pasadas (evita backfill autom\u00e1tico) catchup=False, # Solo una instancia del DAG puede ejecutarse simult\u00e1neamente max_active_runs=1, # Tags para organizar DAGs en la UI tags=['ventas', 'etl', 'produccion', 'diario'], # Documentaci\u00f3n en formato Markdown doc_md=\"\"\" ## Pipeline de Procesamiento de Ventas Este DAG procesa los datos de ventas diarias: 1. Extrae datos de la base de datos transaccional 2. Aplica transformaciones y limpieza 3. Carga los datos al data warehouse 4. Env\u00eda reporte de resumen por email **Dependencias externas**: Base de datos PostgreSQL, S3 bucket **Tiempo estimado**: 45 minutos **Criticidad**: Alta - bloquea reportes ejecutivos \"\"\", ) as dag: # FUNCI\u00d3N PYTHON PERSONALIZADA def procesar_datos_ventas(**context): \"\"\" Funci\u00f3n que procesa los datos de ventas. **context contiene informaci\u00f3n del contexto de ejecuci\u00f3n de Airflow \"\"\" # Obtener la fecha de ejecuci\u00f3n del contexto execution_date = context['execution_date'] # Simular procesamiento de datos logging.info(f\"Procesando datos de ventas para {execution_date}\") # En un caso real, aqu\u00ed har\u00edas: # - Conexi\u00f3n a base de datos # - Extracci\u00f3n de datos # - Transformaciones con pandas/spark # - Validaciones de calidad # Retornar m\u00e9tricas para usar en tareas posteriores return { 'registros_procesados': 15420, 'ventas_totales': 89750.50, 'fecha_proceso': execution_date.strftime('%Y-%m-%d') } def generar_reporte(**context): \"\"\"Genera un reporte basado en los datos procesados\"\"\" # Obtener datos de la tarea anterior usando XCom datos_ventas = context['task_instance'].xcom_pull(task_ids='procesar_datos') reporte = f\"\"\" Reporte de Ventas - {datos_ventas['fecha_proceso']} ================================================ Registros procesados: {datos_ventas['registros_procesados']:,} Ventas totales: ${datos_ventas['ventas_totales']:,.2f} \"\"\" logging.info(reporte) return reporte # DEFINICI\u00d3N DE TAREAS # TAREA 1: Verificaci\u00f3n del sistema verificar_sistema = BashOperator( task_id='verificar_sistema', # Comando bash que verifica conectividad y recursos bash_command=\"\"\" echo \"Verificando sistema...\" df -h | grep -v tmpfs # Verificar espacio en disco echo \"Sistema verificado correctamente\" \"\"\", # Documentaci\u00f3n espec\u00edfica de la tarea doc_md=\"\"\" ### Verificaci\u00f3n del Sistema Valida que el sistema tenga los recursos necesarios: - Espacio en disco suficiente - Conectividad de red - Servicios requeridos activos \"\"\" ) # TAREA 2: Extracci\u00f3n de datos extraer_datos = BashOperator( task_id='extraer_datos', bash_command=\"\"\" echo \"Iniciando extracci\u00f3n de datos...\" # En producci\u00f3n, esto ser\u00eda algo como: # psql -h $DB_HOST -d ventas -c \"COPY (...) TO STDOUT\" > /tmp/ventas_{{ ds }}.csv echo \"Simulando extracci\u00f3n de 15,420 registros\" echo \"Datos extra\u00eddos exitosamente\" \"\"\", # Esta tarea puede fallar ocasionalmente, aumentamos reintentos retries=3, retry_delay=timedelta(minutes=2) ) # TAREA 3: Procesamiento con Python procesar_datos = PythonOperator( task_id='procesar_datos', python_callable=procesar_datos_ventas, # Proporcionar el contexto de Airflow a la funci\u00f3n provide_context=True ) # TAREA 4: Validaci\u00f3n de calidad validar_calidad = BashOperator( task_id='validar_calidad', bash_command=\"\"\" echo \"Ejecutando validaciones de calidad de datos...\" # Simular validaciones echo \"\u2713 Sin valores nulos en campos cr\u00edticos\" echo \"\u2713 Rangos de fechas v\u00e1lidos\" echo \"\u2713 Integridad referencial correcta\" echo \"Validaci\u00f3n completada exitosamente\" \"\"\" ) # TAREA 5: Carga al data warehouse cargar_datos = BashOperator( task_id='cargar_datos_warehouse', bash_command=\"\"\" echo \"Cargando datos al data warehouse...\" # En producci\u00f3n ser\u00eda algo como: # aws s3 cp /tmp/ventas_processed_{{ ds }}.parquet s3://warehouse/ventas/ echo \"Datos cargados exitosamente al data warehouse\" \"\"\" ) # TAREA 6: Generaci\u00f3n de reporte generar_reporte_task = PythonOperator( task_id='generar_reporte', python_callable=generar_reporte, provide_context=True ) # TAREA 7: Env\u00edo de notificaci\u00f3n por email enviar_notificacion = EmailOperator( task_id='enviar_notificacion', to=['gerencia@empresa.com', 'ventas@empresa.com'], subject='Pipeline de Ventas Completado - {{ ds }}', html_content=\"\"\" <h3>Pipeline de Procesamiento de Ventas</h3> <p><strong>Fecha:</strong> {{ ds }}</p> <p><strong>Estado:</strong> Completado exitosamente \u2705</p> <p><strong>Tiempo de ejecuci\u00f3n:</strong> {{ macros.datetime.now() }}</p> <p>Los datos de ventas han sido procesados y est\u00e1n disponibles en el data warehouse.</p> <hr> <small>Este es un mensaje autom\u00e1tico del sistema de orquestaci\u00f3n de datos.</small> \"\"\" ) # TAREA 8: Limpieza de archivos temporales limpiar_archivos = BashOperator( task_id='limpiar_archivos_temporales', bash_command=\"\"\" echo \"Limpiando archivos temporales...\" # rm -f /tmp/ventas_{{ ds }}.* echo \"Limpieza completada\" \"\"\", # Esta tarea no es cr\u00edtica, si falla no debe afectar el pipeline trigger_rule='all_done' # Se ejecuta sin importar si las anteriores fallan ) # DEFINICI\u00d3N DE DEPENDENCIAS # Secuencia lineal b\u00e1sica verificar_sistema >> extraer_datos >> procesar_datos # Despu\u00e9s del procesamiento, dos ramas paralelas procesar_datos >> [validar_calidad, generar_reporte_task] # La carga depende de que tanto el procesamiento como la validaci\u00f3n sean exitosos [procesar_datos, validar_calidad] >> cargar_datos # El reporte se puede generar en paralelo con la carga generar_reporte_task >> enviar_notificacion # La limpieza se ejecuta al final, despu\u00e9s de todo [cargar_datos, enviar_notificacion] >> limpiar_archivos","title":"DAG Completo y Detallado"},{"location":"tema42/#422-operadores-built-in-y-custom-operators","text":"Los operadores son las unidades de ejecuci\u00f3n en Airflow. Representan tareas concretas que se ejecutan en el DAG. Airflow incluye m\u00faltiples operadores built-in y permite crear operadores personalizados para extender su funcionalidad.","title":"4.2.2 Operadores built-in y custom operators"},{"location":"tema42/#operadores-de-ejecucion-basica","text":"Estos operadores permiten ejecutar comandos del sistema, scripts, o funciones personalizadas. BashOperator : Fundamental para ejecutar comandos del sistema, scripts shell y herramientas CLI de big data como Hadoop, Spark-submit, etc. PythonOperator : Cr\u00edtico para ejecutar funciones Python personalizadas, transformaciones de datos y l\u00f3gica de negocio EmailOperator : Esencial para notificaciones y alertas en pipelines de producci\u00f3n from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator def saludar(): print(\"Hola desde PythonOperator\") tarea_bash = BashOperator( task_id='tarea_bash', bash_command='echo \"Ejecutando comando Bash\"', dag=dag ) tarea_python = PythonOperator( task_id='tarea_python', python_callable=saludar, dag=dag )","title":"Operadores de Ejecuci\u00f3n B\u00e1sica"},{"location":"tema42/#operadores-de-bases-de-datos","text":"Estos operadores permiten ejecutar consultas SQL directamente desde Airflow hacia motores relacionales o embebidos. PostgresOperator / MySqlOperator : Para ejecutar consultas SQL en bases de datos relacionales SqliteOperator : \u00datil para pruebas y desarrollo local from airflow.providers.postgres.operators.postgres import PostgresOperator tarea_sql = PostgresOperator( task_id='crear_tabla', postgres_conn_id='mi_conexion_postgres', sql='CREATE TABLE IF NOT EXISTS tabla_ejemplo (id SERIAL PRIMARY KEY);', dag=dag )","title":"Operadores de Bases de Datos"},{"location":"tema42/#operadores-de-transferencia","text":"Son usados para mover datos entre servicios, especialmente en la nube. S3ToRedshiftOperator : Transferir datos desde S3 a Redshift (muy com\u00fan en AWS) BigQueryOperator : Para operaciones en Google BigQuery RedshiftToS3Operator : Exportar datos desde Redshift from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator tarea_transferencia = S3ToRedshiftOperator( task_id='s3_a_redshift', s3_bucket='mi-bucket', s3_key='datos.csv', schema='public', table='mi_tabla', copy_options=['csv'], aws_conn_id='aws_default', redshift_conn_id='redshift_default', dag=dag )","title":"Operadores de Transferencia"},{"location":"tema42/#operadores-de-control-de-flujo","text":"Controlan la l\u00f3gica de ejecuci\u00f3n dentro de un DAG. BranchPythonOperator : Para l\u00f3gica condicional en el pipeline EmptyOperator (antes DummyOperator ): Para puntos de sincronizaci\u00f3n TriggerDagRunOperator : Para activar otros DAGs from airflow.operators.empty import EmptyOperator from airflow.operators.branch import BranchPythonOperator def decidir(): return 'tarea_a' if datetime.now().hour < 12 else 'tarea_b' rama = BranchPythonOperator( task_id='evaluar_ruta', python_callable=decidir, dag=dag ) tarea_a = EmptyOperator(task_id='tarea_a', dag=dag) tarea_b = EmptyOperator(task_id='tarea_b', dag=dag) rama >> [tarea_a, tarea_b]","title":"Operadores de Control de Flujo"},{"location":"tema42/#operadores-para-spark-y-hadoop","text":"Permiten ejecutar aplicaciones en motores de procesamiento distribuido. SparkSubmitOperator : Ejecutar aplicaciones Spark (batch processing) DataprocSubmitJobOperator : Para trabajos en Google Cloud Dataproc EMRStepOperator : Para clusters Amazon EMR from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator spark_job = SparkSubmitOperator( task_id='ejecutar_spark', application='/ruta/a/mi_app_spark.py', conn_id='spark_default', dag=dag )","title":"Operadores para Spark y Hadoop"},{"location":"tema42/#operadores-para-streaming-y-hdfs","text":"Permiten flujos en tiempo real y acceso a HDFS KafkaOperator : Interactuar con Apache Kafka para datos en tiempo real HDFSOperator : Operaciones en Hadoop Distributed File System from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor esperar_archivo = HdfsSensor( task_id='esperar_archivo_hdfs', filepath='/data/input/archivo.csv', hdfs_conn_id='hdfs_default', dag=dag )","title":"Operadores para Streaming y HDFS"},{"location":"tema42/#operadores-para-validacion-de-datos","text":"Permiten realizar validaciones sobre los datos GreatExpectationsOperator : Validaci\u00f3n de calidad de datos Custom Data Validation Operators : Para reglas espec\u00edficas del negocio from airflow.providers.great_expectations.operators.great_expectations import GreatExpectationsOperator validacion = GreatExpectationsOperator( task_id='validar_datos', data_context_root_dir='/opt/ge/', checkpoint_name='check_ventas', dag=dag )","title":"Operadores para Validaci\u00f3n de Datos"},{"location":"tema42/#operadores-para-mlops","text":"Flujos relacionados con procesos de Machine Learning MLflowOperator : Para experimentos y modelos de machine learning KubernetesOperator : Para ejecutar contenedores en Kubernetes from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator ml_entrenamiento = KubernetesPodOperator( task_id='entrenar_modelo', name='ml-train', image='ml/entrenamiento:latest', dag=dag )","title":"Operadores para MLOps"},{"location":"tema42/#423-taskgroups","text":"Cuando los DAGs crecen en complejidad (m\u00e1s de 10-15 tareas), surgen varios problemas: Legibilidad: Dif\u00edcil entender el flujo en la UI de Airflow Mantenimiento: C\u00f3digo desordenado y dif\u00edcil de modificar Reutilizaci\u00f3n: Patrones repetitivos sin forma de encapsular Colaboraci\u00f3n: Equipos diferentes trabajando en el mismo DAG Debugging: Dif\u00edcil identificar grupos de tareas relacionadas La soluci\u00f3n es agrupar tareas l\u00f3gicamente relacionadas usando TaskGroups. Nota: Existe un mecanismo adicional llamado SubDAGs, pero ya es considerado obsoleto.","title":"4.2.3 TaskGroups"},{"location":"tema42/#uso-de-taskgroups","text":"Los TaskGroups son una forma de organizar tareas visualmente en la UI de Airflow sin crear DAGs separados. Introducidos en Airflow 2.0, son la mejor pr\u00e1ctica actual para estructurar DAGs complejos. Ventajas de TaskGroups Organizaci\u00f3n visual: Agrupa tareas en la UI de Airflow Reutilizaci\u00f3n de c\u00f3digo: Encapsula l\u00f3gica repetitiva Mejor rendimiento: No crean procesos separados como SubDAGs Simplicidad: F\u00e1cil de usar y entender Debugging mejorado: Logs y estados agrupados Escalabilidad: Maneja DAGs con cientos de tareas Los TaskGroup permiten agrupar visual y l\u00f3gicamente tareas relacionadas dentro de un DAG. Ejemplo de un TaskGroup : from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator from airflow.utils.task_group import TaskGroup from datetime import datetime, timedelta # Configuraci\u00f3n del DAG with DAG( dag_id='pipeline_con_taskgroups_v1', start_date=datetime(2024, 1, 1), schedule_interval='@daily', catchup=False, tags=['taskgroups', 'ejemplo'] ) as dag: # TASKGROUP 1: PREPARACI\u00d3N DE DATOS with TaskGroup( group_id='preparacion_datos', tooltip='Descarga, validaci\u00f3n y limpieza de datos raw' ) as grupo_preparacion: # Tarea 1: Descargar datos de m\u00faltiples fuentes descargar_datos = BashOperator( task_id='descargar_datos', bash_command=''' echo \"Descargando datos de APIs y bases de datos...\" # Simular descarga de m\u00faltiples fuentes echo \"\u2713 API de ventas: 15,420 registros\" echo \"\u2713 API de clientes: 8,932 registros\" echo \"\u2713 DB transaccional: 45,123 registros\" ''' ) # Tarea 2: Validar integridad de los datos validar_integridad = BashOperator( task_id='validar_integridad', bash_command=''' echo \"Validando integridad de datos descargados...\" echo \"\u2713 Verificando checksums\" echo \"\u2713 Validando esquemas JSON/CSV\" echo \"\u2713 Contando registros esperados\" ''' ) # Tarea 3: Limpiar y estandarizar datos limpiar_datos = PythonOperator( task_id='limpiar_datos', python_callable=lambda: print(\"\"\" Limpiando datos: \u2713 Removiendo duplicados \u2713 Estandarizando formatos de fecha \u2713 Normalizando nombres y direcciones \u2713 Aplicando reglas de negocio \"\"\") ) # Dependencias dentro del TaskGroup descargar_datos >> validar_integridad >> limpiar_datos # TASKGROUP 2: TRANSFORMACIONES DE NEGOCIO with TaskGroup( group_id='transformaciones_negocio', tooltip='Aplicar reglas de negocio y c\u00e1lculos complejos' ) as grupo_transformaciones: def calcular_metricas_ventas(): print(\"Calculando m\u00e9tricas de ventas:\") print(\"\u2713 Revenue por regi\u00f3n\") print(\"\u2713 Customer Lifetime Value\") print(\"\u2713 Churn rate\") print(\"\u2713 Forecasting pr\u00f3ximo trimestre\") return \"M\u00e9tricas calculadas exitosamente\" def aplicar_segmentacion(): print(\"Aplicando segmentaci\u00f3n de clientes:\") print(\"\u2713 RFM Analysis (Recency, Frequency, Monetary)\") print(\"\u2713 Clustering de comportamiento\") print(\"\u2713 Propensi\u00f3n de compra\") return \"Segmentaci\u00f3n completada\" calcular_metricas = PythonOperator( task_id='calcular_metricas_ventas', python_callable=calcular_metricas_ventas ) segmentar_clientes = PythonOperator( task_id='segmentar_clientes', python_callable=aplicar_segmentacion ) # Estas tareas pueden ejecutarse en paralelo # No hay dependencia entre calcular m\u00e9tricas y segmentar clientes pass # Sin dependencias internas = ejecuci\u00f3n paralela # TASKGROUP 3: CARGA DE DATOS with TaskGroup( group_id='carga_datos', tooltip='Carga a diferentes sistemas de destino' ) as grupo_carga: cargar_datawarehouse = BashOperator( task_id='cargar_datawarehouse', bash_command=''' echo \"Cargando datos al data warehouse...\" echo \"\u2713 Upserting tabla dimensiones\" echo \"\u2713 Insertando hechos de ventas\" echo \"\u2713 Actualizando \u00edndices\" ''' ) cargar_data_lake = BashOperator( task_id='cargar_data_lake', bash_command=''' echo \"Cargando datos al data lake...\" echo \"\u2713 Particionando por fecha\" echo \"\u2713 Comprimiendo en formato Parquet\" echo \"\u2713 Actualizando cat\u00e1logo de metadatos\" ''' ) actualizar_cache = BashOperator( task_id='actualizar_cache', bash_command=''' echo \"Actualizando sistemas de cache...\" echo \"\u2713 Redis para m\u00e9tricas en tiempo real\" echo \"\u2713 Elasticsearch para b\u00fasquedas\" echo \"\u2713 CDN para reportes est\u00e1ticos\" ''' ) # Todas las cargas pueden ejecutarse en paralelo pass # Sin dependencias = m\u00e1ximo paralelismo # TAREA INDIVIDUAL: NOTIFICACIONES enviar_notificaciones = BashOperator( task_id='enviar_notificaciones', bash_command=''' echo \"Enviando notificaciones finales...\" echo \"\u2713 Email a stakeholders\" echo \"\u2713 Slack al equipo de datos\" echo \"\u2713 Dashboard actualizado\" ''' ) # DEPENDENCIAS ENTRE TASKGROUPS # Flujo secuencial entre grupos principales grupo_preparacion >> grupo_transformaciones >> grupo_carga # La notificaci\u00f3n final depende de que toda la carga est\u00e9 completa grupo_carga >> enviar_notificaciones","title":"Uso de TaskGroups"},{"location":"tema42/#424-templating-con-jinja2","text":"Apache Airflow aprovecha la potencia de Jinja2 , un motor de plantillas moderno y de alto rendimiento para Python, para permitir la generaci\u00f3n din\u00e1mica de comandos, configuraciones y otros valores dentro de tus DAGs (Directed Acyclic Graphs) y operadores. Esta capacidad de templating es fundamental para crear flujos de trabajo flexibles y reutilizables, ya que te permite adaptar el comportamiento de tus tareas en funci\u00f3n de variables de tiempo de ejecuci\u00f3n, configuraciones del entorno o datos espec\u00edficos.","title":"4.2.4 Templating con Jinja2"},{"location":"tema42/#uso-de-variables-y-contextos","text":"El templating con Jinja2 te permite incrustar variables y acceder al contexto de ejecuci\u00f3n de Airflow directamente dentro de las propiedades de tus operadores que soportan plantillas (identificadas por tener un sufijo _template o ser parte de una lista definida por template_fields en el operador). Considera el siguiente ejemplo con un BashOperator : tarea_con_template = BashOperator( task_id='mostrar_fecha', bash_command='echo \"La fecha de ejecuci\u00f3n es {{ ds }}\"', dag=dag ) En este caso, la cadena bash_command es una plantilla Jinja2. Durante la ejecuci\u00f3n de la tarea {{ ds }} ser\u00e1 reemplazado autom\u00e1ticamente por la fecha de ejecuci\u00f3n actual del DAG en formato YYYY-MM-DD . Esto es \u00fatil para tareas que dependen de la fecha, como el procesamiento de datos diarios o la generaci\u00f3n de informes con nombres de archivo basados en la fecha.","title":"Uso de variables y contextos"},{"location":"tema42/#acceso-a-macros-y-funciones-de-utilidad","text":"Airflow expone un conjunto de macros y funciones de utilidad a las plantillas Jinja2, lo que te permite realizar operaciones comunes sin necesidad de escribir l\u00f3gica Python adicional. Estas macros proporcionan acceso conveniente a informaci\u00f3n clave del contexto de ejecuci\u00f3n y a funciones de manipulaci\u00f3n de fechas, entre otras cosas. Algunas de las macros m\u00e1s utilizadas incluyen: {{ ds }} : Representa la fecha de ejecuci\u00f3n del DAG en formato de cadena YYYY-MM-DD . Es ideal para operaciones de carga de datos diarias. {{ ds_nodash }} : Similar a ds , pero la fecha se presenta sin guiones, por ejemplo, YYYYMMDD . \u00datil para nombres de archivos o rutas sin caracteres especiales. {{ execution_date }} : Es un objeto datetime completo que representa el momento exacto en que la instancia del DAG fue programada para ejecutarse. Este objeto te permite realizar manipulaciones de tiempo m\u00e1s complejas. {{ prev_ds }} : La fecha de ejecuci\u00f3n anterior, si existe, en formato YYYY-MM-DD . {{ next_ds }} : La siguiente fecha de ejecuci\u00f3n programada, si existe, en formato YYYY-MM-DD . {{ macros.ds_add(ds, 7) }} : Esta es una funci\u00f3n de macro que te permite sumar o restar d\u00edas a una fecha dada. En este ejemplo, suma 7 d\u00edas a la fecha de ejecuci\u00f3n ( ds ). Puedes usarla para definir ventanas de tiempo relativas o para procesar datos con un desfase. Por ejemplo, macros.ds_format(ds, \"%Y-%m-%d\", \"%Y/%m/%d\") para cambiar el formato. {{ dag_run }} : Acceso al objeto DagRun completo, lo que te permite obtener informaci\u00f3n m\u00e1s detallada sobre la ejecuci\u00f3n actual del DAG. {{ ti }} : Acceso al objeto TaskInstance actual, \u00fatil para obtener informaci\u00f3n espec\u00edfica de la instancia de la tarea.","title":"Acceso a macros y funciones de utilidad"},{"location":"tema42/#templating-avanzado-en-operadores-python","text":"El templating no se limita solo a operadores de Bash. Muchos operadores de Airflow, incluido el PythonOperator , permiten que los valores sean puestos en un template . Para los PythonOperator que necesitan acceder al contexto de Airflow, puedes usar el argumento provide_context=True . Cuando esta opci\u00f3n est\u00e1 activada, Airflow inyecta un diccionario con el contexto de ejecuci\u00f3n (incluyendo las variables y macros Jinja2 disponibles) como argumentos kwargs al python_callable definido. from airflow.operators.python import PythonOperator def mostrar_contexto(**kwargs): # 'ds' est\u00e1 disponible en kwargs cuando provide_context=True print(f\"La tarea se ejecuta para la fecha: {kwargs['ds']}\") print(f\"Fecha de ejecuci\u00f3n completa: {kwargs['execution_date']}\") # Tambi\u00e9n puedes acceder a otras variables del contexto print(f\"ID de la tarea: {kwargs['task_instance'].task_id}\") tarea_contexto = PythonOperator( task_id='ver_contexto', python_callable=mostrar_contexto, provide_context=True, # Esto inyecta el contexto como kwargs dag=dag ) En este PythonOperator , la funci\u00f3n mostrar_contexto recibe **kwargs , que contendr\u00e1 el diccionario de contexto de Airflow. Dentro de esta funci\u00f3n, puedes acceder a kwargs['ds'] , kwargs['execution_date'] , kwargs['task_instance'] , y cualquier otra variable o macro que normalmente estar\u00eda disponible en las plantillas Jinja2. Esto te brinda un control program\u00e1tico completo sobre c\u00f3mo tus funciones Python interact\u00faan con el entorno de ejecuci\u00f3n de Airflow.","title":"Templating avanzado en operadores Python"},{"location":"tema42/#tarea","text":"Realiza los siguientes ejercicios para afianzar tu dominio sobre DAGs, operadores y tareas en Airflow: Define un DAG diario que ejecute dos tareas: una que imprima la fecha del sistema y otra que salude al usuario. Usa un BranchPythonOperator para decidir entre dos caminos: ejecutar un script Python o un comando Bash, seg\u00fan el d\u00eda de la semana. Agrupa tres tareas de transformaci\u00f3n (descargar, limpiar y cargar datos) usando TaskGroup . Crea un operador personalizado que verifique si existe un archivo en un bucket de S3 y registre el resultado. Implementa templating en un BashOperator para imprimir din\u00e1micamente la fecha de ejecuci\u00f3n y el nombre del DAG.","title":"Tarea"},{"location":"tema43/","text":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow 4.3. Integraci\u00f3n con ecosistema Big Data Objetivo: Comprender y aplicar la integraci\u00f3n de Apache Airflow con herramientas y servicios del ecosistema Big Data, incluyendo motores de procesamiento como Spark, sistemas distribuidos como Hadoop y Hive, almacenamiento como HDFS, conectores en la nube (AWS, GCP, Azure) y operadores para bases de datos y APIs, permitiendo la orquestaci\u00f3n automatizada de flujos de datos complejos en arquitecturas modernas. Introducci\u00f3n: En los entornos de Big Data, los flujos de trabajo rara vez est\u00e1n aislados. Normalmente interact\u00faan con m\u00faltiples plataformas, motores de procesamiento y servicios en la nube. Apache Airflow, como herramienta de orquestaci\u00f3n, proporciona operadores, hooks y mecanismos de conexi\u00f3n para facilitar esta integraci\u00f3n, permitiendo ejecutar tareas en Spark, consultar datos en Hive, mover archivos en HDFS y conectar con servicios en AWS, GCP o Azure. Desarrollo: La integraci\u00f3n de Airflow con el ecosistema Big Data permite automatizar procesos complejos distribuidos, reduciendo errores manuales y mejorando la eficiencia. Usando operadores espec\u00edficos, Airflow se convierte en un controlador central capaz de invocar jobs de Spark, consultar tablas Hive, interactuar con buckets de almacenamiento en la nube o consumir APIs externas. Esta capacidad de integraci\u00f3n lo convierte en una herramienta esencial para arquitecturas de datos modernas orientadas a pipelines escalables y mantenibles. 4.3.1 SparkSubmitOperator y configuraci\u00f3n con Spark Apache Spark es un motor unificado de an\u00e1lisis de datos de c\u00f3digo abierto, dise\u00f1ado para el procesamiento de datos a gran escala. Es ampliamente utilizado en diversas aplicaciones, incluyendo la extracci\u00f3n, transformaci\u00f3n y carga (ETL) de datos, el procesamiento de streaming en tiempo real, el aprendizaje autom\u00e1tico ( Machine Learning ) y los gr\u00e1ficos. Su capacidad para manejar grandes vol\u00famenes de datos de manera distribuida y eficiente lo convierte en una herramienta fundamental en los ecosistemas de big data . Apache Airflow , como orquestador de flujos de trabajo, ofrece una integraci\u00f3n nativa y robusta con Spark, permitiendo que tus DAGs (Directed Acyclic Graphs) lancen y gestionen trabajos Spark directamente. Esta sinergia es crucial para construir pipelines de datos complejos donde el procesamiento distribuido es un requisito. Airflow proporciona el SparkSubmitOperator , una herramienta especializada para esta tarea. Configuraci\u00f3n del SparkSubmitOperator El SparkSubmitOperator en Airflow est\u00e1 dise\u00f1ado para emular la funcionalidad del comando de l\u00ednea de comandos spark-submit . Este comando es el principal script utilizado para enviar aplicaciones (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator spark_task = SparkSubmitOperator( task_id='run_spark_job', application='/path/to/app.py', conn_id='spark_default', executor_memory='2g', total_executor_cores=4, name='example_spark_job', dag=dag ) Orquestaci\u00f3n de un flujo de limpieza de datos con PySpark Imagina que tienes un flujo de trabajo de datos que requiere una fase crucial de limpieza y preparaci\u00f3n de datos a gran escala. Para manejar el volumen y la complejidad de estos datos, has desarrollado un script de Python llamado data_cleaning.py que utiliza las capacidades de procesamiento distribuido de Apache Spark . Este script est\u00e1 dise\u00f1ado para ejecutarse en un cl\u00faster Spark , ya sea local o remoto (como en un entorno de producci\u00f3n). Tu objetivo es integrar la ejecuci\u00f3n de este script Spark en tu DAG de Airflow , de modo que se ejecute como una tarea m\u00e1s dentro de tu pipeline general de datos. Aqu\u00ed es donde el SparkSubmitOperator de Airflow se vuelve invaluable. spark_cleaning = SparkSubmitOperator( task_id='clean_data', application='/opt/airflow/scripts/data_cleaning.py', conn_id='spark_local', application_args=['--input', 'hdfs://data/raw', '--output', 'hdfs://data/clean'], dag=dag ) Este enfoque permite que el procesamiento distribuido de Spark se convierta en una parte integral y orquestada de una cadena de tareas m\u00e1s amplia dentro de un DAG de Airflow. Los beneficios clave incluyen: Orquestaci\u00f3n Centralizada : Airflow gestiona cu\u00e1ndo y c\u00f3mo se ejecuta el trabajo Spark, asegurando que ocurra en el momento adecuado, despu\u00e9s de que las dependencias previas (por ejemplo, la ingesta de datos) se hayan completado y antes de que las tareas posteriores (por ejemplo, la carga a un data warehouse o la ejecuci\u00f3n de un modelo de ML) comiencen. Reintentos Autom\u00e1ticos y Manejo de Errores : Si el trabajo Spark falla, Airflow puede configurarse para reintentar la tarea autom\u00e1ticamente, y su robusto sistema de registro facilita la depuraci\u00f3n de cualquier problema. Visibilidad y Monitoreo : Puedes monitorear el estado de tu trabajo Spark directamente desde la interfaz de usuario de Airflow, viendo si est\u00e1 en ejecuci\u00f3n, ha tenido \u00e9xito o ha fallado, e incluso acceder a los registros del driver de Spark. Parametrizaci\u00f3n Din\u00e1mica : Utilizando el templating de Jinja2 o pasando argumentos din\u00e1micamente, puedes hacer que tus trabajos Spark sean a\u00fan m\u00e1s flexibles, por ejemplo, procesando datos para una fecha espec\u00edfica ( {{ ds }} ) o para un entorno particular. 4.3.2 Integraci\u00f3n con Hadoop, Hive, HDFS Apache Airflow puede interactuar con el ecosistema Hadoop gracias a sus hooks y operadores especializados, facilitando tareas como mover archivos en HDFS o ejecutar queries en Hive. Hadoop y HDFS Airflow puede conectarse con HDFS usando el HdfsHook y HdfsSensor para detectar archivos o mover datos entre sistemas. from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor hdfs_check = HdfsSensor( task_id='check_file_in_hdfs', filepath='/data/input/file.csv', hdfs_conn_id='hdfs_default', dag=dag ) Este sensor espera hasta que el archivo est\u00e9 disponible para iniciar tareas posteriores. Hive Para consultas SQL en Hive, Airflow ofrece el HiveOperator . Tambi\u00e9n se puede usar HivePartitionSensor para esperar hasta que se cree una partici\u00f3n. from airflow.providers.apache.hive.operators.hive import HiveOperator hive_query = HiveOperator( task_id='run_hive_query', hql='SELECT COUNT(*) FROM transactions WHERE amount > 1000;', hive_cli_conn_id='hive_conn', dag=dag ) 4.3.3 Conectores cloud Airflow incluye conectores y operadores especializados para interactuar con servicios cloud como AWS, GCP y Azure. AWS Airflow proporciona operadores para S3, Redshift, Athena, EMR, entre otros. from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator upload_to_s3 = S3CreateObjectOperator( task_id='upload_file', s3_bucket='my-bucket', s3_key='data/output.json', data='{\"status\": \"ok\"}', aws_conn_id='aws_default', dag=dag ) GCP Airflow permite orquestar servicios como BigQuery, GCS, Cloud Functions, Dataproc. from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator bq_query = BigQueryInsertJobOperator( task_id=\"run_bq_query\", configuration={ \"query\": { \"query\": \"SELECT * FROM my_dataset.table WHERE active = TRUE\", \"useLegacySql\": False, } }, gcp_conn_id=\"google_cloud_default\", dag=dag ) Azure Airflow tambi\u00e9n soporta Blob Storage, Data Lake y Synapse Analytics. from airflow.providers.microsoft.azure.operators.wasb_delete_blob import WasbDeleteBlobOperator delete_blob = WasbDeleteBlobOperator( task_id='delete_blob', container_name='datalake', blob_name='old/file.csv', wasb_conn_id='azure_blob_conn', dag=dag ) 4.3.4 Operadores de bases de datos y APIs Airflow facilita el acceso a bases de datos relacionales y APIs mediante operadores y hooks gen\u00e9ricos y espec\u00edficos. Operadores de bases de datos Operadores como PostgresOperator , MySqlOperator , SqliteOperator , entre otros, permiten ejecutar sentencias SQL como parte de un flujo. from airflow.providers.postgres.operators.postgres import PostgresOperator run_sql = PostgresOperator( task_id='insert_data', sql='INSERT INTO logs (message) VALUES (\\'Pipeline success\\');', postgres_conn_id='postgres_default', dag=dag ) APIs con HttpOperator Airflow tambi\u00e9n permite interactuar con APIs REST. from airflow.providers.http.operators.http import SimpleHttpOperator call_api = SimpleHttpOperator( task_id='call_rest_api', http_conn_id='api_service', endpoint='/status', method='GET', dag=dag ) Tarea Realiza los siguientes ejercicios pr\u00e1cticos para afianzar los conocimientos del tema 4.3: Construye un DAG que lance un trabajo PySpark usando SparkSubmitOperator con par\u00e1metros din\u00e1micos enviados desde Variable o Jinja2 . Dise\u00f1a un DAG que espere un archivo en HDFS con HdfsSensor y luego ejecute una consulta sobre una tabla Hive. Crea un DAG con dos tareas: la primera debe cargar un archivo JSON a un bucket de S3, y la segunda debe notificar por API cuando el archivo haya sido subido correctamente. Integra Airflow con BigQuery y dise\u00f1a una consulta parametrizada que se ejecute todos los d\u00edas y almacene los resultados en una tabla destino. Crea un DAG que consuma una API p\u00fablica , almacene la respuesta en una base de datos Postgres y registre el evento en una tabla de auditor\u00eda.","title":"Integraci\u00f3n con ecosistema Big Data"},{"location":"tema43/#4-automatizacion-y-orquestacion-con-apache-airflow","text":"","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"tema43/#43-integracion-con-ecosistema-big-data","text":"Objetivo: Comprender y aplicar la integraci\u00f3n de Apache Airflow con herramientas y servicios del ecosistema Big Data, incluyendo motores de procesamiento como Spark, sistemas distribuidos como Hadoop y Hive, almacenamiento como HDFS, conectores en la nube (AWS, GCP, Azure) y operadores para bases de datos y APIs, permitiendo la orquestaci\u00f3n automatizada de flujos de datos complejos en arquitecturas modernas. Introducci\u00f3n: En los entornos de Big Data, los flujos de trabajo rara vez est\u00e1n aislados. Normalmente interact\u00faan con m\u00faltiples plataformas, motores de procesamiento y servicios en la nube. Apache Airflow, como herramienta de orquestaci\u00f3n, proporciona operadores, hooks y mecanismos de conexi\u00f3n para facilitar esta integraci\u00f3n, permitiendo ejecutar tareas en Spark, consultar datos en Hive, mover archivos en HDFS y conectar con servicios en AWS, GCP o Azure. Desarrollo: La integraci\u00f3n de Airflow con el ecosistema Big Data permite automatizar procesos complejos distribuidos, reduciendo errores manuales y mejorando la eficiencia. Usando operadores espec\u00edficos, Airflow se convierte en un controlador central capaz de invocar jobs de Spark, consultar tablas Hive, interactuar con buckets de almacenamiento en la nube o consumir APIs externas. Esta capacidad de integraci\u00f3n lo convierte en una herramienta esencial para arquitecturas de datos modernas orientadas a pipelines escalables y mantenibles.","title":"4.3. Integraci\u00f3n con ecosistema Big Data"},{"location":"tema43/#431-sparksubmitoperator-y-configuracion-con-spark","text":"Apache Spark es un motor unificado de an\u00e1lisis de datos de c\u00f3digo abierto, dise\u00f1ado para el procesamiento de datos a gran escala. Es ampliamente utilizado en diversas aplicaciones, incluyendo la extracci\u00f3n, transformaci\u00f3n y carga (ETL) de datos, el procesamiento de streaming en tiempo real, el aprendizaje autom\u00e1tico ( Machine Learning ) y los gr\u00e1ficos. Su capacidad para manejar grandes vol\u00famenes de datos de manera distribuida y eficiente lo convierte en una herramienta fundamental en los ecosistemas de big data . Apache Airflow , como orquestador de flujos de trabajo, ofrece una integraci\u00f3n nativa y robusta con Spark, permitiendo que tus DAGs (Directed Acyclic Graphs) lancen y gestionen trabajos Spark directamente. Esta sinergia es crucial para construir pipelines de datos complejos donde el procesamiento distribuido es un requisito. Airflow proporciona el SparkSubmitOperator , una herramienta especializada para esta tarea.","title":"4.3.1 SparkSubmitOperator y configuraci\u00f3n con Spark"},{"location":"tema43/#configuracion-del-sparksubmitoperator","text":"El SparkSubmitOperator en Airflow est\u00e1 dise\u00f1ado para emular la funcionalidad del comando de l\u00ednea de comandos spark-submit . Este comando es el principal script utilizado para enviar aplicaciones (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator spark_task = SparkSubmitOperator( task_id='run_spark_job', application='/path/to/app.py', conn_id='spark_default', executor_memory='2g', total_executor_cores=4, name='example_spark_job', dag=dag )","title":"Configuraci\u00f3n del SparkSubmitOperator"},{"location":"tema43/#orquestacion-de-un-flujo-de-limpieza-de-datos-con-pyspark","text":"Imagina que tienes un flujo de trabajo de datos que requiere una fase crucial de limpieza y preparaci\u00f3n de datos a gran escala. Para manejar el volumen y la complejidad de estos datos, has desarrollado un script de Python llamado data_cleaning.py que utiliza las capacidades de procesamiento distribuido de Apache Spark . Este script est\u00e1 dise\u00f1ado para ejecutarse en un cl\u00faster Spark , ya sea local o remoto (como en un entorno de producci\u00f3n). Tu objetivo es integrar la ejecuci\u00f3n de este script Spark en tu DAG de Airflow , de modo que se ejecute como una tarea m\u00e1s dentro de tu pipeline general de datos. Aqu\u00ed es donde el SparkSubmitOperator de Airflow se vuelve invaluable. spark_cleaning = SparkSubmitOperator( task_id='clean_data', application='/opt/airflow/scripts/data_cleaning.py', conn_id='spark_local', application_args=['--input', 'hdfs://data/raw', '--output', 'hdfs://data/clean'], dag=dag ) Este enfoque permite que el procesamiento distribuido de Spark se convierta en una parte integral y orquestada de una cadena de tareas m\u00e1s amplia dentro de un DAG de Airflow. Los beneficios clave incluyen: Orquestaci\u00f3n Centralizada : Airflow gestiona cu\u00e1ndo y c\u00f3mo se ejecuta el trabajo Spark, asegurando que ocurra en el momento adecuado, despu\u00e9s de que las dependencias previas (por ejemplo, la ingesta de datos) se hayan completado y antes de que las tareas posteriores (por ejemplo, la carga a un data warehouse o la ejecuci\u00f3n de un modelo de ML) comiencen. Reintentos Autom\u00e1ticos y Manejo de Errores : Si el trabajo Spark falla, Airflow puede configurarse para reintentar la tarea autom\u00e1ticamente, y su robusto sistema de registro facilita la depuraci\u00f3n de cualquier problema. Visibilidad y Monitoreo : Puedes monitorear el estado de tu trabajo Spark directamente desde la interfaz de usuario de Airflow, viendo si est\u00e1 en ejecuci\u00f3n, ha tenido \u00e9xito o ha fallado, e incluso acceder a los registros del driver de Spark. Parametrizaci\u00f3n Din\u00e1mica : Utilizando el templating de Jinja2 o pasando argumentos din\u00e1micamente, puedes hacer que tus trabajos Spark sean a\u00fan m\u00e1s flexibles, por ejemplo, procesando datos para una fecha espec\u00edfica ( {{ ds }} ) o para un entorno particular.","title":"Orquestaci\u00f3n de un flujo de limpieza de datos con PySpark"},{"location":"tema43/#432-integracion-con-hadoop-hive-hdfs","text":"Apache Airflow puede interactuar con el ecosistema Hadoop gracias a sus hooks y operadores especializados, facilitando tareas como mover archivos en HDFS o ejecutar queries en Hive.","title":"4.3.2 Integraci\u00f3n con Hadoop, Hive, HDFS"},{"location":"tema43/#hadoop-y-hdfs","text":"Airflow puede conectarse con HDFS usando el HdfsHook y HdfsSensor para detectar archivos o mover datos entre sistemas. from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor hdfs_check = HdfsSensor( task_id='check_file_in_hdfs', filepath='/data/input/file.csv', hdfs_conn_id='hdfs_default', dag=dag ) Este sensor espera hasta que el archivo est\u00e9 disponible para iniciar tareas posteriores.","title":"Hadoop y HDFS"},{"location":"tema43/#hive","text":"Para consultas SQL en Hive, Airflow ofrece el HiveOperator . Tambi\u00e9n se puede usar HivePartitionSensor para esperar hasta que se cree una partici\u00f3n. from airflow.providers.apache.hive.operators.hive import HiveOperator hive_query = HiveOperator( task_id='run_hive_query', hql='SELECT COUNT(*) FROM transactions WHERE amount > 1000;', hive_cli_conn_id='hive_conn', dag=dag )","title":"Hive"},{"location":"tema43/#433-conectores-cloud","text":"Airflow incluye conectores y operadores especializados para interactuar con servicios cloud como AWS, GCP y Azure.","title":"4.3.3 Conectores cloud"},{"location":"tema43/#aws","text":"Airflow proporciona operadores para S3, Redshift, Athena, EMR, entre otros. from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator upload_to_s3 = S3CreateObjectOperator( task_id='upload_file', s3_bucket='my-bucket', s3_key='data/output.json', data='{\"status\": \"ok\"}', aws_conn_id='aws_default', dag=dag )","title":"AWS"},{"location":"tema43/#gcp","text":"Airflow permite orquestar servicios como BigQuery, GCS, Cloud Functions, Dataproc. from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator bq_query = BigQueryInsertJobOperator( task_id=\"run_bq_query\", configuration={ \"query\": { \"query\": \"SELECT * FROM my_dataset.table WHERE active = TRUE\", \"useLegacySql\": False, } }, gcp_conn_id=\"google_cloud_default\", dag=dag )","title":"GCP"},{"location":"tema43/#azure","text":"Airflow tambi\u00e9n soporta Blob Storage, Data Lake y Synapse Analytics. from airflow.providers.microsoft.azure.operators.wasb_delete_blob import WasbDeleteBlobOperator delete_blob = WasbDeleteBlobOperator( task_id='delete_blob', container_name='datalake', blob_name='old/file.csv', wasb_conn_id='azure_blob_conn', dag=dag )","title":"Azure"},{"location":"tema43/#434-operadores-de-bases-de-datos-y-apis","text":"Airflow facilita el acceso a bases de datos relacionales y APIs mediante operadores y hooks gen\u00e9ricos y espec\u00edficos.","title":"4.3.4 Operadores de bases de datos y APIs"},{"location":"tema43/#operadores-de-bases-de-datos","text":"Operadores como PostgresOperator , MySqlOperator , SqliteOperator , entre otros, permiten ejecutar sentencias SQL como parte de un flujo. from airflow.providers.postgres.operators.postgres import PostgresOperator run_sql = PostgresOperator( task_id='insert_data', sql='INSERT INTO logs (message) VALUES (\\'Pipeline success\\');', postgres_conn_id='postgres_default', dag=dag )","title":"Operadores de bases de datos"},{"location":"tema43/#apis-con-httpoperator","text":"Airflow tambi\u00e9n permite interactuar con APIs REST. from airflow.providers.http.operators.http import SimpleHttpOperator call_api = SimpleHttpOperator( task_id='call_rest_api', http_conn_id='api_service', endpoint='/status', method='GET', dag=dag )","title":"APIs con HttpOperator"},{"location":"tema43/#tarea","text":"Realiza los siguientes ejercicios pr\u00e1cticos para afianzar los conocimientos del tema 4.3: Construye un DAG que lance un trabajo PySpark usando SparkSubmitOperator con par\u00e1metros din\u00e1micos enviados desde Variable o Jinja2 . Dise\u00f1a un DAG que espere un archivo en HDFS con HdfsSensor y luego ejecute una consulta sobre una tabla Hive. Crea un DAG con dos tareas: la primera debe cargar un archivo JSON a un bucket de S3, y la segunda debe notificar por API cuando el archivo haya sido subido correctamente. Integra Airflow con BigQuery y dise\u00f1a una consulta parametrizada que se ejecute todos los d\u00edas y almacene los resultados en una tabla destino. Crea un DAG que consuma una API p\u00fablica , almacene la respuesta en una base de datos Postgres y registre el evento en una tabla de auditor\u00eda.","title":"Tarea"},{"location":"tema44/","text":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow 4.4. Monitoreo, logging y manejo de dependencias Objetivo : Comprender y aplicar t\u00e9cnicas de monitoreo, logging y gesti\u00f3n de dependencias en Apache Airflow para asegurar la trazabilidad, confiabilidad y eficiencia en la ejecuci\u00f3n de flujos de trabajo de datos distribuidos. Introducci\u00f3n : En entornos de Big Data, donde los pipelines pueden involucrar decenas o cientos de tareas interdependientes, es fundamental contar con mecanismos robustos para gestionar dependencias, detectar errores r\u00e1pidamente y recibir notificaciones oportunas. Apache Airflow proporciona una infraestructura poderosa para monitorear y depurar flujos de trabajo, manejar relaciones entre tareas, establecer alertas ante fallas, y generar registros detallados para su an\u00e1lisis. Desarrollo : Este tema se enfoca en tres aspectos clave de la operaci\u00f3n de DAGs en producci\u00f3n: c\u00f3mo definir y controlar las dependencias entre tareas y DAGs, c\u00f3mo configurar sistemas de notificaciones que alerten sobre estados cr\u00edticos, y c\u00f3mo aprovechar las capacidades de logging de Airflow para identificar y solucionar errores. Dominar estos elementos es esencial para mantener la confiabilidad y visibilidad de los pipelines en ecosistemas Big Data complejos, tanto on-premise como en la nube. 4.4.1 Gesti\u00f3n de dependencias entre tareas y DAGs La gesti\u00f3n de dependencias en Airflow es crucial para asegurar que las tareas se ejecuten en el orden correcto, respetando los flujos l\u00f3gicos del proceso. Las dependencias pueden definirse tanto dentro de un DAG como entre diferentes DAGs, y su correcta implementaci\u00f3n mejora la modularidad, reutilizaci\u00f3n y control de los flujos. Definici\u00f3n de dependencias entre tareas en un DAG Airflow utiliza un modelo basado en grafos dirigidos ac\u00edclicos (DAGs), donde las tareas est\u00e1n conectadas entre s\u00ed por medio de relaciones de dependencia. Estas se definen de forma declarativa usando los operadores >> (downstream) y << (upstream). from airflow import DAG from airflow.operators.dummy import DummyOperator from datetime import datetime with DAG(dag_id=\"ejemplo_dependencias\", start_date=datetime(2024, 1, 1), schedule_interval='@daily') as dag: inicio = DummyOperator(task_id=\"inicio\") proceso_1 = DummyOperator(task_id=\"proceso_1\") proceso_2 = DummyOperator(task_id=\"proceso_2\") fin = DummyOperator(task_id=\"fin\") inicio >> [proceso_1, proceso_2] >> fin Este patr\u00f3n garantiza que proceso_1 y proceso_2 solo inicien despu\u00e9s de inicio , y que fin espere la finalizaci\u00f3n de ambos. Dependencias entre DAGs (TriggerDagRunOperator y ExternalTaskSensor) Airflow permite establecer dependencias entre DAGs distintos. Esto es \u00fatil para dividir procesos complejos en m\u00f3dulos independientes. TriggerDagRunOperator : inicia otro DAG desde una tarea. ExternalTaskSensor : espera la finalizaci\u00f3n de una tarea en otro DAG. from airflow.operators.dagrun_operator import TriggerDagRunOperator trigger = TriggerDagRunOperator( task_id=\"trigger_otros_dag\", trigger_dag_id=\"subdag_etl\", dag=dag ) 4.4.2 Sistema de alertas y notificaciones Airflow permite configurar alertas autom\u00e1ticas que se disparan cuando las tareas fallan, se retrasan o se comportan de forma inesperada. Estas alertas pueden enviarse por correo electr\u00f3nico, Slack, Microsoft Teams u otros canales, facilitando la respuesta r\u00e1pida a incidentes. Configuraci\u00f3n b\u00e1sica de notificaciones por correo electr\u00f3nico Se pueden configurar notificaciones usando los par\u00e1metros email , email_on_failure , email_on_retry en la definici\u00f3n de tareas. from airflow.operators.dummy import DummyOperator DummyOperator( task_id=\"tarea_con_alerta\", email=[\"alertas@empresa.com\"], email_on_failure=True, retries=1, retry_delay=timedelta(minutes=5), dag=dag ) A nivel global, se configuran en airflow.cfg o variables de entorno ( SMTP_HOST , SMTP_USER , etc.). Integraci\u00f3n con herramientas de monitoreo y mensajer\u00eda Airflow puede integrarse con servicios como Slack o Microsoft Teams usando hooks o callbacks personalizados. Callback para Slack: def enviar_mensaje_slack(context): mensaje = f\"Tarea {context['task_instance_key_str']} fall\u00f3.\" # Aqu\u00ed se llamar\u00eda un webhook Slack from airflow.operators.python import PythonOperator tarea = PythonOperator( task_id=\"tarea_notificada\", python_callable=mi_funcion, on_failure_callback=enviar_mensaje_slack, dag=dag ) Tambi\u00e9n existen integraciones listas como [SlackWebhookOperator]. 4.4.3 Logging y debugging El sistema de logging de Airflow permite capturar, visualizar y analizar los eventos que ocurren durante la ejecuci\u00f3n de tareas. Estos logs son fundamentales para depurar errores, evaluar el comportamiento de los DAGs y hacer troubleshooting en producci\u00f3n. Visualizaci\u00f3n de logs por tarea en la UI Cada tarea ejecutada genera un log accesible desde la interfaz web. Estos logs pueden ser configurados para almacenarse localmente o en la nube (S3, GCS, Azure Blob). Configuraci\u00f3n en airflow.cfg : [logging] base_log_folder = /opt/airflow/logs remote_logging = True remote_log_conn_id = MyS3Connection remote_base_log_folder = s3://airflow-logs Esto asegura que los logs est\u00e9n centralizados y disponibles para auditor\u00eda. Niveles de logging y debugging en desarrollo Durante la fase de desarrollo, es com\u00fan usar niveles de logging como DEBUG , INFO , WARNING , ERROR . import logging def mi_funcion(**kwargs): logging.info(\"Inicio de la funci\u00f3n\") try: resultado = 1 / 0 except ZeroDivisionError: logging.error(\"Divisi\u00f3n por cero detectada\", exc_info=True) Tambi\u00e9n es posible ejecutar DAGs en modo manual ( airflow dags test ) para obtener retroalimentaci\u00f3n inmediata sin esperar al scheduler. Tarea Desarrolla los siguientes ejercicios pr\u00e1cticos: Dise\u00f1a un DAG con 5 tareas, donde se evidencie la gesti\u00f3n correcta de dependencias usando >> y << . Implementa un DAG maestro que dispare otro DAG secundario mediante TriggerDagRunOperator y aseg\u00farate de que este \u00faltimo se ejecute correctamente. Configura una tarea que env\u00ede una notificaci\u00f3n por correo electr\u00f3nico en caso de fallo. Prueba intencionalmente el fallo para verificar el env\u00edo. Integra una alerta personalizada usando una funci\u00f3n Python que simule el env\u00edo de un mensaje a Slack o Teams en caso de fallo. Revisa los logs de ejecuci\u00f3n de un DAG que falle, identifica el error, corr\u00edgelo y vuelve a ejecutar. Adjunta capturas o fragmentos del log que justifiquen tu an\u00e1lisis.","title":"Monitoreo, logging y manejo de dependencias"},{"location":"tema44/#4-automatizacion-y-orquestacion-con-apache-airflow","text":"","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"tema44/#44-monitoreo-logging-y-manejo-de-dependencias","text":"Objetivo : Comprender y aplicar t\u00e9cnicas de monitoreo, logging y gesti\u00f3n de dependencias en Apache Airflow para asegurar la trazabilidad, confiabilidad y eficiencia en la ejecuci\u00f3n de flujos de trabajo de datos distribuidos. Introducci\u00f3n : En entornos de Big Data, donde los pipelines pueden involucrar decenas o cientos de tareas interdependientes, es fundamental contar con mecanismos robustos para gestionar dependencias, detectar errores r\u00e1pidamente y recibir notificaciones oportunas. Apache Airflow proporciona una infraestructura poderosa para monitorear y depurar flujos de trabajo, manejar relaciones entre tareas, establecer alertas ante fallas, y generar registros detallados para su an\u00e1lisis. Desarrollo : Este tema se enfoca en tres aspectos clave de la operaci\u00f3n de DAGs en producci\u00f3n: c\u00f3mo definir y controlar las dependencias entre tareas y DAGs, c\u00f3mo configurar sistemas de notificaciones que alerten sobre estados cr\u00edticos, y c\u00f3mo aprovechar las capacidades de logging de Airflow para identificar y solucionar errores. Dominar estos elementos es esencial para mantener la confiabilidad y visibilidad de los pipelines en ecosistemas Big Data complejos, tanto on-premise como en la nube.","title":"4.4. Monitoreo, logging y manejo de dependencias"},{"location":"tema44/#441-gestion-de-dependencias-entre-tareas-y-dags","text":"La gesti\u00f3n de dependencias en Airflow es crucial para asegurar que las tareas se ejecuten en el orden correcto, respetando los flujos l\u00f3gicos del proceso. Las dependencias pueden definirse tanto dentro de un DAG como entre diferentes DAGs, y su correcta implementaci\u00f3n mejora la modularidad, reutilizaci\u00f3n y control de los flujos.","title":"4.4.1 Gesti\u00f3n de dependencias entre tareas y DAGs"},{"location":"tema44/#definicion-de-dependencias-entre-tareas-en-un-dag","text":"Airflow utiliza un modelo basado en grafos dirigidos ac\u00edclicos (DAGs), donde las tareas est\u00e1n conectadas entre s\u00ed por medio de relaciones de dependencia. Estas se definen de forma declarativa usando los operadores >> (downstream) y << (upstream). from airflow import DAG from airflow.operators.dummy import DummyOperator from datetime import datetime with DAG(dag_id=\"ejemplo_dependencias\", start_date=datetime(2024, 1, 1), schedule_interval='@daily') as dag: inicio = DummyOperator(task_id=\"inicio\") proceso_1 = DummyOperator(task_id=\"proceso_1\") proceso_2 = DummyOperator(task_id=\"proceso_2\") fin = DummyOperator(task_id=\"fin\") inicio >> [proceso_1, proceso_2] >> fin Este patr\u00f3n garantiza que proceso_1 y proceso_2 solo inicien despu\u00e9s de inicio , y que fin espere la finalizaci\u00f3n de ambos.","title":"Definici\u00f3n de dependencias entre tareas en un DAG"},{"location":"tema44/#dependencias-entre-dags-triggerdagrunoperator-y-externaltasksensor","text":"Airflow permite establecer dependencias entre DAGs distintos. Esto es \u00fatil para dividir procesos complejos en m\u00f3dulos independientes. TriggerDagRunOperator : inicia otro DAG desde una tarea. ExternalTaskSensor : espera la finalizaci\u00f3n de una tarea en otro DAG. from airflow.operators.dagrun_operator import TriggerDagRunOperator trigger = TriggerDagRunOperator( task_id=\"trigger_otros_dag\", trigger_dag_id=\"subdag_etl\", dag=dag )","title":"Dependencias entre DAGs (TriggerDagRunOperator y ExternalTaskSensor)"},{"location":"tema44/#442-sistema-de-alertas-y-notificaciones","text":"Airflow permite configurar alertas autom\u00e1ticas que se disparan cuando las tareas fallan, se retrasan o se comportan de forma inesperada. Estas alertas pueden enviarse por correo electr\u00f3nico, Slack, Microsoft Teams u otros canales, facilitando la respuesta r\u00e1pida a incidentes.","title":"4.4.2 Sistema de alertas y notificaciones"},{"location":"tema44/#configuracion-basica-de-notificaciones-por-correo-electronico","text":"Se pueden configurar notificaciones usando los par\u00e1metros email , email_on_failure , email_on_retry en la definici\u00f3n de tareas. from airflow.operators.dummy import DummyOperator DummyOperator( task_id=\"tarea_con_alerta\", email=[\"alertas@empresa.com\"], email_on_failure=True, retries=1, retry_delay=timedelta(minutes=5), dag=dag ) A nivel global, se configuran en airflow.cfg o variables de entorno ( SMTP_HOST , SMTP_USER , etc.).","title":"Configuraci\u00f3n b\u00e1sica de notificaciones por correo electr\u00f3nico"},{"location":"tema44/#integracion-con-herramientas-de-monitoreo-y-mensajeria","text":"Airflow puede integrarse con servicios como Slack o Microsoft Teams usando hooks o callbacks personalizados. Callback para Slack: def enviar_mensaje_slack(context): mensaje = f\"Tarea {context['task_instance_key_str']} fall\u00f3.\" # Aqu\u00ed se llamar\u00eda un webhook Slack from airflow.operators.python import PythonOperator tarea = PythonOperator( task_id=\"tarea_notificada\", python_callable=mi_funcion, on_failure_callback=enviar_mensaje_slack, dag=dag ) Tambi\u00e9n existen integraciones listas como [SlackWebhookOperator].","title":"Integraci\u00f3n con herramientas de monitoreo y mensajer\u00eda"},{"location":"tema44/#443-logging-y-debugging","text":"El sistema de logging de Airflow permite capturar, visualizar y analizar los eventos que ocurren durante la ejecuci\u00f3n de tareas. Estos logs son fundamentales para depurar errores, evaluar el comportamiento de los DAGs y hacer troubleshooting en producci\u00f3n.","title":"4.4.3 Logging y debugging"},{"location":"tema44/#visualizacion-de-logs-por-tarea-en-la-ui","text":"Cada tarea ejecutada genera un log accesible desde la interfaz web. Estos logs pueden ser configurados para almacenarse localmente o en la nube (S3, GCS, Azure Blob). Configuraci\u00f3n en airflow.cfg : [logging] base_log_folder = /opt/airflow/logs remote_logging = True remote_log_conn_id = MyS3Connection remote_base_log_folder = s3://airflow-logs Esto asegura que los logs est\u00e9n centralizados y disponibles para auditor\u00eda.","title":"Visualizaci\u00f3n de logs por tarea en la UI"},{"location":"tema44/#niveles-de-logging-y-debugging-en-desarrollo","text":"Durante la fase de desarrollo, es com\u00fan usar niveles de logging como DEBUG , INFO , WARNING , ERROR . import logging def mi_funcion(**kwargs): logging.info(\"Inicio de la funci\u00f3n\") try: resultado = 1 / 0 except ZeroDivisionError: logging.error(\"Divisi\u00f3n por cero detectada\", exc_info=True) Tambi\u00e9n es posible ejecutar DAGs en modo manual ( airflow dags test ) para obtener retroalimentaci\u00f3n inmediata sin esperar al scheduler.","title":"Niveles de logging y debugging en desarrollo"},{"location":"tema44/#tarea","text":"Desarrolla los siguientes ejercicios pr\u00e1cticos: Dise\u00f1a un DAG con 5 tareas, donde se evidencie la gesti\u00f3n correcta de dependencias usando >> y << . Implementa un DAG maestro que dispare otro DAG secundario mediante TriggerDagRunOperator y aseg\u00farate de que este \u00faltimo se ejecute correctamente. Configura una tarea que env\u00ede una notificaci\u00f3n por correo electr\u00f3nico en caso de fallo. Prueba intencionalmente el fallo para verificar el env\u00edo. Integra una alerta personalizada usando una funci\u00f3n Python que simule el env\u00edo de un mensaje a Slack o Teams en caso de fallo. Revisa los logs de ejecuci\u00f3n de un DAG que falle, identifica el error, corr\u00edgelo y vuelve a ejecutar. Adjunta capturas o fragmentos del log que justifiquen tu an\u00e1lisis.","title":"Tarea"},{"location":"tema51/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Desarrollo del proyecto integrador"},{"location":"tema51/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema51/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema52/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Despliegue en nube"},{"location":"tema52/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema52/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema_databricks/","text":"\u2190 Volver al Inicio Configuraci\u00f3n Cuenta en Databricks Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos. \u2190 Volver al Inicio","title":"Tema databricks"},{"location":"tema_databricks/#configuracion-cuenta-en-databricks","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos. \u2190 Volver al Inicio","title":"Configuraci\u00f3n Cuenta en Databricks"},{"location":"tema_docker/","text":"\u2190 Volver al Inicio Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2 Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ). 1. Instalaci\u00f3n de WSL2 Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a. 2. Instalaci\u00f3n de Docker Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version 3. Ejecutando un contenedor de ejemplo Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios. \u2190 Volver al Inicio","title":"Tema docker"},{"location":"tema_docker/#instalacion-y-configuracion-de-docker-y-wsl2","text":"Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ).","title":"Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2"},{"location":"tema_docker/#1-instalacion-de-wsl2","text":"Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a.","title":"1. Instalaci\u00f3n de WSL2"},{"location":"tema_docker/#2-instalacion-de-docker","text":"Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version","title":"2. Instalaci\u00f3n de Docker"},{"location":"tema_docker/#3-ejecutando-un-contenedor-de-ejemplo","text":"Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios. \u2190 Volver al Inicio","title":"3. Ejecutando un contenedor de ejemplo"},{"location":"tema_python/","text":"\u2190 Volver al Inicio Dominio b\u00e1sico de Python Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave 1. Entorno de Trabajo: Jupyter y Markdown Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown 2. Fundamentos de Python Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos 3. An\u00e1lisis de Datos con Pandas Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas \u2190 Volver al Inicio","title":"Tema python"},{"location":"tema_python/#dominio-basico-de-python","text":"Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave","title":"Dominio b\u00e1sico de Python"},{"location":"tema_python/#1-entorno-de-trabajo-jupyter-y-markdown","text":"Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown","title":"1. Entorno de Trabajo: Jupyter y Markdown"},{"location":"tema_python/#2-fundamentos-de-python","text":"Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos","title":"2. Fundamentos de Python"},{"location":"tema_python/#3-analisis-de-datos-con-pandas","text":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas \u2190 Volver al Inicio","title":"3. An\u00e1lisis de Datos con Pandas"},{"location":"tema_sql/","text":"\u2190 Volver al Inicio Repaso de SQL 1. Consultas B\u00e1sicas (SELECT) El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC; 2. Filtrado de Datos (WHERE) La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia'); 3. Joins (Uniones de Tablas) Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre; 4. Funciones de Agregaci\u00f3n Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5; 5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING) GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000; 6. Inserci\u00f3n de Datos (INSERT) INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75); 7. Actualizaci\u00f3n de Datos (UPDATE) UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 ); 8. Eliminaci\u00f3n de Datos (DELETE) DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) ); 9. Subconsultas Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento ); 10. \u00cdndices y Optimizaci\u00f3n Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20; \u2190 Volver al Inicio","title":"Tema sql"},{"location":"tema_sql/#repaso-de-sql","text":"","title":"Repaso de SQL"},{"location":"tema_sql/#1-consultas-basicas-select","text":"El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC;","title":"1. Consultas B\u00e1sicas (SELECT)"},{"location":"tema_sql/#2-filtrado-de-datos-where","text":"La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia');","title":"2. Filtrado de Datos (WHERE)"},{"location":"tema_sql/#3-joins-uniones-de-tablas","text":"Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre;","title":"3. Joins (Uniones de Tablas)"},{"location":"tema_sql/#4-funciones-de-agregacion","text":"Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5;","title":"4. Funciones de Agregaci\u00f3n"},{"location":"tema_sql/#5-agrupacion-y-filtrado-de-grupos-group-by-y-having","text":"GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000;","title":"5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING)"},{"location":"tema_sql/#6-insercion-de-datos-insert","text":"INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75);","title":"6. Inserci\u00f3n de Datos (INSERT)"},{"location":"tema_sql/#7-actualizacion-de-datos-update","text":"UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 );","title":"7. Actualizaci\u00f3n de Datos (UPDATE)"},{"location":"tema_sql/#8-eliminacion-de-datos-delete","text":"DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) );","title":"8. Eliminaci\u00f3n de Datos (DELETE)"},{"location":"tema_sql/#9-subconsultas","text":"Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento );","title":"9. Subconsultas"},{"location":"tema_sql/#10-indices-y-optimizacion","text":"Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20; \u2190 Volver al Inicio","title":"10. \u00cdndices y Optimizaci\u00f3n"}]}