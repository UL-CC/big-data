# Métodos de Procesamiento y Análisis de Big Data

**Objetivo General**: Al finalizar el módulo, cada estudiante será capaz de diseñar, implementar y ejecutar flujos de procesamiento para grandes volúmenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer información valiosa y apoyar la toma de decisiones basada en datos.

#### Requisitos Previos

- Dominio básico de [Python](tema_python.md)
- Conocimientos en [SQL](tema_sql.md)
- Es necesario tener instalado [Docker/WSL2](tema_docker.md)
- Es necesario tener acceso a [Databricks Community Edition](tema_databricks.md)

## 1. Introducción

**Objetivo**: Al finalizar esta unidad, el estudiante comprenderá los conceptos fundamentales del Big Data, sus desafíos y oportunidades, y será capaz de configurar un entorno básico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets.

- [Fundamentos de Big Data](tema11.md)
- [Introducción al ecosistema Spark](tema12.md)
- [RDD, DataFrame y Dataset](tema13.md)
- [Instalación y configuración de Spark](tema14.md)
- [Primeros pasos con PySpark](tema15.md)

## 2. PySpark y SparkSQL

**Objetivo**: Al finalizar esta unidad, el estudiante será capaz de manipular, transformar y consultar grandes volúmenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimización subyacentes para mejorar el rendimiento de las operaciones.

- [Fundamentos de DataFrames en Spark](tema21.md)
- [Manipulación y Transformación de Datos](tema22.md)
- [Consultas y SQL en Spark](tema23.md)
- [Optimización y Rendimiento](tema24.md)

## 3. Arquitectura y Diseño de Flujos ETL

**Objetivo**: Al finalizar esta unidad, el estudiante será capaz de diseñar, implementar y optimizar flujos ETL (Extracción, Transformación y Carga) escalables, seguros y resilientes, aplicando buenas prácticas y patrones modernos de ingeniería de datos. Utilizando herramientas como Apache Spark y Apache Airflow, podrá conectar múltiples fuentes de datos, gestionar la evolución de esquemas, monitorear el desempeño de los pipelines y aplicar estrategias para asegurar la calidad, eficiencia y sostenibilidad de los procesos tanto en entornos locales como en la nube.

- [Diseño y Orquestación de Pipelines ETL](tema31.md)
- [Conexión a Múltiples Fuentes de Datos](tema32.md)
- [Procesamiento Escalable y Particionamiento](tema33.md)
- [Manejo de Esquemas y Calidad de Datos](tema34.md)
- [Monitorización y Troubleshooting de Pipelines](tema35.md)
- [Seguridad en ETL y Protección de Datos](tema36.md)
- [Patrones de Diseño y Optimización en la Nube](tema37.md)

## 4. Automatización y Orquestación con Apache Airflow

**Objetivo**: Al finalizar esta unidad, el estudiante será capaz de diseñar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecución de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data.

- [Arquitectura y componentes de Airflow](tema41.md)
- [Instalación local usando Docker](tema42.md)
- [DAGs, operadores y tareas](tema43.md)
- [Uso de `BashOperator`, `PythonOperator` y `SparkSubmitOperator`](tema44.md)
- [Monitoreo y manejo de dependencias](tema45.md)

## 5. Proyecto Integrador y Despliegue

**Objetivo**: Al finalizar esta unidad, el estudiante será capaz de implementar un proyecto completo de Big Data, aplicando las metodologías y herramientas aprendidas a lo largo del módulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la solución.

- [Desarrollo del proyecto integrador](tema51.md)
- [Despliegue en nube](tema52.md)

