# 4. Automatizaci√≥n y Orquestaci√≥n con Apache Airflow

## 4.2. DAGs, operadores y tareas

**Objetivo**:

Comprender, dise√±ar y construir flujos de trabajo programables en Apache Airflow utilizando DAGs, operadores y tareas, incluyendo su agrupaci√≥n, parametrizaci√≥n y personalizaci√≥n para gestionar eficientemente flujos de datos complejos en entornos Big Data.

**Introducci√≥n**:

Apache Airflow permite definir y gestionar flujos de trabajo como c√≥digo, a trav√©s de DAGs (Directed Acyclic Graphs), que representan secuencias de tareas con dependencias expl√≠citas. Este enfoque permite un control detallado del orden de ejecuci√≥n, la l√≥gica condicional, la integraci√≥n con diversas herramientas y servicios, y la reutilizaci√≥n de c√≥digo para tareas repetitivas. En este tema se profundiza en c√≥mo construir DAGs robustos, emplear operadores integrados y personalizados, utilizar estructuras avanzadas como TaskGroups y SubDAGs, y aplicar templating din√°mico con Jinja2.

**Desarrollo**:

La definici√≥n de flujos de trabajo (pipelines) en Apache Airflow se basa en DAGs que agrupan tareas individuales y definen la l√≥gica de ejecuci√≥n. Cada tarea puede realizar operaciones diversas como ejecutar comandos del sistema, procesar datos con Spark, mover datos entre servicios cloud, o enviar notificaciones. Airflow proporciona una gran variedad de operadores listos para usar y permite crear operadores personalizados. Tambi√©n ofrece herramientas para organizar tareas y aplicar plantillas din√°micas, lo cual facilita el mantenimiento y escalabilidad de flujos complejos.

### 4.2.1 Definici√≥n y estructura de DAGs

Un **DAG** (Directed Acyclic Graph) es la pieza fundamental de Apache Airflow que representa un flujo de trabajo completo como un grafo dirigido sin ciclos.

Cada DAG puede ser:

- **Dirigido**: Las tareas tienen un orden espec√≠fico de ejecuci√≥n (flujo direccional)
- **Ac√≠clico**: No puede haber bucles infinitos o dependencias circulares
- **Grafo**: Conjunto de nodos (tareas) conectados por aristas (dependencias)

##### Par√°metros Esenciales

Son los par√°metros fundamentales que definen el comportamiento b√°sico de un DAG en Apache Airflow.

```python
# Par√°metros obligatorios
dag_id='nombre_unico_del_dag'     # identificador √∫nico en todo Airflow
start_date=datetime(2024, 1, 1)   # fecha desde cuando puede ejecutarse
schedule_interval='@daily'        # frecuencia de ejecuci√≥n

# Par√°metros importantes
catchup=False                     # si ejecutar ejecuciones pasadas
max_active_runs=1                 # n√∫mero m√°ximo de ejecuciones simult√°neas
default_retries=1                 # reintentos por defecto para todas las tareas
retry_delay=timedelta(minutes=5)  # tiempo entre reintentos
default_args = {                  # par√°metros que heredan todas las tareas del DAG
    'owner': 'team_data',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['alerts@empresa.com']
}
```

**Planificaci√≥n de la Ejecuci√≥n (Scheduling)**

Hay varias formas de configurar la frecuencias de ejecuci√≥n:

```python
# Opciones de schedule_interval
'@once'        # Ejecutar solo una vez
'@hourly'      # Cada hora (0 * * * *)
'@daily'       # Diariamente a medianoche (0 0 * * *)
'@weekly'      # Semanalmente los domingos (0 0 * * 0)
'@monthly'     # Primer d√≠a de cada mes (0 0 1 * *)
'@yearly'      # 1 de enero cada a√±o (0 0 1 1 *)

# Cron personalizado
'30 6 * * 1-5' # 6:30 AM, lunes a viernes
'0 */4 * * *'  # Cada 4 horas

# Program√°tico
timedelta(hours=2)              # Cada 2 horas
timedelta(days=1, hours=12)     # Cada d√≠a y medio
```

##### Estructura b√°sica de un DAG

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='mi_primer_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['ejemplo']
) as dag:

    tarea_1 = BashOperator(
        task_id='imprimir_fecha',
        bash_command='date'
    )

    tarea_2 = BashOperator(
        task_id='mostrar_hola_mundo',
        bash_command='echo "Hola mundo desde Airflow!"'
    )

    tarea_1 >> tarea_2  # Define la dependencia
```

##### DAG Completo y Detallado

**Diagrama**:

```mermaid
flowchart TD
    %% Estilos para diferentes tipos de tareas
    classDef verificacion fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef extraccion fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef procesamiento fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef validacion fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef reporte fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef carga fill:#e0f2f1,stroke:#004d40,stroke-width:2px
    classDef notificacion fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    classDef limpieza fill:#fafafa,stroke:#424242,stroke-width:2px

    %% Definici√≥n de nodos
    A[verificar_sistema<br/>üîç Verificar recursos del sistema]:::verificacion
    B[extraer_datos<br/>üì• Extraer datos de PostgreSQL]:::extraccion
    C[procesar_datos<br/>‚öôÔ∏è Transformar y limpiar datos]:::procesamiento
    D[validar_calidad<br/>‚úÖ Validar calidad de datos]:::validacion
    E[generar_reporte_task<br/>üìä Generar reporte de m√©tricas]:::reporte
    F[cargar_datos_warehouse<br/>üèóÔ∏è Cargar al data warehouse]:::carga
    G[enviar_notificacion<br/>üìß Enviar email de confirmaci√≥n]:::notificacion
    H[limpiar_archivos_temporales<br/>üßπ Limpiar archivos temp]:::limpieza

    %% Flujo de dependencias
    A --> B
    B --> C
    
    %% Ramificaci√≥n paralela despu√©s del procesamiento
    C --> D
    C --> E
    
    %% Convergencia para la carga (requiere procesamiento Y validaci√≥n)
    C --> F
    D --> F
    
    %% Reporte va a notificaci√≥n
    E --> G
    
    %% Limpieza al final (despu√©s de carga Y notificaci√≥n)
    F --> H
    G --> H

    %% Anotaciones explicativas
    subgraph "Fase 1: Preparaci√≥n"
        A
        B
    end
    
    subgraph "Fase 2: Procesamiento"
        C
    end
    
    subgraph "Fase 3: Validaci√≥n y Reportes (Paralelo)"
        D
        E
    end
    
    subgraph "Fase 4: Finalizaci√≥n"
        F
        G
        H
    end
```

**Implementaci√≥n**:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from datetime import datetime, timedelta
import pandas as pd
import logging

# Estos argumentos se aplicar√°n a todas las tareas del DAG
# a menos que se sobrescriban espec√≠ficamente
default_args = {
    'owner': 'equipo_data_engineering',          # Responsable del DAG
    'depends_on_past': False,                    # No depende de ejecuciones anteriores
    'start_date': datetime(2024, 1, 1),          # Fecha de inicio del DAG
    'email_on_failure': True,                    # Enviar email si falla
    'email_on_retry': False,                     # No enviar email en reintento
    'email': ['team@empresa.com'],               # Lista de emails para notificaciones
    'retries': 2,                                # N√∫mero de reintentos por tarea
    'retry_delay': timedelta(minutes=5),         # Tiempo entre reintentos
    'execution_timeout': timedelta(hours=2),     # Timeout m√°ximo por tarea
}

# DEFINICI√ìN DEL DAG
with DAG(
    # Identificador √∫nico del DAG en toda la instancia de Airflow
    dag_id='pipeline_procesamiento_ventas_v2',
    
    # Hereda la configuraci√≥n por defecto definida arriba
    default_args=default_args,
    
    # Descripci√≥n que aparece en la UI de Airflow
    description='Pipeline completo para procesar datos de ventas diarias',
    
    # Frecuencia de ejecuci√≥n - se ejecuta diariamente a las 2:00 AM
    schedule_interval='0 2 * * *',
    
    # No ejecutar DAGs para fechas pasadas (evita backfill autom√°tico)
    catchup=False,
    
    # Solo una instancia del DAG puede ejecutarse simult√°neamente
    max_active_runs=1,
    
    # Tags para organizar DAGs en la UI
    tags=['ventas', 'etl', 'produccion', 'diario'],
    
    # Documentaci√≥n en formato Markdown
    doc_md="""
    ## Pipeline de Procesamiento de Ventas
    
    Este DAG procesa los datos de ventas diarias:
    1. Extrae datos de la base de datos transaccional
    2. Aplica transformaciones y limpieza
    3. Carga los datos al data warehouse
    4. Env√≠a reporte de resumen por email
    
    **Dependencias externas**: Base de datos PostgreSQL, S3 bucket
    **Tiempo estimado**: 45 minutos
    **Criticidad**: Alta - bloquea reportes ejecutivos
    """,
    
) as dag:

    # FUNCI√ìN PYTHON PERSONALIZADA
    def procesar_datos_ventas(**context):
        """
        Funci√≥n que procesa los datos de ventas.
        **context contiene informaci√≥n del contexto de ejecuci√≥n de Airflow
        """
        # Obtener la fecha de ejecuci√≥n del contexto
        execution_date = context['execution_date']
        
        # Simular procesamiento de datos
        logging.info(f"Procesando datos de ventas para {execution_date}")
        
        # En un caso real, aqu√≠ har√≠as:
        # - Conexi√≥n a base de datos
        # - Extracci√≥n de datos
        # - Transformaciones con pandas/spark
        # - Validaciones de calidad
        
        # Retornar m√©tricas para usar en tareas posteriores
        return {
            'registros_procesados': 15420,
            'ventas_totales': 89750.50,
            'fecha_proceso': execution_date.strftime('%Y-%m-%d')
        }

    def generar_reporte(**context):
        """Genera un reporte basado en los datos procesados"""
        # Obtener datos de la tarea anterior usando XCom
        datos_ventas = context['task_instance'].xcom_pull(task_ids='procesar_datos')
        
        reporte = f"""
        Reporte de Ventas - {datos_ventas['fecha_proceso']}
        ================================================
        Registros procesados: {datos_ventas['registros_procesados']:,}
        Ventas totales: ${datos_ventas['ventas_totales']:,.2f}
        """
        
        logging.info(reporte)
        return reporte

    # DEFINICI√ìN DE TAREAS
    
    # TAREA 1: Verificaci√≥n del sistema
    verificar_sistema = BashOperator(
        task_id='verificar_sistema',
        # Comando bash que verifica conectividad y recursos
        bash_command="""
        echo "Verificando sistema..."
        df -h | grep -v tmpfs  # Verificar espacio en disco
        echo "Sistema verificado correctamente"
        """,
        # Documentaci√≥n espec√≠fica de la tarea
        doc_md="""
        ### Verificaci√≥n del Sistema
        Valida que el sistema tenga los recursos necesarios:
        - Espacio en disco suficiente
        - Conectividad de red
        - Servicios requeridos activos
        """
    )

    # TAREA 2: Extracci√≥n de datos
    extraer_datos = BashOperator(
        task_id='extraer_datos',
        bash_command="""
        echo "Iniciando extracci√≥n de datos..."
        # En producci√≥n, esto ser√≠a algo como:
        # psql -h $DB_HOST -d ventas -c "COPY (...) TO STDOUT" > /tmp/ventas_{{ ds }}.csv
        echo "Simulando extracci√≥n de 15,420 registros"
        echo "Datos extra√≠dos exitosamente"
        """,
        # Esta tarea puede fallar ocasionalmente, aumentamos reintentos
        retries=3,
        retry_delay=timedelta(minutes=2)
    )

    # TAREA 3: Procesamiento con Python
    procesar_datos = PythonOperator(
        task_id='procesar_datos',
        python_callable=procesar_datos_ventas,
        # Proporcionar el contexto de Airflow a la funci√≥n
        provide_context=True
    )

    # TAREA 4: Validaci√≥n de calidad
    validar_calidad = BashOperator(
        task_id='validar_calidad',
        bash_command="""
        echo "Ejecutando validaciones de calidad de datos..."
        # Simular validaciones
        echo "‚úì Sin valores nulos en campos cr√≠ticos"
        echo "‚úì Rangos de fechas v√°lidos"
        echo "‚úì Integridad referencial correcta"
        echo "Validaci√≥n completada exitosamente"
        """
    )

    # TAREA 5: Carga al data warehouse
    cargar_datos = BashOperator(
        task_id='cargar_datos_warehouse',
        bash_command="""
        echo "Cargando datos al data warehouse..."
        # En producci√≥n ser√≠a algo como:
        # aws s3 cp /tmp/ventas_processed_{{ ds }}.parquet s3://warehouse/ventas/
        echo "Datos cargados exitosamente al data warehouse"
        """
    )

    # TAREA 6: Generaci√≥n de reporte
    generar_reporte_task = PythonOperator(
        task_id='generar_reporte',
        python_callable=generar_reporte,
        provide_context=True
    )

    # TAREA 7: Env√≠o de notificaci√≥n por email
    enviar_notificacion = EmailOperator(
        task_id='enviar_notificacion',
        to=['gerencia@empresa.com', 'ventas@empresa.com'],
        subject='Pipeline de Ventas Completado - {{ ds }}',
        html_content="""
        <h3>Pipeline de Procesamiento de Ventas</h3>
        <p><strong>Fecha:</strong> {{ ds }}</p>
        <p><strong>Estado:</strong> Completado exitosamente ‚úÖ</p>
        <p><strong>Tiempo de ejecuci√≥n:</strong> {{ macros.datetime.now() }}</p>
        
        <p>Los datos de ventas han sido procesados y est√°n disponibles en el data warehouse.</p>
        
        <hr>
        <small>Este es un mensaje autom√°tico del sistema de orquestaci√≥n de datos.</small>
        """
    )

    # TAREA 8: Limpieza de archivos temporales
    limpiar_archivos = BashOperator(
        task_id='limpiar_archivos_temporales',
        bash_command="""
        echo "Limpiando archivos temporales..."
        # rm -f /tmp/ventas_{{ ds }}.*
        echo "Limpieza completada"
        """,
        # Esta tarea no es cr√≠tica, si falla no debe afectar el pipeline
        trigger_rule='all_done'  # Se ejecuta sin importar si las anteriores fallan
    )

    # DEFINICI√ìN DE DEPENDENCIAS
    
    # Secuencia lineal b√°sica
    verificar_sistema >> extraer_datos >> procesar_datos
    
    # Despu√©s del procesamiento, dos ramas paralelas
    procesar_datos >> [validar_calidad, generar_reporte_task]
    
    # La carga depende de que tanto el procesamiento como la validaci√≥n sean exitosos
    [procesar_datos, validar_calidad] >> cargar_datos
    
    # El reporte se puede generar en paralelo con la carga
    generar_reporte_task >> enviar_notificacion
    
    # La limpieza se ejecuta al final, despu√©s de todo
    [cargar_datos, enviar_notificacion] >> limpiar_archivos

```


### 4.2.2 Operadores built-in y custom operators

Los **operadores** son las unidades de ejecuci√≥n en Airflow. Representan tareas concretas que se ejecutan en el DAG. Airflow incluye m√∫ltiples operadores built-in y permite crear operadores personalizados para extender su funcionalidad.

##### Operadores de Ejecuci√≥n B√°sica

Estos operadores permiten ejecutar comandos del sistema, scripts, o funciones personalizadas.

* `BashOperator`: Fundamental para ejecutar comandos del sistema, scripts shell y herramientas CLI de big data como Hadoop, Spark-submit, etc.
* `PythonOperator`: Cr√≠tico para ejecutar funciones Python personalizadas, transformaciones de datos y l√≥gica de negocio
* `EmailOperator`: Esencial para notificaciones y alertas en pipelines de producci√≥n

```python
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def saludar():
    print("Hola desde PythonOperator")

tarea_bash = BashOperator(
    task_id='tarea_bash',
    bash_command='echo "Ejecutando comando Bash"',
    dag=dag
)

tarea_python = PythonOperator(
    task_id='tarea_python',
    python_callable=saludar,
    dag=dag
)
```

##### Operadores de Bases de Datos

Estos operadores permiten ejecutar consultas SQL directamente desde Airflow hacia motores relacionales o embebidos.

* `PostgresOperator`/`MySqlOperator`: Para ejecutar consultas SQL en bases de datos relacionales
* `SqliteOperator`: √ötil para pruebas y desarrollo local

```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

tarea_sql = PostgresOperator(
    task_id='crear_tabla',
    postgres_conn_id='mi_conexion_postgres',
    sql='CREATE TABLE IF NOT EXISTS tabla_ejemplo (id SERIAL PRIMARY KEY);',
    dag=dag
)
```

##### Operadores de Transferencia

Son usados para mover datos entre servicios, especialmente en la nube.

* `S3ToRedshiftOperator`: Transferir datos desde S3 a Redshift (muy com√∫n en AWS)
* `BigQueryOperator`: Para operaciones en Google BigQuery
* `RedshiftToS3Operator`: Exportar datos desde Redshift

```python
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator

tarea_transferencia = S3ToRedshiftOperator(
    task_id='s3_a_redshift',
    s3_bucket='mi-bucket',
    s3_key='datos.csv',
    schema='public',
    table='mi_tabla',
    copy_options=['csv'],
    aws_conn_id='aws_default',
    redshift_conn_id='redshift_default',
    dag=dag
)
```

##### Operadores de Control de Flujo

Controlan la l√≥gica de ejecuci√≥n dentro de un DAG.

* `BranchPythonOperator`: Para l√≥gica condicional en el pipeline
* `EmptyOperator` (antes `DummyOperator`): Para puntos de sincronizaci√≥n
* `TriggerDagRunOperator`: Para activar otros DAGs

```python
from airflow.operators.empty import EmptyOperator
from airflow.operators.branch import BranchPythonOperator

def decidir():
    return 'tarea_a' if datetime.now().hour < 12 else 'tarea_b'

rama = BranchPythonOperator(
    task_id='evaluar_ruta',
    python_callable=decidir,
    dag=dag
)

tarea_a = EmptyOperator(task_id='tarea_a', dag=dag)
tarea_b = EmptyOperator(task_id='tarea_b', dag=dag)

rama >> [tarea_a, tarea_b]
```

##### Operadores para Spark y Hadoop

Permiten ejecutar aplicaciones en motores de procesamiento distribuido.

* `SparkSubmitOperator`: Ejecutar aplicaciones Spark (batch processing)
* `DataprocSubmitJobOperator`: Para trabajos en Google Cloud Dataproc
* `EMRStepOperator`: Para clusters Amazon EMR

```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_job = SparkSubmitOperator(
    task_id='ejecutar_spark',
    application='/ruta/a/mi_app_spark.py',
    conn_id='spark_default',
    dag=dag
)
```

##### Operadores para Streaming y HDFS

Permiten flujos en tiempo real y acceso a HDFS

* `KafkaOperator`: Interactuar con Apache Kafka para datos en tiempo real
* `HDFSOperator`: Operaciones en Hadoop Distributed File System

```python
from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor

esperar_archivo = HdfsSensor(
    task_id='esperar_archivo_hdfs',
    filepath='/data/input/archivo.csv',
    hdfs_conn_id='hdfs_default',
    dag=dag
)
```

##### Operadores para Validaci√≥n de Datos

Permiten realizar validaciones sobre los datos

* `GreatExpectationsOperator`: Validaci√≥n de calidad de datos
* `Custom Data Validation Operators`: Para reglas espec√≠ficas del negocio

```python
from airflow.providers.great_expectations.operators.great_expectations import GreatExpectationsOperator

validacion = GreatExpectationsOperator(
    task_id='validar_datos',
    data_context_root_dir='/opt/ge/',
    checkpoint_name='check_ventas',
    dag=dag
)
```

##### Operadores para MLOps

Flujos relacionados con procesos de Machine Learning

* `MLflowOperator`: Para experimentos y modelos de machine learning
* `KubernetesOperator`: Para ejecutar contenedores en Kubernetes

```python
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

ml_entrenamiento = KubernetesPodOperator(
    task_id='entrenar_modelo',
    name='ml-train',
    image='ml/entrenamiento:latest',
    dag=dag
)
```

### 4.2.3 TaskGroups

Cuando los DAGs crecen en complejidad (m√°s de 10-15 tareas), surgen varios problemas:

* Legibilidad: Dif√≠cil entender el flujo en la UI de Airflow
* Mantenimiento: C√≥digo desordenado y dif√≠cil de modificar
* Reutilizaci√≥n: Patrones repetitivos sin forma de encapsular
* Colaboraci√≥n: Equipos diferentes trabajando en el mismo DAG
* Debugging: Dif√≠cil identificar grupos de tareas relacionadas

La soluci√≥n es agrupar tareas l√≥gicamente relacionadas usando TaskGroups.

Nota: Existe un mecanismo adicional llamado SubDAGs, pero ya es considerado obsoleto.

##### Uso de TaskGroups

Los TaskGroups son una forma de organizar tareas visualmente en la UI de Airflow sin crear DAGs separados.

Introducidos en Airflow 2.0, son la mejor pr√°ctica actual para estructurar DAGs complejos.

Ventajas de **TaskGroups**

- Organizaci√≥n visual: Agrupa tareas en la UI de Airflow
- Reutilizaci√≥n de c√≥digo: Encapsula l√≥gica repetitiva
- Mejor rendimiento: No crean procesos separados como SubDAGs
- Simplicidad: F√°cil de usar y entender
- Debugging mejorado: Logs y estados agrupados
- Escalabilidad: Maneja DAGs con cientos de tareas

Los `TaskGroup` permiten agrupar visual y l√≥gicamente tareas relacionadas dentro de un DAG.

**Ejemplo de un TaskGroup**:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

# Configuraci√≥n del DAG
with DAG(
    dag_id='pipeline_con_taskgroups_v1',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['taskgroups', 'ejemplo']
) as dag:

    # TASKGROUP 1: PREPARACI√ìN DE DATOS
    with TaskGroup(
        group_id='preparacion_datos',
        tooltip='Descarga, validaci√≥n y limpieza de datos raw'
    ) as grupo_preparacion:
        
        # Tarea 1: Descargar datos de m√∫ltiples fuentes
        descargar_datos = BashOperator(
            task_id='descargar_datos',
            bash_command='''
            echo "Descargando datos de APIs y bases de datos..."
            # Simular descarga de m√∫ltiples fuentes
            echo "‚úì API de ventas: 15,420 registros"
            echo "‚úì API de clientes: 8,932 registros"
            echo "‚úì DB transaccional: 45,123 registros"
            '''
        )
        
        # Tarea 2: Validar integridad de los datos
        validar_integridad = BashOperator(
            task_id='validar_integridad',
            bash_command='''
            echo "Validando integridad de datos descargados..."
            echo "‚úì Verificando checksums"
            echo "‚úì Validando esquemas JSON/CSV"
            echo "‚úì Contando registros esperados"
            '''
        )
        
        # Tarea 3: Limpiar y estandarizar datos
        limpiar_datos = PythonOperator(
            task_id='limpiar_datos',
            python_callable=lambda: print("""
            Limpiando datos:
            ‚úì Removiendo duplicados
            ‚úì Estandarizando formatos de fecha
            ‚úì Normalizando nombres y direcciones
            ‚úì Aplicando reglas de negocio
            """)
        )
        
        # Dependencias dentro del TaskGroup
        descargar_datos >> validar_integridad >> limpiar_datos

    # TASKGROUP 2: TRANSFORMACIONES DE NEGOCIO
    with TaskGroup(
        group_id='transformaciones_negocio',
        tooltip='Aplicar reglas de negocio y c√°lculos complejos'
    ) as grupo_transformaciones:
        
        def calcular_metricas_ventas():
            print("Calculando m√©tricas de ventas:")
            print("‚úì Revenue por regi√≥n")
            print("‚úì Customer Lifetime Value")
            print("‚úì Churn rate")
            print("‚úì Forecasting pr√≥ximo trimestre")
            return "M√©tricas calculadas exitosamente"
        
        def aplicar_segmentacion():
            print("Aplicando segmentaci√≥n de clientes:")
            print("‚úì RFM Analysis (Recency, Frequency, Monetary)")
            print("‚úì Clustering de comportamiento")
            print("‚úì Propensi√≥n de compra")
            return "Segmentaci√≥n completada"
        
        calcular_metricas = PythonOperator(
            task_id='calcular_metricas_ventas',
            python_callable=calcular_metricas_ventas
        )
        
        segmentar_clientes = PythonOperator(
            task_id='segmentar_clientes',
            python_callable=aplicar_segmentacion
        )
        
        # Estas tareas pueden ejecutarse en paralelo
        # No hay dependencia entre calcular m√©tricas y segmentar clientes
        pass  # Sin dependencias internas = ejecuci√≥n paralela

    # TASKGROUP 3: CARGA DE DATOS
    with TaskGroup(
        group_id='carga_datos',
        tooltip='Carga a diferentes sistemas de destino'
    ) as grupo_carga:
        
        cargar_datawarehouse = BashOperator(
            task_id='cargar_datawarehouse',
            bash_command='''
            echo "Cargando datos al data warehouse..."
            echo "‚úì Upserting tabla dimensiones"
            echo "‚úì Insertando hechos de ventas"
            echo "‚úì Actualizando √≠ndices"
            '''
        )
        
        cargar_data_lake = BashOperator(
            task_id='cargar_data_lake',
            bash_command='''
            echo "Cargando datos al data lake..."
            echo "‚úì Particionando por fecha"
            echo "‚úì Comprimiendo en formato Parquet"
            echo "‚úì Actualizando cat√°logo de metadatos"
            '''
        )
        
        actualizar_cache = BashOperator(
            task_id='actualizar_cache',
            bash_command='''
            echo "Actualizando sistemas de cache..."
            echo "‚úì Redis para m√©tricas en tiempo real"
            echo "‚úì Elasticsearch para b√∫squedas"
            echo "‚úì CDN para reportes est√°ticos"
            '''
        )
        
        # Todas las cargas pueden ejecutarse en paralelo
        pass  # Sin dependencias = m√°ximo paralelismo

    # TAREA INDIVIDUAL: NOTIFICACIONES
    enviar_notificaciones = BashOperator(
        task_id='enviar_notificaciones',
        bash_command='''
        echo "Enviando notificaciones finales..."
        echo "‚úì Email a stakeholders"
        echo "‚úì Slack al equipo de datos"
        echo "‚úì Dashboard actualizado"
        '''
    )

    # DEPENDENCIAS ENTRE TASKGROUPS
    
    # Flujo secuencial entre grupos principales
    grupo_preparacion >> grupo_transformaciones >> grupo_carga
    
    # La notificaci√≥n final depende de que toda la carga est√© completa
    grupo_carga >> enviar_notificaciones
```

### 4.2.4 Templating con Jinja2

**Apache Airflow** aprovecha la potencia de **Jinja2**, un motor de plantillas moderno y de alto rendimiento para Python, para permitir la generaci√≥n din√°mica de comandos, configuraciones y otros valores dentro de tus **DAGs** (Directed Acyclic Graphs) y operadores. Esta capacidad de templating es fundamental para crear flujos de trabajo flexibles y reutilizables, ya que te permite adaptar el comportamiento de tus tareas en funci√≥n de variables de tiempo de ejecuci√≥n, configuraciones del entorno o datos espec√≠ficos.

##### Uso de variables y contextos

El templating con Jinja2 te permite incrustar variables y acceder al contexto de ejecuci√≥n de Airflow directamente dentro de las propiedades de tus operadores que soportan plantillas (identificadas por tener un sufijo `_template` o ser parte de una lista definida por `template_fields` en el operador).

Considera el siguiente ejemplo con un `BashOperator`:

```python
tarea_con_template = BashOperator(
    task_id='mostrar_fecha',
    bash_command='echo "La fecha de ejecuci√≥n es {{ ds }}"',
    dag=dag
)
```

En este caso, la cadena `bash_command` es una plantilla Jinja2. Durante la ejecuci√≥n de la tarea `{{ ds }}` ser√° reemplazado autom√°ticamente por la fecha de ejecuci√≥n actual del DAG en formato `YYYY-MM-DD`. Esto es √∫til para tareas que dependen de la fecha, como el procesamiento de datos diarios o la generaci√≥n de informes con nombres de archivo basados en la fecha.

##### Acceso a macros y funciones de utilidad

Airflow expone un conjunto de **macros** y funciones de utilidad a las plantillas Jinja2, lo que te permite realizar operaciones comunes sin necesidad de escribir l√≥gica Python adicional. Estas macros proporcionan acceso conveniente a informaci√≥n clave del contexto de ejecuci√≥n y a funciones de manipulaci√≥n de fechas, entre otras cosas. Algunas de las macros m√°s utilizadas incluyen:

* **`{{ ds }}`**: Representa la **fecha de ejecuci√≥n del DAG** en formato de cadena `YYYY-MM-DD`. Es ideal para operaciones de carga de datos diarias.
* **`{{ ds_nodash }}`**: Similar a `ds`, pero la fecha se presenta sin guiones, por ejemplo, `YYYYMMDD`. √ötil para nombres de archivos o rutas sin caracteres especiales.
* **`{{ execution_date }}`**: Es un **objeto `datetime` completo** que representa el momento exacto en que la instancia del DAG fue programada para ejecutarse. Este objeto te permite realizar manipulaciones de tiempo m√°s complejas.
* **`{{ prev_ds }}`**: La fecha de ejecuci√≥n anterior, si existe, en formato `YYYY-MM-DD`.
* **`{{ next_ds }}`**: La siguiente fecha de ejecuci√≥n programada, si existe, en formato `YYYY-MM-DD`.
* **`{{ macros.ds_add(ds, 7) }}`**: Esta es una funci√≥n de macro que te permite **sumar o restar d√≠as** a una fecha dada. En este ejemplo, suma 7 d√≠as a la fecha de ejecuci√≥n (`ds`). Puedes usarla para definir ventanas de tiempo relativas o para procesar datos con un desfase. Por ejemplo, `macros.ds_format(ds, "%Y-%m-%d", "%Y/%m/%d")` para cambiar el formato.
* **`{{ dag_run }}`**: Acceso al objeto `DagRun` completo, lo que te permite obtener informaci√≥n m√°s detallada sobre la ejecuci√≥n actual del DAG.
* **`{{ ti }}`**: Acceso al objeto `TaskInstance` actual, √∫til para obtener informaci√≥n espec√≠fica de la instancia de la tarea.

##### Templating avanzado en operadores Python

El _templating_ no se limita solo a operadores de Bash. Muchos operadores de Airflow, incluido el `PythonOperator`, permiten que los valores sean puestos en un _template_. Para los `PythonOperator` que necesitan acceder al contexto de Airflow, puedes usar el argumento `provide_context=True`. Cuando esta opci√≥n est√° activada, Airflow inyecta un diccionario con el contexto de ejecuci√≥n (incluyendo las variables y macros Jinja2 disponibles) como argumentos `kwargs` al `python_callable` definido.

```python
from airflow.operators.python import PythonOperator

def mostrar_contexto(**kwargs):
    # 'ds' est√° disponible en kwargs cuando provide_context=True
    print(f"La tarea se ejecuta para la fecha: {kwargs['ds']}")
    print(f"Fecha de ejecuci√≥n completa: {kwargs['execution_date']}")
    # Tambi√©n puedes acceder a otras variables del contexto
    print(f"ID de la tarea: {kwargs['task_instance'].task_id}")

tarea_contexto = PythonOperator(
    task_id='ver_contexto',
    python_callable=mostrar_contexto,
    provide_context=True,  # Esto inyecta el contexto como kwargs
    dag=dag
)
```

En este `PythonOperator`, la funci√≥n `mostrar_contexto` recibe `**kwargs`, que contendr√° el diccionario de contexto de Airflow. Dentro de esta funci√≥n, puedes acceder a `kwargs['ds']`, `kwargs['execution_date']`, `kwargs['task_instance']`, y cualquier otra variable o macro que normalmente estar√≠a disponible en las plantillas Jinja2. Esto te brinda un control program√°tico completo sobre c√≥mo tus funciones Python interact√∫an con el entorno de ejecuci√≥n de Airflow.

## Tarea

Realiza los siguientes ejercicios para afianzar tu dominio sobre DAGs, operadores y tareas en Airflow:

1. **Define un DAG diario** que ejecute dos tareas: una que imprima la fecha del sistema y otra que salude al usuario.
2. **Usa un `BranchPythonOperator`** para decidir entre dos caminos: ejecutar un script Python o un comando Bash, seg√∫n el d√≠a de la semana.
3. **Agrupa tres tareas de transformaci√≥n** (descargar, limpiar y cargar datos) usando `TaskGroup`.
4. **Crea un operador personalizado** que verifique si existe un archivo en un bucket de S3 y registre el resultado.
5. **Implementa templating** en un `BashOperator` para imprimir din√°micamente la fecha de ejecuci√≥n y el nombre del DAG.

