<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Fundamentos de DataFrames en Spark - Métodos de Procesamiento y Análisis de Big Data</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Fundamentos de DataFrames en Spark";
        var mkdocs_page_input_path = "tema21.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Métodos de Procesamiento y Análisis de Big Data
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Inicio</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Introducción</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema11/">Fundamentos de Big Data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema12/">Introducción al ecosistema Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema13/">RDD, DataFrame y Dataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema14/">Instalación y configuración de Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema15/">Primeros pasos con PySpark</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">PySpark y SparkSQL</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Fundamentos de DataFrames en Spark</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#tema-21-fundamentos-de-dataframes-en-spark">Tema 2.1 Fundamentos de DataFrames en Spark</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#211-operaciones-con-dataframes">2.1.1 Operaciones con DataFrames</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#creacion-de-dataframes">Creación de DataFrames</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#transformaciones-de-dataframes">Transformaciones de DataFrames</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#acciones-de-dataframes">Acciones de DataFrames</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#212-esquemas-y-tipos-de-datos-complejos">2.1.2 Esquemas y tipos de datos complejos</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#inferencia-y-definicion-explicita-de-esquemas">Inferencia y Definición Explícita de Esquemas</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#tipos-de-datos-complejos">Tipos de Datos Complejos</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#213-lectura-y-escritura-en-formatos-populares-parquet-avro-orc-csv-json">2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON)</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#lectura-de-datos">Lectura de Datos</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#escritura-de-datos">Escritura de Datos</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#tarea">Tarea</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema22/">Manipulación y Transformación de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema23/">Consultas y SQL en Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema24/">Optimización y Rendimiento</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Arquitectura y Diseño de Flujos ETL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema31/">Diseño y Orquestación de Pipelines ETL</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema32/">Conexión a Múltiples Fuentes de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema33/">Procesamiento Escalable y Particionamiento</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema34/">Manejo de Esquemas y Calidad de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema35/">Monitorización y Troubleshooting de Pipelines</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema36/">Seguridad en ETL y Protección de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema37/">Patrones de Diseño y Optimización en la Nube</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Automatización y Orquestación con Apache Airflow</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema41/">Arquitectura y componentes de Airflow</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema42/">Instalación local usando Docker</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema43/">DAGs, operadores y tareas</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema44/">Uso de `BashOperator`, `PythonOperator` y `SparkSubmitOperator`</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema45/">Monitoreo y manejo de dependencias</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Proyecto Integrador y Despliegue</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema51/">Desarrollo del proyecto integrador</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema52/">Despliegue en nube</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Métodos de Procesamiento y Análisis de Big Data</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">PySpark y SparkSQL</li>
      <li class="breadcrumb-item active">Fundamentos de DataFrames en Spark</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2-pyspark-y-sparksql">2. PySpark y SparkSQL</h1>
<h2 id="tema-21-fundamentos-de-dataframes-en-spark">Tema 2.1 Fundamentos de DataFrames en Spark</h2>
<p><strong>Objetivo</strong>:</p>
<p>Al finalizar este tema, el estudiante será capaz de comprender la estructura y el funcionamiento de los DataFrames de Apache Spark, realizar operaciones fundamentales de manipulación de datos, gestionar esquemas y tipos de datos complejos, y leer y escribir datos en los formatos más comunes utilizados en entornos de Big Data.</p>
<p><strong>Introducción</strong>:</p>
<p>En el vasto universo del Big Data, la capacidad de procesar y analizar volúmenes masivos de información es crucial. Apache Spark, con su motor de procesamiento distribuido, se ha consolidado como una herramienta indispensable para esta tarea. En el corazón de su eficiencia y facilidad de uso se encuentran los DataFrames, una abstracción de datos distribuida que organiza los datos en columnas con nombre, similar a una tabla en una base de datos relacional o una hoja de cálculo. Esta estructura permite a los desarrolladores trabajar con datos de forma intuitiva y optimizada, aprovechando el poder de Spark para el procesamiento paralelo y distribuido.</p>
<p><strong>Desarrollo</strong>:</p>
<p>Este tema se centrará en los pilares de la manipulación de datos en Spark a través de los DataFrames. Exploraremos cómo los DataFrames facilitan las operaciones de transformación y consulta, abstraen la complejidad de la distribución de datos y proporcionan un API robusto para interactuar con ellos. Abordaremos desde las operaciones básicas como selección y filtrado, hasta la comprensión de los esquemas y tipos de datos complejos, y la interacción con una variedad de formatos de archivo estándar de la industria.</p>
<h3 id="211-operaciones-con-dataframes">2.1.1 Operaciones con DataFrames</h3>
<p>Las operaciones con DataFrames en Spark son el núcleo de la manipulación de datos, permitiendo seleccionar, filtrar, agregar, unir y realizar una multitud de transformaciones sobre conjuntos de datos distribuidos de manera eficiente. A diferencia de los RDDs, los DataFrames ofrecen un nivel de abstracción superior, permitiendo a Spark optimizar internamente las operaciones gracias a la información del esquema.</p>
<h5 id="creacion-de-dataframes">Creación de DataFrames</h5>
<p>La creación de DataFrames es el primer paso para trabajar con datos en PySpark. Se pueden crear a partir de diversas fuentes, como listas de Python, RDDs existentes, o leyendo directamente desde archivos.</p>
<ol>
<li><strong>Desde una lista de tuplas o diccionarios:</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName(&quot;DataFrameOperations&quot;).getOrCreate()

# Opción 1: Usando una lista de tuplas y definiendo el esquema
data_tuples = [(&quot;Alice&quot;, 1, &quot;NY&quot;), (&quot;Bob&quot;, 2, &quot;LA&quot;), (&quot;Charlie&quot;, 3, &quot;CHI&quot;)]
schema_tuples = StructType([
    StructField(&quot;name&quot;, StringType(), True),
    StructField(&quot;id&quot;, IntegerType(), True),
    StructField(&quot;city&quot;, StringType(), True)
])
df_from_tuples = spark.createDataFrame(data_tuples, schema=schema_tuples)
df_from_tuples.show()
# Resultado:
# +-------+---+----+
# |   name| id|city|
# +-------+---+----+
# |  Alice|  1|  NY|
# |    Bob|  2|  LA|
# |Charlie|  3| CHI|
# +-------+---+----+

# Opción 2: Usando una lista de diccionarios (Spark infiere el esquema)
data_dicts = [{&quot;name&quot;: &quot;Alice&quot;, &quot;id&quot;: 1, &quot;city&quot;: &quot;NY&quot;},
              {&quot;name&quot;: &quot;Bob&quot;, &quot;id&quot;: 2, &quot;city&quot;: &quot;LA&quot;},
              {&quot;name&quot;: &quot;Charlie&quot;, &quot;id&quot;: 3, &quot;city&quot;: &quot;CHI&quot;}]
df_from_dicts = spark.createDataFrame(data_dicts)
df_from_dicts.show()
# Resultado:
# +-------+---+----+
# |   city| id|name|
# +-------+---+----+
# |     NY|  1|Alice|
# |     LA|  2|  Bob|
# |    CHI|  3|Charlie|
# +-------+---+----+
</code></pre>
<ol>
<li><strong>Desde un RDD existente:</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;DataFrameOperations&quot;).getOrCreate()

rdd = spark.sparkContext.parallelize([(&quot;Alice&quot;, 1), (&quot;Bob&quot;, 2), (&quot;Charlie&quot;, 3)])
df_from_rdd = spark.createDataFrame(rdd, [&quot;name&quot;, &quot;id&quot;])
df_from_rdd.show()
# Resultado:
# +-------+---+
# |   name| id|
# +-------+---+
# |  Alice|  1|
# |    Bob|  2|
# |Charlie|  3|
# +-------+---+
</code></pre>
<ol>
<li><strong>Desde un archivo (se verá en 2.1.3):</strong></li>
</ol>
<pre><code class="language-python"># df = spark.read.csv(&quot;path/to/your/file.csv&quot;, header=True, inferSchema=True)
# df.show()
</code></pre>
<h5 id="transformaciones-de-dataframes">Transformaciones de DataFrames</h5>
<p>Las transformaciones en DataFrames son operaciones <em>lazy</em> (perezosas), lo que significa que no se ejecutan hasta que se invoca una acción. Esto permite a Spark optimizar el plan de ejecución.</p>
<ol>
<li><strong>Selección de columnas (<code>select</code> y <code>withColumn</code>):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.appName(&quot;DataFrameTransformations&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 1, &quot;NY&quot;), (&quot;Bob&quot;, 2, &quot;LA&quot;), (&quot;Charlie&quot;, 3, &quot;CHI&quot;)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;id&quot;, &quot;city&quot;])

# Seleccionar columnas específicas
df.select(&quot;name&quot;, &quot;city&quot;).show()
# Resultado:
# +-------+----+
# |   name|city|
# +-------+----+
# |  Alice|  NY|
# |    Bob|  LA|
# |Charlie| CHI|
# +-------+----+

# Renombrar una columna al seleccionar
df.select(col(&quot;name&quot;).alias(&quot;full_name&quot;), &quot;city&quot;).show()
# Resultado:
# +---------+----+
# |full_name|city|
# +---------+----+
# |    Alice|  NY|
# |      Bob|  LA|
# |  Charlie| CHI|
# +---------+----+

# Añadir una nueva columna
df.withColumn(&quot;country&quot;, lit(&quot;USA&quot;)).show()
# Resultado:
# +-------+---+----+-------+
# |   name| id|city|country|
# +-------+---+----+-------+
# |  Alice|  1|  NY|    USA|
# |    Bob|  2|  LA|    USA|
# |Charlie|  3| CHI|    USA|
# +-------+---+----+-------+

# Modificar una columna existente (ejemplo: incrementar id)
df.withColumn(&quot;id_plus_10&quot;, col(&quot;id&quot;) + 10).show()
# Resultado:
# +-------+---+----+----------+
# |   name| id|city|id_plus_10|
# +-------+---+----+----------+
# |  Alice|  1|  NY|        11|
# |    Bob|  2|  LA|        12|
# |Charlie|  3| CHI|        13|
# +-------+---+----+----------+
</code></pre>
<ol>
<li><strong>Filtrado de filas (<code>filter</code> o <code>where</code>):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName(&quot;DataFrameFiltering&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 25, &quot;NY&quot;), (&quot;Bob&quot;, 30, &quot;LA&quot;), (&quot;Charlie&quot;, 22, &quot;CHI&quot;), (&quot;David&quot;, 35, &quot;NY&quot;)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;])

# Filtrar por una condición simple
df.filter(col(&quot;age&quot;) &gt; 25).show()
# Resultado:
# +-----+---+----+
# | name|age|city|
# +-----+---+----+
# |  Bob| 30|  LA|
# |David| 35|  NY|
# +-----+---+----+

# Filtrar por múltiples condiciones
df.filter((col(&quot;age&quot;) &gt; 20) &amp; (col(&quot;city&quot;) == &quot;NY&quot;)).show()
# Resultado:
# +-----+---+----+
# | name|age|city|
# +-----+---+----+
# |Alice| 25|  NY|
# |David| 35|  NY|
# +-----+---+----+

# Usando el método where (alias de filter)
df.where(col(&quot;name&quot;).like(&quot;A%&quot;)).show()
# Resultado:
# +-----+---+----+
# | name|age|city|
# +-----+---+----+
# |Alice| 25|  NY|
# +-----+---+----+
</code></pre>
<ol>
<li><strong>Agregaciones (<code>groupBy</code> y funciones de agregación):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, count, sum, min, max

spark = SparkSession.builder.appName(&quot;DataFrameAggregations&quot;).getOrCreate()

data = [(&quot;Dept1&quot;, &quot;Alice&quot;, 1000), (&quot;Dept1&quot;, &quot;Bob&quot;, 1200), (&quot;Dept2&quot;, &quot;Charlie&quot;, 900), (&quot;Dept2&quot;, &quot;David&quot;, 1500)]
df = spark.createDataFrame(data, [&quot;department&quot;, &quot;name&quot;, &quot;salary&quot;])

# Contar empleados por departamento
df.groupBy(&quot;department&quot;).count().show()
# Resultado:
# +----------+-----+
# |department|count|
# +----------+-----+
# |     Dept1|    2|
# |     Dept2|    2|
# +----------+-----+

# Calcular salario promedio y máximo por departamento
df.groupBy(&quot;department&quot;).agg(avg(&quot;salary&quot;).alias(&quot;avg_salary&quot;),
                             max(&quot;salary&quot;).alias(&quot;max_salary&quot;)).show()
# Resultado:
# +----------+----------+----------+
# |department|avg_salary|max_salary|
# +----------+----------+----------+
# |     Dept1|    1100.0|      1200|
# |     Dept2|    1200.0|      1500|
# +----------+----------+----------+

# Sumar salarios por departamento
df.groupBy(&quot;department&quot;).agg(sum(&quot;salary&quot;).alias(&quot;total_salary&quot;)).show()
# Resultado:
# +----------+------------+
# |department|total_salary|
# +----------+------------+
# |     Dept1|        2200|
# |     Dept2|        2400|
# +----------+------------+
</code></pre>
<h5 id="acciones-de-dataframes">Acciones de DataFrames</h5>
<p>Las acciones son operaciones que disparan la ejecución del plan de transformaciones y devuelven un resultado a la aplicación del controlador o escriben datos en un sistema de almacenamiento.</p>
<ol>
<li><strong>Mostrar datos (<code>show</code>):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;DataFrameActions&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 25), (&quot;Bob&quot;, 30)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;])

# Mostrar las primeras filas del DataFrame
df.show()
# Resultado:
# +-----+---+
# | name|age|
# +-----+---+
# |Alice| 25|
# |  Bob| 30|
# +-----+---+

# Mostrar un número específico de filas y truncar el contenido de las columnas si es largo
df.show(numRows=1, truncate=False)
# Resultado:
# +-----+---+
# |name |age|
# +-----+---+
# |Alice|25 |
# +-----+---+
</code></pre>
<ol>
<li><strong>Contar filas (<code>count</code>):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;DataFrameActions&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Charlie&quot;, 22)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;])

# Contar el número total de filas en el DataFrame
num_rows = df.count()
print(f&quot;Número de filas: {num_rows}&quot;)
# Resultado: Número de filas: 3
</code></pre>
<ol>
<li><strong>Recopilar datos (<code>collect</code>, <code>take</code>, <code>first</code>):</strong></li>
</ol>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;DataFrameActions&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Charlie&quot;, 22)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;])

# Recopilar todos los datos del DataFrame en una lista de Rows en el driver
all_data = df.collect()
print(f&quot;Todos los datos: {all_data}&quot;)
# Resultado: Todos los datos: [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=22)]

# Tomar las primeras N filas
first_two = df.take(2)
print(f&quot;Primeras 2 filas: {first_two}&quot;)
# Resultado: Primeras 2 filas: [Row(name='Alice', age=25), Row(name='Bob', age=30)]

# Obtener la primera fila
first_row = df.first()
print(f&quot;Primera fila: {first_row}&quot;)
# Resultado: Primera fila: Row(name='Alice', age=25)
</code></pre>
<h3 id="212-esquemas-y-tipos-de-datos-complejos">2.1.2 Esquemas y tipos de datos complejos</h3>
<p>El esquema de un DataFrame es una estructura fundamental que define los nombres de las columnas y sus tipos de datos correspondientes. Esta metadata es crucial para la optimización de Spark, ya que le permite saber cómo se organizan los datos y aplicar optimizaciones de tipo de datos y de columna. La inferencia de esquema y la definición explícita son dos formas de manejarlo, y Spark también soporta tipos de datos complejos como <code>ArrayType</code>, <code>MapType</code> y <code>StructType</code> para manejar estructuras anidadas.</p>
<h5 id="inferencia-y-definicion-explicita-de-esquemas">Inferencia y Definición Explícita de Esquemas</h5>
<p>La forma en que Spark determina el esquema de un DataFrame es vital para la integridad y eficiencia del procesamiento de datos.</p>
<ol>
<li><strong>Inferencia de esquema (<code>inferSchema=True</code>):</strong></li>
</ol>
<p>Spark puede intentar adivinar el esquema de un archivo de datos (CSV, JSON, Parquet, etc.) leyendo una muestra. Esto es conveniente para la exploración inicial, pero puede ser propenso a errores, especialmente con datos inconsistentes o tipos ambiguos.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;SchemaInference&quot;).getOrCreate()

# Creando un archivo CSV de ejemplo
data_csv = &quot;&quot;&quot;name,age,city
Alice,25,NY
Bob,30,LA
Charlie,null,CHI
David,35,NY&quot;&quot;&quot;
with open(&quot;data.csv&quot;, &quot;w&quot;) as f:
    f.write(data_csv)

# Inferencia de esquema al leer un CSV
df_inferred = spark.read.csv(&quot;data.csv&quot;, header=True, inferSchema=True)
df_inferred.printSchema()
# Resultado (ejemplo):
# root
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- city: string (nullable = true)

# Observar que &quot;age&quot; se infirió como IntegerType, lo cual es correcto si no hay valores no numéricos.
# Si hubiera un valor no numérico, podría inferirse como StringType o fallar la inferencia.
</code></pre>
<ol>
<li><strong>Definición explícita de esquema (<code>StructType</code> y <code>StructField</code>):</strong></li>
</ol>
<p>Es la forma más robusta y recomendada para entornos de producción. Permite controlar con precisión los tipos de datos y la nulabilidad, evitando problemas de inferencia y mejorando el rendimiento.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType

spark = SparkSession.builder.appName(&quot;ExplicitSchema&quot;).getOrCreate()

# Definir un esquema explícito
custom_schema = StructType([
    StructField(&quot;employee_name&quot;, StringType(), True),
    StructField(&quot;employee_id&quot;, IntegerType(), False), # Not nullable
    StructField(&quot;salary&quot;, DoubleType(), True),
    StructField(&quot;is_active&quot;, BooleanType(), True)
])

data = [(&quot;Alice&quot;, 1, 50000.0, True), (&quot;Bob&quot;, 2, 60000.50, False), (&quot;Charlie&quot;, 3, 75000.0, True)]
df_explicit = spark.createDataFrame(data, schema=custom_schema)
df_explicit.printSchema()
# Resultado:
# root
#  |-- employee_name: string (nullable = true)
#  |-- employee_id: integer (nullable = false)
#  |-- salary: double (nullable = true)
#  |-- is_active: boolean (nullable = true)

# Intentar insertar un valor nulo en una columna no nula causaría un error o comportamiento inesperado
# data_error = [(&quot;David&quot;, None, 80000.0, True)] # Esto generaría un error si intentas crear el DF
# df_error = spark.createDataFrame(data_error, schema=custom_schema)
</code></pre>
<ol>
<li><strong>Acceder y manipular el esquema (<code>df.schema</code>):</strong></li>
</ol>
<p>El esquema de un DataFrame es accesible a través del atributo <code>.schema</code>, que devuelve un objeto <code>StructType</code>.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName(&quot;SchemaAccess&quot;).getOrCreate()

data = [(&quot;Alice&quot;, 1), (&quot;Bob&quot;, 2)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;id&quot;])

# Acceder al esquema
print(df.schema)
# Resultado: StructType([StructField('name', StringType(), True), StructField('id', LongType(), True)])

# Iterar sobre los campos del esquema
for field in df.schema:
    print(f&quot;Nombre de columna: {field.name}, Tipo: {field.dataType}, Nulable: {field.nullable}&quot;)
# Resultado:
# Nombre de columna: name, Tipo: StringType, Nulable: True
# Nombre de columna: id, Tipo: LongType, Nulable: True
</code></pre>
<h5 id="tipos-de-datos-complejos">Tipos de Datos Complejos</h5>
<p>Spark permite manejar estructuras de datos más allá de los tipos atómicos, lo que es fundamental para trabajar con datos semi-estructurados y anidados como JSON.</p>
<ol>
<li><strong><code>StructType</code> (Estructuras Anidadas/Registros):</strong></li>
</ol>
<p>Representa una estructura similar a un objeto o un registro, donde cada campo tiene un nombre y un tipo de datos. Permite modelar objetos complejos dentro de una columna.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName(&quot;ComplexTypes&quot;).getOrCreate()

# Definir un esquema con un StructType anidado
address_schema = StructType([
    StructField(&quot;street&quot;, StringType(), True),
    StructField(&quot;city&quot;, StringType(), True),
    StructField(&quot;zip&quot;, StringType(), True)
])

person_schema = StructType([
    StructField(&quot;name&quot;, StringType(), True),
    StructField(&quot;age&quot;, IntegerType(), True),
    StructField(&quot;address&quot;, address_schema, True) # Columna de tipo StructType
])

data = [
    (&quot;Alice&quot;, 25, (&quot;123 Main St&quot;, &quot;NY&quot;, &quot;10001&quot;)),
    (&quot;Bob&quot;, 30, (&quot;456 Oak Ave&quot;, &quot;LA&quot;, &quot;90001&quot;))
]

df = spark.createDataFrame(data, schema=person_schema)
df.printSchema()
# Resultado:
# root
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- address: struct (nullable = true)
#  |    |-- street: string (nullable = true)
#  |    |-- city: string (nullable = true)
#  |    |-- zip: string (nullable = true)

df.show(truncate=False)
# Resultado:
# +-----+---+-------------------------+
# |name |age|address                  |
# +-----+---+-------------------------+
# |Alice|25 |{123 Main St, NY, 10001} |
# |Bob  |30 |{456 Oak Ave, LA, 90001} |
# +-----+---+-------------------------+

# Acceder a campos anidados
df.select(&quot;name&quot;, &quot;address.city&quot;).show()
# Resultado:
# +-----+----+
# |name |city|
# +-----+----+
# |Alice|NY  |
# |Bob  |LA  |
# +-----+----+
</code></pre>
<ol>
<li><strong><code>ArrayType</code> (Arrays/Listas):</strong></li>
</ol>
<p>Representa una colección de elementos del mismo tipo. Útil para modelar listas o arreglos de datos dentro de una columna.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType

spark = SparkSession.builder.appName(&quot;ComplexTypesArray&quot;).getOrCreate()

# Definir un esquema con un ArrayType
course_schema = StructType([
    StructField(&quot;student_name&quot;, StringType(), True),
    StructField(&quot;grades&quot;, ArrayType(IntegerType()), True) # Columna de tipo ArrayType
])

data = [
    (&quot;Alice&quot;, [90, 85, 92]),
    (&quot;Bob&quot;, [78, 80]),
    (&quot;Charlie&quot;, [])
]

df = spark.createDataFrame(data, schema=course_schema)
df.printSchema()
# Resultado:
# root
#  |-- student_name: string (nullable = true)
#  |-- grades: array (nullable = true)
#  |    |-- element: integer (containsNull = true)

df.show(truncate=False)
# Resultado:
# +------------+----------+
# |student_name|grades    |
# +------------+----------+
# |Alice       |[90, 85, 92]|
# |Bob         |[78, 80]  |
# |Charlie     |[]        |
# +------------+----------+

# Acceder a elementos de array (requiere funciones de Spark)
from pyspark.sql.functions import size, array_contains
df.select(&quot;student_name&quot;, size(&quot;grades&quot;).alias(&quot;num_grades&quot;)).show()
# Resultado:
# +------------+----------+
# |student_name|num_grades|
# +------------+----------+
# |       Alice|         3|
# |         Bob|         2|
# |     Charlie|         0|
# +------------+----------+

df.filter(array_contains(&quot;grades&quot;, 90)).show()
# Resultado:
# +------------+----------+
# |student_name|    grades|
# +------------+----------+
# |       Alice|[90, 85, 92]|
# +------------+----------+
</code></pre>
<ol>
<li><strong><code>MapType</code> (Mapas/Diccionarios):</strong></li>
</ol>
<p>Representa una colección de pares clave-valor. Útil para datos que se asemejan a diccionarios o JSON con claves dinámicas.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, MapType

spark = SparkSession.builder.appName(&quot;ComplexTypesMap&quot;).getOrCreate()

# Definir un esquema con un MapType
product_schema = StructType([
    StructField(&quot;product_id&quot;, StringType(), True),
    StructField(&quot;attributes&quot;, MapType(StringType(), StringType()), True) # Columna de tipo MapType
])

data = [
    (&quot;P101&quot;, {&quot;color&quot;: &quot;red&quot;, &quot;size&quot;: &quot;M&quot;, &quot;material&quot;: &quot;cotton&quot;}),
    (&quot;P102&quot;, {&quot;color&quot;: &quot;blue&quot;, &quot;size&quot;: &quot;L&quot;}),
    (&quot;P103&quot;, {})
]

df = spark.createDataFrame(data, schema=product_schema)
df.printSchema()
# Resultado:
# root
#  |-- product_id: string (nullable = true)
#  |-- attributes: map (nullable = true)
#  |    |-- key: string
#  |    |-- value: string (containsNull = true)

df.show(truncate=False)
# Resultado:
# +----------+-----------------------------------+
# |product_id|attributes                         |
# +----------+-----------------------------------+
# |P101      |{color -&gt; red, size -&gt; M, material -&gt; cotton}|
# |P102      |{color -&gt; blue, size -&gt; L}         |
# |P103      |{}                                 |
# +----------+-----------------------------------+

# Acceder a elementos de mapa (se usa con `getItem` o notación de corchetes)
from pyspark.sql.functions import col
df.select(&quot;product_id&quot;, col(&quot;attributes&quot;)[&quot;color&quot;].alias(&quot;product_color&quot;)).show()
# Resultado:
# +----------+-------------+
# |product_id|product_color|
# +----------+-------------+
# |      P101|          red|
# |      P102|         blue|
# |      P103|         null|
# +----------+-------------+
</code></pre>
<h3 id="213-lectura-y-escritura-en-formatos-populares-parquet-avro-orc-csv-json">2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON)</h3>
<p>Spark es versátil en la lectura y escritura de datos, soportando una amplia gama de formatos de archivo. La elección del formato adecuado es crucial para el rendimiento y la eficiencia del almacenamiento en entornos de Big Data. Los formatos columnares como Parquet y ORC son altamente recomendados para el análisis debido a su eficiencia en la lectura y compresión.</p>
<h5 id="lectura-de-datos">Lectura de Datos</h5>
<p>La lectura de datos es la base para cualquier análisis. Spark proporciona un API <code>spark.read</code> muy flexible para cargar datos desde diversas fuentes.</p>
<ol>
<li><strong>Lectura de archivos CSV:</strong></li>
</ol>
<p>Ideal para datos tabulares simples. Es importante configurar <code>header=True</code> si el archivo tiene encabezados y <code>inferSchema=True</code> para que Spark intente adivinar los tipos de datos.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;ReadWriteCSV&quot;).getOrCreate()

# Crear un archivo CSV de ejemplo
data_csv = &quot;&quot;&quot;id,name,age,city
1,Alice,25,New York
2,Bob,30,Los Angeles
3,Charlie,22,Chicago&quot;&quot;&quot;
with open(&quot;users.csv&quot;, &quot;w&quot;) as f:
    f.write(data_csv)

# Leer un archivo CSV
df_csv = spark.read.csv(&quot;users.csv&quot;, header=True, inferSchema=True)
df_csv.printSchema()
df_csv.show()
# Resultado:
# root
#  |-- id: integer (nullable = true)
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- city: string (nullable = true)
# +---+-------+---+----------+
# | id|   name|age|      city|
# +---+-------+---+----------+
# |  1|  Alice| 25|  New York|
# |  2|    Bob| 30|Los Angeles|
# |  3|Charlie| 22|   Chicago|
# +---+-------+---+----------+
</code></pre>
<ol>
<li><strong>Lectura de archivos JSON:</strong></li>
</ol>
<p>Útil para datos semi-estructurados. Spark puede inferir el esquema automáticamente.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;ReadWriteJSON&quot;).getOrCreate()

# Crear un archivo JSON de ejemplo
data_json = &quot;&quot;&quot;
{&quot;id&quot;: 1, &quot;name&quot;: &quot;Alice&quot;, &quot;hobbies&quot;: [&quot;reading&quot;, &quot;hiking&quot;]}
{&quot;id&quot;: 2, &quot;name&quot;: &quot;Bob&quot;, &quot;hobbies&quot;: [&quot;gaming&quot;]}
{&quot;id&quot;: 3, &quot;name&quot;: &quot;Charlie&quot;, &quot;hobbies&quot;: []}
&quot;&quot;&quot;
with open(&quot;users.json&quot;, &quot;w&quot;) as f:
    f.write(data_json)

# Leer un archivo JSON
df_json = spark.read.json(&quot;users.json&quot;)
df_json.printSchema()
df_json.show(truncate=False)
# Resultado:
# root
#  |-- hobbies: array (nullable = true)
#  |    |-- element: string (containsNull = true)
#  |-- id: long (nullable = true)
#  |-- name: string (nullable = true)
# +--------------------+---+-------+
# |hobbies             |id |name   |
# +--------------------+---+-------+
# |[reading, hiking]   |1  |Alice  |
# |[gaming]            |2  |Bob    |
# |[]                  |3  |Charlie|
# +--------------------+---+-------+
</code></pre>
<ol>
<li><strong>Lectura de archivos Parquet:</strong></li>
</ol>
<p>Formato columnar altamente eficiente para Big Data. Spark lo usa como formato por defecto y es altamente optimizado para el rendimiento.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;ReadWriteParquet&quot;).getOrCreate()

# Primero, creamos un DataFrame y lo guardamos como Parquet
data = [(&quot;Alice&quot;, 25, &quot;NY&quot;), (&quot;Bob&quot;, 30, &quot;LA&quot;)]
df = spark.createDataFrame(data, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;])
df.write.mode(&quot;overwrite&quot;).parquet(&quot;users.parquet&quot;)

# Leer un archivo Parquet
df_parquet = spark.read.parquet(&quot;users.parquet&quot;)
df_parquet.printSchema()
df_parquet.show()
# Resultado:
# root
#  |-- name: string (nullable = true)
#  |-- age: long (nullable = true)
#  |-- city: string (nullable = true)
# +-----+---+----+
# | name|age|city|
# +-----+---+----+
# |Alice| 25|  NY|
# |  Bob| 30|  LA|
# +-----+---+----+
</code></pre>
<ol>
<li><strong>Lectura de archivos ORC:</strong></li>
</ol>
<p>Otro formato columnar optimizado para Big Data, desarrollado por Apache Hive. Ofrece compresión y rendimiento similares a Parquet.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;ReadWriteORC&quot;).getOrCreate()

# Primero, creamos un DataFrame y lo guardamos como ORC
data = [(&quot;ProductA&quot;, 100, &quot;Electronics&quot;), (&quot;ProductB&quot;, 50, &quot;Books&quot;)]
df = spark.createDataFrame(data, [&quot;product_name&quot;, &quot;price&quot;, &quot;category&quot;])
df.write.mode(&quot;overwrite&quot;).orc(&quot;products.orc&quot;)

# Leer un archivo ORC
df_orc = spark.read.orc(&quot;products.orc&quot;)
df_orc.printSchema()
df_orc.show()
# Resultado:
# root
#  |-- product_name: string (nullable = true)
#  |-- price: long (nullable = true)
#  |-- category: string (nullable = true)
# +------------+-----+-----------+
# |product_name|price|   category|
# +------------+-----+-----------+
# |    ProductA|  100|Electronics|
# |    ProductB|   50|      Books|
# +------------+-----+-----------+
</code></pre>
<ol>
<li><strong>Lectura de archivos Avro:</strong></li>
</ol>
<p>Formato de serialización de datos basado en esquema. Requiere el paquete <code>spark-avro</code>.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;ReadWriteAvro&quot;) \
    .config(&quot;spark.jars.packages&quot;, &quot;org.apache.spark:spark-avro_2.12:3.5.0&quot;) \
    .getOrCreate()

# Primero, creamos un DataFrame y lo guardamos como Avro
data = [(&quot;Event1&quot;, &quot;typeA&quot;, 1678886400), (&quot;Event2&quot;, &quot;typeB&quot;, 1678886460)]
df = spark.createDataFrame(data, [&quot;event_id&quot;, &quot;event_type&quot;, &quot;timestamp&quot;])
df.write.mode(&quot;overwrite&quot;).format(&quot;avro&quot;).save(&quot;events.avro&quot;)

# Leer un archivo Avro
df_avro = spark.read.format(&quot;avro&quot;).load(&quot;events.avro&quot;)
df_avro.printSchema()
df_avro.show()
# Resultado:
# root
#  |-- event_id: string (nullable = true)
#  |-- event_type: string (nullable = true)
#  |-- timestamp: long (nullable = true)
# +--------+----------+----------+
# |event_id|event_type| timestamp|
# +--------+----------+----------+
# |  Event1|     typeA|1678886400|
# |  Event2|     typeB|1678886460|
# +--------+----------+----------+
</code></pre>
<h5 id="escritura-de-datos">Escritura de Datos</h5>
<p>La escritura de DataFrames a diferentes formatos es tan importante como su lectura, ya que permite persistir los resultados de las transformaciones y compartirlos con otras aplicaciones o sistemas.</p>
<ol>
<li><strong>Modos de escritura (<code>mode</code>):</strong></li>
</ol>
<p>Cuando se escribe un DataFrame, es fundamental especificar el modo de escritura para evitar pérdidas de datos o errores.</p>
<ul>
<li><code>overwrite</code>: Sobrescribe los datos existentes en la ubicación de destino.</li>
<li><code>append</code>: Añade los datos al final de los datos existentes.</li>
<li><code>ignore</code>: Si los datos ya existen, la operación de escritura no hace nada.</li>
<li><code>error</code> (o <code>errorIfExists</code>): Lanza un error si los datos ya existen (modo por defecto).</li>
</ul>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;WriteModes&quot;).getOrCreate()

data = [(&quot;A&quot;, 1), (&quot;B&quot;, 2)]
df = spark.createDataFrame(data, [&quot;col1&quot;, &quot;col2&quot;])

# Escribir en modo 'overwrite'
df.write.mode(&quot;overwrite&quot;).parquet(&quot;output_data.parquet&quot;)

# Escribir en modo 'append'
data_new = [(&quot;C&quot;, 3)]
df_new = spark.createDataFrame(data_new, [&quot;col1&quot;, &quot;col2&quot;])
df_new.write.mode(&quot;append&quot;).parquet(&quot;output_data.parquet&quot;)

# Verificar el contenido
spark.read.parquet(&quot;output_data.parquet&quot;).show()
# Resultado:
# +----+----+
# |col1|col2|
# +----+----+
# |   A|   1|
# |   B|   2|
# |   C|   3|
# +----+----+

# Escribir en modo 'ignore' (si el archivo ya existe, no hará nada)
df.write.mode(&quot;ignore&quot;).csv(&quot;output_csv&quot;, header=True)
</code></pre>
<ol>
<li><strong>Particionamiento de salida (<code>partitionBy</code>):</strong></li>
</ol>
<p>Permite organizar los datos en el sistema de archivos subyacente (HDFS, S3, ADLS) en directorios basados en el valor de una o más columnas. Esto mejora el rendimiento de lectura para consultas que filtran por las columnas de partición.</p>
<pre><code class="language-python">from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;PartitionBy&quot;).getOrCreate()

data = [(&quot;Sales&quot;, 2023, 100), (&quot;Sales&quot;, 2024, 120), (&quot;Marketing&quot;, 2023, 80), (&quot;Marketing&quot;, 2024, 90)]
df = spark.createDataFrame(data, [&quot;department&quot;, &quot;year&quot;, &quot;revenue&quot;])

# Escribir con particionamiento por 'department' y 'year'
df.write.mode(&quot;overwrite&quot;).partitionBy(&quot;department&quot;, &quot;year&quot;).parquet(&quot;department_yearly_revenue.parquet&quot;)

# Esto creará una estructura de directorios como:
# department_yearly_revenue.parquet/department=Sales/year=2023/part-....parquet
# department_yearly_revenue.parquet/department=Sales/year=2024/part-....parquet
# ...
</code></pre>
<ol>
<li><strong>Manejo de directorios de salida:</strong></li>
</ol>
<p>Spark crea un directorio para cada operación de escritura. Dentro de este directorio, se encuentran los archivos de datos (partes) y un archivo <code>_SUCCESS</code> si la operación fue exitosa.</p>
<pre><code class="language-python"># Después de ejecutar una escritura, puedes explorar la estructura de directorios.
# Por ejemplo, para el caso de Parquet sin particionamiento:
# ls -R users.parquet/
# Resultado (ejemplo):
# users.parquet/:
# _SUCCESS/
# part-00000-....snappy.parquet/
</code></pre>
<h2 id="tarea">Tarea</h2>
<p><strong>Ejercicios con PySpark</strong>:</p>
<ol>
<li>
<p>Crea un DataFrame a partir de la siguiente lista de tuplas: <code>[("Juan", "Perez", 30, "Ingeniero"), ("Maria", "Lopez", 25, "Doctora"), ("Carlos", "Gomez", 35, "Abogado")]</code>. Define el esquema explícitamente con las columnas <code>nombre</code>, <code>apellido</code>, <code>edad</code> (entero) y <code>profesion</code>. Luego, muestra el esquema y las primeras filas del DataFrame.</p>
</li>
<li>
<p>Dado el DataFrame del ejercicio 1, selecciona únicamente las columnas <code>nombre</code> y <code>profesion</code>. Además, renombra la columna <code>nombre</code> a <code>primer_nombre</code> en el DataFrame resultante.</p>
</li>
<li>
<p>Al DataFrame original del ejercicio 1, añade una nueva columna llamada <code>salario_base</code> con un valor fijo de <code>50000</code>. Luego, crea otra columna <code>salario_ajustado</code> que sea <code>salario_base</code> más <code>edad * 100</code>.</p>
</li>
<li>
<p>Filtra el DataFrame resultante del ejercicio 3 para mostrar solo las personas cuya <code>edad</code> sea mayor a <code>28</code> Y su <code>profesion</code> sea <code>Ingeniero</code> o <code>Abogado</code>.</p>
</li>
<li>
<p>Utilizando el DataFrame original del ejercicio 1, calcula el promedio de <code>edad</code> y la cantidad total de personas.</p>
</li>
<li>
<p>Crea un DataFrame de empleados que incluya una columna <code>contacto</code> de tipo <code>StructType</code> con <code>email</code> y <code>telefono</code> como subcampos. Los datos de ejemplo podrían ser: <code>[("Alice", {"email": "alice@example.com", "telefono": "123-456-7890"})]</code>. Muestra el esquema y accede al <code>email</code> de Alice.</p>
</li>
<li>
<p>Crea un DataFrame de estudiantes con una columna <code>cursos_inscritos</code> de tipo <code>ArrayType(StringType())</code>. Ejemplo de datos: <code>[("Bob", ["Matemáticas", "Física"]), ("Eve", ["Química"])]</code>. Muestra el esquema y filtra los estudiantes que estén inscritos en <code>Matemáticas</code>.</p>
</li>
<li>
<p>Crea un archivo CSV llamado <code>productos.csv</code> con los siguientes datos (incluye encabezado):</p>
</li>
</ol>
<pre><code class="language-csv">producto_id,nombre,precio,cantidad
1,Laptop,1200.50,10
2,Mouse,25.00,50
3,Teclado,75.99,30
</code></pre>
<p>Lee este archivo en un DataFrame, infiriendo el esquema y mostrando el esquema y el contenido.</p>
<ol>
<li>
<p>Crea un DataFrame con columnas <code>region</code>, <code>mes</code> y <code>ventas</code>. Los datos de ejemplo: <code>[("Norte", "Enero", 1000), ("Sur", "Enero", 800), ("Norte", "Febrero", 1100), ("Sur", "Febrero", 900)]</code>. Guarda este DataFrame como archivos Parquet, particionando por <code>region</code> y <code>mes</code>. Luego, lee solo las ventas de la región <code>Norte</code> en <code>Enero</code> para verificar la partición.</p>
</li>
<li>
<p>Crea un archivo JSON llamado <code>config.json</code> con los siguientes datos (cada objeto en una línea):</p>
</li>
</ol>
<pre><code class="language-json">{&quot;id&quot;: 1, &quot;settings&quot;: {&quot;theme&quot;: &quot;dark&quot;, &quot;notifications&quot;: true}}
{&quot;id&quot;: 2, &quot;settings&quot;: {&quot;theme&quot;: &quot;light&quot;, &quot;notifications&quot;: false}}
</code></pre>
<p>Lee este archivo en un DataFrame y muestra el <code>theme</code> para cada ID.</p>
<p><strong>Ejercicios con SparkSQL</strong>:</p>
<ol>
<li>
<p>Crea el mismo DataFrame del Ejercicio 1 de PySpark (empleados). Registra este DataFrame como una vista temporal llamada <code>empleados_temp</code>. Luego, ejecuta una consulta SQL para seleccionar todos los empleados.</p>
</li>
<li>
<p>Usando la vista <code>empleados_temp</code>, escribe una consulta SparkSQL para seleccionar <code>nombre</code>, <code>apellido</code> y <code>profesion</code> de los empleados con <code>edad</code> menor a <code>30</code>.</p>
</li>
<li>
<p>Sobre la vista <code>empleados_temp</code>, realiza una consulta SQL que seleccione <code>nombre</code> como <code>primer_nombre</code> y <code>profesion</code> como <code>ocupacion</code>.</p>
</li>
<li>
<p>Utilizando <code>empleados_temp</code>, añade una columna calculada llamada <code>edad_futura</code> que sea la <code>edad</code> actual más <code>5</code>.</p>
</li>
<li>
<p>Crea una vista temporal a partir de un DataFrame de <code>ventas</code> con columnas <code>producto</code>, <code>region</code> y <code>cantidad</code>. Datos de ejemplo: <code>[("Laptop", "Norte", 5), ("Mouse", "Norte", 10), ("Laptop", "Sur", 3)]</code>. Calcula la <code>SUM</code> de <code>cantidad</code> por <code>producto</code> usando SparkSQL.</p>
</li>
<li>
<p>Partiendo del DataFrame de empleados con <code>contacto</code> (email y telefono) del Ejercicio 6 de PySpark, crea una vista temporal. Luego, usa SparkSQL para seleccionar el <code>nombre</code> del empleado y su <code>contacto.email</code>.</p>
</li>
<li>
<p>Utilizando el DataFrame de estudiantes con <code>cursos_inscritos</code> del Ejercicio 7 de PySpark, crea una vista temporal. Escribe una consulta SparkSQL para seleccionar los estudiantes que tienen <code>Matemáticas</code> en su lista de <code>cursos_inscritos</code> (puedes necesitar una función SQL de Spark para arrays).</p>
</li>
<li>
<p>Crea el archivo <code>productos.csv</code> del Ejercicio 8 de PySpark. Luego, usando <code>spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("productos.csv")</code>, crea un DataFrame y regístralo como vista temporal <code>productos_temp</code>. Finalmente, selecciona todos los productos con un <code>precio</code> mayor a <code>50</code>.</p>
</li>
<li>
<p>Guarda un DataFrame (por ejemplo, el de ventas del Ejercicio 9 de PySpark) como archivos Parquet en una ubicación específica (ej: <code>"data/ventas_particionadas"</code>). Luego, crea una tabla externa de SparkSQL apuntando a esa ubicación (<code>CREATE TABLE ... USING PARQUET LOCATION ...</code>). Finalmente, consulta las ventas de una <code>region</code> específica directamente desde la tabla SQL.</p>
</li>
<li>
<p>Usando el archivo <code>config.json</code> del Ejercicio 10 de PySpark, lee el JSON y crea una vista temporal <code>config_temp</code>. Escribe una consulta SparkSQL para extraer el valor del <code>theme</code> de la columna <code>settings</code> para cada <code>id</code> (esto requerirá desanidación o funciones JSON de SparkSQL).</p>
</li>
</ol>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../tema15/" class="btn btn-neutral float-left" title="Primeros pasos con PySpark"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tema22/" class="btn btn-neutral float-right" title="Manipulación y Transformación de Datos">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../tema15/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tema22/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
