<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>DAGs, operadores y tareas - M√©todos de Procesamiento y An√°lisis de Big Data</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "DAGs, operadores y tareas";
        var mkdocs_page_input_path = "tema42.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> M√©todos de Procesamiento y An√°lisis de Big Data
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Inicio</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Introducci√≥n</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema11/">Fundamentos de Big Data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema12/">Introducci√≥n al ecosistema Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema13/">RDD, DataFrame y Dataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema14/">Instalaci√≥n y configuraci√≥n de Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema15/">Primeros pasos con PySpark</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">PySpark y SparkSQL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema21/">Fundamentos de DataFrames en Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema22/">Manipulaci√≥n y Transformaci√≥n de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema23/">Consultas y SQL en Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema24/">Optimizaci√≥n y Rendimiento</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Arquitectura y Dise√±o de Flujos ETL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema31/">Dise√±o y Orquestaci√≥n de Pipelines ETL</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema32/">Conexi√≥n a M√∫ltiples Fuentes de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema33/">Procesamiento Escalable y Particionamiento</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema34/">Manejo de Esquemas y Calidad de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema35/">Monitorizaci√≥n y Troubleshooting de Pipelines</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema36/">Seguridad en ETL y Protecci√≥n de Datos</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema37/">Patrones de Dise√±o y Optimizaci√≥n en la Nube</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Automatizaci√≥n y Orquestaci√≥n con Apache Airflow</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../tema41/">Arquitectura y componentes de Airflow</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">DAGs, operadores y tareas</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#42-dags-operadores-y-tareas">4.2. DAGs, operadores y tareas</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#421-definicion-y-estructura-de-dags">4.2.1 Definici√≥n y estructura de DAGs</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#parametros-esenciales">Par√°metros Esenciales</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#estructura-basica-de-un-dag">Estructura b√°sica de un DAG</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#dag-completo-y-detallado">DAG Completo y Detallado</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#422-operadores-built-in-y-custom-operators">4.2.2 Operadores built-in y custom operators</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-de-ejecucion-basica">Operadores de Ejecuci√≥n B√°sica</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-de-bases-de-datos">Operadores de Bases de Datos</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-de-transferencia">Operadores de Transferencia</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-de-control-de-flujo">Operadores de Control de Flujo</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-para-spark-y-hadoop">Operadores para Spark y Hadoop</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-para-streaming-y-hdfs">Operadores para Streaming y HDFS</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-para-validacion-de-datos">Operadores para Validaci√≥n de Datos</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#operadores-para-mlops">Operadores para MLOps</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#423-taskgroups">4.2.3 TaskGroups</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#uso-de-taskgroups">Uso de TaskGroups</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#424-templating-con-jinja2">4.2.4 Templating con Jinja2</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#uso-de-variables-y-contextos">Uso de variables y contextos</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#acceso-a-macros-y-funciones-de-utilidad">Acceso a macros y funciones de utilidad</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#templating-avanzado-en-operadores-python">Templating avanzado en operadores Python</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#tarea">Tarea</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema43/">Integraci√≥n con ecosistema Big Data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema44/">Monitoreo, logging y manejo de dependencias</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Proyecto Integrador y Despliegue</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tema51/">Desarrollo del proyecto integrador</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tema52/">Despliegue en nube</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">M√©todos de Procesamiento y An√°lisis de Big Data</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Automatizaci√≥n y Orquestaci√≥n con Apache Airflow</li>
      <li class="breadcrumb-item active">DAGs, operadores y tareas</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="4-automatizacion-y-orquestacion-con-apache-airflow">4. Automatizaci√≥n y Orquestaci√≥n con Apache Airflow</h1>
<h2 id="42-dags-operadores-y-tareas">4.2. DAGs, operadores y tareas</h2>
<p><strong>Objetivo</strong>:</p>
<p>Comprender, dise√±ar y construir flujos de trabajo programables en Apache Airflow utilizando DAGs, operadores y tareas, incluyendo su agrupaci√≥n, parametrizaci√≥n y personalizaci√≥n para gestionar eficientemente flujos de datos complejos en entornos Big Data.</p>
<p><strong>Introducci√≥n</strong>:</p>
<p>Apache Airflow permite definir y gestionar flujos de trabajo como c√≥digo, a trav√©s de DAGs (Directed Acyclic Graphs), que representan secuencias de tareas con dependencias expl√≠citas. Este enfoque permite un control detallado del orden de ejecuci√≥n, la l√≥gica condicional, la integraci√≥n con diversas herramientas y servicios, y la reutilizaci√≥n de c√≥digo para tareas repetitivas. En este tema se profundiza en c√≥mo construir DAGs robustos, emplear operadores integrados y personalizados, utilizar estructuras avanzadas como TaskGroups y SubDAGs, y aplicar templating din√°mico con Jinja2.</p>
<p><strong>Desarrollo</strong>:</p>
<p>La definici√≥n de flujos de trabajo (pipelines) en Apache Airflow se basa en DAGs que agrupan tareas individuales y definen la l√≥gica de ejecuci√≥n. Cada tarea puede realizar operaciones diversas como ejecutar comandos del sistema, procesar datos con Spark, mover datos entre servicios cloud, o enviar notificaciones. Airflow proporciona una gran variedad de operadores listos para usar y permite crear operadores personalizados. Tambi√©n ofrece herramientas para organizar tareas y aplicar plantillas din√°micas, lo cual facilita el mantenimiento y escalabilidad de flujos complejos.</p>
<h3 id="421-definicion-y-estructura-de-dags">4.2.1 Definici√≥n y estructura de DAGs</h3>
<p>Un <strong>DAG</strong> (Directed Acyclic Graph) es la pieza fundamental de Apache Airflow que representa un flujo de trabajo completo como un grafo dirigido sin ciclos.</p>
<p>Cada DAG puede ser:</p>
<ul>
<li><strong>Dirigido</strong>: Las tareas tienen un orden espec√≠fico de ejecuci√≥n (flujo direccional)</li>
<li><strong>Ac√≠clico</strong>: No puede haber bucles infinitos o dependencias circulares</li>
<li><strong>Grafo</strong>: Conjunto de nodos (tareas) conectados por aristas (dependencias)</li>
</ul>
<h5 id="parametros-esenciales">Par√°metros Esenciales</h5>
<p>Son los par√°metros fundamentales que definen el comportamiento b√°sico de un DAG en Apache Airflow.</p>
<pre><code class="language-python"># Par√°metros obligatorios
dag_id='nombre_unico_del_dag'     # identificador √∫nico en todo Airflow
start_date=datetime(2024, 1, 1)   # fecha desde cuando puede ejecutarse
schedule_interval='@daily'        # frecuencia de ejecuci√≥n

# Par√°metros importantes
catchup=False                     # si ejecutar ejecuciones pasadas
max_active_runs=1                 # n√∫mero m√°ximo de ejecuciones simult√°neas
default_retries=1                 # reintentos por defecto para todas las tareas
retry_delay=timedelta(minutes=5)  # tiempo entre reintentos
default_args = {                  # par√°metros que heredan todas las tareas del DAG
    'owner': 'team_data',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['alerts@empresa.com']
}
</code></pre>
<p><strong>Planificaci√≥n de la Ejecuci√≥n (Scheduling)</strong></p>
<p>Hay varias formas de configurar la frecuencias de ejecuci√≥n:</p>
<pre><code class="language-python"># Opciones de schedule_interval
'@once'        # Ejecutar solo una vez
'@hourly'      # Cada hora (0 * * * *)
'@daily'       # Diariamente a medianoche (0 0 * * *)
'@weekly'      # Semanalmente los domingos (0 0 * * 0)
'@monthly'     # Primer d√≠a de cada mes (0 0 1 * *)
'@yearly'      # 1 de enero cada a√±o (0 0 1 1 *)

# Cron personalizado
'30 6 * * 1-5' # 6:30 AM, lunes a viernes
'0 */4 * * *'  # Cada 4 horas

# Program√°tico
timedelta(hours=2)              # Cada 2 horas
timedelta(days=1, hours=12)     # Cada d√≠a y medio
</code></pre>
<h5 id="estructura-basica-de-un-dag">Estructura b√°sica de un DAG</h5>
<pre><code class="language-python">from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='mi_primer_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['ejemplo']
) as dag:

    tarea_1 = BashOperator(
        task_id='imprimir_fecha',
        bash_command='date'
    )

    tarea_2 = BashOperator(
        task_id='mostrar_hola_mundo',
        bash_command='echo &quot;Hola mundo desde Airflow!&quot;'
    )

    tarea_1 &gt;&gt; tarea_2  # Define la dependencia
</code></pre>
<h5 id="dag-completo-y-detallado">DAG Completo y Detallado</h5>
<p><strong>Diagrama</strong>:</p>
<pre><code class="language-mermaid">flowchart TD
    %% Estilos para diferentes tipos de tareas
    classDef verificacion fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef extraccion fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef procesamiento fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef validacion fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef reporte fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef carga fill:#e0f2f1,stroke:#004d40,stroke-width:2px
    classDef notificacion fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    classDef limpieza fill:#fafafa,stroke:#424242,stroke-width:2px

    %% Definici√≥n de nodos
    A[verificar_sistema&lt;br/&gt;üîç Verificar recursos del sistema]:::verificacion
    B[extraer_datos&lt;br/&gt;üì• Extraer datos de PostgreSQL]:::extraccion
    C[procesar_datos&lt;br/&gt;‚öôÔ∏è Transformar y limpiar datos]:::procesamiento
    D[validar_calidad&lt;br/&gt;‚úÖ Validar calidad de datos]:::validacion
    E[generar_reporte_task&lt;br/&gt;üìä Generar reporte de m√©tricas]:::reporte
    F[cargar_datos_warehouse&lt;br/&gt;üèóÔ∏è Cargar al data warehouse]:::carga
    G[enviar_notificacion&lt;br/&gt;üìß Enviar email de confirmaci√≥n]:::notificacion
    H[limpiar_archivos_temporales&lt;br/&gt;üßπ Limpiar archivos temp]:::limpieza

    %% Flujo de dependencias
    A --&gt; B
    B --&gt; C

    %% Ramificaci√≥n paralela despu√©s del procesamiento
    C --&gt; D
    C --&gt; E

    %% Convergencia para la carga (requiere procesamiento Y validaci√≥n)
    C --&gt; F
    D --&gt; F

    %% Reporte va a notificaci√≥n
    E --&gt; G

    %% Limpieza al final (despu√©s de carga Y notificaci√≥n)
    F --&gt; H
    G --&gt; H

    %% Anotaciones explicativas
    subgraph &quot;Fase 1: Preparaci√≥n&quot;
        A
        B
    end

    subgraph &quot;Fase 2: Procesamiento&quot;
        C
    end

    subgraph &quot;Fase 3: Validaci√≥n y Reportes (Paralelo)&quot;
        D
        E
    end

    subgraph &quot;Fase 4: Finalizaci√≥n&quot;
        F
        G
        H
    end
</code></pre>
<p><strong>Implementaci√≥n</strong>:</p>
<pre><code class="language-python">from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from datetime import datetime, timedelta
import pandas as pd
import logging

# Estos argumentos se aplicar√°n a todas las tareas del DAG
# a menos que se sobrescriban espec√≠ficamente
default_args = {
    'owner': 'equipo_data_engineering',          # Responsable del DAG
    'depends_on_past': False,                    # No depende de ejecuciones anteriores
    'start_date': datetime(2024, 1, 1),          # Fecha de inicio del DAG
    'email_on_failure': True,                    # Enviar email si falla
    'email_on_retry': False,                     # No enviar email en reintento
    'email': ['team@empresa.com'],               # Lista de emails para notificaciones
    'retries': 2,                                # N√∫mero de reintentos por tarea
    'retry_delay': timedelta(minutes=5),         # Tiempo entre reintentos
    'execution_timeout': timedelta(hours=2),     # Timeout m√°ximo por tarea
}

# DEFINICI√ìN DEL DAG
with DAG(
    # Identificador √∫nico del DAG en toda la instancia de Airflow
    dag_id='pipeline_procesamiento_ventas_v2',

    # Hereda la configuraci√≥n por defecto definida arriba
    default_args=default_args,

    # Descripci√≥n que aparece en la UI de Airflow
    description='Pipeline completo para procesar datos de ventas diarias',

    # Frecuencia de ejecuci√≥n - se ejecuta diariamente a las 2:00 AM
    schedule_interval='0 2 * * *',

    # No ejecutar DAGs para fechas pasadas (evita backfill autom√°tico)
    catchup=False,

    # Solo una instancia del DAG puede ejecutarse simult√°neamente
    max_active_runs=1,

    # Tags para organizar DAGs en la UI
    tags=['ventas', 'etl', 'produccion', 'diario'],

    # Documentaci√≥n en formato Markdown
    doc_md=&quot;&quot;&quot;
    ## Pipeline de Procesamiento de Ventas

    Este DAG procesa los datos de ventas diarias:
    1. Extrae datos de la base de datos transaccional
    2. Aplica transformaciones y limpieza
    3. Carga los datos al data warehouse
    4. Env√≠a reporte de resumen por email

    **Dependencias externas**: Base de datos PostgreSQL, S3 bucket
    **Tiempo estimado**: 45 minutos
    **Criticidad**: Alta - bloquea reportes ejecutivos
    &quot;&quot;&quot;,

) as dag:

    # FUNCI√ìN PYTHON PERSONALIZADA
    def procesar_datos_ventas(**context):
        &quot;&quot;&quot;
        Funci√≥n que procesa los datos de ventas.
        **context contiene informaci√≥n del contexto de ejecuci√≥n de Airflow
        &quot;&quot;&quot;
        # Obtener la fecha de ejecuci√≥n del contexto
        execution_date = context['execution_date']

        # Simular procesamiento de datos
        logging.info(f&quot;Procesando datos de ventas para {execution_date}&quot;)

        # En un caso real, aqu√≠ har√≠as:
        # - Conexi√≥n a base de datos
        # - Extracci√≥n de datos
        # - Transformaciones con pandas/spark
        # - Validaciones de calidad

        # Retornar m√©tricas para usar en tareas posteriores
        return {
            'registros_procesados': 15420,
            'ventas_totales': 89750.50,
            'fecha_proceso': execution_date.strftime('%Y-%m-%d')
        }

    def generar_reporte(**context):
        &quot;&quot;&quot;Genera un reporte basado en los datos procesados&quot;&quot;&quot;
        # Obtener datos de la tarea anterior usando XCom
        datos_ventas = context['task_instance'].xcom_pull(task_ids='procesar_datos')

        reporte = f&quot;&quot;&quot;
        Reporte de Ventas - {datos_ventas['fecha_proceso']}
        ================================================
        Registros procesados: {datos_ventas['registros_procesados']:,}
        Ventas totales: ${datos_ventas['ventas_totales']:,.2f}
        &quot;&quot;&quot;

        logging.info(reporte)
        return reporte

    # DEFINICI√ìN DE TAREAS

    # TAREA 1: Verificaci√≥n del sistema
    verificar_sistema = BashOperator(
        task_id='verificar_sistema',
        # Comando bash que verifica conectividad y recursos
        bash_command=&quot;&quot;&quot;
        echo &quot;Verificando sistema...&quot;
        df -h | grep -v tmpfs  # Verificar espacio en disco
        echo &quot;Sistema verificado correctamente&quot;
        &quot;&quot;&quot;,
        # Documentaci√≥n espec√≠fica de la tarea
        doc_md=&quot;&quot;&quot;
        ### Verificaci√≥n del Sistema
        Valida que el sistema tenga los recursos necesarios:
        - Espacio en disco suficiente
        - Conectividad de red
        - Servicios requeridos activos
        &quot;&quot;&quot;
    )

    # TAREA 2: Extracci√≥n de datos
    extraer_datos = BashOperator(
        task_id='extraer_datos',
        bash_command=&quot;&quot;&quot;
        echo &quot;Iniciando extracci√≥n de datos...&quot;
        # En producci√≥n, esto ser√≠a algo como:
        # psql -h $DB_HOST -d ventas -c &quot;COPY (...) TO STDOUT&quot; &gt; /tmp/ventas_{{ ds }}.csv
        echo &quot;Simulando extracci√≥n de 15,420 registros&quot;
        echo &quot;Datos extra√≠dos exitosamente&quot;
        &quot;&quot;&quot;,
        # Esta tarea puede fallar ocasionalmente, aumentamos reintentos
        retries=3,
        retry_delay=timedelta(minutes=2)
    )

    # TAREA 3: Procesamiento con Python
    procesar_datos = PythonOperator(
        task_id='procesar_datos',
        python_callable=procesar_datos_ventas,
        # Proporcionar el contexto de Airflow a la funci√≥n
        provide_context=True
    )

    # TAREA 4: Validaci√≥n de calidad
    validar_calidad = BashOperator(
        task_id='validar_calidad',
        bash_command=&quot;&quot;&quot;
        echo &quot;Ejecutando validaciones de calidad de datos...&quot;
        # Simular validaciones
        echo &quot;‚úì Sin valores nulos en campos cr√≠ticos&quot;
        echo &quot;‚úì Rangos de fechas v√°lidos&quot;
        echo &quot;‚úì Integridad referencial correcta&quot;
        echo &quot;Validaci√≥n completada exitosamente&quot;
        &quot;&quot;&quot;
    )

    # TAREA 5: Carga al data warehouse
    cargar_datos = BashOperator(
        task_id='cargar_datos_warehouse',
        bash_command=&quot;&quot;&quot;
        echo &quot;Cargando datos al data warehouse...&quot;
        # En producci√≥n ser√≠a algo como:
        # aws s3 cp /tmp/ventas_processed_{{ ds }}.parquet s3://warehouse/ventas/
        echo &quot;Datos cargados exitosamente al data warehouse&quot;
        &quot;&quot;&quot;
    )

    # TAREA 6: Generaci√≥n de reporte
    generar_reporte_task = PythonOperator(
        task_id='generar_reporte',
        python_callable=generar_reporte,
        provide_context=True
    )

    # TAREA 7: Env√≠o de notificaci√≥n por email
    enviar_notificacion = EmailOperator(
        task_id='enviar_notificacion',
        to=['gerencia@empresa.com', 'ventas@empresa.com'],
        subject='Pipeline de Ventas Completado - {{ ds }}',
        html_content=&quot;&quot;&quot;
        &lt;h3&gt;Pipeline de Procesamiento de Ventas&lt;/h3&gt;
        &lt;p&gt;&lt;strong&gt;Fecha:&lt;/strong&gt; {{ ds }}&lt;/p&gt;
        &lt;p&gt;&lt;strong&gt;Estado:&lt;/strong&gt; Completado exitosamente ‚úÖ&lt;/p&gt;
        &lt;p&gt;&lt;strong&gt;Tiempo de ejecuci√≥n:&lt;/strong&gt; {{ macros.datetime.now() }}&lt;/p&gt;

        &lt;p&gt;Los datos de ventas han sido procesados y est√°n disponibles en el data warehouse.&lt;/p&gt;

        &lt;hr&gt;
        &lt;small&gt;Este es un mensaje autom√°tico del sistema de orquestaci√≥n de datos.&lt;/small&gt;
        &quot;&quot;&quot;
    )

    # TAREA 8: Limpieza de archivos temporales
    limpiar_archivos = BashOperator(
        task_id='limpiar_archivos_temporales',
        bash_command=&quot;&quot;&quot;
        echo &quot;Limpiando archivos temporales...&quot;
        # rm -f /tmp/ventas_{{ ds }}.*
        echo &quot;Limpieza completada&quot;
        &quot;&quot;&quot;,
        # Esta tarea no es cr√≠tica, si falla no debe afectar el pipeline
        trigger_rule='all_done'  # Se ejecuta sin importar si las anteriores fallan
    )

    # DEFINICI√ìN DE DEPENDENCIAS

    # Secuencia lineal b√°sica
    verificar_sistema &gt;&gt; extraer_datos &gt;&gt; procesar_datos

    # Despu√©s del procesamiento, dos ramas paralelas
    procesar_datos &gt;&gt; [validar_calidad, generar_reporte_task]

    # La carga depende de que tanto el procesamiento como la validaci√≥n sean exitosos
    [procesar_datos, validar_calidad] &gt;&gt; cargar_datos

    # El reporte se puede generar en paralelo con la carga
    generar_reporte_task &gt;&gt; enviar_notificacion

    # La limpieza se ejecuta al final, despu√©s de todo
    [cargar_datos, enviar_notificacion] &gt;&gt; limpiar_archivos

</code></pre>
<h3 id="422-operadores-built-in-y-custom-operators">4.2.2 Operadores built-in y custom operators</h3>
<p>Los <strong>operadores</strong> son las unidades de ejecuci√≥n en Airflow. Representan tareas concretas que se ejecutan en el DAG. Airflow incluye m√∫ltiples operadores built-in y permite crear operadores personalizados para extender su funcionalidad.</p>
<h5 id="operadores-de-ejecucion-basica">Operadores de Ejecuci√≥n B√°sica</h5>
<p>Estos operadores permiten ejecutar comandos del sistema, scripts, o funciones personalizadas.</p>
<ul>
<li><code>BashOperator</code>: Fundamental para ejecutar comandos del sistema, scripts shell y herramientas CLI de big data como Hadoop, Spark-submit, etc.</li>
<li><code>PythonOperator</code>: Cr√≠tico para ejecutar funciones Python personalizadas, transformaciones de datos y l√≥gica de negocio</li>
<li><code>EmailOperator</code>: Esencial para notificaciones y alertas en pipelines de producci√≥n</li>
</ul>
<pre><code class="language-python">from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

def saludar():
    print(&quot;Hola desde PythonOperator&quot;)

tarea_bash = BashOperator(
    task_id='tarea_bash',
    bash_command='echo &quot;Ejecutando comando Bash&quot;',
    dag=dag
)

tarea_python = PythonOperator(
    task_id='tarea_python',
    python_callable=saludar,
    dag=dag
)
</code></pre>
<h5 id="operadores-de-bases-de-datos">Operadores de Bases de Datos</h5>
<p>Estos operadores permiten ejecutar consultas SQL directamente desde Airflow hacia motores relacionales o embebidos.</p>
<ul>
<li><code>PostgresOperator</code>/<code>MySqlOperator</code>: Para ejecutar consultas SQL en bases de datos relacionales</li>
<li><code>SqliteOperator</code>: √ötil para pruebas y desarrollo local</li>
</ul>
<pre><code class="language-python">from airflow.providers.postgres.operators.postgres import PostgresOperator

tarea_sql = PostgresOperator(
    task_id='crear_tabla',
    postgres_conn_id='mi_conexion_postgres',
    sql='CREATE TABLE IF NOT EXISTS tabla_ejemplo (id SERIAL PRIMARY KEY);',
    dag=dag
)
</code></pre>
<h5 id="operadores-de-transferencia">Operadores de Transferencia</h5>
<p>Son usados para mover datos entre servicios, especialmente en la nube.</p>
<ul>
<li><code>S3ToRedshiftOperator</code>: Transferir datos desde S3 a Redshift (muy com√∫n en AWS)</li>
<li><code>BigQueryOperator</code>: Para operaciones en Google BigQuery</li>
<li><code>RedshiftToS3Operator</code>: Exportar datos desde Redshift</li>
</ul>
<pre><code class="language-python">from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator

tarea_transferencia = S3ToRedshiftOperator(
    task_id='s3_a_redshift',
    s3_bucket='mi-bucket',
    s3_key='datos.csv',
    schema='public',
    table='mi_tabla',
    copy_options=['csv'],
    aws_conn_id='aws_default',
    redshift_conn_id='redshift_default',
    dag=dag
)
</code></pre>
<h5 id="operadores-de-control-de-flujo">Operadores de Control de Flujo</h5>
<p>Controlan la l√≥gica de ejecuci√≥n dentro de un DAG.</p>
<ul>
<li><code>BranchPythonOperator</code>: Para l√≥gica condicional en el pipeline</li>
<li><code>EmptyOperator</code> (antes <code>DummyOperator</code>): Para puntos de sincronizaci√≥n</li>
<li><code>TriggerDagRunOperator</code>: Para activar otros DAGs</li>
</ul>
<pre><code class="language-python">from airflow.operators.empty import EmptyOperator
from airflow.operators.branch import BranchPythonOperator

def decidir():
    return 'tarea_a' if datetime.now().hour &lt; 12 else 'tarea_b'

rama = BranchPythonOperator(
    task_id='evaluar_ruta',
    python_callable=decidir,
    dag=dag
)

tarea_a = EmptyOperator(task_id='tarea_a', dag=dag)
tarea_b = EmptyOperator(task_id='tarea_b', dag=dag)

rama &gt;&gt; [tarea_a, tarea_b]
</code></pre>
<h5 id="operadores-para-spark-y-hadoop">Operadores para Spark y Hadoop</h5>
<p>Permiten ejecutar aplicaciones en motores de procesamiento distribuido.</p>
<ul>
<li><code>SparkSubmitOperator</code>: Ejecutar aplicaciones Spark (batch processing)</li>
<li><code>DataprocSubmitJobOperator</code>: Para trabajos en Google Cloud Dataproc</li>
<li><code>EMRStepOperator</code>: Para clusters Amazon EMR</li>
</ul>
<pre><code class="language-python">from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_job = SparkSubmitOperator(
    task_id='ejecutar_spark',
    application='/ruta/a/mi_app_spark.py',
    conn_id='spark_default',
    dag=dag
)
</code></pre>
<h5 id="operadores-para-streaming-y-hdfs">Operadores para Streaming y HDFS</h5>
<p>Permiten flujos en tiempo real y acceso a HDFS</p>
<ul>
<li><code>KafkaOperator</code>: Interactuar con Apache Kafka para datos en tiempo real</li>
<li><code>HDFSOperator</code>: Operaciones en Hadoop Distributed File System</li>
</ul>
<pre><code class="language-python">from airflow.providers.apache.hdfs.sensors.hdfs import HdfsSensor

esperar_archivo = HdfsSensor(
    task_id='esperar_archivo_hdfs',
    filepath='/data/input/archivo.csv',
    hdfs_conn_id='hdfs_default',
    dag=dag
)
</code></pre>
<h5 id="operadores-para-validacion-de-datos">Operadores para Validaci√≥n de Datos</h5>
<p>Permiten realizar validaciones sobre los datos</p>
<ul>
<li><code>GreatExpectationsOperator</code>: Validaci√≥n de calidad de datos</li>
<li><code>Custom Data Validation Operators</code>: Para reglas espec√≠ficas del negocio</li>
</ul>
<pre><code class="language-python">from airflow.providers.great_expectations.operators.great_expectations import GreatExpectationsOperator

validacion = GreatExpectationsOperator(
    task_id='validar_datos',
    data_context_root_dir='/opt/ge/',
    checkpoint_name='check_ventas',
    dag=dag
)
</code></pre>
<h5 id="operadores-para-mlops">Operadores para MLOps</h5>
<p>Flujos relacionados con procesos de Machine Learning</p>
<ul>
<li><code>MLflowOperator</code>: Para experimentos y modelos de machine learning</li>
<li><code>KubernetesOperator</code>: Para ejecutar contenedores en Kubernetes</li>
</ul>
<pre><code class="language-python">from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

ml_entrenamiento = KubernetesPodOperator(
    task_id='entrenar_modelo',
    name='ml-train',
    image='ml/entrenamiento:latest',
    dag=dag
)
</code></pre>
<h3 id="423-taskgroups">4.2.3 TaskGroups</h3>
<p>Cuando los DAGs crecen en complejidad (m√°s de 10-15 tareas), surgen varios problemas:</p>
<ul>
<li>Legibilidad: Dif√≠cil entender el flujo en la UI de Airflow</li>
<li>Mantenimiento: C√≥digo desordenado y dif√≠cil de modificar</li>
<li>Reutilizaci√≥n: Patrones repetitivos sin forma de encapsular</li>
<li>Colaboraci√≥n: Equipos diferentes trabajando en el mismo DAG</li>
<li>Debugging: Dif√≠cil identificar grupos de tareas relacionadas</li>
</ul>
<p>La soluci√≥n es agrupar tareas l√≥gicamente relacionadas usando TaskGroups.</p>
<p>Nota: Existe un mecanismo adicional llamado SubDAGs, pero ya es considerado obsoleto.</p>
<h5 id="uso-de-taskgroups">Uso de TaskGroups</h5>
<p>Los TaskGroups son una forma de organizar tareas visualmente en la UI de Airflow sin crear DAGs separados.</p>
<p>Introducidos en Airflow 2.0, son la mejor pr√°ctica actual para estructurar DAGs complejos.</p>
<p>Ventajas de <strong>TaskGroups</strong></p>
<ul>
<li>Organizaci√≥n visual: Agrupa tareas en la UI de Airflow</li>
<li>Reutilizaci√≥n de c√≥digo: Encapsula l√≥gica repetitiva</li>
<li>Mejor rendimiento: No crean procesos separados como SubDAGs</li>
<li>Simplicidad: F√°cil de usar y entender</li>
<li>Debugging mejorado: Logs y estados agrupados</li>
<li>Escalabilidad: Maneja DAGs con cientos de tareas</li>
</ul>
<p>Los <code>TaskGroup</code> permiten agrupar visual y l√≥gicamente tareas relacionadas dentro de un DAG.</p>
<p><strong>Ejemplo de un TaskGroup</strong>:</p>
<pre><code class="language-python">from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

# Configuraci√≥n del DAG
with DAG(
    dag_id='pipeline_con_taskgroups_v1',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['taskgroups', 'ejemplo']
) as dag:

    # TASKGROUP 1: PREPARACI√ìN DE DATOS
    with TaskGroup(
        group_id='preparacion_datos',
        tooltip='Descarga, validaci√≥n y limpieza de datos raw'
    ) as grupo_preparacion:

        # Tarea 1: Descargar datos de m√∫ltiples fuentes
        descargar_datos = BashOperator(
            task_id='descargar_datos',
            bash_command='''
            echo &quot;Descargando datos de APIs y bases de datos...&quot;
            # Simular descarga de m√∫ltiples fuentes
            echo &quot;‚úì API de ventas: 15,420 registros&quot;
            echo &quot;‚úì API de clientes: 8,932 registros&quot;
            echo &quot;‚úì DB transaccional: 45,123 registros&quot;
            '''
        )

        # Tarea 2: Validar integridad de los datos
        validar_integridad = BashOperator(
            task_id='validar_integridad',
            bash_command='''
            echo &quot;Validando integridad de datos descargados...&quot;
            echo &quot;‚úì Verificando checksums&quot;
            echo &quot;‚úì Validando esquemas JSON/CSV&quot;
            echo &quot;‚úì Contando registros esperados&quot;
            '''
        )

        # Tarea 3: Limpiar y estandarizar datos
        limpiar_datos = PythonOperator(
            task_id='limpiar_datos',
            python_callable=lambda: print(&quot;&quot;&quot;
            Limpiando datos:
            ‚úì Removiendo duplicados
            ‚úì Estandarizando formatos de fecha
            ‚úì Normalizando nombres y direcciones
            ‚úì Aplicando reglas de negocio
            &quot;&quot;&quot;)
        )

        # Dependencias dentro del TaskGroup
        descargar_datos &gt;&gt; validar_integridad &gt;&gt; limpiar_datos

    # TASKGROUP 2: TRANSFORMACIONES DE NEGOCIO
    with TaskGroup(
        group_id='transformaciones_negocio',
        tooltip='Aplicar reglas de negocio y c√°lculos complejos'
    ) as grupo_transformaciones:

        def calcular_metricas_ventas():
            print(&quot;Calculando m√©tricas de ventas:&quot;)
            print(&quot;‚úì Revenue por regi√≥n&quot;)
            print(&quot;‚úì Customer Lifetime Value&quot;)
            print(&quot;‚úì Churn rate&quot;)
            print(&quot;‚úì Forecasting pr√≥ximo trimestre&quot;)
            return &quot;M√©tricas calculadas exitosamente&quot;

        def aplicar_segmentacion():
            print(&quot;Aplicando segmentaci√≥n de clientes:&quot;)
            print(&quot;‚úì RFM Analysis (Recency, Frequency, Monetary)&quot;)
            print(&quot;‚úì Clustering de comportamiento&quot;)
            print(&quot;‚úì Propensi√≥n de compra&quot;)
            return &quot;Segmentaci√≥n completada&quot;

        calcular_metricas = PythonOperator(
            task_id='calcular_metricas_ventas',
            python_callable=calcular_metricas_ventas
        )

        segmentar_clientes = PythonOperator(
            task_id='segmentar_clientes',
            python_callable=aplicar_segmentacion
        )

        # Estas tareas pueden ejecutarse en paralelo
        # No hay dependencia entre calcular m√©tricas y segmentar clientes
        pass  # Sin dependencias internas = ejecuci√≥n paralela

    # TASKGROUP 3: CARGA DE DATOS
    with TaskGroup(
        group_id='carga_datos',
        tooltip='Carga a diferentes sistemas de destino'
    ) as grupo_carga:

        cargar_datawarehouse = BashOperator(
            task_id='cargar_datawarehouse',
            bash_command='''
            echo &quot;Cargando datos al data warehouse...&quot;
            echo &quot;‚úì Upserting tabla dimensiones&quot;
            echo &quot;‚úì Insertando hechos de ventas&quot;
            echo &quot;‚úì Actualizando √≠ndices&quot;
            '''
        )

        cargar_data_lake = BashOperator(
            task_id='cargar_data_lake',
            bash_command='''
            echo &quot;Cargando datos al data lake...&quot;
            echo &quot;‚úì Particionando por fecha&quot;
            echo &quot;‚úì Comprimiendo en formato Parquet&quot;
            echo &quot;‚úì Actualizando cat√°logo de metadatos&quot;
            '''
        )

        actualizar_cache = BashOperator(
            task_id='actualizar_cache',
            bash_command='''
            echo &quot;Actualizando sistemas de cache...&quot;
            echo &quot;‚úì Redis para m√©tricas en tiempo real&quot;
            echo &quot;‚úì Elasticsearch para b√∫squedas&quot;
            echo &quot;‚úì CDN para reportes est√°ticos&quot;
            '''
        )

        # Todas las cargas pueden ejecutarse en paralelo
        pass  # Sin dependencias = m√°ximo paralelismo

    # TAREA INDIVIDUAL: NOTIFICACIONES
    enviar_notificaciones = BashOperator(
        task_id='enviar_notificaciones',
        bash_command='''
        echo &quot;Enviando notificaciones finales...&quot;
        echo &quot;‚úì Email a stakeholders&quot;
        echo &quot;‚úì Slack al equipo de datos&quot;
        echo &quot;‚úì Dashboard actualizado&quot;
        '''
    )

    # DEPENDENCIAS ENTRE TASKGROUPS

    # Flujo secuencial entre grupos principales
    grupo_preparacion &gt;&gt; grupo_transformaciones &gt;&gt; grupo_carga

    # La notificaci√≥n final depende de que toda la carga est√© completa
    grupo_carga &gt;&gt; enviar_notificaciones
</code></pre>
<h3 id="424-templating-con-jinja2">4.2.4 Templating con Jinja2</h3>
<p><strong>Apache Airflow</strong> aprovecha la potencia de <strong>Jinja2</strong>, un motor de plantillas moderno y de alto rendimiento para Python, para permitir la generaci√≥n din√°mica de comandos, configuraciones y otros valores dentro de tus <strong>DAGs</strong> (Directed Acyclic Graphs) y operadores. Esta capacidad de templating es fundamental para crear flujos de trabajo flexibles y reutilizables, ya que te permite adaptar el comportamiento de tus tareas en funci√≥n de variables de tiempo de ejecuci√≥n, configuraciones del entorno o datos espec√≠ficos.</p>
<h5 id="uso-de-variables-y-contextos">Uso de variables y contextos</h5>
<p>El templating con Jinja2 te permite incrustar variables y acceder al contexto de ejecuci√≥n de Airflow directamente dentro de las propiedades de tus operadores que soportan plantillas (identificadas por tener un sufijo <code>_template</code> o ser parte de una lista definida por <code>template_fields</code> en el operador).</p>
<p>Considera el siguiente ejemplo con un <code>BashOperator</code>:</p>
<pre><code class="language-python">tarea_con_template = BashOperator(
    task_id='mostrar_fecha',
    bash_command='echo &quot;La fecha de ejecuci√≥n es {{ ds }}&quot;',
    dag=dag
)
</code></pre>
<p>En este caso, la cadena <code>bash_command</code> es una plantilla Jinja2. Durante la ejecuci√≥n de la tarea <code>{{ ds }}</code> ser√° reemplazado autom√°ticamente por la fecha de ejecuci√≥n actual del DAG en formato <code>YYYY-MM-DD</code>. Esto es √∫til para tareas que dependen de la fecha, como el procesamiento de datos diarios o la generaci√≥n de informes con nombres de archivo basados en la fecha.</p>
<h5 id="acceso-a-macros-y-funciones-de-utilidad">Acceso a macros y funciones de utilidad</h5>
<p>Airflow expone un conjunto de <strong>macros</strong> y funciones de utilidad a las plantillas Jinja2, lo que te permite realizar operaciones comunes sin necesidad de escribir l√≥gica Python adicional. Estas macros proporcionan acceso conveniente a informaci√≥n clave del contexto de ejecuci√≥n y a funciones de manipulaci√≥n de fechas, entre otras cosas. Algunas de las macros m√°s utilizadas incluyen:</p>
<ul>
<li><strong><code>{{ ds }}</code></strong>: Representa la <strong>fecha de ejecuci√≥n del DAG</strong> en formato de cadena <code>YYYY-MM-DD</code>. Es ideal para operaciones de carga de datos diarias.</li>
<li><strong><code>{{ ds_nodash }}</code></strong>: Similar a <code>ds</code>, pero la fecha se presenta sin guiones, por ejemplo, <code>YYYYMMDD</code>. √ötil para nombres de archivos o rutas sin caracteres especiales.</li>
<li><strong><code>{{ execution_date }}</code></strong>: Es un <strong>objeto <code>datetime</code> completo</strong> que representa el momento exacto en que la instancia del DAG fue programada para ejecutarse. Este objeto te permite realizar manipulaciones de tiempo m√°s complejas.</li>
<li><strong><code>{{ prev_ds }}</code></strong>: La fecha de ejecuci√≥n anterior, si existe, en formato <code>YYYY-MM-DD</code>.</li>
<li><strong><code>{{ next_ds }}</code></strong>: La siguiente fecha de ejecuci√≥n programada, si existe, en formato <code>YYYY-MM-DD</code>.</li>
<li><strong><code>{{ macros.ds_add(ds, 7) }}</code></strong>: Esta es una funci√≥n de macro que te permite <strong>sumar o restar d√≠as</strong> a una fecha dada. En este ejemplo, suma 7 d√≠as a la fecha de ejecuci√≥n (<code>ds</code>). Puedes usarla para definir ventanas de tiempo relativas o para procesar datos con un desfase. Por ejemplo, <code>macros.ds_format(ds, "%Y-%m-%d", "%Y/%m/%d")</code> para cambiar el formato.</li>
<li><strong><code>{{ dag_run }}</code></strong>: Acceso al objeto <code>DagRun</code> completo, lo que te permite obtener informaci√≥n m√°s detallada sobre la ejecuci√≥n actual del DAG.</li>
<li><strong><code>{{ ti }}</code></strong>: Acceso al objeto <code>TaskInstance</code> actual, √∫til para obtener informaci√≥n espec√≠fica de la instancia de la tarea.</li>
</ul>
<h5 id="templating-avanzado-en-operadores-python">Templating avanzado en operadores Python</h5>
<p>El <em>templating</em> no se limita solo a operadores de Bash. Muchos operadores de Airflow, incluido el <code>PythonOperator</code>, permiten que los valores sean puestos en un <em>template</em>. Para los <code>PythonOperator</code> que necesitan acceder al contexto de Airflow, puedes usar el argumento <code>provide_context=True</code>. Cuando esta opci√≥n est√° activada, Airflow inyecta un diccionario con el contexto de ejecuci√≥n (incluyendo las variables y macros Jinja2 disponibles) como argumentos <code>kwargs</code> al <code>python_callable</code> definido.</p>
<pre><code class="language-python">from airflow.operators.python import PythonOperator

def mostrar_contexto(**kwargs):
    # 'ds' est√° disponible en kwargs cuando provide_context=True
    print(f&quot;La tarea se ejecuta para la fecha: {kwargs['ds']}&quot;)
    print(f&quot;Fecha de ejecuci√≥n completa: {kwargs['execution_date']}&quot;)
    # Tambi√©n puedes acceder a otras variables del contexto
    print(f&quot;ID de la tarea: {kwargs['task_instance'].task_id}&quot;)

tarea_contexto = PythonOperator(
    task_id='ver_contexto',
    python_callable=mostrar_contexto,
    provide_context=True,  # Esto inyecta el contexto como kwargs
    dag=dag
)
</code></pre>
<p>En este <code>PythonOperator</code>, la funci√≥n <code>mostrar_contexto</code> recibe <code>**kwargs</code>, que contendr√° el diccionario de contexto de Airflow. Dentro de esta funci√≥n, puedes acceder a <code>kwargs['ds']</code>, <code>kwargs['execution_date']</code>, <code>kwargs['task_instance']</code>, y cualquier otra variable o macro que normalmente estar√≠a disponible en las plantillas Jinja2. Esto te brinda un control program√°tico completo sobre c√≥mo tus funciones Python interact√∫an con el entorno de ejecuci√≥n de Airflow.</p>
<h2 id="tarea">Tarea</h2>
<p>Realiza los siguientes ejercicios para afianzar tu dominio sobre DAGs, operadores y tareas en Airflow:</p>
<ol>
<li><strong>Define un DAG diario</strong> que ejecute dos tareas: una que imprima la fecha del sistema y otra que salude al usuario.</li>
<li><strong>Usa un <code>BranchPythonOperator</code></strong> para decidir entre dos caminos: ejecutar un script Python o un comando Bash, seg√∫n el d√≠a de la semana.</li>
<li><strong>Agrupa tres tareas de transformaci√≥n</strong> (descargar, limpiar y cargar datos) usando <code>TaskGroup</code>.</li>
<li><strong>Crea un operador personalizado</strong> que verifique si existe un archivo en un bucket de S3 y registre el resultado.</li>
<li><strong>Implementa templating</strong> en un <code>BashOperator</code> para imprimir din√°micamente la fecha de ejecuci√≥n y el nombre del DAG.</li>
</ol>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../tema41/" class="btn btn-neutral float-left" title="Arquitectura y componentes de Airflow"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tema43/" class="btn btn-neutral float-right" title="Integraci√≥n con ecosistema Big Data">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../tema41/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tema43/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
