{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos. Requisitos Previos Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition 1. Introducci\u00f3n Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark 2. PySpark y SparkSQL Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Fundamentos de DataFrames en Spark Manipulaci\u00f3n y Transformaci\u00f3n de Datos Consultas y SQL en Spark Optimizaci\u00f3n y Rendimiento 3. Arquitectura y Dise\u00f1o de Flujos ETL Objetivo : Al terminar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables y eficientes utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye la habilidad de conectar diversas fuentes de datos, gestionar el particionamiento para el rendimiento y aplicar las mejores pr\u00e1cticas de la industria para asegurar la integridad y la calidad de los datos. Dise\u00f1o de pipelines ETL Conexi\u00f3n a m\u00faltiples fuentes Procesos escalables y particionamiento Buenas pr\u00e1cticas para ETL en Spark Introducci\u00f3n a Apache Airflow 4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow Instalaci\u00f3n local usando Docker DAGs, operadores y tareas Uso de BashOperator , PythonOperator y SparkSubmitOperator Monitoreo y manejo de dependencias 5. Proyecto Integrador y Despliegue Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"Inicio"},{"location":"#metodos-de-procesamiento-y-analisis-de-big-data","text":"Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos.","title":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data"},{"location":"#requisitos-previos","text":"Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition","title":"Requisitos Previos"},{"location":"#1-introduccion","text":"Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark","title":"1. Introducci\u00f3n"},{"location":"#2-pyspark-y-sparksql","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Fundamentos de DataFrames en Spark Manipulaci\u00f3n y Transformaci\u00f3n de Datos Consultas y SQL en Spark Optimizaci\u00f3n y Rendimiento","title":"2. PySpark y SparkSQL"},{"location":"#3-arquitectura-y-diseno-de-flujos-etl","text":"Objetivo : Al terminar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables y eficientes utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye la habilidad de conectar diversas fuentes de datos, gestionar el particionamiento para el rendimiento y aplicar las mejores pr\u00e1cticas de la industria para asegurar la integridad y la calidad de los datos. Dise\u00f1o de pipelines ETL Conexi\u00f3n a m\u00faltiples fuentes Procesos escalables y particionamiento Buenas pr\u00e1cticas para ETL en Spark Introducci\u00f3n a Apache Airflow","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"#4-automatizacion-y-orquestacion-con-apache-airflow","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow Instalaci\u00f3n local usando Docker DAGs, operadores y tareas Uso de BashOperator , PythonOperator y SparkSubmitOperator Monitoreo y manejo de dependencias","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"#5-proyecto-integrador-y-despliegue","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"5. Proyecto Integrador y Despliegue"},{"location":"ejemplo_json/","text":"Ejemplo JSON { \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_json/#ejemplo-json","text":"{ \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_log/","text":"Ejemplo LOG de acceso web 192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_log/#ejemplo-log-de-acceso-web","text":"192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_xml/","text":"Ejemplo XML <?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"ejemplo_xml/#ejemplo-xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"tema11/","text":"1. Introducci\u00f3n Tema 1.1. Fundamentos de Big Data Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados. 1.1.1 \u00bfQu\u00e9 es Big Data? Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales. 1.1.2 Las 5 V s del Big Data: Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data: Volumen Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad. Velocidad Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones. Variedad Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas. Veracidad Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados. Valor Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha. 1.1.3 Desaf\u00edos del Big Data: El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos. 1.1.4 Oportunidades del Big Data: A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros. 1.1.5 Tipos de Datos en Big Data: Datos Estructurados Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo . Datos Semi-estructurados Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor. Datos No Estructurados Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre. 1.1.6 Modelos de Procesamiento de Datos Procesamiento Batch (por Lotes) Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa. Procesamiento en Tiempo Real (Streaming) Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real. Procesamiento Interactivo (Ad-Hoc/Exploratorio) Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido. Tarea Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Fundamentos de Big Data"},{"location":"tema11/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema11/#tema-11-fundamentos-de-big-data","text":"Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados.","title":"Tema 1.1. Fundamentos de Big Data"},{"location":"tema11/#111-que-es-big-data","text":"Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales.","title":"1.1.1 \u00bfQu\u00e9 es Big Data?"},{"location":"tema11/#112-las-5-vs-del-big-data","text":"Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data:","title":"1.1.2 Las 5 Vs del Big Data:"},{"location":"tema11/#volumen","text":"Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad.","title":"Volumen"},{"location":"tema11/#velocidad","text":"Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones.","title":"Velocidad"},{"location":"tema11/#variedad","text":"Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas.","title":"Variedad"},{"location":"tema11/#veracidad","text":"Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados.","title":"Veracidad"},{"location":"tema11/#valor","text":"Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha.","title":"Valor"},{"location":"tema11/#113-desafios-del-big-data","text":"El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos.","title":"1.1.3 Desaf\u00edos del Big Data:"},{"location":"tema11/#114-oportunidades-del-big-data","text":"A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros.","title":"1.1.4 Oportunidades del Big Data:"},{"location":"tema11/#115-tipos-de-datos-en-big-data","text":"","title":"1.1.5 Tipos de Datos en Big Data:"},{"location":"tema11/#datos-estructurados","text":"Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo .","title":"Datos Estructurados"},{"location":"tema11/#datos-semi-estructurados","text":"Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor.","title":"Datos Semi-estructurados"},{"location":"tema11/#datos-no-estructurados","text":"Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre.","title":"Datos No Estructurados"},{"location":"tema11/#116-modelos-de-procesamiento-de-datos","text":"","title":"1.1.6 Modelos de Procesamiento de Datos"},{"location":"tema11/#procesamiento-batch-por-lotes","text":"Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa.","title":"Procesamiento Batch (por Lotes)"},{"location":"tema11/#procesamiento-en-tiempo-real-streaming","text":"Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real.","title":"Procesamiento en Tiempo Real (Streaming)"},{"location":"tema11/#procesamiento-interactivo-ad-hocexploratorio","text":"Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido.","title":"Procesamiento Interactivo (Ad-Hoc/Exploratorio)"},{"location":"tema11/#tarea","text":"Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Tarea"},{"location":"tema12/","text":"1. Introducci\u00f3n Tema 1.2 Introducci\u00f3n al ecosistema Spark Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo. 1.2.1 \u00bfQu\u00e9 es Apache Spark? Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos. Procesamiento en memoria Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria. Resilient Distributed Datasets (RDDs) Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave. 1.2.2 Componentes principales de Spark Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero. Spark Core Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia. Spark SQL Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional. Spark Streaming Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos. MLlib MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos. GraphX GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n. 1.2.3 Arquitectura de Spark La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva. Spark Driver El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones. Spark Executor Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos. Cluster Manager El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias. 1.2.4 Interacci\u00f3n con Spark La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente. Spark Shell El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande. Spark Submit spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL. Spark UI La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria. 1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas. Particionamiento de datos El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes. Persistencia de datos (Caching) La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido. Lazy Evaluation (Evaluaci\u00f3n Perezosa) La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia. 1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas. Spark vs. Hadoop MapReduce Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente. Tarea Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema12/#tema-12-introduccion-al-ecosistema-spark","text":"Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo.","title":"Tema 1.2 Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#121-que-es-apache-spark","text":"Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos.","title":"1.2.1 \u00bfQu\u00e9 es Apache Spark?"},{"location":"tema12/#procesamiento-en-memoria","text":"Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria.","title":"Procesamiento en memoria"},{"location":"tema12/#resilient-distributed-datasets-rdds","text":"Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave.","title":"Resilient Distributed Datasets (RDDs)"},{"location":"tema12/#122-componentes-principales-de-spark","text":"Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero.","title":"1.2.2 Componentes principales de Spark"},{"location":"tema12/#spark-core","text":"Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia.","title":"Spark Core"},{"location":"tema12/#spark-sql","text":"Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional.","title":"Spark SQL"},{"location":"tema12/#spark-streaming","text":"Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos.","title":"Spark Streaming"},{"location":"tema12/#mllib","text":"MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos.","title":"MLlib"},{"location":"tema12/#graphx","text":"GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n.","title":"GraphX"},{"location":"tema12/#123-arquitectura-de-spark","text":"La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva.","title":"1.2.3 Arquitectura de Spark"},{"location":"tema12/#spark-driver","text":"El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones.","title":"Spark Driver"},{"location":"tema12/#spark-executor","text":"Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos.","title":"Spark Executor"},{"location":"tema12/#cluster-manager","text":"El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias.","title":"Cluster Manager"},{"location":"tema12/#124-interaccion-con-spark","text":"La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente.","title":"1.2.4 Interacci\u00f3n con Spark"},{"location":"tema12/#spark-shell","text":"El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande.","title":"Spark Shell"},{"location":"tema12/#spark-submit","text":"spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL.","title":"Spark Submit"},{"location":"tema12/#spark-ui","text":"La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria.","title":"Spark UI"},{"location":"tema12/#125-conceptos-fundamentales-de-procesamiento-distribuido-en-spark","text":"El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas.","title":"1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark"},{"location":"tema12/#particionamiento-de-datos","text":"El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes.","title":"Particionamiento de datos"},{"location":"tema12/#persistencia-de-datos-caching","text":"La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido.","title":"Persistencia de datos (Caching)"},{"location":"tema12/#lazy-evaluation-evaluacion-perezosa","text":"La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia.","title":"Lazy Evaluation (Evaluaci\u00f3n Perezosa)"},{"location":"tema12/#126-comparacion-de-spark-con-otras-herramientas-big-data","text":"Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas.","title":"1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data"},{"location":"tema12/#spark-vs-hadoop-mapreduce","text":"Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente.","title":"Spark vs. Hadoop MapReduce"},{"location":"tema12/#tarea","text":"Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Tarea"},{"location":"tema13/","text":"1. Introducci\u00f3n Tema 1.3 RDD, DataFrame y Dataset Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas. 1.3.1 RDD (Resilient Distributed Datasets) Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje. Caracter\u00edsticas clave de los RDDs Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark. Operaciones de transformaci\u00f3n Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo. Operaciones de acci\u00f3n Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido. 1.3.2 DataFrame Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados. Ventajas sobre los RDDs Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos. Creaci\u00f3n y manipulaci\u00f3n de DataFrames Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) . 1.3.3 Dataset El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark. Seguridad de tipos (Type-safety) La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador. Encoders Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado. 1.3.4 Comparaci\u00f3n y Casos de Uso La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n. Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir. Tarea Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"RDD, DataFrame y Dataset"},{"location":"tema13/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema13/#tema-13-rdd-dataframe-y-dataset","text":"Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas.","title":"Tema 1.3 RDD, DataFrame y Dataset"},{"location":"tema13/#131-rdd-resilient-distributed-datasets","text":"Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje.","title":"1.3.1 RDD (Resilient Distributed Datasets)"},{"location":"tema13/#caracteristicas-clave-de-los-rdds","text":"Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark.","title":"Caracter\u00edsticas clave de los RDDs"},{"location":"tema13/#operaciones-de-transformacion","text":"Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo.","title":"Operaciones de transformaci\u00f3n"},{"location":"tema13/#operaciones-de-accion","text":"Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido.","title":"Operaciones de acci\u00f3n"},{"location":"tema13/#132-dataframe","text":"Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados.","title":"1.3.2 DataFrame"},{"location":"tema13/#ventajas-sobre-los-rdds","text":"Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos.","title":"Ventajas sobre los RDDs"},{"location":"tema13/#creacion-y-manipulacion-de-dataframes","text":"Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) .","title":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames"},{"location":"tema13/#133-dataset","text":"El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark.","title":"1.3.3 Dataset"},{"location":"tema13/#seguridad-de-tipos-type-safety","text":"La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador.","title":"Seguridad de tipos (Type-safety)"},{"location":"tema13/#encoders","text":"Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado.","title":"Encoders"},{"location":"tema13/#134-comparacion-y-casos-de-uso","text":"La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n.","title":"1.3.4 Comparaci\u00f3n y Casos de Uso"},{"location":"tema13/#tabla-comparativa-detallada-rdd-vs-dataframe-vs-dataset","text":"Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir.","title":"Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset"},{"location":"tema13/#tarea","text":"Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"Tarea"},{"location":"tema14/","text":"1. Introducci\u00f3n Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente. 1.4.1 Requisitos previos para la instalaci\u00f3n Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo. Java Development Kit (JDK) Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n. Python (para PySpark) Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior. Apache Hadoop (opcional, para HDFS y YARN) Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop). 1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento. Descarga de Spark El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/ Configuraci\u00f3n del modo Standalone El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado. Configuraci\u00f3n con YARN (Yet Another Resource Negotiator) YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos. Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo) La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental. 1.4.3 Spark en entornos de nube (AWS, Azure, GCP) La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos. Servicios gestionados de Spark en la nube Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres. Ventajas de los servicios gestionados Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones. Configuraci\u00f3n de acceso a datos en la nube Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP. 1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas) Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows. Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark) Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal. Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs) Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888 Tarea Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema14/#tema-14-instalacion-y-configuracion-de-spark","text":"Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente.","title":"Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#141-requisitos-previos-para-la-instalacion","text":"Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo.","title":"1.4.1 Requisitos previos para la instalaci\u00f3n"},{"location":"tema14/#java-development-kit-jdk","text":"Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n.","title":"Java Development Kit (JDK)"},{"location":"tema14/#python-para-pyspark","text":"Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior.","title":"Python (para PySpark)"},{"location":"tema14/#apache-hadoop-opcional-para-hdfs-y-yarn","text":"Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop).","title":"Apache Hadoop (opcional, para HDFS y YARN)"},{"location":"tema14/#142-instalacion-de-un-cluster-spark-on-premise","text":"Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento.","title":"1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise"},{"location":"tema14/#descarga-de-spark","text":"El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/","title":"Descarga de Spark"},{"location":"tema14/#configuracion-del-modo-standalone","text":"El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado.","title":"Configuraci\u00f3n del modo Standalone"},{"location":"tema14/#configuracion-con-yarn-yet-another-resource-negotiator","text":"YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos.","title":"Configuraci\u00f3n con YARN (Yet Another Resource Negotiator)"},{"location":"tema14/#optimizacion-de-la-configuracion-memoria-cores-paralelismo","text":"La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental.","title":"Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo)"},{"location":"tema14/#143-spark-en-entornos-de-nube-aws-azure-gcp","text":"La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos.","title":"1.4.3 Spark en entornos de nube (AWS, Azure, GCP)"},{"location":"tema14/#servicios-gestionados-de-spark-en-la-nube","text":"Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres.","title":"Servicios gestionados de Spark en la nube"},{"location":"tema14/#ventajas-de-los-servicios-gestionados","text":"Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones.","title":"Ventajas de los servicios gestionados"},{"location":"tema14/#configuracion-de-acceso-a-datos-en-la-nube","text":"Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP.","title":"Configuraci\u00f3n de acceso a datos en la nube"},{"location":"tema14/#144-instalacion-de-spark-en-windows-para-practicas","text":"Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows.","title":"1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas)"},{"location":"tema14/#uso-de-imagenes-docker-para-spark-ej-bitnamispark","text":"Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal.","title":"Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark)"},{"location":"tema14/#configuracion-de-entorno-de-desarrollo-jupyter-ides","text":"Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888","title":"Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs)"},{"location":"tema14/#tarea","text":"Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Tarea"},{"location":"tema15/","text":"1. Introducci\u00f3n Tema 1.5 Primeros pasos con PySpark Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados. 1.5.1 Entorno de desarrollo para PySpark Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE. Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv) Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows). Instalaci\u00f3n de PySpark ( pip install pyspark ) Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa. Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm) Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente. Acceso a la Spark UI La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores. 1.5.2 Inicializaci\u00f3n de SparkSession La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz. El papel de SparkSession SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL). Creaci\u00f3n de una SparkSession La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts. 1.5.3 Carga de datos con PySpark Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos. Lectura de archivos CSV (inferSchema, header, delimiter) El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema) Lectura de archivos JSON Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea. 1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames. Visualizaci\u00f3n de datos ( show() , printSchema() , describe() ) Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50 Selecci\u00f3n de columnas ( select() ) La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show() Filtrado de filas ( filter() / where() ) Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show() 1.5.5 Operaciones comunes de transformaci\u00f3n Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos. Renombrar y eliminar columnas ( withColumnRenamed() , drop() ) Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show() A\u00f1adir y modificar columnas ( withColumn() ) El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show() Operaciones de agregaci\u00f3n ( groupBy() , agg() , sum() , avg() , min() , max() ) Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show() 1.5.6 Escritura de datos con PySpark Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes. Guardar DataFrames en formato CSV, JSON, Parquet PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\") Modos de escritura (append, overwrite, ignore, errorIfExists) PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists Particionamiento de la salida ( partitionBy() ) El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True) Tarea Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Primeros pasos con PySpark"},{"location":"tema15/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema15/#tema-15-primeros-pasos-con-pyspark","text":"Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados.","title":"Tema 1.5 Primeros pasos con PySpark"},{"location":"tema15/#151-entorno-de-desarrollo-para-pyspark","text":"Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE.","title":"1.5.1 Entorno de desarrollo para PySpark"},{"location":"tema15/#configuracion-de-un-entorno-python-anacondaminiconda-virtualenv","text":"Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows).","title":"Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv)"},{"location":"tema15/#instalacion-de-pyspark-pip-install-pyspark","text":"Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa.","title":"Instalaci\u00f3n de PySpark (pip install pyspark)"},{"location":"tema15/#integracion-con-jupyter-notebooks-o-ides-vs-code-pycharm","text":"Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente.","title":"Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm)"},{"location":"tema15/#acceso-a-la-spark-ui","text":"La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores.","title":"Acceso a la Spark UI"},{"location":"tema15/#152-inicializacion-de-sparksession","text":"La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz.","title":"1.5.2 Inicializaci\u00f3n de SparkSession"},{"location":"tema15/#el-papel-de-sparksession","text":"SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL).","title":"El papel de SparkSession"},{"location":"tema15/#creacion-de-una-sparksession","text":"La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts.","title":"Creaci\u00f3n de una SparkSession"},{"location":"tema15/#153-carga-de-datos-con-pyspark","text":"Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos.","title":"1.5.3 Carga de datos con PySpark"},{"location":"tema15/#lectura-de-archivos-csv-inferschema-header-delimiter","text":"El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema)","title":"Lectura de archivos CSV (inferSchema, header, delimiter)"},{"location":"tema15/#lectura-de-archivos-json","text":"Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea.","title":"Lectura de archivos JSON"},{"location":"tema15/#154-exploracion-basica-de-dataframes","text":"Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames.","title":"1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames"},{"location":"tema15/#visualizacion-de-datos-show-printschema-describe","text":"Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50","title":"Visualizaci\u00f3n de datos (show(), printSchema(), describe())"},{"location":"tema15/#seleccion-de-columnas-select","text":"La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show()","title":"Selecci\u00f3n de columnas (select())"},{"location":"tema15/#filtrado-de-filas-filter-where","text":"Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show()","title":"Filtrado de filas (filter() / where())"},{"location":"tema15/#155-operaciones-comunes-de-transformacion","text":"Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos.","title":"1.5.5 Operaciones comunes de transformaci\u00f3n"},{"location":"tema15/#renombrar-y-eliminar-columnas-withcolumnrenamed-drop","text":"Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show()","title":"Renombrar y eliminar columnas (withColumnRenamed(), drop())"},{"location":"tema15/#anadir-y-modificar-columnas-withcolumn","text":"El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show()","title":"A\u00f1adir y modificar columnas (withColumn())"},{"location":"tema15/#operaciones-de-agregacion-groupby-agg-sum-avg-min-max","text":"Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show()","title":"Operaciones de agregaci\u00f3n (groupBy(), agg(), sum(), avg(), min(), max())"},{"location":"tema15/#156-escritura-de-datos-con-pyspark","text":"Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes.","title":"1.5.6 Escritura de datos con PySpark"},{"location":"tema15/#guardar-dataframes-en-formato-csv-json-parquet","text":"PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\")","title":"Guardar DataFrames en formato CSV, JSON, Parquet"},{"location":"tema15/#modos-de-escritura-append-overwrite-ignore-errorifexists","text":"PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists","title":"Modos de escritura (append, overwrite, ignore, errorIfExists)"},{"location":"tema15/#particionamiento-de-la-salida-partitionby","text":"El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True)","title":"Particionamiento de la salida (partitionBy())"},{"location":"tema15/#tarea","text":"Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Tarea"},{"location":"tema21/","text":"2. PySpark y SparkSQL Tema 2.1 Fundamentos de DataFrames en Spark Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender la estructura y el funcionamiento de los DataFrames de Apache Spark, realizar operaciones fundamentales de manipulaci\u00f3n de datos, gestionar esquemas y tipos de datos complejos, y leer y escribir datos en los formatos m\u00e1s comunes utilizados en entornos de Big Data. Introducci\u00f3n : En el vasto universo del Big Data, la capacidad de procesar y analizar vol\u00famenes masivos de informaci\u00f3n es crucial. Apache Spark, con su motor de procesamiento distribuido, se ha consolidado como una herramienta indispensable para esta tarea. En el coraz\u00f3n de su eficiencia y facilidad de uso se encuentran los DataFrames, una abstracci\u00f3n de datos distribuida que organiza los datos en columnas con nombre, similar a una tabla en una base de datos relacional o una hoja de c\u00e1lculo. Esta estructura permite a los desarrolladores trabajar con datos de forma intuitiva y optimizada, aprovechando el poder de Spark para el procesamiento paralelo y distribuido. Desarrollo : Este tema se centrar\u00e1 en los pilares de la manipulaci\u00f3n de datos en Spark a trav\u00e9s de los DataFrames. Exploraremos c\u00f3mo los DataFrames facilitan las operaciones de transformaci\u00f3n y consulta, abstraen la complejidad de la distribuci\u00f3n de datos y proporcionan un API robusto para interactuar con ellos. Abordaremos desde las operaciones b\u00e1sicas como selecci\u00f3n y filtrado, hasta la comprensi\u00f3n de los esquemas y tipos de datos complejos, y la interacci\u00f3n con una variedad de formatos de archivo est\u00e1ndar de la industria. 2.1.1 Operaciones con DataFrames Las operaciones con DataFrames en Spark son el n\u00facleo de la manipulaci\u00f3n de datos, permitiendo seleccionar, filtrar, agregar, unir y realizar una multitud de transformaciones sobre conjuntos de datos distribuidos de manera eficiente. A diferencia de los RDDs, los DataFrames ofrecen un nivel de abstracci\u00f3n superior, permitiendo a Spark optimizar internamente las operaciones gracias a la informaci\u00f3n del esquema. Creaci\u00f3n de DataFrames La creaci\u00f3n de DataFrames es el primer paso para trabajar con datos en PySpark. Se pueden crear a partir de diversas fuentes, como listas de Python, RDDs existentes, o leyendo directamente desde archivos. Desde una lista de tuplas o diccionarios: from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() # Opci\u00f3n 1: Usando una lista de tuplas y definiendo el esquema data_tuples = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] schema_tuples = StructType([ StructField(\"name\", StringType(), True), StructField(\"id\", IntegerType(), True), StructField(\"city\", StringType(), True) ]) df_from_tuples = spark.createDataFrame(data_tuples, schema=schema_tuples) df_from_tuples.show() # Resultado: # +-------+---+----+ # | name| id|city| # +-------+---+----+ # | Alice| 1| NY| # | Bob| 2| LA| # |Charlie| 3| CHI| # +-------+---+----+ # Opci\u00f3n 2: Usando una lista de diccionarios (Spark infiere el esquema) data_dicts = [{\"name\": \"Alice\", \"id\": 1, \"city\": \"NY\"}, {\"name\": \"Bob\", \"id\": 2, \"city\": \"LA\"}, {\"name\": \"Charlie\", \"id\": 3, \"city\": \"CHI\"}] df_from_dicts = spark.createDataFrame(data_dicts) df_from_dicts.show() # Resultado: # +-------+---+----+ # | city| id|name| # +-------+---+----+ # | NY| 1|Alice| # | LA| 2| Bob| # | CHI| 3|Charlie| # +-------+---+----+ Desde un RDD existente: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]) df_from_rdd = spark.createDataFrame(rdd, [\"name\", \"id\"]) df_from_rdd.show() # Resultado: # +-------+---+ # | name| id| # +-------+---+ # | Alice| 1| # | Bob| 2| # |Charlie| 3| # +-------+---+ Desde un archivo (se ver\u00e1 en 2.1.3): # df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True) # df.show() Transformaciones de DataFrames Las transformaciones en DataFrames son operaciones lazy (perezosas), lo que significa que no se ejecutan hasta que se invoca una acci\u00f3n. Esto permite a Spark optimizar el plan de ejecuci\u00f3n. Selecci\u00f3n de columnas ( select y withColumn ): from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate() data = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"id\", \"city\"]) # Seleccionar columnas espec\u00edficas df.select(\"name\", \"city\").show() # Resultado: # +-------+----+ # | name|city| # +-------+----+ # | Alice| NY| # | Bob| LA| # |Charlie| CHI| # +-------+----+ # Renombrar una columna al seleccionar df.select(col(\"name\").alias(\"full_name\"), \"city\").show() # Resultado: # +---------+----+ # |full_name|city| # +---------+----+ # | Alice| NY| # | Bob| LA| # | Charlie| CHI| # +---------+----+ # A\u00f1adir una nueva columna df.withColumn(\"country\", lit(\"USA\")).show() # Resultado: # +-------+---+----+-------+ # | name| id|city|country| # +-------+---+----+-------+ # | Alice| 1| NY| USA| # | Bob| 2| LA| USA| # |Charlie| 3| CHI| USA| # +-------+---+----+-------+ # Modificar una columna existente (ejemplo: incrementar id) df.withColumn(\"id_plus_10\", col(\"id\") + 10).show() # Resultado: # +-------+---+----+----------+ # | name| id|city|id_plus_10| # +-------+---+----+----------+ # | Alice| 1| NY| 11| # | Bob| 2| LA| 12| # |Charlie| 3| CHI| 13| # +-------+---+----+----------+ Filtrado de filas ( filter o where ): from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"DataFrameFiltering\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\"), (\"David\", 35, \"NY\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Filtrar por una condici\u00f3n simple df.filter(col(\"age\") > 25).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # | Bob| 30| LA| # |David| 35| NY| # +-----+---+----+ # Filtrar por m\u00faltiples condiciones df.filter((col(\"age\") > 20) & (col(\"city\") == \"NY\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # |David| 35| NY| # +-----+---+----+ # Usando el m\u00e9todo where (alias de filter) df.where(col(\"name\").like(\"A%\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # +-----+---+----+ Agregaciones ( groupBy y funciones de agregaci\u00f3n): from pyspark.sql import SparkSession from pyspark.sql.functions import avg, count, sum, min, max spark = SparkSession.builder.appName(\"DataFrameAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000), (\"Dept1\", \"Bob\", 1200), (\"Dept2\", \"Charlie\", 900), (\"Dept2\", \"David\", 1500)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"]) # Contar empleados por departamento df.groupBy(\"department\").count().show() # Resultado: # +----------+-----+ # |department|count| # +----------+-----+ # | Dept1| 2| # | Dept2| 2| # +----------+-----+ # Calcular salario promedio y m\u00e1ximo por departamento df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"), max(\"salary\").alias(\"max_salary\")).show() # Resultado: # +----------+----------+----------+ # |department|avg_salary|max_salary| # +----------+----------+----------+ # | Dept1| 1100.0| 1200| # | Dept2| 1200.0| 1500| # +----------+----------+----------+ # Sumar salarios por departamento df.groupBy(\"department\").agg(sum(\"salary\").alias(\"total_salary\")).show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | Dept1| 2200| # | Dept2| 2400| # +----------+------------+ Acciones de DataFrames Las acciones son operaciones que disparan la ejecuci\u00f3n del plan de transformaciones y devuelven un resultado a la aplicaci\u00f3n del controlador o escriben datos en un sistema de almacenamiento. Mostrar datos ( show ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Mostrar las primeras filas del DataFrame df.show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ # Mostrar un n\u00famero espec\u00edfico de filas y truncar el contenido de las columnas si es largo df.show(numRows=1, truncate=False) # Resultado: # +-----+---+ # |name |age| # +-----+---+ # |Alice|25 | # +-----+---+ Contar filas ( count ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Contar el n\u00famero total de filas en el DataFrame num_rows = df.count() print(f\"N\u00famero de filas: {num_rows}\") # Resultado: N\u00famero de filas: 3 Recopilar datos ( collect , take , first ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Recopilar todos los datos del DataFrame en una lista de Rows en el driver all_data = df.collect() print(f\"Todos los datos: {all_data}\") # Resultado: Todos los datos: [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=22)] # Tomar las primeras N filas first_two = df.take(2) print(f\"Primeras 2 filas: {first_two}\") # Resultado: Primeras 2 filas: [Row(name='Alice', age=25), Row(name='Bob', age=30)] # Obtener la primera fila first_row = df.first() print(f\"Primera fila: {first_row}\") # Resultado: Primera fila: Row(name='Alice', age=25) 2.1.2 Esquemas y tipos de datos complejos El esquema de un DataFrame es una estructura fundamental que define los nombres de las columnas y sus tipos de datos correspondientes. Esta metadata es crucial para la optimizaci\u00f3n de Spark, ya que le permite saber c\u00f3mo se organizan los datos y aplicar optimizaciones de tipo de datos y de columna. La inferencia de esquema y la definici\u00f3n expl\u00edcita son dos formas de manejarlo, y Spark tambi\u00e9n soporta tipos de datos complejos como ArrayType , MapType y StructType para manejar estructuras anidadas. Inferencia y Definici\u00f3n Expl\u00edcita de Esquemas La forma en que Spark determina el esquema de un DataFrame es vital para la integridad y eficiencia del procesamiento de datos. Inferencia de esquema ( inferSchema=True ): Spark puede intentar adivinar el esquema de un archivo de datos (CSV, JSON, Parquet, etc.) leyendo una muestra. Esto es conveniente para la exploraci\u00f3n inicial, pero puede ser propenso a errores, especialmente con datos inconsistentes o tipos ambiguos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SchemaInference\").getOrCreate() # Creando un archivo CSV de ejemplo data_csv = \"\"\"name,age,city Alice,25,NY Bob,30,LA Charlie,null,CHI David,35,NY\"\"\" with open(\"data.csv\", \"w\") as f: f.write(data_csv) # Inferencia de esquema al leer un CSV df_inferred = spark.read.csv(\"data.csv\", header=True, inferSchema=True) df_inferred.printSchema() # Resultado (ejemplo): # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # Observar que \"age\" se infiri\u00f3 como IntegerType, lo cual es correcto si no hay valores no num\u00e9ricos. # Si hubiera un valor no num\u00e9rico, podr\u00eda inferirse como StringType o fallar la inferencia. Definici\u00f3n expl\u00edcita de esquema ( StructType y StructField ): Es la forma m\u00e1s robusta y recomendada para entornos de producci\u00f3n. Permite controlar con precisi\u00f3n los tipos de datos y la nulabilidad, evitando problemas de inferencia y mejorando el rendimiento. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType spark = SparkSession.builder.appName(\"ExplicitSchema\").getOrCreate() # Definir un esquema expl\u00edcito custom_schema = StructType([ StructField(\"employee_name\", StringType(), True), StructField(\"employee_id\", IntegerType(), False), # Not nullable StructField(\"salary\", DoubleType(), True), StructField(\"is_active\", BooleanType(), True) ]) data = [(\"Alice\", 1, 50000.0, True), (\"Bob\", 2, 60000.50, False), (\"Charlie\", 3, 75000.0, True)] df_explicit = spark.createDataFrame(data, schema=custom_schema) df_explicit.printSchema() # Resultado: # root # |-- employee_name: string (nullable = true) # |-- employee_id: integer (nullable = false) # |-- salary: double (nullable = true) # |-- is_active: boolean (nullable = true) # Intentar insertar un valor nulo en una columna no nula causar\u00eda un error o comportamiento inesperado # data_error = [(\"David\", None, 80000.0, True)] # Esto generar\u00eda un error si intentas crear el DF # df_error = spark.createDataFrame(data_error, schema=custom_schema) Acceder y manipular el esquema ( df.schema ): El esquema de un DataFrame es accesible a trav\u00e9s del atributo .schema , que devuelve un objeto StructType . from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"SchemaAccess\").getOrCreate() data = [(\"Alice\", 1), (\"Bob\", 2)] df = spark.createDataFrame(data, [\"name\", \"id\"]) # Acceder al esquema print(df.schema) # Resultado: StructType([StructField('name', StringType(), True), StructField('id', LongType(), True)]) # Iterar sobre los campos del esquema for field in df.schema: print(f\"Nombre de columna: {field.name}, Tipo: {field.dataType}, Nulable: {field.nullable}\") # Resultado: # Nombre de columna: name, Tipo: StringType, Nulable: True # Nombre de columna: id, Tipo: LongType, Nulable: True Tipos de Datos Complejos Spark permite manejar estructuras de datos m\u00e1s all\u00e1 de los tipos at\u00f3micos, lo que es fundamental para trabajar con datos semi-estructurados y anidados como JSON. StructType (Estructuras Anidadas/Registros): Representa una estructura similar a un objeto o un registro, donde cada campo tiene un nombre y un tipo de datos. Permite modelar objetos complejos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypes\").getOrCreate() # Definir un esquema con un StructType anidado address_schema = StructType([ StructField(\"street\", StringType(), True), StructField(\"city\", StringType(), True), StructField(\"zip\", StringType(), True) ]) person_schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"address\", address_schema, True) # Columna de tipo StructType ]) data = [ (\"Alice\", 25, (\"123 Main St\", \"NY\", \"10001\")), (\"Bob\", 30, (\"456 Oak Ave\", \"LA\", \"90001\")) ] df = spark.createDataFrame(data, schema=person_schema) df.printSchema() # Resultado: # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- address: struct (nullable = true) # | |-- street: string (nullable = true) # | |-- city: string (nullable = true) # | |-- zip: string (nullable = true) df.show(truncate=False) # Resultado: # +-----+---+-------------------------+ # |name |age|address | # +-----+---+-------------------------+ # |Alice|25 |{123 Main St, NY, 10001} | # |Bob |30 |{456 Oak Ave, LA, 90001} | # +-----+---+-------------------------+ # Acceder a campos anidados df.select(\"name\", \"address.city\").show() # Resultado: # +-----+----+ # |name |city| # +-----+----+ # |Alice|NY | # |Bob |LA | # +-----+----+ ArrayType (Arrays/Listas): Representa una colecci\u00f3n de elementos del mismo tipo. \u00datil para modelar listas o arreglos de datos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypesArray\").getOrCreate() # Definir un esquema con un ArrayType course_schema = StructType([ StructField(\"student_name\", StringType(), True), StructField(\"grades\", ArrayType(IntegerType()), True) # Columna de tipo ArrayType ]) data = [ (\"Alice\", [90, 85, 92]), (\"Bob\", [78, 80]), (\"Charlie\", []) ] df = spark.createDataFrame(data, schema=course_schema) df.printSchema() # Resultado: # root # |-- student_name: string (nullable = true) # |-- grades: array (nullable = true) # | |-- element: integer (containsNull = true) df.show(truncate=False) # Resultado: # +------------+----------+ # |student_name|grades | # +------------+----------+ # |Alice |[90, 85, 92]| # |Bob |[78, 80] | # |Charlie |[] | # +------------+----------+ # Acceder a elementos de array (requiere funciones de Spark) from pyspark.sql.functions import size, array_contains df.select(\"student_name\", size(\"grades\").alias(\"num_grades\")).show() # Resultado: # +------------+----------+ # |student_name|num_grades| # +------------+----------+ # | Alice| 3| # | Bob| 2| # | Charlie| 0| # +------------+----------+ df.filter(array_contains(\"grades\", 90)).show() # Resultado: # +------------+----------+ # |student_name| grades| # +------------+----------+ # | Alice|[90, 85, 92]| # +------------+----------+ MapType (Mapas/Diccionarios): Representa una colecci\u00f3n de pares clave-valor. \u00datil para datos que se asemejan a diccionarios o JSON con claves din\u00e1micas. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, MapType spark = SparkSession.builder.appName(\"ComplexTypesMap\").getOrCreate() # Definir un esquema con un MapType product_schema = StructType([ StructField(\"product_id\", StringType(), True), StructField(\"attributes\", MapType(StringType(), StringType()), True) # Columna de tipo MapType ]) data = [ (\"P101\", {\"color\": \"red\", \"size\": \"M\", \"material\": \"cotton\"}), (\"P102\", {\"color\": \"blue\", \"size\": \"L\"}), (\"P103\", {}) ] df = spark.createDataFrame(data, schema=product_schema) df.printSchema() # Resultado: # root # |-- product_id: string (nullable = true) # |-- attributes: map (nullable = true) # | |-- key: string # | |-- value: string (containsNull = true) df.show(truncate=False) # Resultado: # +----------+-----------------------------------+ # |product_id|attributes | # +----------+-----------------------------------+ # |P101 |{color -> red, size -> M, material -> cotton}| # |P102 |{color -> blue, size -> L} | # |P103 |{} | # +----------+-----------------------------------+ # Acceder a elementos de mapa (se usa con `getItem` o notaci\u00f3n de corchetes) from pyspark.sql.functions import col df.select(\"product_id\", col(\"attributes\")[\"color\"].alias(\"product_color\")).show() # Resultado: # +----------+-------------+ # |product_id|product_color| # +----------+-------------+ # | P101| red| # | P102| blue| # | P103| null| # +----------+-------------+ 2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON) Spark es vers\u00e1til en la lectura y escritura de datos, soportando una amplia gama de formatos de archivo. La elecci\u00f3n del formato adecuado es crucial para el rendimiento y la eficiencia del almacenamiento en entornos de Big Data. Los formatos columnares como Parquet y ORC son altamente recomendados para el an\u00e1lisis debido a su eficiencia en la lectura y compresi\u00f3n. Lectura de Datos La lectura de datos es la base para cualquier an\u00e1lisis. Spark proporciona un API spark.read muy flexible para cargar datos desde diversas fuentes. Lectura de archivos CSV: Ideal para datos tabulares simples. Es importante configurar header=True si el archivo tiene encabezados y inferSchema=True para que Spark intente adivinar los tipos de datos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteCSV\").getOrCreate() # Crear un archivo CSV de ejemplo data_csv = \"\"\"id,name,age,city 1,Alice,25,New York 2,Bob,30,Los Angeles 3,Charlie,22,Chicago\"\"\" with open(\"users.csv\", \"w\") as f: f.write(data_csv) # Leer un archivo CSV df_csv = spark.read.csv(\"users.csv\", header=True, inferSchema=True) df_csv.printSchema() df_csv.show() # Resultado: # root # |-- id: integer (nullable = true) # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # +---+-------+---+----------+ # | id| name|age| city| # +---+-------+---+----------+ # | 1| Alice| 25| New York| # | 2| Bob| 30|Los Angeles| # | 3|Charlie| 22| Chicago| # +---+-------+---+----------+ Lectura de archivos JSON: \u00datil para datos semi-estructurados. Spark puede inferir el esquema autom\u00e1ticamente. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteJSON\").getOrCreate() # Crear un archivo JSON de ejemplo data_json = \"\"\" {\"id\": 1, \"name\": \"Alice\", \"hobbies\": [\"reading\", \"hiking\"]} {\"id\": 2, \"name\": \"Bob\", \"hobbies\": [\"gaming\"]} {\"id\": 3, \"name\": \"Charlie\", \"hobbies\": []} \"\"\" with open(\"users.json\", \"w\") as f: f.write(data_json) # Leer un archivo JSON df_json = spark.read.json(\"users.json\") df_json.printSchema() df_json.show(truncate=False) # Resultado: # root # |-- hobbies: array (nullable = true) # | |-- element: string (containsNull = true) # |-- id: long (nullable = true) # |-- name: string (nullable = true) # +--------------------+---+-------+ # |hobbies |id |name | # +--------------------+---+-------+ # |[reading, hiking] |1 |Alice | # |[gaming] |2 |Bob | # |[] |3 |Charlie| # +--------------------+---+-------+ Lectura de archivos Parquet: Formato columnar altamente eficiente para Big Data. Spark lo usa como formato por defecto y es altamente optimizado para el rendimiento. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteParquet\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Parquet data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.write.mode(\"overwrite\").parquet(\"users.parquet\") # Leer un archivo Parquet df_parquet = spark.read.parquet(\"users.parquet\") df_parquet.printSchema() df_parquet.show() # Resultado: # root # |-- name: string (nullable = true) # |-- age: long (nullable = true) # |-- city: string (nullable = true) # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ Lectura de archivos ORC: Otro formato columnar optimizado para Big Data, desarrollado por Apache Hive. Ofrece compresi\u00f3n y rendimiento similares a Parquet. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteORC\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como ORC data = [(\"ProductA\", 100, \"Electronics\"), (\"ProductB\", 50, \"Books\")] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\"]) df.write.mode(\"overwrite\").orc(\"products.orc\") # Leer un archivo ORC df_orc = spark.read.orc(\"products.orc\") df_orc.printSchema() df_orc.show() # Resultado: # root # |-- product_name: string (nullable = true) # |-- price: long (nullable = true) # |-- category: string (nullable = true) # +------------+-----+-----------+ # |product_name|price| category| # +------------+-----+-----------+ # | ProductA| 100|Electronics| # | ProductB| 50| Books| # +------------+-----+-----------+ Lectura de archivos Avro: Formato de serializaci\u00f3n de datos basado en esquema. Requiere el paquete spark-avro . from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteAvro\") \\ .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\ .getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Avro data = [(\"Event1\", \"typeA\", 1678886400), (\"Event2\", \"typeB\", 1678886460)] df = spark.createDataFrame(data, [\"event_id\", \"event_type\", \"timestamp\"]) df.write.mode(\"overwrite\").format(\"avro\").save(\"events.avro\") # Leer un archivo Avro df_avro = spark.read.format(\"avro\").load(\"events.avro\") df_avro.printSchema() df_avro.show() # Resultado: # root # |-- event_id: string (nullable = true) # |-- event_type: string (nullable = true) # |-- timestamp: long (nullable = true) # +--------+----------+----------+ # |event_id|event_type| timestamp| # +--------+----------+----------+ # | Event1| typeA|1678886400| # | Event2| typeB|1678886460| # +--------+----------+----------+ Escritura de Datos La escritura de DataFrames a diferentes formatos es tan importante como su lectura, ya que permite persistir los resultados de las transformaciones y compartirlos con otras aplicaciones o sistemas. Modos de escritura ( mode ): Cuando se escribe un DataFrame, es fundamental especificar el modo de escritura para evitar p\u00e9rdidas de datos o errores. overwrite : Sobrescribe los datos existentes en la ubicaci\u00f3n de destino. append : A\u00f1ade los datos al final de los datos existentes. ignore : Si los datos ya existen, la operaci\u00f3n de escritura no hace nada. error (o errorIfExists ): Lanza un error si los datos ya existen (modo por defecto). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WriteModes\").getOrCreate() data = [(\"A\", 1), (\"B\", 2)] df = spark.createDataFrame(data, [\"col1\", \"col2\"]) # Escribir en modo 'overwrite' df.write.mode(\"overwrite\").parquet(\"output_data.parquet\") # Escribir en modo 'append' data_new = [(\"C\", 3)] df_new = spark.createDataFrame(data_new, [\"col1\", \"col2\"]) df_new.write.mode(\"append\").parquet(\"output_data.parquet\") # Verificar el contenido spark.read.parquet(\"output_data.parquet\").show() # Resultado: # +----+----+ # |col1|col2| # +----+----+ # | A| 1| # | B| 2| # | C| 3| # +----+----+ # Escribir en modo 'ignore' (si el archivo ya existe, no har\u00e1 nada) df.write.mode(\"ignore\").csv(\"output_csv\", header=True) Particionamiento de salida ( partitionBy ): Permite organizar los datos en el sistema de archivos subyacente (HDFS, S3, ADLS) en directorios basados en el valor de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por las columnas de partici\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionBy\").getOrCreate() data = [(\"Sales\", 2023, 100), (\"Sales\", 2024, 120), (\"Marketing\", 2023, 80), (\"Marketing\", 2024, 90)] df = spark.createDataFrame(data, [\"department\", \"year\", \"revenue\"]) # Escribir con particionamiento por 'department' y 'year' df.write.mode(\"overwrite\").partitionBy(\"department\", \"year\").parquet(\"department_yearly_revenue.parquet\") # Esto crear\u00e1 una estructura de directorios como: # department_yearly_revenue.parquet/department=Sales/year=2023/part-....parquet # department_yearly_revenue.parquet/department=Sales/year=2024/part-....parquet # ... Manejo de directorios de salida: Spark crea un directorio para cada operaci\u00f3n de escritura. Dentro de este directorio, se encuentran los archivos de datos (partes) y un archivo _SUCCESS si la operaci\u00f3n fue exitosa. # Despu\u00e9s de ejecutar una escritura, puedes explorar la estructura de directorios. # Por ejemplo, para el caso de Parquet sin particionamiento: # ls -R users.parquet/ # Resultado (ejemplo): # users.parquet/: # _SUCCESS/ # part-00000-....snappy.parquet/ Tarea Ejercicios con PySpark : Crea un DataFrame a partir de la siguiente lista de tuplas: [(\"Juan\", \"Perez\", 30, \"Ingeniero\"), (\"Maria\", \"Lopez\", 25, \"Doctora\"), (\"Carlos\", \"Gomez\", 35, \"Abogado\")] . Define el esquema expl\u00edcitamente con las columnas nombre , apellido , edad (entero) y profesion . Luego, muestra el esquema y las primeras filas del DataFrame. Dado el DataFrame del ejercicio 1, selecciona \u00fanicamente las columnas nombre y profesion . Adem\u00e1s, renombra la columna nombre a primer_nombre en el DataFrame resultante. Al DataFrame original del ejercicio 1, a\u00f1ade una nueva columna llamada salario_base con un valor fijo de 50000 . Luego, crea otra columna salario_ajustado que sea salario_base m\u00e1s edad * 100 . Filtra el DataFrame resultante del ejercicio 3 para mostrar solo las personas cuya edad sea mayor a 28 Y su profesion sea Ingeniero o Abogado . Utilizando el DataFrame original del ejercicio 1, calcula el promedio de edad y la cantidad total de personas. Crea un DataFrame de empleados que incluya una columna contacto de tipo StructType con email y telefono como subcampos. Los datos de ejemplo podr\u00edan ser: [(\"Alice\", {\"email\": \"alice@example.com\", \"telefono\": \"123-456-7890\"})] . Muestra el esquema y accede al email de Alice. Crea un DataFrame de estudiantes con una columna cursos_inscritos de tipo ArrayType(StringType()) . Ejemplo de datos: [(\"Bob\", [\"Matem\u00e1ticas\", \"F\u00edsica\"]), (\"Eve\", [\"Qu\u00edmica\"])] . Muestra el esquema y filtra los estudiantes que est\u00e9n inscritos en Matem\u00e1ticas . Crea un archivo CSV llamado productos.csv con los siguientes datos (incluye encabezado): producto_id,nombre,precio,cantidad 1,Laptop,1200.50,10 2,Mouse,25.00,50 3,Teclado,75.99,30 Lee este archivo en un DataFrame, infiriendo el esquema y mostrando el esquema y el contenido. Crea un DataFrame con columnas region , mes y ventas . Los datos de ejemplo: [(\"Norte\", \"Enero\", 1000), (\"Sur\", \"Enero\", 800), (\"Norte\", \"Febrero\", 1100), (\"Sur\", \"Febrero\", 900)] . Guarda este DataFrame como archivos Parquet, particionando por region y mes . Luego, lee solo las ventas de la regi\u00f3n Norte en Enero para verificar la partici\u00f3n. Crea un archivo JSON llamado config.json con los siguientes datos (cada objeto en una l\u00ednea): {\"id\": 1, \"settings\": {\"theme\": \"dark\", \"notifications\": true}} {\"id\": 2, \"settings\": {\"theme\": \"light\", \"notifications\": false}} Lee este archivo en un DataFrame y muestra el theme para cada ID. Ejercicios con SparkSQL : Crea el mismo DataFrame del Ejercicio 1 de PySpark (empleados). Registra este DataFrame como una vista temporal llamada empleados_temp . Luego, ejecuta una consulta SQL para seleccionar todos los empleados. Usando la vista empleados_temp , escribe una consulta SparkSQL para seleccionar nombre , apellido y profesion de los empleados con edad menor a 30 . Sobre la vista empleados_temp , realiza una consulta SQL que seleccione nombre como primer_nombre y profesion como ocupacion . Utilizando empleados_temp , a\u00f1ade una columna calculada llamada edad_futura que sea la edad actual m\u00e1s 5 . Crea una vista temporal a partir de un DataFrame de ventas con columnas producto , region y cantidad . Datos de ejemplo: [(\"Laptop\", \"Norte\", 5), (\"Mouse\", \"Norte\", 10), (\"Laptop\", \"Sur\", 3)] . Calcula la SUM de cantidad por producto usando SparkSQL. Partiendo del DataFrame de empleados con contacto (email y telefono) del Ejercicio 6 de PySpark, crea una vista temporal. Luego, usa SparkSQL para seleccionar el nombre del empleado y su contacto.email . Utilizando el DataFrame de estudiantes con cursos_inscritos del Ejercicio 7 de PySpark, crea una vista temporal. Escribe una consulta SparkSQL para seleccionar los estudiantes que tienen Matem\u00e1ticas en su lista de cursos_inscritos (puedes necesitar una funci\u00f3n SQL de Spark para arrays). Crea el archivo productos.csv del Ejercicio 8 de PySpark. Luego, usando spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"productos.csv\") , crea un DataFrame y reg\u00edstralo como vista temporal productos_temp . Finalmente, selecciona todos los productos con un precio mayor a 50 . Guarda un DataFrame (por ejemplo, el de ventas del Ejercicio 9 de PySpark) como archivos Parquet en una ubicaci\u00f3n espec\u00edfica (ej: \"data/ventas_particionadas\" ). Luego, crea una tabla externa de SparkSQL apuntando a esa ubicaci\u00f3n ( CREATE TABLE ... USING PARQUET LOCATION ... ). Finalmente, consulta las ventas de una region espec\u00edfica directamente desde la tabla SQL. Usando el archivo config.json del Ejercicio 10 de PySpark, lee el JSON y crea una vista temporal config_temp . Escribe una consulta SparkSQL para extraer el valor del theme de la columna settings para cada id (esto requerir\u00e1 desanidaci\u00f3n o funciones JSON de SparkSQL).","title":"Fundamentos de DataFrames en Spark"},{"location":"tema21/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema21/#tema-21-fundamentos-de-dataframes-en-spark","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender la estructura y el funcionamiento de los DataFrames de Apache Spark, realizar operaciones fundamentales de manipulaci\u00f3n de datos, gestionar esquemas y tipos de datos complejos, y leer y escribir datos en los formatos m\u00e1s comunes utilizados en entornos de Big Data. Introducci\u00f3n : En el vasto universo del Big Data, la capacidad de procesar y analizar vol\u00famenes masivos de informaci\u00f3n es crucial. Apache Spark, con su motor de procesamiento distribuido, se ha consolidado como una herramienta indispensable para esta tarea. En el coraz\u00f3n de su eficiencia y facilidad de uso se encuentran los DataFrames, una abstracci\u00f3n de datos distribuida que organiza los datos en columnas con nombre, similar a una tabla en una base de datos relacional o una hoja de c\u00e1lculo. Esta estructura permite a los desarrolladores trabajar con datos de forma intuitiva y optimizada, aprovechando el poder de Spark para el procesamiento paralelo y distribuido. Desarrollo : Este tema se centrar\u00e1 en los pilares de la manipulaci\u00f3n de datos en Spark a trav\u00e9s de los DataFrames. Exploraremos c\u00f3mo los DataFrames facilitan las operaciones de transformaci\u00f3n y consulta, abstraen la complejidad de la distribuci\u00f3n de datos y proporcionan un API robusto para interactuar con ellos. Abordaremos desde las operaciones b\u00e1sicas como selecci\u00f3n y filtrado, hasta la comprensi\u00f3n de los esquemas y tipos de datos complejos, y la interacci\u00f3n con una variedad de formatos de archivo est\u00e1ndar de la industria.","title":"Tema 2.1 Fundamentos de DataFrames en Spark"},{"location":"tema21/#211-operaciones-con-dataframes","text":"Las operaciones con DataFrames en Spark son el n\u00facleo de la manipulaci\u00f3n de datos, permitiendo seleccionar, filtrar, agregar, unir y realizar una multitud de transformaciones sobre conjuntos de datos distribuidos de manera eficiente. A diferencia de los RDDs, los DataFrames ofrecen un nivel de abstracci\u00f3n superior, permitiendo a Spark optimizar internamente las operaciones gracias a la informaci\u00f3n del esquema.","title":"2.1.1 Operaciones con DataFrames"},{"location":"tema21/#creacion-de-dataframes","text":"La creaci\u00f3n de DataFrames es el primer paso para trabajar con datos en PySpark. Se pueden crear a partir de diversas fuentes, como listas de Python, RDDs existentes, o leyendo directamente desde archivos. Desde una lista de tuplas o diccionarios: from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() # Opci\u00f3n 1: Usando una lista de tuplas y definiendo el esquema data_tuples = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] schema_tuples = StructType([ StructField(\"name\", StringType(), True), StructField(\"id\", IntegerType(), True), StructField(\"city\", StringType(), True) ]) df_from_tuples = spark.createDataFrame(data_tuples, schema=schema_tuples) df_from_tuples.show() # Resultado: # +-------+---+----+ # | name| id|city| # +-------+---+----+ # | Alice| 1| NY| # | Bob| 2| LA| # |Charlie| 3| CHI| # +-------+---+----+ # Opci\u00f3n 2: Usando una lista de diccionarios (Spark infiere el esquema) data_dicts = [{\"name\": \"Alice\", \"id\": 1, \"city\": \"NY\"}, {\"name\": \"Bob\", \"id\": 2, \"city\": \"LA\"}, {\"name\": \"Charlie\", \"id\": 3, \"city\": \"CHI\"}] df_from_dicts = spark.createDataFrame(data_dicts) df_from_dicts.show() # Resultado: # +-------+---+----+ # | city| id|name| # +-------+---+----+ # | NY| 1|Alice| # | LA| 2| Bob| # | CHI| 3|Charlie| # +-------+---+----+ Desde un RDD existente: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate() rdd = spark.sparkContext.parallelize([(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]) df_from_rdd = spark.createDataFrame(rdd, [\"name\", \"id\"]) df_from_rdd.show() # Resultado: # +-------+---+ # | name| id| # +-------+---+ # | Alice| 1| # | Bob| 2| # |Charlie| 3| # +-------+---+ Desde un archivo (se ver\u00e1 en 2.1.3): # df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True) # df.show()","title":"Creaci\u00f3n de DataFrames"},{"location":"tema21/#transformaciones-de-dataframes","text":"Las transformaciones en DataFrames son operaciones lazy (perezosas), lo que significa que no se ejecutan hasta que se invoca una acci\u00f3n. Esto permite a Spark optimizar el plan de ejecuci\u00f3n. Selecci\u00f3n de columnas ( select y withColumn ): from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit spark = SparkSession.builder.appName(\"DataFrameTransformations\").getOrCreate() data = [(\"Alice\", 1, \"NY\"), (\"Bob\", 2, \"LA\"), (\"Charlie\", 3, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"id\", \"city\"]) # Seleccionar columnas espec\u00edficas df.select(\"name\", \"city\").show() # Resultado: # +-------+----+ # | name|city| # +-------+----+ # | Alice| NY| # | Bob| LA| # |Charlie| CHI| # +-------+----+ # Renombrar una columna al seleccionar df.select(col(\"name\").alias(\"full_name\"), \"city\").show() # Resultado: # +---------+----+ # |full_name|city| # +---------+----+ # | Alice| NY| # | Bob| LA| # | Charlie| CHI| # +---------+----+ # A\u00f1adir una nueva columna df.withColumn(\"country\", lit(\"USA\")).show() # Resultado: # +-------+---+----+-------+ # | name| id|city|country| # +-------+---+----+-------+ # | Alice| 1| NY| USA| # | Bob| 2| LA| USA| # |Charlie| 3| CHI| USA| # +-------+---+----+-------+ # Modificar una columna existente (ejemplo: incrementar id) df.withColumn(\"id_plus_10\", col(\"id\") + 10).show() # Resultado: # +-------+---+----+----------+ # | name| id|city|id_plus_10| # +-------+---+----+----------+ # | Alice| 1| NY| 11| # | Bob| 2| LA| 12| # |Charlie| 3| CHI| 13| # +-------+---+----+----------+ Filtrado de filas ( filter o where ): from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"DataFrameFiltering\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\"), (\"David\", 35, \"NY\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Filtrar por una condici\u00f3n simple df.filter(col(\"age\") > 25).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # | Bob| 30| LA| # |David| 35| NY| # +-----+---+----+ # Filtrar por m\u00faltiples condiciones df.filter((col(\"age\") > 20) & (col(\"city\") == \"NY\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # |David| 35| NY| # +-----+---+----+ # Usando el m\u00e9todo where (alias de filter) df.where(col(\"name\").like(\"A%\")).show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # +-----+---+----+ Agregaciones ( groupBy y funciones de agregaci\u00f3n): from pyspark.sql import SparkSession from pyspark.sql.functions import avg, count, sum, min, max spark = SparkSession.builder.appName(\"DataFrameAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000), (\"Dept1\", \"Bob\", 1200), (\"Dept2\", \"Charlie\", 900), (\"Dept2\", \"David\", 1500)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\"]) # Contar empleados por departamento df.groupBy(\"department\").count().show() # Resultado: # +----------+-----+ # |department|count| # +----------+-----+ # | Dept1| 2| # | Dept2| 2| # +----------+-----+ # Calcular salario promedio y m\u00e1ximo por departamento df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"), max(\"salary\").alias(\"max_salary\")).show() # Resultado: # +----------+----------+----------+ # |department|avg_salary|max_salary| # +----------+----------+----------+ # | Dept1| 1100.0| 1200| # | Dept2| 1200.0| 1500| # +----------+----------+----------+ # Sumar salarios por departamento df.groupBy(\"department\").agg(sum(\"salary\").alias(\"total_salary\")).show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | Dept1| 2200| # | Dept2| 2400| # +----------+------------+","title":"Transformaciones de DataFrames"},{"location":"tema21/#acciones-de-dataframes","text":"Las acciones son operaciones que disparan la ejecuci\u00f3n del plan de transformaciones y devuelven un resultado a la aplicaci\u00f3n del controlador o escriben datos en un sistema de almacenamiento. Mostrar datos ( show ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Mostrar las primeras filas del DataFrame df.show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ # Mostrar un n\u00famero espec\u00edfico de filas y truncar el contenido de las columnas si es largo df.show(numRows=1, truncate=False) # Resultado: # +-----+---+ # |name |age| # +-----+---+ # |Alice|25 | # +-----+---+ Contar filas ( count ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Contar el n\u00famero total de filas en el DataFrame num_rows = df.count() print(f\"N\u00famero de filas: {num_rows}\") # Resultado: N\u00famero de filas: 3 Recopilar datos ( collect , take , first ): from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameActions\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Recopilar todos los datos del DataFrame en una lista de Rows en el driver all_data = df.collect() print(f\"Todos los datos: {all_data}\") # Resultado: Todos los datos: [Row(name='Alice', age=25), Row(name='Bob', age=30), Row(name='Charlie', age=22)] # Tomar las primeras N filas first_two = df.take(2) print(f\"Primeras 2 filas: {first_two}\") # Resultado: Primeras 2 filas: [Row(name='Alice', age=25), Row(name='Bob', age=30)] # Obtener la primera fila first_row = df.first() print(f\"Primera fila: {first_row}\") # Resultado: Primera fila: Row(name='Alice', age=25)","title":"Acciones de DataFrames"},{"location":"tema21/#212-esquemas-y-tipos-de-datos-complejos","text":"El esquema de un DataFrame es una estructura fundamental que define los nombres de las columnas y sus tipos de datos correspondientes. Esta metadata es crucial para la optimizaci\u00f3n de Spark, ya que le permite saber c\u00f3mo se organizan los datos y aplicar optimizaciones de tipo de datos y de columna. La inferencia de esquema y la definici\u00f3n expl\u00edcita son dos formas de manejarlo, y Spark tambi\u00e9n soporta tipos de datos complejos como ArrayType , MapType y StructType para manejar estructuras anidadas.","title":"2.1.2 Esquemas y tipos de datos complejos"},{"location":"tema21/#inferencia-y-definicion-explicita-de-esquemas","text":"La forma en que Spark determina el esquema de un DataFrame es vital para la integridad y eficiencia del procesamiento de datos. Inferencia de esquema ( inferSchema=True ): Spark puede intentar adivinar el esquema de un archivo de datos (CSV, JSON, Parquet, etc.) leyendo una muestra. Esto es conveniente para la exploraci\u00f3n inicial, pero puede ser propenso a errores, especialmente con datos inconsistentes o tipos ambiguos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SchemaInference\").getOrCreate() # Creando un archivo CSV de ejemplo data_csv = \"\"\"name,age,city Alice,25,NY Bob,30,LA Charlie,null,CHI David,35,NY\"\"\" with open(\"data.csv\", \"w\") as f: f.write(data_csv) # Inferencia de esquema al leer un CSV df_inferred = spark.read.csv(\"data.csv\", header=True, inferSchema=True) df_inferred.printSchema() # Resultado (ejemplo): # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # Observar que \"age\" se infiri\u00f3 como IntegerType, lo cual es correcto si no hay valores no num\u00e9ricos. # Si hubiera un valor no num\u00e9rico, podr\u00eda inferirse como StringType o fallar la inferencia. Definici\u00f3n expl\u00edcita de esquema ( StructType y StructField ): Es la forma m\u00e1s robusta y recomendada para entornos de producci\u00f3n. Permite controlar con precisi\u00f3n los tipos de datos y la nulabilidad, evitando problemas de inferencia y mejorando el rendimiento. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType spark = SparkSession.builder.appName(\"ExplicitSchema\").getOrCreate() # Definir un esquema expl\u00edcito custom_schema = StructType([ StructField(\"employee_name\", StringType(), True), StructField(\"employee_id\", IntegerType(), False), # Not nullable StructField(\"salary\", DoubleType(), True), StructField(\"is_active\", BooleanType(), True) ]) data = [(\"Alice\", 1, 50000.0, True), (\"Bob\", 2, 60000.50, False), (\"Charlie\", 3, 75000.0, True)] df_explicit = spark.createDataFrame(data, schema=custom_schema) df_explicit.printSchema() # Resultado: # root # |-- employee_name: string (nullable = true) # |-- employee_id: integer (nullable = false) # |-- salary: double (nullable = true) # |-- is_active: boolean (nullable = true) # Intentar insertar un valor nulo en una columna no nula causar\u00eda un error o comportamiento inesperado # data_error = [(\"David\", None, 80000.0, True)] # Esto generar\u00eda un error si intentas crear el DF # df_error = spark.createDataFrame(data_error, schema=custom_schema) Acceder y manipular el esquema ( df.schema ): El esquema de un DataFrame es accesible a trav\u00e9s del atributo .schema , que devuelve un objeto StructType . from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"SchemaAccess\").getOrCreate() data = [(\"Alice\", 1), (\"Bob\", 2)] df = spark.createDataFrame(data, [\"name\", \"id\"]) # Acceder al esquema print(df.schema) # Resultado: StructType([StructField('name', StringType(), True), StructField('id', LongType(), True)]) # Iterar sobre los campos del esquema for field in df.schema: print(f\"Nombre de columna: {field.name}, Tipo: {field.dataType}, Nulable: {field.nullable}\") # Resultado: # Nombre de columna: name, Tipo: StringType, Nulable: True # Nombre de columna: id, Tipo: LongType, Nulable: True","title":"Inferencia y Definici\u00f3n Expl\u00edcita de Esquemas"},{"location":"tema21/#tipos-de-datos-complejos","text":"Spark permite manejar estructuras de datos m\u00e1s all\u00e1 de los tipos at\u00f3micos, lo que es fundamental para trabajar con datos semi-estructurados y anidados como JSON. StructType (Estructuras Anidadas/Registros): Representa una estructura similar a un objeto o un registro, donde cada campo tiene un nombre y un tipo de datos. Permite modelar objetos complejos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypes\").getOrCreate() # Definir un esquema con un StructType anidado address_schema = StructType([ StructField(\"street\", StringType(), True), StructField(\"city\", StringType(), True), StructField(\"zip\", StringType(), True) ]) person_schema = StructType([ StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"address\", address_schema, True) # Columna de tipo StructType ]) data = [ (\"Alice\", 25, (\"123 Main St\", \"NY\", \"10001\")), (\"Bob\", 30, (\"456 Oak Ave\", \"LA\", \"90001\")) ] df = spark.createDataFrame(data, schema=person_schema) df.printSchema() # Resultado: # root # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- address: struct (nullable = true) # | |-- street: string (nullable = true) # | |-- city: string (nullable = true) # | |-- zip: string (nullable = true) df.show(truncate=False) # Resultado: # +-----+---+-------------------------+ # |name |age|address | # +-----+---+-------------------------+ # |Alice|25 |{123 Main St, NY, 10001} | # |Bob |30 |{456 Oak Ave, LA, 90001} | # +-----+---+-------------------------+ # Acceder a campos anidados df.select(\"name\", \"address.city\").show() # Resultado: # +-----+----+ # |name |city| # +-----+----+ # |Alice|NY | # |Bob |LA | # +-----+----+ ArrayType (Arrays/Listas): Representa una colecci\u00f3n de elementos del mismo tipo. \u00datil para modelar listas o arreglos de datos dentro de una columna. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType spark = SparkSession.builder.appName(\"ComplexTypesArray\").getOrCreate() # Definir un esquema con un ArrayType course_schema = StructType([ StructField(\"student_name\", StringType(), True), StructField(\"grades\", ArrayType(IntegerType()), True) # Columna de tipo ArrayType ]) data = [ (\"Alice\", [90, 85, 92]), (\"Bob\", [78, 80]), (\"Charlie\", []) ] df = spark.createDataFrame(data, schema=course_schema) df.printSchema() # Resultado: # root # |-- student_name: string (nullable = true) # |-- grades: array (nullable = true) # | |-- element: integer (containsNull = true) df.show(truncate=False) # Resultado: # +------------+----------+ # |student_name|grades | # +------------+----------+ # |Alice |[90, 85, 92]| # |Bob |[78, 80] | # |Charlie |[] | # +------------+----------+ # Acceder a elementos de array (requiere funciones de Spark) from pyspark.sql.functions import size, array_contains df.select(\"student_name\", size(\"grades\").alias(\"num_grades\")).show() # Resultado: # +------------+----------+ # |student_name|num_grades| # +------------+----------+ # | Alice| 3| # | Bob| 2| # | Charlie| 0| # +------------+----------+ df.filter(array_contains(\"grades\", 90)).show() # Resultado: # +------------+----------+ # |student_name| grades| # +------------+----------+ # | Alice|[90, 85, 92]| # +------------+----------+ MapType (Mapas/Diccionarios): Representa una colecci\u00f3n de pares clave-valor. \u00datil para datos que se asemejan a diccionarios o JSON con claves din\u00e1micas. from pyspark.sql import SparkSession from pyspark.sql.types import StructType, StructField, StringType, MapType spark = SparkSession.builder.appName(\"ComplexTypesMap\").getOrCreate() # Definir un esquema con un MapType product_schema = StructType([ StructField(\"product_id\", StringType(), True), StructField(\"attributes\", MapType(StringType(), StringType()), True) # Columna de tipo MapType ]) data = [ (\"P101\", {\"color\": \"red\", \"size\": \"M\", \"material\": \"cotton\"}), (\"P102\", {\"color\": \"blue\", \"size\": \"L\"}), (\"P103\", {}) ] df = spark.createDataFrame(data, schema=product_schema) df.printSchema() # Resultado: # root # |-- product_id: string (nullable = true) # |-- attributes: map (nullable = true) # | |-- key: string # | |-- value: string (containsNull = true) df.show(truncate=False) # Resultado: # +----------+-----------------------------------+ # |product_id|attributes | # +----------+-----------------------------------+ # |P101 |{color -> red, size -> M, material -> cotton}| # |P102 |{color -> blue, size -> L} | # |P103 |{} | # +----------+-----------------------------------+ # Acceder a elementos de mapa (se usa con `getItem` o notaci\u00f3n de corchetes) from pyspark.sql.functions import col df.select(\"product_id\", col(\"attributes\")[\"color\"].alias(\"product_color\")).show() # Resultado: # +----------+-------------+ # |product_id|product_color| # +----------+-------------+ # | P101| red| # | P102| blue| # | P103| null| # +----------+-------------+","title":"Tipos de Datos Complejos"},{"location":"tema21/#213-lectura-y-escritura-en-formatos-populares-parquet-avro-orc-csv-json","text":"Spark es vers\u00e1til en la lectura y escritura de datos, soportando una amplia gama de formatos de archivo. La elecci\u00f3n del formato adecuado es crucial para el rendimiento y la eficiencia del almacenamiento en entornos de Big Data. Los formatos columnares como Parquet y ORC son altamente recomendados para el an\u00e1lisis debido a su eficiencia en la lectura y compresi\u00f3n.","title":"2.1.3 Lectura y escritura en formatos populares (Parquet, Avro, ORC, CSV, JSON)"},{"location":"tema21/#lectura-de-datos","text":"La lectura de datos es la base para cualquier an\u00e1lisis. Spark proporciona un API spark.read muy flexible para cargar datos desde diversas fuentes. Lectura de archivos CSV: Ideal para datos tabulares simples. Es importante configurar header=True si el archivo tiene encabezados y inferSchema=True para que Spark intente adivinar los tipos de datos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteCSV\").getOrCreate() # Crear un archivo CSV de ejemplo data_csv = \"\"\"id,name,age,city 1,Alice,25,New York 2,Bob,30,Los Angeles 3,Charlie,22,Chicago\"\"\" with open(\"users.csv\", \"w\") as f: f.write(data_csv) # Leer un archivo CSV df_csv = spark.read.csv(\"users.csv\", header=True, inferSchema=True) df_csv.printSchema() df_csv.show() # Resultado: # root # |-- id: integer (nullable = true) # |-- name: string (nullable = true) # |-- age: integer (nullable = true) # |-- city: string (nullable = true) # +---+-------+---+----------+ # | id| name|age| city| # +---+-------+---+----------+ # | 1| Alice| 25| New York| # | 2| Bob| 30|Los Angeles| # | 3|Charlie| 22| Chicago| # +---+-------+---+----------+ Lectura de archivos JSON: \u00datil para datos semi-estructurados. Spark puede inferir el esquema autom\u00e1ticamente. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteJSON\").getOrCreate() # Crear un archivo JSON de ejemplo data_json = \"\"\" {\"id\": 1, \"name\": \"Alice\", \"hobbies\": [\"reading\", \"hiking\"]} {\"id\": 2, \"name\": \"Bob\", \"hobbies\": [\"gaming\"]} {\"id\": 3, \"name\": \"Charlie\", \"hobbies\": []} \"\"\" with open(\"users.json\", \"w\") as f: f.write(data_json) # Leer un archivo JSON df_json = spark.read.json(\"users.json\") df_json.printSchema() df_json.show(truncate=False) # Resultado: # root # |-- hobbies: array (nullable = true) # | |-- element: string (containsNull = true) # |-- id: long (nullable = true) # |-- name: string (nullable = true) # +--------------------+---+-------+ # |hobbies |id |name | # +--------------------+---+-------+ # |[reading, hiking] |1 |Alice | # |[gaming] |2 |Bob | # |[] |3 |Charlie| # +--------------------+---+-------+ Lectura de archivos Parquet: Formato columnar altamente eficiente para Big Data. Spark lo usa como formato por defecto y es altamente optimizado para el rendimiento. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteParquet\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Parquet data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.write.mode(\"overwrite\").parquet(\"users.parquet\") # Leer un archivo Parquet df_parquet = spark.read.parquet(\"users.parquet\") df_parquet.printSchema() df_parquet.show() # Resultado: # root # |-- name: string (nullable = true) # |-- age: long (nullable = true) # |-- city: string (nullable = true) # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ Lectura de archivos ORC: Otro formato columnar optimizado para Big Data, desarrollado por Apache Hive. Ofrece compresi\u00f3n y rendimiento similares a Parquet. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteORC\").getOrCreate() # Primero, creamos un DataFrame y lo guardamos como ORC data = [(\"ProductA\", 100, \"Electronics\"), (\"ProductB\", 50, \"Books\")] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\"]) df.write.mode(\"overwrite\").orc(\"products.orc\") # Leer un archivo ORC df_orc = spark.read.orc(\"products.orc\") df_orc.printSchema() df_orc.show() # Resultado: # root # |-- product_name: string (nullable = true) # |-- price: long (nullable = true) # |-- category: string (nullable = true) # +------------+-----+-----------+ # |product_name|price| category| # +------------+-----+-----------+ # | ProductA| 100|Electronics| # | ProductB| 50| Books| # +------------+-----+-----------+ Lectura de archivos Avro: Formato de serializaci\u00f3n de datos basado en esquema. Requiere el paquete spark-avro . from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ReadWriteAvro\") \\ .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\ .getOrCreate() # Primero, creamos un DataFrame y lo guardamos como Avro data = [(\"Event1\", \"typeA\", 1678886400), (\"Event2\", \"typeB\", 1678886460)] df = spark.createDataFrame(data, [\"event_id\", \"event_type\", \"timestamp\"]) df.write.mode(\"overwrite\").format(\"avro\").save(\"events.avro\") # Leer un archivo Avro df_avro = spark.read.format(\"avro\").load(\"events.avro\") df_avro.printSchema() df_avro.show() # Resultado: # root # |-- event_id: string (nullable = true) # |-- event_type: string (nullable = true) # |-- timestamp: long (nullable = true) # +--------+----------+----------+ # |event_id|event_type| timestamp| # +--------+----------+----------+ # | Event1| typeA|1678886400| # | Event2| typeB|1678886460| # +--------+----------+----------+","title":"Lectura de Datos"},{"location":"tema21/#escritura-de-datos","text":"La escritura de DataFrames a diferentes formatos es tan importante como su lectura, ya que permite persistir los resultados de las transformaciones y compartirlos con otras aplicaciones o sistemas. Modos de escritura ( mode ): Cuando se escribe un DataFrame, es fundamental especificar el modo de escritura para evitar p\u00e9rdidas de datos o errores. overwrite : Sobrescribe los datos existentes en la ubicaci\u00f3n de destino. append : A\u00f1ade los datos al final de los datos existentes. ignore : Si los datos ya existen, la operaci\u00f3n de escritura no hace nada. error (o errorIfExists ): Lanza un error si los datos ya existen (modo por defecto). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WriteModes\").getOrCreate() data = [(\"A\", 1), (\"B\", 2)] df = spark.createDataFrame(data, [\"col1\", \"col2\"]) # Escribir en modo 'overwrite' df.write.mode(\"overwrite\").parquet(\"output_data.parquet\") # Escribir en modo 'append' data_new = [(\"C\", 3)] df_new = spark.createDataFrame(data_new, [\"col1\", \"col2\"]) df_new.write.mode(\"append\").parquet(\"output_data.parquet\") # Verificar el contenido spark.read.parquet(\"output_data.parquet\").show() # Resultado: # +----+----+ # |col1|col2| # +----+----+ # | A| 1| # | B| 2| # | C| 3| # +----+----+ # Escribir en modo 'ignore' (si el archivo ya existe, no har\u00e1 nada) df.write.mode(\"ignore\").csv(\"output_csv\", header=True) Particionamiento de salida ( partitionBy ): Permite organizar los datos en el sistema de archivos subyacente (HDFS, S3, ADLS) en directorios basados en el valor de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por las columnas de partici\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionBy\").getOrCreate() data = [(\"Sales\", 2023, 100), (\"Sales\", 2024, 120), (\"Marketing\", 2023, 80), (\"Marketing\", 2024, 90)] df = spark.createDataFrame(data, [\"department\", \"year\", \"revenue\"]) # Escribir con particionamiento por 'department' y 'year' df.write.mode(\"overwrite\").partitionBy(\"department\", \"year\").parquet(\"department_yearly_revenue.parquet\") # Esto crear\u00e1 una estructura de directorios como: # department_yearly_revenue.parquet/department=Sales/year=2023/part-....parquet # department_yearly_revenue.parquet/department=Sales/year=2024/part-....parquet # ... Manejo de directorios de salida: Spark crea un directorio para cada operaci\u00f3n de escritura. Dentro de este directorio, se encuentran los archivos de datos (partes) y un archivo _SUCCESS si la operaci\u00f3n fue exitosa. # Despu\u00e9s de ejecutar una escritura, puedes explorar la estructura de directorios. # Por ejemplo, para el caso de Parquet sin particionamiento: # ls -R users.parquet/ # Resultado (ejemplo): # users.parquet/: # _SUCCESS/ # part-00000-....snappy.parquet/","title":"Escritura de Datos"},{"location":"tema21/#tarea","text":"Ejercicios con PySpark : Crea un DataFrame a partir de la siguiente lista de tuplas: [(\"Juan\", \"Perez\", 30, \"Ingeniero\"), (\"Maria\", \"Lopez\", 25, \"Doctora\"), (\"Carlos\", \"Gomez\", 35, \"Abogado\")] . Define el esquema expl\u00edcitamente con las columnas nombre , apellido , edad (entero) y profesion . Luego, muestra el esquema y las primeras filas del DataFrame. Dado el DataFrame del ejercicio 1, selecciona \u00fanicamente las columnas nombre y profesion . Adem\u00e1s, renombra la columna nombre a primer_nombre en el DataFrame resultante. Al DataFrame original del ejercicio 1, a\u00f1ade una nueva columna llamada salario_base con un valor fijo de 50000 . Luego, crea otra columna salario_ajustado que sea salario_base m\u00e1s edad * 100 . Filtra el DataFrame resultante del ejercicio 3 para mostrar solo las personas cuya edad sea mayor a 28 Y su profesion sea Ingeniero o Abogado . Utilizando el DataFrame original del ejercicio 1, calcula el promedio de edad y la cantidad total de personas. Crea un DataFrame de empleados que incluya una columna contacto de tipo StructType con email y telefono como subcampos. Los datos de ejemplo podr\u00edan ser: [(\"Alice\", {\"email\": \"alice@example.com\", \"telefono\": \"123-456-7890\"})] . Muestra el esquema y accede al email de Alice. Crea un DataFrame de estudiantes con una columna cursos_inscritos de tipo ArrayType(StringType()) . Ejemplo de datos: [(\"Bob\", [\"Matem\u00e1ticas\", \"F\u00edsica\"]), (\"Eve\", [\"Qu\u00edmica\"])] . Muestra el esquema y filtra los estudiantes que est\u00e9n inscritos en Matem\u00e1ticas . Crea un archivo CSV llamado productos.csv con los siguientes datos (incluye encabezado): producto_id,nombre,precio,cantidad 1,Laptop,1200.50,10 2,Mouse,25.00,50 3,Teclado,75.99,30 Lee este archivo en un DataFrame, infiriendo el esquema y mostrando el esquema y el contenido. Crea un DataFrame con columnas region , mes y ventas . Los datos de ejemplo: [(\"Norte\", \"Enero\", 1000), (\"Sur\", \"Enero\", 800), (\"Norte\", \"Febrero\", 1100), (\"Sur\", \"Febrero\", 900)] . Guarda este DataFrame como archivos Parquet, particionando por region y mes . Luego, lee solo las ventas de la regi\u00f3n Norte en Enero para verificar la partici\u00f3n. Crea un archivo JSON llamado config.json con los siguientes datos (cada objeto en una l\u00ednea): {\"id\": 1, \"settings\": {\"theme\": \"dark\", \"notifications\": true}} {\"id\": 2, \"settings\": {\"theme\": \"light\", \"notifications\": false}} Lee este archivo en un DataFrame y muestra el theme para cada ID. Ejercicios con SparkSQL : Crea el mismo DataFrame del Ejercicio 1 de PySpark (empleados). Registra este DataFrame como una vista temporal llamada empleados_temp . Luego, ejecuta una consulta SQL para seleccionar todos los empleados. Usando la vista empleados_temp , escribe una consulta SparkSQL para seleccionar nombre , apellido y profesion de los empleados con edad menor a 30 . Sobre la vista empleados_temp , realiza una consulta SQL que seleccione nombre como primer_nombre y profesion como ocupacion . Utilizando empleados_temp , a\u00f1ade una columna calculada llamada edad_futura que sea la edad actual m\u00e1s 5 . Crea una vista temporal a partir de un DataFrame de ventas con columnas producto , region y cantidad . Datos de ejemplo: [(\"Laptop\", \"Norte\", 5), (\"Mouse\", \"Norte\", 10), (\"Laptop\", \"Sur\", 3)] . Calcula la SUM de cantidad por producto usando SparkSQL. Partiendo del DataFrame de empleados con contacto (email y telefono) del Ejercicio 6 de PySpark, crea una vista temporal. Luego, usa SparkSQL para seleccionar el nombre del empleado y su contacto.email . Utilizando el DataFrame de estudiantes con cursos_inscritos del Ejercicio 7 de PySpark, crea una vista temporal. Escribe una consulta SparkSQL para seleccionar los estudiantes que tienen Matem\u00e1ticas en su lista de cursos_inscritos (puedes necesitar una funci\u00f3n SQL de Spark para arrays). Crea el archivo productos.csv del Ejercicio 8 de PySpark. Luego, usando spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"productos.csv\") , crea un DataFrame y reg\u00edstralo como vista temporal productos_temp . Finalmente, selecciona todos los productos con un precio mayor a 50 . Guarda un DataFrame (por ejemplo, el de ventas del Ejercicio 9 de PySpark) como archivos Parquet en una ubicaci\u00f3n espec\u00edfica (ej: \"data/ventas_particionadas\" ). Luego, crea una tabla externa de SparkSQL apuntando a esa ubicaci\u00f3n ( CREATE TABLE ... USING PARQUET LOCATION ... ). Finalmente, consulta las ventas de una region espec\u00edfica directamente desde la tabla SQL. Usando el archivo config.json del Ejercicio 10 de PySpark, lee el JSON y crea una vista temporal config_temp . Escribe una consulta SparkSQL para extraer el valor del theme de la columna settings para cada id (esto requerir\u00e1 desanidaci\u00f3n o funciones JSON de SparkSQL).","title":"Tarea"},{"location":"tema22/","text":"2. PySpark y SparkSQL Tema 2.2 Manipulaci\u00f3n y Transformaci\u00f3n de Datos Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de aplicar t\u00e9cnicas avanzadas de manipulaci\u00f3n y transformaci\u00f3n sobre DataFrames de Apache Spark, utilizando un amplio rango de funciones integradas, creando funciones personalizadas (UDFs) para l\u00f3gica espec\u00edfica, y comprendiendo c\u00f3mo el particionamiento y el paralelismo influyen en el procesamiento eficiente de grandes vol\u00famenes de datos distribuidos. Introducci\u00f3n : La capacidad de transformar datos brutos en informaci\u00f3n valiosa es el coraz\u00f3n del an\u00e1lisis de Big Data. En Apache Spark, los DataFrames no solo ofrecen una interfaz intuitiva para trabajar con datos estructurados y semi-estructurados, sino que tambi\u00e9n proporcionan un conjunto robusto de operaciones y funciones para la manipulaci\u00f3n y limpieza de datos a escala. Desde simples selecciones y filtrados hasta complejas agregaciones y uniones, Spark permite a los usuarios moldear sus datos para satisfacer las necesidades de an\u00e1lisis, modelado o visualizaci\u00f3n, todo ello aprovechando su arquitectura distribuida subyacente. Desarrollo : Este tema profundiza en las capacidades de manipulaci\u00f3n de DataFrames de Spark. Retomaremos y ampliaremos las operaciones fundamentales, exploraremos el vasto cat\u00e1logo de funciones integradas de Spark SQL y aprenderemos a extender esta funcionalidad creando nuestras propias funciones definidas por el usuario (UDFs). Adem\u00e1s, abordaremos conceptos cruciales como el particionamiento y el paralelismo, fundamentales para optimizar el rendimiento y escalar el procesamiento de datos distribuidos de manera efectiva. Comprender estos conceptos es clave para escribir c\u00f3digo Spark eficiente y robusto en escenarios de Big Data. 2.2.1 Operaciones con DataFrames (selecci\u00f3n, filtrado, agregaciones) Si bien ya se introdujeron las operaciones b\u00e1sicas en el tema 2.1, esta secci\u00f3n se enfoca en profundizar y mostrar ejemplos m\u00e1s avanzados y combinaciones de estas operaciones, destacando su poder para la limpieza y preparaci\u00f3n de datos. La flexibilidad del API de DataFrames permite encadenar m\u00faltiples transformaciones, construyendo flujos de trabajo de datos complejos de manera legible y eficiente. Selecci\u00f3n Avanzada de Columnas M\u00e1s all\u00e1 de simplemente elegir columnas por nombre, Spark ofrece potentes capacidades para manipular columnas existentes o crear nuevas basadas en expresiones complejas. Seleccionar y Renombrar M\u00faltiples Columnas din\u00e1micamente: Es com\u00fan necesitar seleccionar un subconjunto de columnas y, al mismo tiempo, renombrarlas. Esto se puede hacer de forma program\u00e1tica utilizando listas de columnas y aplicando aliases. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedSelection\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\"), (\"Charlie\", 22, \"CHI\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Seleccionar y renombrar m\u00faltiples columnas selected_cols = [col(\"name\").alias(\"full_name\"), col(\"age\"), col(\"city\").alias(\"location\")] df.select(*selected_cols).show() # Resultado: # +---------+---+--------+ # |full_name|age|location| # +---------+---+--------+ # | Alice| 25| NY| # | Bob| 30| LA| # | Charlie| 22| CHI| # +---------+---+--------+ Uso de expresiones SQL en select : Spark permite incrustar expresiones SQL directamente dentro de la funci\u00f3n select para mayor flexibilidad, especialmente cuando se trabaja con funciones complejas o l\u00f3gicas condicionales. from pyspark.sql import SparkSession from pyspark.sql.functions import expr spark = SparkSession.builder.appName(\"SqlExpressions\").getOrCreate() data = [(\"Alice\", 25, 50000), (\"Bob\", 30, 60000), (\"Charlie\", 22, 45000)] df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"]) # Calcular un bono basado en el salario usando una expresi\u00f3n SQL df.select(\"name\", \"salary\", expr(\"salary * 0.10 AS bonus\")).show() # Resultado: # +-------+------+-------+ # | name|salary| bonus| # +-------+------+-------+ # | Alice| 50000| 5000.0| # | Bob| 60000| 6000.0| # |Charlie| 45000| 4500.0| # +-------+------+-------+ Eliminar columnas ( drop ): Es una operaci\u00f3n com\u00fan para limpiar DataFrames, eliminando columnas que no son relevantes para el an\u00e1lisis posterior. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropColumn\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Eliminar una sola columna df.drop(\"country\").show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ # Eliminar m\u00faltiples columnas df.drop(\"age\", \"city\").show() # Resultado: # +-----+-------+ # | name|country| # +-----+-------+ # |Alice| USA| # | Bob| USA| # +-----+-------+ Filtrado Avanzado de Filas Las condiciones de filtrado pueden ser muy complejas, combinando m\u00faltiples operadores l\u00f3gicos y funciones. Combinar m\u00faltiples condiciones con & (AND), | (OR), ~ (NOT): Permite construir filtros sofisticados para aislar subconjuntos de datos espec\u00edficos. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedFiltering\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\"), (\"Mouse\", 25, \"Electronics\"), (\"Book\", 15, \"Books\"), (\"Monitor\", 300, \"Electronics\"), (\"Pen\", 2, \"Office\")] df = spark.createDataFrame(data, [\"product\", \"price\", \"category\"]) # Filtrar productos de \"Electronics\" con precio > 100 O productos de \"Books\" df.filter( (col(\"category\") == \"Electronics\") & (col(\"price\") > 100) | (col(\"category\") == \"Books\") ).show() # Resultado: # +--------+-----+-----------+ # | product|price| category| # +--------+-----+-----------+ # | Laptop| 1200|Electronics| # | Book| 15| Books| # | Monitor| 300|Electronics| # +--------+-----+-----------+ Uso de isin para filtrar por m\u00faltiples valores en una columna: Una forma concisa de filtrar filas donde una columna toma uno de varios valores posibles. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"FilteringIsIn\").getOrCreate() data = [(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"CHI\"), (\"David\", \"NY\")] df = spark.createDataFrame(data, [\"name\", \"city\"]) # Filtrar por ciudades espec\u00edficas df.filter(col(\"city\").isin(\"NY\", \"LA\")).show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # |David| NY| # +-----+----+ Manejo de valores nulos en filtros ( isNull , isNotNull , na.drop ): Es crucial manejar los valores nulos al filtrar para evitar resultados inesperados o errores. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"HandlingNullsFiltering\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 22), (\"David\", None)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Filtrar filas donde 'age' no es nulo df.filter(col(\"age\").isNotNull()).show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ # Filtrar filas donde 'age' es nulo df.filter(col(\"age\").isNull()).show() # Resultado: # +-----+----+ # | name| age| # +-----+----+ # | Bob|null| # |David|null| # +-----+----+ # Eliminar filas con cualquier valor nulo df.na.drop().show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ Agregaciones Avanzadas y Agrupamiento Las agregaciones son potentes para resumir datos. Spark permite agregaciones por m\u00faltiples columnas, con funciones de ventana y pivoteo. Agregaciones sobre m\u00faltiples columnas con agg : Permite aplicar m\u00faltiples funciones de agregaci\u00f3n a diferentes columnas en una sola operaci\u00f3n de groupBy . from pyspark.sql import SparkSession from pyspark.sql.functions import avg, sum, count, col spark = SparkSession.builder.appName(\"MultiAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000, 5), (\"Dept1\", \"Bob\", 1200, 8), (\"Dept2\", \"Charlie\", 900, 3), (\"Dept2\", \"David\", 1500, 10)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\", \"projects_completed\"]) # Agregaciones m\u00faltiples por departamento df.groupBy(\"department\").agg( avg(col(\"salary\")).alias(\"avg_salary\"), sum(col(\"projects_completed\")).alias(\"total_projects\"), count(col(\"name\")).alias(\"num_employees\") ).show() # Resultado: # +----------+----------+--------------+-------------+ # |department|avg_salary|total_projects|num_employees| # +----------+----------+--------------+-------------+ # | Dept1| 1100.0| 13| 2| # | Dept2| 1200.0| 13| 2| # +----------+----------+--------------+-------------+ Pivoteo de datos ( pivot ): Transforma filas en columnas, muy \u00fatil para an\u00e1lisis de series temporales o reportes. from pyspark.sql import SparkSession from pyspark.sql.functions import sum spark = SparkSession.builder.appName(\"PivotOperation\").getOrCreate() data = [(\"Sales\", \"Jan\", 100), (\"Sales\", \"Feb\", 120), (\"Marketing\", \"Jan\", 80), (\"Marketing\", \"Feb\", 90), (\"Sales\", \"Mar\", 150)] df = spark.createDataFrame(data, [\"department\", \"month\", \"revenue\"]) # Pivoteo de ingresos por departamento y mes df.groupBy(\"department\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).agg(sum(\"revenue\")).show() # Resultado: # +----------+---+---+---+ # |department|Jan|Feb|Mar| # +----------+---+---+---+ # | Sales|100|120|150| # | Marketing| 80| 90|null| # +----------+---+---+---+ Uniones de DataFrames ( join ): Combina dos DataFrames en funci\u00f3n de una o m\u00e1s claves comunes. Spark soporta varios tipos de uniones (inner, outer, left, right, anti, semi). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameJoins\").getOrCreate() # DataFrame de empleados employees_data = [(\"Alice\", 1, \"Sales\"), (\"Bob\", 2, \"HR\"), (\"Charlie\", 3, \"IT\")] employees_df = spark.createDataFrame(employees_data, [\"name\", \"emp_id\", \"dept_id\"]) # DataFrame de departamentos departments_data = [(1, \"Sales\", \"NY\"), (2, \"HR\", \"LA\"), (4, \"Finance\", \"CHI\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"]) # Inner Join: solo filas que coinciden en ambas tablas employees_df.join(departments_df, on=\"dept_id\", how=\"inner\").show() # Resultado: # +-------+-----+---------+---------+--------+ # |dept_id| name| emp_id|dept_name|location| # +-------+-----+---------+---------+--------+ # | 1|Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # +-------+-----+---------+---------+--------+ # Left Outer Join: todas las filas de la izquierda, y las que coinciden de la derecha employees_df.join(departments_df, on=\"dept_id\", how=\"left_outer\").show() # Resultado: # +-------+-------+------+---------+---------+--------+ # |dept_id| name|emp_id|dept_name|location| # +-------+-------+------+---------+---------+--------+ # | 1| Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # | 3|Charlie| 3| null| null| # +-------+-------+------+---------+---------+--------+ 2.2.2 Funciones integradas y definidas por el usuario (UDFs) Spark proporciona una rica biblioteca de funciones integradas ( pyspark.sql.functions ) que cubren una amplia gama de transformaciones de datos. Sin embargo, cuando la l\u00f3gica de negocio es muy espec\u00edfica y no est\u00e1 cubierta por las funciones existentes, las Funciones Definidas por el Usuario (UDFs) permiten extender la funcionalidad de Spark utilizando c\u00f3digo Python. Funciones Integradas de Spark SQL Estas funciones son altamente optimizadas y deben ser la primera opci\u00f3n para cualquier transformaci\u00f3n. Cubren operaciones num\u00e9ricas, de cadena, de fecha y hora, y de manipulaci\u00f3n de arrays y mapas. Funciones de cadena ( substring , concat_ws , length , lower , upper ): \u00datiles para manipular texto en columnas. from pyspark.sql import SparkSession from pyspark.sql.functions import col, substring, concat_ws, length, lower, upper spark = SparkSession.builder.appName(\"StringFunctions\").getOrCreate() data = [(\"john doe\",), (\"JANE SMITH\",)] df = spark.createDataFrame(data, [\"full_name\"]) # Extraer subcadena df.withColumn(\"first_3_chars\", substring(col(\"full_name\"), 1, 3)).show() # Resultado: # +----------+-------------+ # | full_name|first_3_chars| # +----------+-------------+ # | john doe| joh| # |JANE SMITH| JAN| # +----------+-------------+ # Concatenar con separador (necesita m\u00e1s columnas para ser \u00fatil, ejemplo conceptual) df.withColumn(\"formatted_name\", concat_ws(\", \", lower(col(\"full_name\")))).show() # En este caso, solo convierte a min\u00fasculas # Resultado: # +----------+--------------+ # | full_name|formatted_name| # +----------+--------------+ # | john doe| john doe| # |JANE SMITH| jane smith| # +----------+--------------+ # Obtener longitud y convertir a may\u00fasculas/min\u00fasculas df.select(col(\"full_name\"), length(col(\"full_name\")).alias(\"name_length\"), lower(col(\"full_name\")).alias(\"lower_name\"), upper(col(\"full_name\")).alias(\"upper_name\")).show() # Resultado: # +----------+-----------+----------+----------+ # | full_name|name_length|lower_name|upper_name| # +----------+-----------+----------+----------+ # | john doe| 8| john doe| JOHN DOE| # |JANE SMITH| 10|jane smith|JANE SMITH| # +----------+-----------+----------+----------+ Funciones de fecha y hora ( current_date , datediff , year , month , to_date , to_timestamp ): Esenciales para el procesamiento de datos temporales. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_date, datediff, year, month, to_date, to_timestamp spark = SparkSession.builder.appName(\"DateFunctions\").getOrCreate() data = [(\"2023-01-15\",), (\"2024-03-01\",)] df = spark.createDataFrame(data, [\"event_date\"]) # Convertir a tipo Date y obtener el a\u00f1o y mes df.withColumn(\"event_date_parsed\", to_date(col(\"event_date\"))) \\ .withColumn(\"current_date\", current_date()) \\ .withColumn(\"days_since_event\", datediff(col(\"current_date\"), col(\"event_date_parsed\"))) \\ .withColumn(\"event_year\", year(col(\"event_date_parsed\"))) \\ .withColumn(\"event_month\", month(col(\"event_date_parsed\"))) \\ .show() # Resultado (valores de d\u00edas_since_event variar\u00e1n con la fecha actual): # +----------+-----------------+------------+----------------+----------+-----------+ # |event_date|event_date_parsed|current_date|days_since_event|event_year|event_month| # +----------+-----------------+------------+----------------+----------+-----------+ # |2023-01-15| 2023-01-15| 2025-05-31| 867| 2023| 1| # |2024-03-01| 2024-03-01| 2025-05-31| 457| 2024| 3| # +----------+-----------------+------------+----------------+----------+-----------+ # Convertir a timestamp df_ts = spark.createDataFrame([(\"2023-01-15 10:30:00\",)], [\"datetime_str\"]) df_ts.withColumn(\"parsed_timestamp\", to_timestamp(col(\"datetime_str\"))).show() # Resultado: # +-------------------+--------------------+ # | datetime_str| parsed_timestamp| # +-------------------+--------------------+ # |2023-01-15 10:30:00|2023-01-15 10:30:00| # +-------------------+--------------------+ Funciones condicionales ( when , otherwise ): Permiten aplicar l\u00f3gica condicional para crear nuevas columnas o modificar existentes. from pyspark.sql import SparkSession from pyspark.sql.functions import col, when spark = SparkSession.builder.appName(\"ConditionalFunctions\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 35, \"LA\"), (\"Charlie\", 17, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Clasificar edad en categor\u00edas df.withColumn(\"age_group\", when(col(\"age\") < 18, \"Minor\") .when(col(\"age\") >= 18, \"Adult\") .otherwise(\"Unknown\") ).show() # Resultado: # +-------+---+----+---------+ # | name|age|city|age_group| # +-------+---+----+---------+ # | Alice| 25| NY| Adult| # | Bob| 35| LA| Adult| # |Charlie| 17| CHI| Minor| # +-------+---+----+---------+ Funciones Definidas por el Usuario (UDFs) Las UDFs permiten a los desarrolladores de Python extender la funcionalidad de Spark implementando l\u00f3gica personalizada. Aunque potentes, pueden tener un impacto en el rendimiento debido a la serializaci\u00f3n y deserializaci\u00f3n de datos entre la JVM (donde Spark se ejecuta) y el proceso Python. Creaci\u00f3n de UDFs simples: Se definen como funciones Python normales y luego se registran en Spark usando udf de pyspark.sql.functions . Es crucial especificar el tipo de retorno. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import StringType spark = SparkSession.builder.appName(\"SimpleUDF\").getOrCreate() data = [(\"alice\",), (\"BOB\",), (\"charlie\",)] df = spark.createDataFrame(data, [\"name\"]) # Definir una funci\u00f3n Python para capitalizar la primera letra def capitalize_name(name): return name.capitalize() if name else None # Registrar la UDF con el tipo de retorno capitalize_udf = udf(capitalize_name, StringType()) # Aplicar la UDF al DataFrame df.withColumn(\"capitalized_name\", capitalize_udf(col(\"name\"))).show() # Resultado: # +-------+----------------+ # | name|capitalized_name| # +-------+----------------+ # | alice| Alice| # | BOB| Bob| # |charlie| Charlie| # +-------+----------------+ UDFs con m\u00faltiples argumentos: Las UDFs pueden aceptar m\u00faltiples columnas como entrada. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import DoubleType spark = SparkSession.builder.appName(\"MultiArgUDF\").getOrCreate() data = [(1000, 0.10), (1200, 0.05), (800, 0.15)] df = spark.createDataFrame(data, [\"base_salary\", \"bonus_rate\"]) # Funci\u00f3n Python para calcular el salario total def calculate_total_salary(base_salary, bonus_rate): if base_salary is None or bonus_rate is None: return None return base_salary * (1 + bonus_rate) # Registrar la UDF total_salary_udf = udf(calculate_total_salary, DoubleType()) # Aplicar la UDF df.withColumn(\"total_salary\", total_salary_udf(col(\"base_salary\"), col(\"bonus_rate\"))).show() # Resultado: # +-----------+----------+------------+ # |base_salary|bonus_rate|total_salary| # +-----------+----------+------------+ # | 1000| 0.10| 1100.0| # | 1200| 0.05| 1260.0| # | 800| 0.15| 920.0| # +-----------+----------+------------+ Consideraciones de rendimiento de UDFs (Vectorized UDFs con Pandas): Para mitigar el costo de serializaci\u00f3n/deserializaci\u00f3n, Spark 2.3+ introdujo las UDFs vectorizadas (anteriormente \"Pandas UDFs\"). Estas UDFs operan en pandas.Series o pandas.DataFrame en lugar de una fila a la vez, lo que reduce la sobrecarga y mejora significativamente el rendimiento para ciertas operaciones. from pyspark.sql import SparkSession from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import DoubleType, StringType spark = SparkSession.builder.appName(\"PandasUDF\").getOrCreate() data = [(1, 10.0), (2, 20.0), (3, 30.0)] df = spark.createDataFrame(data, [\"id\", \"value\"]) # Pandas UDF para escalar valores (Series a Series) @pandas_udf(DoubleType(), PandasUDFType.SCALAR) def multiply_by_two(value: float) -> float: return value * 2 df.withColumn(\"value_doubled\", multiply_by_two(col(\"value\"))).show() # Resultado: # +---+-----+-------------+ # | id|value|value_doubled| # +---+-----+-------------+ # | 1| 10.0| 20.0| # | 2| 20.0| 40.0| # | 3| 30.0| 60.0| # +---+-----+-------------+ # Pandas UDF para agregaci\u00f3n (Series a escalar) @pandas_udf(StringType(), PandasUDFType.GROUPED_AGG) def concat_strings(col_series): return \"_\".join(col_series.astype(str)) df_agg = spark.createDataFrame([(\"A\", \"x\"), (\"A\", \"y\"), (\"B\", \"z\")], [\"group\", \"value\"]) df_agg.groupBy(\"group\").agg(concat_strings(col(\"value\")).alias(\"concatenated_values\")).show() # Resultado: # +-----+-------------------+ # |group|concatenated_values| # +-----+-------------------+ # | A| x_y| # | B| z| # +-----+-------------------+ 2.2.3 Particionamiento y paralelismo El particionamiento es un concepto fundamental en Spark que define c\u00f3mo se distribuyen los datos en el cl\u00faster. Un particionamiento adecuado es clave para optimizar el rendimiento de las operaciones, especialmente las que involucran shuffles (intercambio de datos entre nodos). El paralelismo se refiere a la cantidad de tareas que Spark puede ejecutar simult\u00e1neamente. Conceptos de Particionamiento Los datos en Spark se dividen en \"particiones\" l\u00f3gicas, cada una de las cuales es procesada por una tarea. La forma en que los datos se particionan afecta el rendimiento y la eficiencia de la computaci\u00f3n. \u00bfQu\u00e9 es una partici\u00f3n en Spark?: Una partici\u00f3n es una divisi\u00f3n l\u00f3gica de los datos de un RDD o DataFrame. Cada partici\u00f3n se puede almacenar en un nodo diferente del cl\u00faster y se procesa de forma independiente y paralela. M\u00e1s particiones no siempre es mejor; el n\u00famero \u00f3ptimo depende del tama\u00f1o de los datos, el n\u00famero de n\u00facleos del cl\u00faster y la naturaleza de las operaciones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionsConcept\").getOrCreate() data = [(i,) for i in range(100)] df = spark.createDataFrame(data, [\"value\"]) # Obtener el n\u00famero de particiones actual (por defecto Spark usa el n\u00famero de n\u00facleos disponibles o spark.sql.shuffle.partitions) print(f\"N\u00famero inicial de particiones: {df.rdd.getNumPartitions()}\") # Podemos re-particionar un DataFrame (esto implica un shuffle) df_repartitioned = df.repartition(10) print(f\"N\u00famero de particiones despu\u00e9s de repartition: {df_repartitioned.rdd.getNumPartitions()}\") # Una partici\u00f3n es como un bloque de trabajo para un n\u00facleo de CPU. # Si tenemos 100 particiones y 20 n\u00facleos, cada n\u00facleo procesar\u00e1 5 particiones en paralelo (idealmente). Visualizaci\u00f3n del n\u00famero de particiones ( df.rdd.getNumPartitions() ): Es importante saber cu\u00e1ntas particiones tiene un DataFrame para entender c\u00f3mo se distribuir\u00e1 el trabajo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CheckPartitions\").getOrCreate() # Leer un archivo de ejemplo para ver sus particiones # Si no tienes un archivo grande, crea uno peque\u00f1o y l\u00e9elo data = [(i, f\"Name_{i}\") for i in range(1000)] df_large = spark.createDataFrame(data, [\"id\", \"name\"]) df_large.write.mode(\"overwrite\").parquet(\"large_data.parquet\") # Leer el DataFrame df_read = spark.read.parquet(\"large_data.parquet\") # Ver el n\u00famero de particiones print(f\"N\u00famero de particiones del DataFrame le\u00eddo: {df_read.rdd.getNumPartitions()}\") # Generalmente, Spark intenta que el n\u00famero de particiones sea cercano al tama\u00f1o de bloque del HDFS (128MB por defecto) # o al n\u00famero de n\u00facleos en el cluster. Impacto del particionamiento en el rendimiento (Shuffle): Las operaciones que requieren agrupar o unir datos (como groupBy , join , orderBy ) a menudo implican un \"shuffle\", donde los datos se mueven entre los nodos del cl\u00faster. Un n\u00famero incorrecto de particiones puede llevar a un shuffle ineficiente, causando cuellos de botella. Demasiadas particiones peque\u00f1as pueden generar mucha sobrecarga, mientras que muy pocas pueden limitar el paralelismo. Skewed data: Si los datos est\u00e1n muy desequilibrados en las particiones (algunas particiones tienen muchos m\u00e1s datos que otras), esto puede causar cuellos de botella donde una o pocas tareas tardan mucho m\u00e1s en completarse. Small files problem: Si hay muchas particiones muy peque\u00f1as, la sobrecarga de gestionar cada una puede ser mayor que el tiempo de procesamiento real. Control del Particionamiento y Paralelismo Spark ofrece mecanismos para controlar c\u00f3mo se particionan los datos y el nivel de paralelismo. repartition() y coalesce() : repartition() crea un nuevo RDD/DataFrame con un n\u00famero especificado de particiones, distribuyendo los datos uniformemente. Esto siempre implica un shuffle completo. coalesce() reduce el n\u00famero de particiones sin shuffle si es posible (solo reduce el n\u00famero de particiones en el mismo nodo), o con un shuffle m\u00ednimo si se requiere. Es m\u00e1s eficiente para reducir particiones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"RepartitionCoalesce\").getOrCreate() data = [(i,) for i in range(1000)] df = spark.createDataFrame(data, [\"value\"]) print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\") # Var\u00eda seg\u00fan la configuraci\u00f3n local # Reparticionar a 4 particiones (siempre con shuffle) df_repartitioned = df.repartition(4) print(f\"Particiones despu\u00e9s de repartition(4): {df_repartitioned.rdd.getNumPartitions()}\") # Coalesce a 2 particiones (puede evitar shuffle si los datos ya est\u00e1n en menos de 2) df_coalesced = df_repartitioned.coalesce(2) print(f\"Particiones despu\u00e9s de coalesce(2): {df_coalesced.rdd.getNumPartitions()}\") # Coalesce a 1 partici\u00f3n (siempre implicar\u00e1 un shuffle si hay m\u00e1s de 1 partici\u00f3n) df_single_partition = df.coalesce(1) print(f\"Particiones despu\u00e9s de coalesce(1): {df_single_partition.rdd.getNumPartitions()}\") Configuraci\u00f3n de spark.sql.shuffle.partitions : Este par\u00e1metro controla el n\u00famero de particiones que Spark utiliza por defecto despu\u00e9s de una operaci\u00f3n de shuffle. Un valor bien ajustado puede mejorar dr\u00e1sticamente el rendimiento de las operaciones de agregaci\u00f3n y uni\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import count # Configurar el n\u00famero de particiones de shuffle antes de crear la SparkSession spark = SparkSession.builder \\ .appName(\"ShufflePartitions\") \\ .config(\"spark.sql.shuffle.partitions\", \"8\") \\ .getOrCreate() print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\") data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4), (\"B\", 5)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # La operaci\u00f3n groupBy implicar\u00e1 un shuffle, y el resultado tendr\u00e1 8 particiones. df_agg = df.groupBy(\"key\").agg(count(\"*\")).repartition(1) # Repartition para mostrar en una sola salida df_agg.show() # Para verificar el n\u00famero de particiones de la salida de la agregaci\u00f3n (antes del repartition) # print(df_agg.rdd.getNumPartitions()) Estrategias para optimizar particiones en Joins: Broadcast Join: Cuando un DataFrame es peque\u00f1o (por defecto, menos de spark.sql.autoBroadcastJoinThreshold ), Spark puede \"broadcast\" (transmitir) el DataFrame peque\u00f1o a todos los nodos del cl\u00faster, evitando el shuffle de la tabla grande. Es muy eficiente. Hash Join: Cuando la clave de uni\u00f3n est\u00e1 particionada de forma similar en ambos DataFrames, Spark puede realizar un Hash Join, que es eficiente ya que los datos con la misma clave ya est\u00e1n en las mismas particiones. Sort-Merge Join: El join por defecto si las tablas no se pueden transmitir y no est\u00e1n co-ubicadas. Implica ordenar y fusionar las particiones, lo que puede ser costoso. from pyspark.sql import SparkSession from pyspark.sql.functions import broadcast # Para forzar un broadcast join spark = SparkSession.builder \\ .appName(\"JoinStrategies\") \\ .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") # Configurar el umbral de broadcast (por defecto 10MB) .getOrCreate() # DataFrame peque\u00f1o (menos de 10MB de datos) small_df = spark.createDataFrame([(1, \"DeptA\"), (2, \"DeptB\")], [\"dept_id\", \"dept_name\"]) # DataFrame grande large_df = spark.createDataFrame([(101, \"Alice\", 1), (102, \"Bob\", 2), (103, \"Charlie\", 1)], [\"emp_id\", \"emp_name\", \"dept_id\"]) # Spark autom\u00e1ticamente intentar\u00e1 un Broadcast Join si small_df est\u00e1 por debajo del umbral large_df.join(small_df, \"dept_id\", \"inner\").explain() # Ver el plan de ejecuci\u00f3n para confirmar BroadcastHashJoin # Resultado de explain() deber\u00eda mostrar \"*BroadcastHashJoin\" # Forzar un Broadcast Join (\u00fatil si Spark no lo infiere autom\u00e1ticamente o para depuraci\u00f3n) large_df.join(broadcast(small_df), \"dept_id\", \"inner\").explain() 2.2.4 Manejo de datos distribuidos Trabajar con datos distribuidos implica m\u00e1s que solo particionar. Se trata de entender c\u00f3mo Spark gestiona la memoria, el almacenamiento en cach\u00e9 y la persistencia para optimizar el acceso a los datos, y c\u00f3mo maneja los \"shuffles\" que son costosos. Persistencia y Almacenamiento en Cach\u00e9 Para evitar recalcular DataFrames que se utilizan m\u00faltiples veces, Spark permite persistirlos en memoria o en disco. cache() y persist() : cache() es un alias de persist() con el nivel de almacenamiento por defecto ( MEMORY_AND_DISK ). Almacena el DataFrame en la memoria del cl\u00faster para un acceso r\u00e1pido en operaciones futuras. persist() permite especificar diferentes niveles de almacenamiento (solo memoria, solo disco, memoria y disco, con o sin serializaci\u00f3n, con o sin replicaci\u00f3n). from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"Persistence\").getOrCreate() data = [(i, f\"Item_{i}\") for i in range(100000)] df = spark.createDataFrame(data, [\"id\", \"description\"]) # Persistir en memoria y disco (comportamiento por defecto de cache()) df.cache() # Equivalente a df.persist(StorageLevel.MEMORY_AND_DISK) # La primera acci\u00f3n dispara la carga y el almacenamiento en cach\u00e9 df.count() print(f\"DataFrame cached. Number of partitions: {df.rdd.getNumPartitions()}\") # Las acciones subsiguientes ser\u00e1n m\u00e1s r\u00e1pidas df.filter(df.id > 50000).show(5) # Persistir solo en memoria (m\u00e1s r\u00e1pido si los datos caben en memoria) df_mem_only = df.persist(StorageLevel.MEMORY_ONLY) df_mem_only.count() print(f\"DataFrame persisted in MEMORY_ONLY. Number of partitions: {df_mem_only.rdd.getNumPartitions()}\") # Despersistir (liberar los datos de cach\u00e9) df.unpersist() df_mem_only.unpersist() Niveles de almacenamiento ( StorageLevel ): Permiten un control granular sobre c\u00f3mo se almacenan los datos persistidos, equilibrando velocidad y tolerancia a fallos. MEMORY_ONLY : Guarda el RDD deserializado como objetos Python en la JVM. Si no cabe, recalcula. MEMORY_AND_DISK : Guarda en memoria; si no cabe, se desborda a disco. MEMORY_ONLY_SER : Igual que MEMORY_ONLY, pero los datos est\u00e1n serializados (ahorra espacio, pero m\u00e1s lento de acceder). MEMORY_AND_DISK_SER : Igual que MEMORY_AND_DISK, pero serializado. DISK_ONLY : Solo guarda en disco. Versiones con _2 al final (ej. MEMORY_ONLY_2 ): Replica la partici\u00f3n en dos nodos, para tolerancia a fallos. from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"StorageLevels\").getOrCreate() data = [(i,) for i in range(1000000)] df = spark.createDataFrame(data, [\"value\"]) # Persistir en disco para mayor confiabilidad en caso de poca memoria df.persist(StorageLevel.DISK_ONLY) df.count() # Dispara la persistencia print(\"DataFrame persisted to DISK_ONLY.\") # Persistir con replicaci\u00f3n (para tolerancia a fallos) df.persist(StorageLevel.MEMORY_AND_DISK_2) df.count() # Dispara la persistencia print(\"DataFrame persisted to MEMORY_AND_DISK_2 (replicated).\") df.unpersist() \u00bfCu\u00e1ndo usar cache() / persist() ?: Cuando un DataFrame se usa en m\u00faltiples acciones (ej. count() , show() , luego filter() , join() ). Cuando un DataFrame es el resultado de transformaciones costosas (ej. join complejos, agregaciones pesadas). Antes de aplicar algoritmos iterativos (ej. Machine Learning), donde los datos se leen repetidamente. En puntos intermedios de un flujo de trabajo de ETL complejo que se reutilizan. Comprensi\u00f3n y Optimizaci\u00f3n de Shuffles Los shuffles son la operaci\u00f3n m\u00e1s costosa en Spark porque implican la transferencia de datos a trav\u00e9s de la red entre diferentes nodos. Minimizar o optimizar los shuffles es clave para el rendimiento. Identificaci\u00f3n de operaciones que causan Shuffle: Cualquier operaci\u00f3n que requiera que Spark reorganice los datos a trav\u00e9s del cl\u00faster, como: groupBy() join() (excepto Broadcast Joins) orderBy() y sort() repartition() Ventanas anal\u00edticas (ciertas operaciones) from pyspark.sql import SparkSession from pyspark.sql.functions import count spark = SparkSession.builder.appName(\"IdentifyShuffles\").getOrCreate() data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # explain() muestra el plan de ejecuci\u00f3n y ayuda a identificar shuffles df.groupBy(\"key\").agg(count(\"value\")).explain() # En el plan de ejecuci\u00f3n, buscar etapas como \"Exchange\", \"Sort\", \"HashPartitioning\" # Estas indican una operaci\u00f3n de shuffle. # Ejemplo de salida parcial de explain: # == Physical Plan == # *(2) HashAggregate(keys=[key#123], functions=[count(value#124)]) # +- Exchange hashpartitioning(key#123, 200), [id=#12] <--- Esto es un shuffle # +- *(1) HashAggregate(keys=[key#123], functions=[partial_count(value#124)]) # +- *(1) Project [key#123, value#124] # +- *(1) Scan ExistingRDD Mitigaci\u00f3n de Shuffles (ej. Broadcast Join, Co-locaci\u00f3n de datos): Broadcast Join: Como se mencion\u00f3, usar broadcast() para DataFrames peque\u00f1os evita el shuffle de la tabla grande. Co-locaci\u00f3n de datos: Si los datos que se van a unir o agrupar ya est\u00e1n particionados de forma similar en el sistema de archivos (ej. Parquet particionado por la clave de uni\u00f3n), Spark puede evitar shuffles completos. Esto se logra mediante la escritura de datos particionados ( df.write.partitionBy(...) ). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ShuffleMitigation\").getOrCreate() # Peque\u00f1o DataFrame de lookup lookup_data = [(1, \"RegionA\"), (2, \"RegionB\")] lookup_df = spark.createDataFrame(lookup_data, [\"region_id\", \"region_name\"]) # DataFrame grande sales_data = [(101, 1, 100), (102, 2, 150), (103, 1, 200)] sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"region_id\", \"amount\"]) # Forzar Broadcast Join para evitar shuffle de sales_df from pyspark.sql.functions import broadcast sales_df.join(broadcast(lookup_df), \"region_id\").explain() # Deber\u00edas ver BroadcastHashJoin en el plan # Ejemplo de co-locaci\u00f3n con escritura particionada (requiere planificaci\u00f3n previa) # df_large.write.partitionBy(\"join_key\").parquet(\"path/to/partitioned_data\") # df_other_large.write.partitionBy(\"join_key\").parquet(\"path/to/other_partitioned_data\") # Luego, al unirlos, si Spark detecta que est\u00e1n co-ubicados, puede usar un SortMergeJoin m\u00e1s eficiente Manejo de datos desequilibrados (Skewed Data): Cuando un valor de clave tiene significativamente m\u00e1s filas que otros, la partici\u00f3n correspondiente se convierte en un cuello de botella. Salting: A\u00f1adir un sufijo aleatorio a la clave desequilibrada en el DataFrame grande y replicar las filas de la clave desequilibrada en el DataFrame peque\u00f1o con los mismos sufijos. Spark 3.x Adaptive Query Execution (AQE): AQE puede detectar y manejar skew de forma autom\u00e1tica durante la ejecuci\u00f3n de los joins, dividiendo las particiones grandes en subparticiones m\u00e1s peque\u00f1as. Habilitar spark.sql.adaptive.enabled a true (es por defecto en Spark 3.2+). from pyspark.sql import SparkSession from pyspark.sql.functions import lit, rand, concat spark = SparkSession.builder \\ .appName(\"SkewedJoin\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") # Habilitar AQE .getOrCreate() # Simular datos desequilibrados: 'HotKey' tiene muchos m\u00e1s registros data_large = [(i, \"NormalKey\") for i in range(1000)] + \\ [(i, \"HotKey\") for i in range(9000)] large_df = spark.createDataFrame(data_large, [\"id\", \"join_key\"]) data_small = [(\"NormalKey\", \"ValueA\"), (\"HotKey\", \"ValueB\")] small_df = spark.createDataFrame(data_small, [\"join_key\", \"value\"]) # Uni\u00f3n est\u00e1ndar (puede ser lenta debido a HotKey si AQE no est\u00e1 activo o no es suficiente) result_df = large_df.join(small_df, \"join_key\", \"inner\") result_df.explain() # Observar si AQE detecta el skew (si est\u00e1 habilitado) # Ejemplo de Salting (manual): # A\u00f1adir un \"salt\" aleatorio a las claves num_salt_buckets = 10 salted_large_df = large_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (rand() * num_salt_buckets).cast(\"int\"))) salted_small_df = small_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (lit(0) + rand() * num_salt_buckets).cast(\"int\"))) # Si 'HotKey' tiene mucho skew, se replicar\u00eda 'HotKey' en small_df para cada sufijo de salt. # Luego, se unir\u00eda por (join_key, salted_key) # Uni\u00f3n con salting: # result_salted = salted_large_df.join(salted_small_df, on=\"salted_key\", how=\"inner\") # result_salted.show() Tarea Crea un DataFrame de pedidos con las columnas order_id , customer_id , order_date (formato 'YYYY-MM-DD'), y amount (valor num\u00e9rico). Datos de ejemplo: [(\"ORD001\", \"C001\", \"2023-01-10\", 150.75), (\"ORD002\", \"C002\", \"2023-01-15\", 200.00), (\"ORD003\", \"C001\", \"2023-02-01\", 50.25), (\"ORD004\", \"C003\", \"2023-02-05\", 300.00)] Realiza las siguientes transformaciones: * A\u00f1ade una columna order_year que contenga solo el a\u00f1o de order_date . * A\u00f1ade una columna order_month que contenga el n\u00famero del mes de order_date . * A\u00f1ade una columna is_high_value que sea True si amount es mayor o igual a 100 , de lo contrario False . * Muestra el DataFrame resultante. Usando el DataFrame de pedidos del ejercicio 1, calcula el total_amount_spent y el num_orders por customer_id . Muestra los resultados. Crea un DataFrame de productos con columnas product_name y category . Datos de ejemplo: [(\"laptop Dell\", \"electronics\"), (\"teclado logitech\", \"electronics\"), (\"libro de cocina\", \"books\"), (\"Auriculares sony\", \"electronics\")] Realiza las siguientes transformaciones: * Normaliza la columna product_name a min\u00fasculas. * A\u00f1ade una columna brand que extraiga la primera palabra de product_name . * A\u00f1ade una columna product_type basada en la category : si es \"electronics\", \"Tech Gadget\"; si es \"books\", \"Reading Material\"; de lo contrario \"Other\". * Muestra el DataFrame resultante. Define una UDF de PySpark llamada classify_amount que tome un amount num\u00e9rico y devuelva una cadena: \"Peque\u00f1o\" si amount < 50 , \"Mediano\" si 50 <= amount < 200 , y \"Grande\" si amount >= 200 . Aplica esta UDF al DataFrame de pedidos del ejercicio 1 para crear una nueva columna amount_category . Muestra el DataFrame. Crea un segundo DataFrame de clientes con las columnas customer_id y customer_name . Datos de ejemplo: [(\"C001\", \"Ana Garcia\"), (\"C002\", \"Pedro Ruiz\"), (\"C003\", \"Laura Sanz\"), (\"C004\", \"Diego Marin\")] Realiza un INNER JOIN entre el DataFrame de pedidos (ejercicio 1) y el DataFrame de clientes para mostrar los order_id , customer_name y amount de cada pedido. Realiza un LEFT OUTER JOIN de clientes (izquierda) con pedidos (derecha). Muestra todos los clientes y sus pedidos (si los tienen). Filtra el resultado para mostrar solo los clientes que no han realizado ning\u00fan pedido (es decir, donde order_id es nulo). Crea un DataFrame de ventas_regionales con columnas region , product_category y sales_value . Datos de ejemplo: [(\"Norte\", \"Electronics\", 1000), (\"Norte\", \"Books\", 500), (\"Sur\", \"Electronics\", 1200), (\"Sur\", \"Books\", 600), (\"Centro\", \"Electronics\", 800)] Pivotea este DataFrame para mostrar sales_value por region (filas) y product_category (columnas). Crea un DataFrame con 1,000,000 de filas y dos columnas: id (entero secuencial) y random_value (n\u00famero aleatorio). Guarda este DataFrame en formato Parquet en un directorio temporal ( /tmp/large_data.parquet ). Lee el DataFrame de nuevo y averigua su n\u00famero de particiones. Reparticiona el DataFrame a 10 particiones y persist\u00e9lo en memoria ( MEMORY_AND_DISK ). Realiza una operaci\u00f3n de conteo y luego despersiste el DataFrame. Configura spark.sql.shuffle.partitions a un valor bajo (ej. 2) y luego a un valor m\u00e1s alto (ej. 20). Crea un DataFrame con una columna category que tenga 5 valores \u00fanicos y una columna value . Realiza una operaci\u00f3n groupBy por category y suma value . * Usa explain() para observar los planes de ejecuci\u00f3n y c\u00f3mo el n\u00famero de particiones de shuffle cambia. * (Opcional, para un entorno de cl\u00faster) Intenta medir el tiempo de ejecuci\u00f3n en ambos casos para ver el impacto. Crea un DataFrame dim_customers muy peque\u00f1o (ej. 10 filas, customer_id , customer_name ). Crea un DataFrame fact_transactions muy grande (ej. 1,000,000 filas, transaction_id , customer_id , amount ). Realiza un INNER JOIN entre fact_transactions y dim_customers en customer_id . * Usa .explain() para verificar si Spark ha utilizado autom\u00e1ticamente un BroadcastHashJoin . * Intenta forzar un BroadcastHashJoin si no se aplica autom\u00e1ticamente utilizando broadcast(dim_customers) en la uni\u00f3n, y verifica de nuevo el plan de ejecuci\u00f3n.","title":"Manipulaci\u00f3n y Transformaci\u00f3n de Datos"},{"location":"tema22/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema22/#tema-22-manipulacion-y-transformacion-de-datos","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de aplicar t\u00e9cnicas avanzadas de manipulaci\u00f3n y transformaci\u00f3n sobre DataFrames de Apache Spark, utilizando un amplio rango de funciones integradas, creando funciones personalizadas (UDFs) para l\u00f3gica espec\u00edfica, y comprendiendo c\u00f3mo el particionamiento y el paralelismo influyen en el procesamiento eficiente de grandes vol\u00famenes de datos distribuidos. Introducci\u00f3n : La capacidad de transformar datos brutos en informaci\u00f3n valiosa es el coraz\u00f3n del an\u00e1lisis de Big Data. En Apache Spark, los DataFrames no solo ofrecen una interfaz intuitiva para trabajar con datos estructurados y semi-estructurados, sino que tambi\u00e9n proporcionan un conjunto robusto de operaciones y funciones para la manipulaci\u00f3n y limpieza de datos a escala. Desde simples selecciones y filtrados hasta complejas agregaciones y uniones, Spark permite a los usuarios moldear sus datos para satisfacer las necesidades de an\u00e1lisis, modelado o visualizaci\u00f3n, todo ello aprovechando su arquitectura distribuida subyacente. Desarrollo : Este tema profundiza en las capacidades de manipulaci\u00f3n de DataFrames de Spark. Retomaremos y ampliaremos las operaciones fundamentales, exploraremos el vasto cat\u00e1logo de funciones integradas de Spark SQL y aprenderemos a extender esta funcionalidad creando nuestras propias funciones definidas por el usuario (UDFs). Adem\u00e1s, abordaremos conceptos cruciales como el particionamiento y el paralelismo, fundamentales para optimizar el rendimiento y escalar el procesamiento de datos distribuidos de manera efectiva. Comprender estos conceptos es clave para escribir c\u00f3digo Spark eficiente y robusto en escenarios de Big Data.","title":"Tema 2.2 Manipulaci\u00f3n y Transformaci\u00f3n de Datos"},{"location":"tema22/#221-operaciones-con-dataframes-seleccion-filtrado-agregaciones","text":"Si bien ya se introdujeron las operaciones b\u00e1sicas en el tema 2.1, esta secci\u00f3n se enfoca en profundizar y mostrar ejemplos m\u00e1s avanzados y combinaciones de estas operaciones, destacando su poder para la limpieza y preparaci\u00f3n de datos. La flexibilidad del API de DataFrames permite encadenar m\u00faltiples transformaciones, construyendo flujos de trabajo de datos complejos de manera legible y eficiente.","title":"2.2.1 Operaciones con DataFrames (selecci\u00f3n, filtrado, agregaciones)"},{"location":"tema22/#seleccion-avanzada-de-columnas","text":"M\u00e1s all\u00e1 de simplemente elegir columnas por nombre, Spark ofrece potentes capacidades para manipular columnas existentes o crear nuevas basadas en expresiones complejas. Seleccionar y Renombrar M\u00faltiples Columnas din\u00e1micamente: Es com\u00fan necesitar seleccionar un subconjunto de columnas y, al mismo tiempo, renombrarlas. Esto se puede hacer de forma program\u00e1tica utilizando listas de columnas y aplicando aliases. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedSelection\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\"), (\"Charlie\", 22, \"CHI\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Seleccionar y renombrar m\u00faltiples columnas selected_cols = [col(\"name\").alias(\"full_name\"), col(\"age\"), col(\"city\").alias(\"location\")] df.select(*selected_cols).show() # Resultado: # +---------+---+--------+ # |full_name|age|location| # +---------+---+--------+ # | Alice| 25| NY| # | Bob| 30| LA| # | Charlie| 22| CHI| # +---------+---+--------+ Uso de expresiones SQL en select : Spark permite incrustar expresiones SQL directamente dentro de la funci\u00f3n select para mayor flexibilidad, especialmente cuando se trabaja con funciones complejas o l\u00f3gicas condicionales. from pyspark.sql import SparkSession from pyspark.sql.functions import expr spark = SparkSession.builder.appName(\"SqlExpressions\").getOrCreate() data = [(\"Alice\", 25, 50000), (\"Bob\", 30, 60000), (\"Charlie\", 22, 45000)] df = spark.createDataFrame(data, [\"name\", \"age\", \"salary\"]) # Calcular un bono basado en el salario usando una expresi\u00f3n SQL df.select(\"name\", \"salary\", expr(\"salary * 0.10 AS bonus\")).show() # Resultado: # +-------+------+-------+ # | name|salary| bonus| # +-------+------+-------+ # | Alice| 50000| 5000.0| # | Bob| 60000| 6000.0| # |Charlie| 45000| 4500.0| # +-------+------+-------+ Eliminar columnas ( drop ): Es una operaci\u00f3n com\u00fan para limpiar DataFrames, eliminando columnas que no son relevantes para el an\u00e1lisis posterior. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropColumn\").getOrCreate() data = [(\"Alice\", 25, \"NY\", \"USA\"), (\"Bob\", 30, \"LA\", \"USA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\", \"country\"]) # Eliminar una sola columna df.drop(\"country\").show() # Resultado: # +-----+---+----+ # | name|age|city| # +-----+---+----+ # |Alice| 25| NY| # | Bob| 30| LA| # +-----+---+----+ # Eliminar m\u00faltiples columnas df.drop(\"age\", \"city\").show() # Resultado: # +-----+-------+ # | name|country| # +-----+-------+ # |Alice| USA| # | Bob| USA| # +-----+-------+","title":"Selecci\u00f3n Avanzada de Columnas"},{"location":"tema22/#filtrado-avanzado-de-filas","text":"Las condiciones de filtrado pueden ser muy complejas, combinando m\u00faltiples operadores l\u00f3gicos y funciones. Combinar m\u00faltiples condiciones con & (AND), | (OR), ~ (NOT): Permite construir filtros sofisticados para aislar subconjuntos de datos espec\u00edficos. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"AdvancedFiltering\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\"), (\"Mouse\", 25, \"Electronics\"), (\"Book\", 15, \"Books\"), (\"Monitor\", 300, \"Electronics\"), (\"Pen\", 2, \"Office\")] df = spark.createDataFrame(data, [\"product\", \"price\", \"category\"]) # Filtrar productos de \"Electronics\" con precio > 100 O productos de \"Books\" df.filter( (col(\"category\") == \"Electronics\") & (col(\"price\") > 100) | (col(\"category\") == \"Books\") ).show() # Resultado: # +--------+-----+-----------+ # | product|price| category| # +--------+-----+-----------+ # | Laptop| 1200|Electronics| # | Book| 15| Books| # | Monitor| 300|Electronics| # +--------+-----+-----------+ Uso de isin para filtrar por m\u00faltiples valores en una columna: Una forma concisa de filtrar filas donde una columna toma uno de varios valores posibles. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"FilteringIsIn\").getOrCreate() data = [(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"CHI\"), (\"David\", \"NY\")] df = spark.createDataFrame(data, [\"name\", \"city\"]) # Filtrar por ciudades espec\u00edficas df.filter(col(\"city\").isin(\"NY\", \"LA\")).show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # |David| NY| # +-----+----+ Manejo de valores nulos en filtros ( isNull , isNotNull , na.drop ): Es crucial manejar los valores nulos al filtrar para evitar resultados inesperados o errores. from pyspark.sql import SparkSession from pyspark.sql.functions import col spark = SparkSession.builder.appName(\"HandlingNullsFiltering\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", None), (\"Charlie\", 22), (\"David\", None)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Filtrar filas donde 'age' no es nulo df.filter(col(\"age\").isNotNull()).show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+ # Filtrar filas donde 'age' es nulo df.filter(col(\"age\").isNull()).show() # Resultado: # +-----+----+ # | name| age| # +-----+----+ # | Bob|null| # |David|null| # +-----+----+ # Eliminar filas con cualquier valor nulo df.na.drop().show() # Resultado: # +-------+---+ # | name|age| # +-------+---+ # | Alice| 25| # |Charlie| 22| # +-------+---+","title":"Filtrado Avanzado de Filas"},{"location":"tema22/#agregaciones-avanzadas-y-agrupamiento","text":"Las agregaciones son potentes para resumir datos. Spark permite agregaciones por m\u00faltiples columnas, con funciones de ventana y pivoteo. Agregaciones sobre m\u00faltiples columnas con agg : Permite aplicar m\u00faltiples funciones de agregaci\u00f3n a diferentes columnas en una sola operaci\u00f3n de groupBy . from pyspark.sql import SparkSession from pyspark.sql.functions import avg, sum, count, col spark = SparkSession.builder.appName(\"MultiAggregations\").getOrCreate() data = [(\"Dept1\", \"Alice\", 1000, 5), (\"Dept1\", \"Bob\", 1200, 8), (\"Dept2\", \"Charlie\", 900, 3), (\"Dept2\", \"David\", 1500, 10)] df = spark.createDataFrame(data, [\"department\", \"name\", \"salary\", \"projects_completed\"]) # Agregaciones m\u00faltiples por departamento df.groupBy(\"department\").agg( avg(col(\"salary\")).alias(\"avg_salary\"), sum(col(\"projects_completed\")).alias(\"total_projects\"), count(col(\"name\")).alias(\"num_employees\") ).show() # Resultado: # +----------+----------+--------------+-------------+ # |department|avg_salary|total_projects|num_employees| # +----------+----------+--------------+-------------+ # | Dept1| 1100.0| 13| 2| # | Dept2| 1200.0| 13| 2| # +----------+----------+--------------+-------------+ Pivoteo de datos ( pivot ): Transforma filas en columnas, muy \u00fatil para an\u00e1lisis de series temporales o reportes. from pyspark.sql import SparkSession from pyspark.sql.functions import sum spark = SparkSession.builder.appName(\"PivotOperation\").getOrCreate() data = [(\"Sales\", \"Jan\", 100), (\"Sales\", \"Feb\", 120), (\"Marketing\", \"Jan\", 80), (\"Marketing\", \"Feb\", 90), (\"Sales\", \"Mar\", 150)] df = spark.createDataFrame(data, [\"department\", \"month\", \"revenue\"]) # Pivoteo de ingresos por departamento y mes df.groupBy(\"department\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).agg(sum(\"revenue\")).show() # Resultado: # +----------+---+---+---+ # |department|Jan|Feb|Mar| # +----------+---+---+---+ # | Sales|100|120|150| # | Marketing| 80| 90|null| # +----------+---+---+---+ Uniones de DataFrames ( join ): Combina dos DataFrames en funci\u00f3n de una o m\u00e1s claves comunes. Spark soporta varios tipos de uniones (inner, outer, left, right, anti, semi). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DataFrameJoins\").getOrCreate() # DataFrame de empleados employees_data = [(\"Alice\", 1, \"Sales\"), (\"Bob\", 2, \"HR\"), (\"Charlie\", 3, \"IT\")] employees_df = spark.createDataFrame(employees_data, [\"name\", \"emp_id\", \"dept_id\"]) # DataFrame de departamentos departments_data = [(1, \"Sales\", \"NY\"), (2, \"HR\", \"LA\"), (4, \"Finance\", \"CHI\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\", \"location\"]) # Inner Join: solo filas que coinciden en ambas tablas employees_df.join(departments_df, on=\"dept_id\", how=\"inner\").show() # Resultado: # +-------+-----+---------+---------+--------+ # |dept_id| name| emp_id|dept_name|location| # +-------+-----+---------+---------+--------+ # | 1|Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # +-------+-----+---------+---------+--------+ # Left Outer Join: todas las filas de la izquierda, y las que coinciden de la derecha employees_df.join(departments_df, on=\"dept_id\", how=\"left_outer\").show() # Resultado: # +-------+-------+------+---------+---------+--------+ # |dept_id| name|emp_id|dept_name|location| # +-------+-------+------+---------+---------+--------+ # | 1| Alice| 1| Sales| NY| # | 2| Bob| 2| HR| LA| # | 3|Charlie| 3| null| null| # +-------+-------+------+---------+---------+--------+","title":"Agregaciones Avanzadas y Agrupamiento"},{"location":"tema22/#222-funciones-integradas-y-definidas-por-el-usuario-udfs","text":"Spark proporciona una rica biblioteca de funciones integradas ( pyspark.sql.functions ) que cubren una amplia gama de transformaciones de datos. Sin embargo, cuando la l\u00f3gica de negocio es muy espec\u00edfica y no est\u00e1 cubierta por las funciones existentes, las Funciones Definidas por el Usuario (UDFs) permiten extender la funcionalidad de Spark utilizando c\u00f3digo Python.","title":"2.2.2 Funciones integradas y definidas por el usuario (UDFs)"},{"location":"tema22/#funciones-integradas-de-spark-sql","text":"Estas funciones son altamente optimizadas y deben ser la primera opci\u00f3n para cualquier transformaci\u00f3n. Cubren operaciones num\u00e9ricas, de cadena, de fecha y hora, y de manipulaci\u00f3n de arrays y mapas. Funciones de cadena ( substring , concat_ws , length , lower , upper ): \u00datiles para manipular texto en columnas. from pyspark.sql import SparkSession from pyspark.sql.functions import col, substring, concat_ws, length, lower, upper spark = SparkSession.builder.appName(\"StringFunctions\").getOrCreate() data = [(\"john doe\",), (\"JANE SMITH\",)] df = spark.createDataFrame(data, [\"full_name\"]) # Extraer subcadena df.withColumn(\"first_3_chars\", substring(col(\"full_name\"), 1, 3)).show() # Resultado: # +----------+-------------+ # | full_name|first_3_chars| # +----------+-------------+ # | john doe| joh| # |JANE SMITH| JAN| # +----------+-------------+ # Concatenar con separador (necesita m\u00e1s columnas para ser \u00fatil, ejemplo conceptual) df.withColumn(\"formatted_name\", concat_ws(\", \", lower(col(\"full_name\")))).show() # En este caso, solo convierte a min\u00fasculas # Resultado: # +----------+--------------+ # | full_name|formatted_name| # +----------+--------------+ # | john doe| john doe| # |JANE SMITH| jane smith| # +----------+--------------+ # Obtener longitud y convertir a may\u00fasculas/min\u00fasculas df.select(col(\"full_name\"), length(col(\"full_name\")).alias(\"name_length\"), lower(col(\"full_name\")).alias(\"lower_name\"), upper(col(\"full_name\")).alias(\"upper_name\")).show() # Resultado: # +----------+-----------+----------+----------+ # | full_name|name_length|lower_name|upper_name| # +----------+-----------+----------+----------+ # | john doe| 8| john doe| JOHN DOE| # |JANE SMITH| 10|jane smith|JANE SMITH| # +----------+-----------+----------+----------+ Funciones de fecha y hora ( current_date , datediff , year , month , to_date , to_timestamp ): Esenciales para el procesamiento de datos temporales. from pyspark.sql import SparkSession from pyspark.sql.functions import col, current_date, datediff, year, month, to_date, to_timestamp spark = SparkSession.builder.appName(\"DateFunctions\").getOrCreate() data = [(\"2023-01-15\",), (\"2024-03-01\",)] df = spark.createDataFrame(data, [\"event_date\"]) # Convertir a tipo Date y obtener el a\u00f1o y mes df.withColumn(\"event_date_parsed\", to_date(col(\"event_date\"))) \\ .withColumn(\"current_date\", current_date()) \\ .withColumn(\"days_since_event\", datediff(col(\"current_date\"), col(\"event_date_parsed\"))) \\ .withColumn(\"event_year\", year(col(\"event_date_parsed\"))) \\ .withColumn(\"event_month\", month(col(\"event_date_parsed\"))) \\ .show() # Resultado (valores de d\u00edas_since_event variar\u00e1n con la fecha actual): # +----------+-----------------+------------+----------------+----------+-----------+ # |event_date|event_date_parsed|current_date|days_since_event|event_year|event_month| # +----------+-----------------+------------+----------------+----------+-----------+ # |2023-01-15| 2023-01-15| 2025-05-31| 867| 2023| 1| # |2024-03-01| 2024-03-01| 2025-05-31| 457| 2024| 3| # +----------+-----------------+------------+----------------+----------+-----------+ # Convertir a timestamp df_ts = spark.createDataFrame([(\"2023-01-15 10:30:00\",)], [\"datetime_str\"]) df_ts.withColumn(\"parsed_timestamp\", to_timestamp(col(\"datetime_str\"))).show() # Resultado: # +-------------------+--------------------+ # | datetime_str| parsed_timestamp| # +-------------------+--------------------+ # |2023-01-15 10:30:00|2023-01-15 10:30:00| # +-------------------+--------------------+ Funciones condicionales ( when , otherwise ): Permiten aplicar l\u00f3gica condicional para crear nuevas columnas o modificar existentes. from pyspark.sql import SparkSession from pyspark.sql.functions import col, when spark = SparkSession.builder.appName(\"ConditionalFunctions\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 35, \"LA\"), (\"Charlie\", 17, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # Clasificar edad en categor\u00edas df.withColumn(\"age_group\", when(col(\"age\") < 18, \"Minor\") .when(col(\"age\") >= 18, \"Adult\") .otherwise(\"Unknown\") ).show() # Resultado: # +-------+---+----+---------+ # | name|age|city|age_group| # +-------+---+----+---------+ # | Alice| 25| NY| Adult| # | Bob| 35| LA| Adult| # |Charlie| 17| CHI| Minor| # +-------+---+----+---------+","title":"Funciones Integradas de Spark SQL"},{"location":"tema22/#funciones-definidas-por-el-usuario-udfs","text":"Las UDFs permiten a los desarrolladores de Python extender la funcionalidad de Spark implementando l\u00f3gica personalizada. Aunque potentes, pueden tener un impacto en el rendimiento debido a la serializaci\u00f3n y deserializaci\u00f3n de datos entre la JVM (donde Spark se ejecuta) y el proceso Python. Creaci\u00f3n de UDFs simples: Se definen como funciones Python normales y luego se registran en Spark usando udf de pyspark.sql.functions . Es crucial especificar el tipo de retorno. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import StringType spark = SparkSession.builder.appName(\"SimpleUDF\").getOrCreate() data = [(\"alice\",), (\"BOB\",), (\"charlie\",)] df = spark.createDataFrame(data, [\"name\"]) # Definir una funci\u00f3n Python para capitalizar la primera letra def capitalize_name(name): return name.capitalize() if name else None # Registrar la UDF con el tipo de retorno capitalize_udf = udf(capitalize_name, StringType()) # Aplicar la UDF al DataFrame df.withColumn(\"capitalized_name\", capitalize_udf(col(\"name\"))).show() # Resultado: # +-------+----------------+ # | name|capitalized_name| # +-------+----------------+ # | alice| Alice| # | BOB| Bob| # |charlie| Charlie| # +-------+----------------+ UDFs con m\u00faltiples argumentos: Las UDFs pueden aceptar m\u00faltiples columnas como entrada. from pyspark.sql import SparkSession from pyspark.sql.functions import udf, col from pyspark.sql.types import DoubleType spark = SparkSession.builder.appName(\"MultiArgUDF\").getOrCreate() data = [(1000, 0.10), (1200, 0.05), (800, 0.15)] df = spark.createDataFrame(data, [\"base_salary\", \"bonus_rate\"]) # Funci\u00f3n Python para calcular el salario total def calculate_total_salary(base_salary, bonus_rate): if base_salary is None or bonus_rate is None: return None return base_salary * (1 + bonus_rate) # Registrar la UDF total_salary_udf = udf(calculate_total_salary, DoubleType()) # Aplicar la UDF df.withColumn(\"total_salary\", total_salary_udf(col(\"base_salary\"), col(\"bonus_rate\"))).show() # Resultado: # +-----------+----------+------------+ # |base_salary|bonus_rate|total_salary| # +-----------+----------+------------+ # | 1000| 0.10| 1100.0| # | 1200| 0.05| 1260.0| # | 800| 0.15| 920.0| # +-----------+----------+------------+ Consideraciones de rendimiento de UDFs (Vectorized UDFs con Pandas): Para mitigar el costo de serializaci\u00f3n/deserializaci\u00f3n, Spark 2.3+ introdujo las UDFs vectorizadas (anteriormente \"Pandas UDFs\"). Estas UDFs operan en pandas.Series o pandas.DataFrame en lugar de una fila a la vez, lo que reduce la sobrecarga y mejora significativamente el rendimiento para ciertas operaciones. from pyspark.sql import SparkSession from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import DoubleType, StringType spark = SparkSession.builder.appName(\"PandasUDF\").getOrCreate() data = [(1, 10.0), (2, 20.0), (3, 30.0)] df = spark.createDataFrame(data, [\"id\", \"value\"]) # Pandas UDF para escalar valores (Series a Series) @pandas_udf(DoubleType(), PandasUDFType.SCALAR) def multiply_by_two(value: float) -> float: return value * 2 df.withColumn(\"value_doubled\", multiply_by_two(col(\"value\"))).show() # Resultado: # +---+-----+-------------+ # | id|value|value_doubled| # +---+-----+-------------+ # | 1| 10.0| 20.0| # | 2| 20.0| 40.0| # | 3| 30.0| 60.0| # +---+-----+-------------+ # Pandas UDF para agregaci\u00f3n (Series a escalar) @pandas_udf(StringType(), PandasUDFType.GROUPED_AGG) def concat_strings(col_series): return \"_\".join(col_series.astype(str)) df_agg = spark.createDataFrame([(\"A\", \"x\"), (\"A\", \"y\"), (\"B\", \"z\")], [\"group\", \"value\"]) df_agg.groupBy(\"group\").agg(concat_strings(col(\"value\")).alias(\"concatenated_values\")).show() # Resultado: # +-----+-------------------+ # |group|concatenated_values| # +-----+-------------------+ # | A| x_y| # | B| z| # +-----+-------------------+","title":"Funciones Definidas por el Usuario (UDFs)"},{"location":"tema22/#223-particionamiento-y-paralelismo","text":"El particionamiento es un concepto fundamental en Spark que define c\u00f3mo se distribuyen los datos en el cl\u00faster. Un particionamiento adecuado es clave para optimizar el rendimiento de las operaciones, especialmente las que involucran shuffles (intercambio de datos entre nodos). El paralelismo se refiere a la cantidad de tareas que Spark puede ejecutar simult\u00e1neamente.","title":"2.2.3 Particionamiento y paralelismo"},{"location":"tema22/#conceptos-de-particionamiento","text":"Los datos en Spark se dividen en \"particiones\" l\u00f3gicas, cada una de las cuales es procesada por una tarea. La forma en que los datos se particionan afecta el rendimiento y la eficiencia de la computaci\u00f3n. \u00bfQu\u00e9 es una partici\u00f3n en Spark?: Una partici\u00f3n es una divisi\u00f3n l\u00f3gica de los datos de un RDD o DataFrame. Cada partici\u00f3n se puede almacenar en un nodo diferente del cl\u00faster y se procesa de forma independiente y paralela. M\u00e1s particiones no siempre es mejor; el n\u00famero \u00f3ptimo depende del tama\u00f1o de los datos, el n\u00famero de n\u00facleos del cl\u00faster y la naturaleza de las operaciones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"PartitionsConcept\").getOrCreate() data = [(i,) for i in range(100)] df = spark.createDataFrame(data, [\"value\"]) # Obtener el n\u00famero de particiones actual (por defecto Spark usa el n\u00famero de n\u00facleos disponibles o spark.sql.shuffle.partitions) print(f\"N\u00famero inicial de particiones: {df.rdd.getNumPartitions()}\") # Podemos re-particionar un DataFrame (esto implica un shuffle) df_repartitioned = df.repartition(10) print(f\"N\u00famero de particiones despu\u00e9s de repartition: {df_repartitioned.rdd.getNumPartitions()}\") # Una partici\u00f3n es como un bloque de trabajo para un n\u00facleo de CPU. # Si tenemos 100 particiones y 20 n\u00facleos, cada n\u00facleo procesar\u00e1 5 particiones en paralelo (idealmente). Visualizaci\u00f3n del n\u00famero de particiones ( df.rdd.getNumPartitions() ): Es importante saber cu\u00e1ntas particiones tiene un DataFrame para entender c\u00f3mo se distribuir\u00e1 el trabajo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CheckPartitions\").getOrCreate() # Leer un archivo de ejemplo para ver sus particiones # Si no tienes un archivo grande, crea uno peque\u00f1o y l\u00e9elo data = [(i, f\"Name_{i}\") for i in range(1000)] df_large = spark.createDataFrame(data, [\"id\", \"name\"]) df_large.write.mode(\"overwrite\").parquet(\"large_data.parquet\") # Leer el DataFrame df_read = spark.read.parquet(\"large_data.parquet\") # Ver el n\u00famero de particiones print(f\"N\u00famero de particiones del DataFrame le\u00eddo: {df_read.rdd.getNumPartitions()}\") # Generalmente, Spark intenta que el n\u00famero de particiones sea cercano al tama\u00f1o de bloque del HDFS (128MB por defecto) # o al n\u00famero de n\u00facleos en el cluster. Impacto del particionamiento en el rendimiento (Shuffle): Las operaciones que requieren agrupar o unir datos (como groupBy , join , orderBy ) a menudo implican un \"shuffle\", donde los datos se mueven entre los nodos del cl\u00faster. Un n\u00famero incorrecto de particiones puede llevar a un shuffle ineficiente, causando cuellos de botella. Demasiadas particiones peque\u00f1as pueden generar mucha sobrecarga, mientras que muy pocas pueden limitar el paralelismo. Skewed data: Si los datos est\u00e1n muy desequilibrados en las particiones (algunas particiones tienen muchos m\u00e1s datos que otras), esto puede causar cuellos de botella donde una o pocas tareas tardan mucho m\u00e1s en completarse. Small files problem: Si hay muchas particiones muy peque\u00f1as, la sobrecarga de gestionar cada una puede ser mayor que el tiempo de procesamiento real.","title":"Conceptos de Particionamiento"},{"location":"tema22/#control-del-particionamiento-y-paralelismo","text":"Spark ofrece mecanismos para controlar c\u00f3mo se particionan los datos y el nivel de paralelismo. repartition() y coalesce() : repartition() crea un nuevo RDD/DataFrame con un n\u00famero especificado de particiones, distribuyendo los datos uniformemente. Esto siempre implica un shuffle completo. coalesce() reduce el n\u00famero de particiones sin shuffle si es posible (solo reduce el n\u00famero de particiones en el mismo nodo), o con un shuffle m\u00ednimo si se requiere. Es m\u00e1s eficiente para reducir particiones. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"RepartitionCoalesce\").getOrCreate() data = [(i,) for i in range(1000)] df = spark.createDataFrame(data, [\"value\"]) print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\") # Var\u00eda seg\u00fan la configuraci\u00f3n local # Reparticionar a 4 particiones (siempre con shuffle) df_repartitioned = df.repartition(4) print(f\"Particiones despu\u00e9s de repartition(4): {df_repartitioned.rdd.getNumPartitions()}\") # Coalesce a 2 particiones (puede evitar shuffle si los datos ya est\u00e1n en menos de 2) df_coalesced = df_repartitioned.coalesce(2) print(f\"Particiones despu\u00e9s de coalesce(2): {df_coalesced.rdd.getNumPartitions()}\") # Coalesce a 1 partici\u00f3n (siempre implicar\u00e1 un shuffle si hay m\u00e1s de 1 partici\u00f3n) df_single_partition = df.coalesce(1) print(f\"Particiones despu\u00e9s de coalesce(1): {df_single_partition.rdd.getNumPartitions()}\") Configuraci\u00f3n de spark.sql.shuffle.partitions : Este par\u00e1metro controla el n\u00famero de particiones que Spark utiliza por defecto despu\u00e9s de una operaci\u00f3n de shuffle. Un valor bien ajustado puede mejorar dr\u00e1sticamente el rendimiento de las operaciones de agregaci\u00f3n y uni\u00f3n. from pyspark.sql import SparkSession from pyspark.sql.functions import count # Configurar el n\u00famero de particiones de shuffle antes de crear la SparkSession spark = SparkSession.builder \\ .appName(\"ShufflePartitions\") \\ .config(\"spark.sql.shuffle.partitions\", \"8\") \\ .getOrCreate() print(f\"spark.sql.shuffle.partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\") data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4), (\"B\", 5)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # La operaci\u00f3n groupBy implicar\u00e1 un shuffle, y el resultado tendr\u00e1 8 particiones. df_agg = df.groupBy(\"key\").agg(count(\"*\")).repartition(1) # Repartition para mostrar en una sola salida df_agg.show() # Para verificar el n\u00famero de particiones de la salida de la agregaci\u00f3n (antes del repartition) # print(df_agg.rdd.getNumPartitions()) Estrategias para optimizar particiones en Joins: Broadcast Join: Cuando un DataFrame es peque\u00f1o (por defecto, menos de spark.sql.autoBroadcastJoinThreshold ), Spark puede \"broadcast\" (transmitir) el DataFrame peque\u00f1o a todos los nodos del cl\u00faster, evitando el shuffle de la tabla grande. Es muy eficiente. Hash Join: Cuando la clave de uni\u00f3n est\u00e1 particionada de forma similar en ambos DataFrames, Spark puede realizar un Hash Join, que es eficiente ya que los datos con la misma clave ya est\u00e1n en las mismas particiones. Sort-Merge Join: El join por defecto si las tablas no se pueden transmitir y no est\u00e1n co-ubicadas. Implica ordenar y fusionar las particiones, lo que puede ser costoso. from pyspark.sql import SparkSession from pyspark.sql.functions import broadcast # Para forzar un broadcast join spark = SparkSession.builder \\ .appName(\"JoinStrategies\") \\ .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") # Configurar el umbral de broadcast (por defecto 10MB) .getOrCreate() # DataFrame peque\u00f1o (menos de 10MB de datos) small_df = spark.createDataFrame([(1, \"DeptA\"), (2, \"DeptB\")], [\"dept_id\", \"dept_name\"]) # DataFrame grande large_df = spark.createDataFrame([(101, \"Alice\", 1), (102, \"Bob\", 2), (103, \"Charlie\", 1)], [\"emp_id\", \"emp_name\", \"dept_id\"]) # Spark autom\u00e1ticamente intentar\u00e1 un Broadcast Join si small_df est\u00e1 por debajo del umbral large_df.join(small_df, \"dept_id\", \"inner\").explain() # Ver el plan de ejecuci\u00f3n para confirmar BroadcastHashJoin # Resultado de explain() deber\u00eda mostrar \"*BroadcastHashJoin\" # Forzar un Broadcast Join (\u00fatil si Spark no lo infiere autom\u00e1ticamente o para depuraci\u00f3n) large_df.join(broadcast(small_df), \"dept_id\", \"inner\").explain()","title":"Control del Particionamiento y Paralelismo"},{"location":"tema22/#224-manejo-de-datos-distribuidos","text":"Trabajar con datos distribuidos implica m\u00e1s que solo particionar. Se trata de entender c\u00f3mo Spark gestiona la memoria, el almacenamiento en cach\u00e9 y la persistencia para optimizar el acceso a los datos, y c\u00f3mo maneja los \"shuffles\" que son costosos.","title":"2.2.4 Manejo de datos distribuidos"},{"location":"tema22/#persistencia-y-almacenamiento-en-cache","text":"Para evitar recalcular DataFrames que se utilizan m\u00faltiples veces, Spark permite persistirlos en memoria o en disco. cache() y persist() : cache() es un alias de persist() con el nivel de almacenamiento por defecto ( MEMORY_AND_DISK ). Almacena el DataFrame en la memoria del cl\u00faster para un acceso r\u00e1pido en operaciones futuras. persist() permite especificar diferentes niveles de almacenamiento (solo memoria, solo disco, memoria y disco, con o sin serializaci\u00f3n, con o sin replicaci\u00f3n). from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"Persistence\").getOrCreate() data = [(i, f\"Item_{i}\") for i in range(100000)] df = spark.createDataFrame(data, [\"id\", \"description\"]) # Persistir en memoria y disco (comportamiento por defecto de cache()) df.cache() # Equivalente a df.persist(StorageLevel.MEMORY_AND_DISK) # La primera acci\u00f3n dispara la carga y el almacenamiento en cach\u00e9 df.count() print(f\"DataFrame cached. Number of partitions: {df.rdd.getNumPartitions()}\") # Las acciones subsiguientes ser\u00e1n m\u00e1s r\u00e1pidas df.filter(df.id > 50000).show(5) # Persistir solo en memoria (m\u00e1s r\u00e1pido si los datos caben en memoria) df_mem_only = df.persist(StorageLevel.MEMORY_ONLY) df_mem_only.count() print(f\"DataFrame persisted in MEMORY_ONLY. Number of partitions: {df_mem_only.rdd.getNumPartitions()}\") # Despersistir (liberar los datos de cach\u00e9) df.unpersist() df_mem_only.unpersist() Niveles de almacenamiento ( StorageLevel ): Permiten un control granular sobre c\u00f3mo se almacenan los datos persistidos, equilibrando velocidad y tolerancia a fallos. MEMORY_ONLY : Guarda el RDD deserializado como objetos Python en la JVM. Si no cabe, recalcula. MEMORY_AND_DISK : Guarda en memoria; si no cabe, se desborda a disco. MEMORY_ONLY_SER : Igual que MEMORY_ONLY, pero los datos est\u00e1n serializados (ahorra espacio, pero m\u00e1s lento de acceder). MEMORY_AND_DISK_SER : Igual que MEMORY_AND_DISK, pero serializado. DISK_ONLY : Solo guarda en disco. Versiones con _2 al final (ej. MEMORY_ONLY_2 ): Replica la partici\u00f3n en dos nodos, para tolerancia a fallos. from pyspark.sql import SparkSession from pyspark.storagelevel import StorageLevel spark = SparkSession.builder.appName(\"StorageLevels\").getOrCreate() data = [(i,) for i in range(1000000)] df = spark.createDataFrame(data, [\"value\"]) # Persistir en disco para mayor confiabilidad en caso de poca memoria df.persist(StorageLevel.DISK_ONLY) df.count() # Dispara la persistencia print(\"DataFrame persisted to DISK_ONLY.\") # Persistir con replicaci\u00f3n (para tolerancia a fallos) df.persist(StorageLevel.MEMORY_AND_DISK_2) df.count() # Dispara la persistencia print(\"DataFrame persisted to MEMORY_AND_DISK_2 (replicated).\") df.unpersist() \u00bfCu\u00e1ndo usar cache() / persist() ?: Cuando un DataFrame se usa en m\u00faltiples acciones (ej. count() , show() , luego filter() , join() ). Cuando un DataFrame es el resultado de transformaciones costosas (ej. join complejos, agregaciones pesadas). Antes de aplicar algoritmos iterativos (ej. Machine Learning), donde los datos se leen repetidamente. En puntos intermedios de un flujo de trabajo de ETL complejo que se reutilizan.","title":"Persistencia y Almacenamiento en Cach\u00e9"},{"location":"tema22/#comprension-y-optimizacion-de-shuffles","text":"Los shuffles son la operaci\u00f3n m\u00e1s costosa en Spark porque implican la transferencia de datos a trav\u00e9s de la red entre diferentes nodos. Minimizar o optimizar los shuffles es clave para el rendimiento. Identificaci\u00f3n de operaciones que causan Shuffle: Cualquier operaci\u00f3n que requiera que Spark reorganice los datos a trav\u00e9s del cl\u00faster, como: groupBy() join() (excepto Broadcast Joins) orderBy() y sort() repartition() Ventanas anal\u00edticas (ciertas operaciones) from pyspark.sql import SparkSession from pyspark.sql.functions import count spark = SparkSession.builder.appName(\"IdentifyShuffles\").getOrCreate() data = [(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)] df = spark.createDataFrame(data, [\"key\", \"value\"]) # explain() muestra el plan de ejecuci\u00f3n y ayuda a identificar shuffles df.groupBy(\"key\").agg(count(\"value\")).explain() # En el plan de ejecuci\u00f3n, buscar etapas como \"Exchange\", \"Sort\", \"HashPartitioning\" # Estas indican una operaci\u00f3n de shuffle. # Ejemplo de salida parcial de explain: # == Physical Plan == # *(2) HashAggregate(keys=[key#123], functions=[count(value#124)]) # +- Exchange hashpartitioning(key#123, 200), [id=#12] <--- Esto es un shuffle # +- *(1) HashAggregate(keys=[key#123], functions=[partial_count(value#124)]) # +- *(1) Project [key#123, value#124] # +- *(1) Scan ExistingRDD Mitigaci\u00f3n de Shuffles (ej. Broadcast Join, Co-locaci\u00f3n de datos): Broadcast Join: Como se mencion\u00f3, usar broadcast() para DataFrames peque\u00f1os evita el shuffle de la tabla grande. Co-locaci\u00f3n de datos: Si los datos que se van a unir o agrupar ya est\u00e1n particionados de forma similar en el sistema de archivos (ej. Parquet particionado por la clave de uni\u00f3n), Spark puede evitar shuffles completos. Esto se logra mediante la escritura de datos particionados ( df.write.partitionBy(...) ). from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"ShuffleMitigation\").getOrCreate() # Peque\u00f1o DataFrame de lookup lookup_data = [(1, \"RegionA\"), (2, \"RegionB\")] lookup_df = spark.createDataFrame(lookup_data, [\"region_id\", \"region_name\"]) # DataFrame grande sales_data = [(101, 1, 100), (102, 2, 150), (103, 1, 200)] sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"region_id\", \"amount\"]) # Forzar Broadcast Join para evitar shuffle de sales_df from pyspark.sql.functions import broadcast sales_df.join(broadcast(lookup_df), \"region_id\").explain() # Deber\u00edas ver BroadcastHashJoin en el plan # Ejemplo de co-locaci\u00f3n con escritura particionada (requiere planificaci\u00f3n previa) # df_large.write.partitionBy(\"join_key\").parquet(\"path/to/partitioned_data\") # df_other_large.write.partitionBy(\"join_key\").parquet(\"path/to/other_partitioned_data\") # Luego, al unirlos, si Spark detecta que est\u00e1n co-ubicados, puede usar un SortMergeJoin m\u00e1s eficiente Manejo de datos desequilibrados (Skewed Data): Cuando un valor de clave tiene significativamente m\u00e1s filas que otros, la partici\u00f3n correspondiente se convierte en un cuello de botella. Salting: A\u00f1adir un sufijo aleatorio a la clave desequilibrada en el DataFrame grande y replicar las filas de la clave desequilibrada en el DataFrame peque\u00f1o con los mismos sufijos. Spark 3.x Adaptive Query Execution (AQE): AQE puede detectar y manejar skew de forma autom\u00e1tica durante la ejecuci\u00f3n de los joins, dividiendo las particiones grandes en subparticiones m\u00e1s peque\u00f1as. Habilitar spark.sql.adaptive.enabled a true (es por defecto en Spark 3.2+). from pyspark.sql import SparkSession from pyspark.sql.functions import lit, rand, concat spark = SparkSession.builder \\ .appName(\"SkewedJoin\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") # Habilitar AQE .getOrCreate() # Simular datos desequilibrados: 'HotKey' tiene muchos m\u00e1s registros data_large = [(i, \"NormalKey\") for i in range(1000)] + \\ [(i, \"HotKey\") for i in range(9000)] large_df = spark.createDataFrame(data_large, [\"id\", \"join_key\"]) data_small = [(\"NormalKey\", \"ValueA\"), (\"HotKey\", \"ValueB\")] small_df = spark.createDataFrame(data_small, [\"join_key\", \"value\"]) # Uni\u00f3n est\u00e1ndar (puede ser lenta debido a HotKey si AQE no est\u00e1 activo o no es suficiente) result_df = large_df.join(small_df, \"join_key\", \"inner\") result_df.explain() # Observar si AQE detecta el skew (si est\u00e1 habilitado) # Ejemplo de Salting (manual): # A\u00f1adir un \"salt\" aleatorio a las claves num_salt_buckets = 10 salted_large_df = large_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (rand() * num_salt_buckets).cast(\"int\"))) salted_small_df = small_df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (lit(0) + rand() * num_salt_buckets).cast(\"int\"))) # Si 'HotKey' tiene mucho skew, se replicar\u00eda 'HotKey' en small_df para cada sufijo de salt. # Luego, se unir\u00eda por (join_key, salted_key) # Uni\u00f3n con salting: # result_salted = salted_large_df.join(salted_small_df, on=\"salted_key\", how=\"inner\") # result_salted.show()","title":"Comprensi\u00f3n y Optimizaci\u00f3n de Shuffles"},{"location":"tema22/#tarea","text":"Crea un DataFrame de pedidos con las columnas order_id , customer_id , order_date (formato 'YYYY-MM-DD'), y amount (valor num\u00e9rico). Datos de ejemplo: [(\"ORD001\", \"C001\", \"2023-01-10\", 150.75), (\"ORD002\", \"C002\", \"2023-01-15\", 200.00), (\"ORD003\", \"C001\", \"2023-02-01\", 50.25), (\"ORD004\", \"C003\", \"2023-02-05\", 300.00)] Realiza las siguientes transformaciones: * A\u00f1ade una columna order_year que contenga solo el a\u00f1o de order_date . * A\u00f1ade una columna order_month que contenga el n\u00famero del mes de order_date . * A\u00f1ade una columna is_high_value que sea True si amount es mayor o igual a 100 , de lo contrario False . * Muestra el DataFrame resultante. Usando el DataFrame de pedidos del ejercicio 1, calcula el total_amount_spent y el num_orders por customer_id . Muestra los resultados. Crea un DataFrame de productos con columnas product_name y category . Datos de ejemplo: [(\"laptop Dell\", \"electronics\"), (\"teclado logitech\", \"electronics\"), (\"libro de cocina\", \"books\"), (\"Auriculares sony\", \"electronics\")] Realiza las siguientes transformaciones: * Normaliza la columna product_name a min\u00fasculas. * A\u00f1ade una columna brand que extraiga la primera palabra de product_name . * A\u00f1ade una columna product_type basada en la category : si es \"electronics\", \"Tech Gadget\"; si es \"books\", \"Reading Material\"; de lo contrario \"Other\". * Muestra el DataFrame resultante. Define una UDF de PySpark llamada classify_amount que tome un amount num\u00e9rico y devuelva una cadena: \"Peque\u00f1o\" si amount < 50 , \"Mediano\" si 50 <= amount < 200 , y \"Grande\" si amount >= 200 . Aplica esta UDF al DataFrame de pedidos del ejercicio 1 para crear una nueva columna amount_category . Muestra el DataFrame. Crea un segundo DataFrame de clientes con las columnas customer_id y customer_name . Datos de ejemplo: [(\"C001\", \"Ana Garcia\"), (\"C002\", \"Pedro Ruiz\"), (\"C003\", \"Laura Sanz\"), (\"C004\", \"Diego Marin\")] Realiza un INNER JOIN entre el DataFrame de pedidos (ejercicio 1) y el DataFrame de clientes para mostrar los order_id , customer_name y amount de cada pedido. Realiza un LEFT OUTER JOIN de clientes (izquierda) con pedidos (derecha). Muestra todos los clientes y sus pedidos (si los tienen). Filtra el resultado para mostrar solo los clientes que no han realizado ning\u00fan pedido (es decir, donde order_id es nulo). Crea un DataFrame de ventas_regionales con columnas region , product_category y sales_value . Datos de ejemplo: [(\"Norte\", \"Electronics\", 1000), (\"Norte\", \"Books\", 500), (\"Sur\", \"Electronics\", 1200), (\"Sur\", \"Books\", 600), (\"Centro\", \"Electronics\", 800)] Pivotea este DataFrame para mostrar sales_value por region (filas) y product_category (columnas). Crea un DataFrame con 1,000,000 de filas y dos columnas: id (entero secuencial) y random_value (n\u00famero aleatorio). Guarda este DataFrame en formato Parquet en un directorio temporal ( /tmp/large_data.parquet ). Lee el DataFrame de nuevo y averigua su n\u00famero de particiones. Reparticiona el DataFrame a 10 particiones y persist\u00e9lo en memoria ( MEMORY_AND_DISK ). Realiza una operaci\u00f3n de conteo y luego despersiste el DataFrame. Configura spark.sql.shuffle.partitions a un valor bajo (ej. 2) y luego a un valor m\u00e1s alto (ej. 20). Crea un DataFrame con una columna category que tenga 5 valores \u00fanicos y una columna value . Realiza una operaci\u00f3n groupBy por category y suma value . * Usa explain() para observar los planes de ejecuci\u00f3n y c\u00f3mo el n\u00famero de particiones de shuffle cambia. * (Opcional, para un entorno de cl\u00faster) Intenta medir el tiempo de ejecuci\u00f3n en ambos casos para ver el impacto. Crea un DataFrame dim_customers muy peque\u00f1o (ej. 10 filas, customer_id , customer_name ). Crea un DataFrame fact_transactions muy grande (ej. 1,000,000 filas, transaction_id , customer_id , amount ). Realiza un INNER JOIN entre fact_transactions y dim_customers en customer_id . * Usa .explain() para verificar si Spark ha utilizado autom\u00e1ticamente un BroadcastHashJoin . * Intenta forzar un BroadcastHashJoin si no se aplica autom\u00e1ticamente utilizando broadcast(dim_customers) en la uni\u00f3n, y verifica de nuevo el plan de ejecuci\u00f3n.","title":"Tarea"},{"location":"tema23/","text":"2. PySpark y SparkSQL Tema 2.3 Consultas y SQL en Spark Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender los fundamentos de SparkSQL como una interfaz declarativa para el procesamiento de datos distribuidos, ejecutar consultas SQL directamente sobre DataFrames y fuentes de datos, crear y gestionar vistas temporales, y aplicar sentencias SQL avanzadas para realizar an\u00e1lisis complejos y transformaciones eficientes de grandes vol\u00famenes de datos. Introducci\u00f3n : Spark SQL es un m\u00f3dulo de Apache Spark para trabajar con datos estructurados. Proporciona una interfaz unificada para interactuar con datos utilizando consultas SQL est\u00e1ndar, lo que lo convierte en una herramienta invaluable para analistas de datos, ingenieros y cient\u00edficos que ya est\u00e1n familiarizados con el lenguaje SQL. Permite a los usuarios consultar datos almacenados en DataFrames, as\u00ed como en diversas fuentes de datos como Parquet, ORC, JSON, CSV, bases de datos JDBC y Hive, aprovechando al mismo tiempo el motor de ejecuci\u00f3n optimizado de Spark para lograr un rendimiento excepcional en escala de Big Data. Desarrollo : Este tema explorar\u00e1 c\u00f3mo Spark SQL integra la potencia de SQL con la escalabilidad de Spark. Iniciaremos con los fundamentos de Spark SQL, comprendiendo c\u00f3mo los DataFrames pueden ser vistos y consultados como tablas relacionales. Luego, avanzaremos a la ejecuci\u00f3n de consultas SQL b\u00e1sicas y la creaci\u00f3n y gesti\u00f3n de vistas temporales, que son esenciales para estructurar flujos de trabajo SQL. Finalmente, nos sumergiremos en consultas SQL avanzadas, incluyendo uniones complejas, subconsultas, funciones de ventana y CTEs (Common Table Expressions), demostrando c\u00f3mo Spark SQL puede manejar escenarios de an\u00e1lisis y transformaci\u00f3n de datos altamente sofisticados. 2.3.1 Fundamentos de SparkSQL Spark SQL permite la ejecuci\u00f3n de consultas SQL sobre datos estructurados o semi-estructurados. Esencialmente, act\u00faa como un motor SQL distribuido, permitiendo a los usuarios interactuar con DataFrames como si fueran tablas de bases de datos tradicionales, combinando la familiaridad de SQL con el poder de procesamiento de Spark. La relaci\u00f3n entre DataFrames y Tablas/Vistas en SparkSQL La clave de SparkSQL reside en su capacidad para mapear DataFrames a estructuras relacionales como tablas o vistas. Esto permite que los datos en un DataFrame sean consultados usando sintaxis SQL est\u00e1ndar, lo que facilita la integraci\u00f3n para usuarios con experiencia en bases de datos relacionales. DataFrames como la base de SparkSQL: Todos los datos en SparkSQL se manejan como DataFrames. Cuando se ejecuta una consulta SQL, Spark la analiza, la optimiza y la ejecuta sobre los DataFrames subyacentes. El resultado de una consulta SQL es siempre un DataFrame. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLFundamentals\").getOrCreate() # Crear un DataFrame data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # El DataFrame es la representaci\u00f3n en memoria df.show() # +-------+---+----+ # | name|age|city| # +-------+---+----+ # | Alice| 25| NY| # | Bob| 30| LA| # |Charlie| 22| CHI| # +-------+---+----+ # Sin una vista temporal, no podemos consultarlo directamente con spark.sql try: spark.sql(\"SELECT * FROM my_table\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_table; spark.sql() para ejecutar consultas SQL: Esta es la funci\u00f3n principal para ejecutar consultas SQL directamente en el contexto de Spark. Las consultas se escriben como cadenas de texto. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLQuery\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Crear una vista temporal para poder consultar el DataFrame con SQL df.createOrReplaceTempView(\"people\") # Ejecutar una consulta SQL result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 25\") result_df.show() # Resultado: # +----+---+ # |name|age| # +----+---+ # | Bob| 30| # +----+---+ Optimizador Catalyst y Generaci\u00f3n de C\u00f3digo Tungsten: Spark SQL utiliza dos componentes clave para la optimizaci\u00f3n y ejecuci\u00f3n de consultas: Catalyst Optimizer: Un optimizador de consulta basado en reglas y costos que genera planes de ejecuci\u00f3n eficientes. Puede realizar optimizaciones como predicado pushdown, column pruning y reordenamiento de joins. Tungsten: Un motor de ejecuci\u00f3n que genera c\u00f3digo optimizado en tiempo de ejecuci\u00f3n para DataFrames, mejorando la eficiencia de la CPU y el uso de memoria a trav\u00e9s de t\u00e9cnicas como la eliminaci\u00f3n de punteros y la gesti\u00f3n expl\u00edcita de memoria. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CatalystTungsten\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\", 10), (\"Mouse\", 25, \"Electronics\", 50), (\"Book\", 15, \"Books\", 100)] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\", \"stock\"]) df.createOrReplaceTempView(\"products\") # Observar el plan de ejecuci\u00f3n l\u00f3gico y f\u00edsico (Catalyst y Tungsten en acci\u00f3n) # Explain muestra c\u00f3mo Spark traduce la consulta SQL a una serie de operaciones optimizadas spark.sql(\"SELECT product_name, price FROM products WHERE category = 'Electronics' AND price > 100\").explain(extended=True) # La salida mostrar\u00e1 el plan l\u00f3gico (Parsed, Analyzed, Optimized) y el plan f\u00edsico. # Notar \"PushedFilters\" y \"PushedProjections\" que son optimizaciones de Catalyst. # Los operadores f\u00edsicos son implementaciones optimizadas por Tungsten. 2.3.2 Consultas b\u00e1sicas con SparkSQL Una vez que un DataFrame se ha registrado como una vista temporal, se pueden realizar las operaciones de SQL m\u00e1s comunes sobre \u00e9l. Estas operaciones son equivalentes a las transformaciones de DataFrame API, pero expresadas en un lenguaje declarativo. SELECT y FROM La base de cualquier consulta SQL, permitiendo especificar qu\u00e9 columnas se quieren recuperar y de qu\u00e9 fuente de datos. Selecci\u00f3n de todas las columnas ( SELECT * ): Recupera todas las columnas de una vista o tabla. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"people\") spark.sql(\"SELECT * FROM people\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ Selecci\u00f3n de columnas espec\u00edficas ( SELECT column1, column2 ): Permite especificar las columnas que se desean recuperar, optimizando el rendimiento al no leer datos innecesarios. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.createOrReplaceTempView(\"users\") spark.sql(\"SELECT name, city FROM users\").show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # +-----+----+ Renombrar columnas con AS : Asigna un alias a una columna para hacer el resultado m\u00e1s legible o para evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"ProductA\", 100), (\"ProductB\", 200)] df = spark.createDataFrame(data, [\"item_name\", \"item_price\"]) df.createOrReplaceTempView(\"items\") spark.sql(\"SELECT item_name AS product, item_price AS price FROM items\").show() # Resultado: # +--------+-----+ # | product|price| # +--------+-----+ # |ProductA| 100| # |ProductB| 200| # +--------+-----+ WHERE (Filtrado) La cl\u00e1usula WHERE se utiliza para filtrar filas bas\u00e1ndose en una o m\u00e1s condiciones, al igual que en SQL tradicional. Condiciones de igualdad y desigualdad ( = , != , < , > , <= , >= ): Filtra filas donde una columna cumple una condici\u00f3n de comparaci\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhere\").getOrCreate() data = [(\"Juan\", 30), (\"Maria\", 25), (\"Pedro\", 35)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"employees\") spark.sql(\"SELECT name, age FROM employees WHERE age > 28\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # | Juan| 30| # |Pedro| 35| # +-----+---+ Operadores l\u00f3gicos ( AND , OR , NOT ): Combina m\u00faltiples condiciones de filtrado. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereLogic\").getOrCreate() data = [(\"Shirt\", \"Blue\", \"Small\"), (\"Pants\", \"Black\", \"Medium\"), (\"T-Shirt\", \"Red\", \"Large\")] df = spark.createDataFrame(data, [\"item\", \"color\", \"size\"]) df.createOrReplaceTempView(\"apparel\") spark.sql(\"SELECT item, color FROM apparel WHERE color = 'Blue' OR size = 'Large'\").show() # Resultado: # +-------+-----+ # | item|color| # +-------+-----+ # | Shirt| Blue| # |T-Shirt| Red| # +-------+-----+ LIKE , IN , BETWEEN , IS NULL / IS NOT NULL : Funciones de filtrado comunes para patrones, listas de valores, rangos y valores nulos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereSpecial\").getOrCreate() data = [(\"Apple\", 1.0), (\"Banana\", 0.5), (\"Cherry\", 2.0), (\"Date\", None)] df = spark.createDataFrame(data, [\"fruit\", \"price\"]) df.createOrReplaceTempView(\"fruits\") # LIKE spark.sql(\"SELECT fruit FROM fruits WHERE fruit LIKE 'B%'\").show() # Resultado: # +------+ # | fruit| # +------+ # |Banana| # +------+ # IN spark.sql(\"SELECT fruit FROM fruits WHERE fruit IN ('Apple', 'Cherry')\").show() # Resultado: # +------+ # | fruit| # +------+ # | Apple| # |Cherry| # +------+ # BETWEEN spark.sql(\"SELECT fruit, price FROM fruits WHERE price BETWEEN 0.8 AND 1.5\").show() # Resultado: # +-----+-----+ # |fruit|price| # +-----+-----+ # |Apple| 1.0| # +-----+-----+ # IS NULL / IS NOT NULL spark.sql(\"SELECT fruit, price FROM fruits WHERE price IS NULL\").show() # Resultado: # +----+-----+ # |fruit|price| # +----+-----+ # |Date| null| # +----+-----+ GROUP BY y HAVING (Agregaci\u00f3n) GROUP BY se utiliza para agrupar filas que tienen los mismos valores en una o m\u00e1s columnas, permitiendo aplicar funciones de agregaci\u00f3n. HAVING se usa para filtrar los resultados de las agregaciones. Funciones de agregaci\u00f3n ( COUNT , SUM , AVG , MIN , MAX ): Permiten resumir datos dentro de cada grupo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLAggregation\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") spark.sql(\"SELECT department, SUM(salary) AS total_salary, COUNT(*) AS num_employees FROM salaries GROUP BY department\").show() # Resultado: # +----------+------------+-------------+ # |department|total_salary|num_employees| # +----------+------------+-------------+ # | DeptA| 300| 2| # | DeptB| 250| 2| # | DeptC| 50| 1| # +----------+------------+-------------+ HAVING para filtrar resultados de agregaci\u00f3n: Aplica condiciones de filtrado despu\u00e9s de que las agregaciones se han calculado, a diferencia de WHERE que filtra antes. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLHaving\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") # Sumar salarios por departamento, pero solo para aquellos departamentos con un total de salarios > 200 spark.sql(\"SELECT department, SUM(salary) AS total_salary FROM salaries GROUP BY department HAVING SUM(salary) > 200\").show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | DeptA| 300| # +----------+------------+ 2.3.3 Creaci\u00f3n y uso de vistas temporales Las vistas temporales en SparkSQL son referencias a DataFrames que se registran en el cat\u00e1logo de Spark como si fueran tablas de bases de datos. Son \"temporales\" porque solo duran mientras la SparkSession est\u00e9 activa, o hasta que se eliminen expl\u00edcitamente. Son esenciales para permitir que las consultas SQL interact\u00faen con los DataFrames. Vistas Temporales Las vistas temporales son la forma principal de exponer un DataFrame para ser consultado mediante SQL. createTempView() vs. createOrReplaceTempView() : createTempView(view_name) : Crea una vista temporal con el nombre especificado. Si ya existe una vista con ese nombre, lanzar\u00e1 un error. createOrReplaceTempView(view_name) : Crea o reemplaza una vista temporal. Si una vista con el mismo nombre ya existe, la reemplazar\u00e1 sin lanzar un error. Esta es la m\u00e1s com\u00fanmente utilizada. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"TempViews\").getOrCreate() data1 = [(\"Laptop\", 1200)] df1 = spark.createDataFrame(data1, [\"item\", \"price\"]) data2 = [(\"Monitor\", 300)] df2 = spark.createDataFrame(data2, [\"item\", \"price\"]) # Usar createTempView (fallar\u00e1 si se ejecuta dos veces sin borrar) df1.createTempView(\"products_v1\") spark.sql(\"SELECT * FROM products_v1\").show() # Usar createOrReplaceTempView (permite sobrescribir) df2.createOrReplaceTempView(\"products_v1\") # Reemplaza la vista existente spark.sql(\"SELECT * FROM products_v1\").show() # Resultado: # +-------+-----+ # | item|price| # +-------+-----+ # |Monitor| 300| # +-------+-----+ Alcance de las vistas temporales (sesi\u00f3n): Las vistas temporales son locales a la SparkSession en la que se crearon. No son visibles para otras SparkSession s ni persisten despu\u00e9s de que la SparkSession finaliza. from pyspark.sql import SparkSession spark1 = SparkSession.builder.appName(\"Session1\").getOrCreate() spark2 = SparkSession.builder.appName(\"Session2\").getOrCreate() df_session1 = spark1.createDataFrame([(\"Data1\",)], [\"col\"]) df_session1.createOrReplaceTempView(\"my_data_session1\") # Consulta en la misma sesi\u00f3n (spark1) spark1.sql(\"SELECT * FROM my_data_session1\").show() # Intentar consultar desde otra sesi\u00f3n (spark2) - esto fallar\u00e1 try: spark2.sql(\"SELECT * FROM my_data_session1\").show() except Exception as e: print(f\"Error esperado en spark2: {e}\") # Resultado: Error esperado en spark2: Table or view not found: my_data_session1; Eliminaci\u00f3n de vistas temporales ( dropTempView() ): Aunque son temporales, es buena pr\u00e1ctica eliminarlas expl\u00edcitamente si ya no se necesitan para liberar recursos o evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropView\").getOrCreate() df = spark.createDataFrame([(\"test\",)], [\"col\"]) df.createOrReplaceTempView(\"my_test_view\") spark.sql(\"SELECT * FROM my_test_view\").show() spark.catalog.dropTempView(\"my_test_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM my_test_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_test_view; Vistas Globales Temporales A diferencia de las vistas temporales, las vistas globales temporales son visibles a trav\u00e9s de todas las SparkSession s dentro de la misma aplicaci\u00f3n Spark. Esto las hace \u00fatiles para compartir datos entre diferentes m\u00f3dulos o usuarios de una misma aplicaci\u00f3n. createGlobalTempView() vs. createOrReplaceGlobalTempView() : createGlobalTempView(view_name) : Crea una vista global temporal. Lanza un error si ya existe. createOrReplaceGlobalTempView(view_name) : Crea o reemplaza una vista global temporal. Las vistas globales temporales se acceden con el prefijo global_temp . view_name . from pyspark.sql import SparkSession # Session 1 spark1 = SparkSession.builder.appName(\"GlobalViewSession1\").getOrCreate() df_global = spark1.createDataFrame([(\"GlobalData\",)], [\"data_col\"]) df_global.createOrReplaceGlobalTempView(\"shared_data\") spark1.sql(\"SELECT * FROM global_temp.shared_data\").show() # Session 2 spark2 = SparkSession.builder.appName(\"GlobalViewSession2\").getOrCreate() # La vista global es accesible desde otra SparkSession spark2.sql(\"SELECT * FROM global_temp.shared_data\").show() # Resultado (desde spark2): # +----------+ # | data_col| # +----------+ # |GlobalData| # +----------+ Alcance de las vistas globales temporales (aplicaci\u00f3n Spark): Persisten mientras la aplicaci\u00f3n Spark est\u00e9 activa (es decir, el proceso SparkContext est\u00e9 en ejecuci\u00f3n). Se eliminan cuando la aplicaci\u00f3n finaliza. El ejemplo anterior demuestra el alcance a trav\u00e9s de sesiones, una vez que el script o la aplicaci\u00f3n Spark finalizan, la vista global temporal se destruye. Eliminaci\u00f3n de vistas globales temporales ( dropGlobalTempView() ): Se pueden eliminar expl\u00edcitamente si ya no se necesitan. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropGlobalView\").getOrCreate() df = spark.createDataFrame([(\"global test\",)], [\"col\"]) df.createOrReplaceGlobalTempView(\"my_global_view\") spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() spark.catalog.dropGlobalTempView(\"my_global_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_global_view; 2.3.4 Consultas avanzadas con SparkSQL SparkSQL no se limita a consultas b\u00e1sicas; soporta caracter\u00edsticas SQL avanzadas que son fundamentales para an\u00e1lisis complejos y transformaciones de datos, como uniones complejas, subconsultas, funciones de ventana y Common Table Expressions (CTEs). Uniones (JOINs) avanzadas M\u00e1s all\u00e1 del INNER JOIN , SparkSQL soporta una gama completa de tipos de uniones, incluyendo LEFT OUTER , RIGHT OUTER , FULL OUTER , LEFT SEMI y LEFT ANTI JOIN . Tipos de JOINs ( INNER , LEFT OUTER , RIGHT OUTER , FULL OUTER ): Comprender las diferencias entre los tipos de uniones es crucial para obtener los resultados deseados. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"AdvancedJoins\").getOrCreate() # Tabla de empleados employees_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 103)] employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"]) employees_df.createOrReplaceTempView(\"employees\") # Tabla de departamentos departments_data = [(101, \"Sales\"), (102, \"HR\"), (104, \"IT\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"]) departments_df.createOrReplaceTempView(\"departments\") # LEFT OUTER JOIN (todas las filas de la izquierda, coincidencias de la derecha o NULLs) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e LEFT OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Alice| Sales| # | Bob| HR| # | Charlie| null| # +---------+---------+ # FULL OUTER JOIN (todas las filas de ambas tablas, con NULLs donde no hay coincidencia) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e FULL OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Charlie| null| # | Alice| Sales| # | Bob| HR| # | null| IT| # +---------+---------+ LEFT SEMI JOIN y LEFT ANTI JOIN (Subconsultas optimizadas): LEFT SEMI JOIN : Retorna las filas del lado izquierdo (primera tabla) que tienen una coincidencia en el lado derecho (segunda tabla). No incluye columnas de la tabla derecha. Es similar a un INNER JOIN pero solo retorna columnas de la tabla izquierda y es m\u00e1s eficiente. LEFT ANTI JOIN : Retorna las filas del lado izquierdo que no tienen una coincidencia en el lado derecho. \u00datil para encontrar registros hu\u00e9rfanos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SemiAntiJoins\").getOrCreate() # Tabla de clientes customers_data = [(1, \"Ana\"), (2, \"Luis\"), (3, \"Marta\"), (4, \"Pedro\")] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\"]) customers_df.createOrReplaceTempView(\"customers\") # Tabla de pedidos orders_data = [(101, 1, 50), (102, 2, 75), (103, 1, 120)] # Marta y Pedro no tienen pedidos orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\"]) orders_df.createOrReplaceTempView(\"orders\") # LEFT SEMI JOIN (clientes que tienen al menos un pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT SEMI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Ana| # | Luis| # +-------------+ # LEFT ANTI JOIN (clientes que NO tienen ning\u00fan pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT ANTI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Marta| # | Pedro| # +-------------+ Subconsultas y CTEs (Common Table Expressions) Las subconsultas y CTEs permiten dividir consultas complejas en partes m\u00e1s manejables, mejorando la legibilidad y, en algunos casos, la optimizaci\u00f3n. Subconsultas en WHERE (IN, EXISTS): Permiten filtrar resultados bas\u00e1ndose en los valores de otra consulta. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Subqueries\").getOrCreate() # Tabla de productos products_data = [(1, \"Laptop\", 1200), (2, \"Mouse\", 25), (3, \"Keyboard\", 75), (4, \"Monitor\", 300)] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"]) products_df.createOrReplaceTempView(\"products\") # Tabla de stock stock_data = [(1, 10), (3, 5), (5, 20)] # Producto 2 y 4 no tienen stock stock_df = spark.createDataFrame(stock_data, [\"product_id\", \"quantity\"]) stock_df.createOrReplaceTempView(\"stock\") # Seleccionar productos que tienen stock (usando IN) spark.sql(\"\"\" SELECT product_name, price FROM products WHERE product_id IN (SELECT product_id FROM stock) \"\"\").show() # Resultado: # +------------+-----+ # |product_name|price| # +------------+-----+ # | Laptop| 1200| # | Keyboard| 75| # +------------+-----+ CTEs (Common Table Expressions) con WITH : Las CTEs definen un conjunto de resultados temporal y nombrado que se puede referenciar dentro de una \u00fanica sentencia SELECT , INSERT , UPDATE o DELETE . Mejoran la legibilidad y la modularidad de las consultas complejas. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CTEs\").getOrCreate() # Tabla de ventas sales_data = [(\"RegionA\", \"Q1\", 1000), (\"RegionA\", \"Q2\", 1200), (\"RegionB\", \"Q1\", 800), (\"RegionB\", \"Q2\", 900), (\"RegionA\", \"Q3\", 1500)] sales_df = spark.createDataFrame(sales_data, [\"region\", \"quarter\", \"revenue\"]) sales_df.createOrReplaceTempView(\"sales\") # Calcular el promedio de ingresos por regi\u00f3n usando una CTE spark.sql(\"\"\" WITH RegionalRevenue AS ( SELECT region, SUM(revenue) AS total_region_revenue FROM sales GROUP BY region ) SELECT r.region, r.total_region_revenue, (SELECT AVG(total_region_revenue) FROM RegionalRevenue) AS overall_avg_revenue FROM RegionalRevenue r \"\"\").show() # Resultado: # +-------+------------------+-------------------+ # | region|total_region_revenue|overall_avg_revenue| # +-------+------------------+-------------------+ # |RegionA| 3700| 2200.0| # |RegionB| 1700| 2200.0| # +-------+------------------+-------------------+ Funciones de Ventana (Window Functions): Permiten realizar c\u00e1lculos sobre un conjunto de filas relacionadas con la fila actual ( PARTITION BY , ORDER BY , ROWS BETWEEN / RANGE BETWEEN ). Son muy potentes para c\u00e1lculos anal\u00edticos como promedios m\u00f3viles, rankings, sumas acumuladas, etc. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate() # Tabla de transacciones transactions_data = [(\"StoreA\", \"2023-01-01\", 100), (\"StoreA\", \"2023-01-02\", 150), (\"StoreA\", \"2023-01-03\", 80), (\"StoreB\", \"2023-01-01\", 200), (\"StoreB\", \"2023-01-02\", 120)] transactions_df = spark.createDataFrame(transactions_data, [\"store_id\", \"sale_date\", \"amount\"]) transactions_df.createOrReplaceTempView(\"transactions\") # Calcular la suma acumulada de ventas por tienda, ordenada por fecha spark.sql(\"\"\" SELECT store_id, sale_date, amount, SUM(amount) OVER (PARTITION BY store_id ORDER BY sale_date) AS running_total FROM transactions ORDER BY store_id, sale_date \"\"\").show() # Resultado: # +--------+----------+------+-------------+ # |store_id| sale_date|amount|running_total| # +--------+----------+------+-------------+ # | StoreA|2023-01-01| 100| 100| # | StoreA|2023-01-02| 150| 250| # | StoreA|2023-01-03| 80| 330| # | StoreB|2023-01-01| 200| 200| # | StoreB|2023-01-02| 120| 320| # +--------+----------+------+-------------+ # Ranking de transacciones por tienda spark.sql(\"\"\" SELECT store_id, sale_date, amount, ROW_NUMBER() OVER (PARTITION BY store_id ORDER BY amount DESC) AS rank_by_amount FROM transactions ORDER BY store_id, rank_by_amount \"\"\").show() # Resultado: # +--------+----------+------+------------+ # |store_id| sale_date|amount|rank_by_amount| # +--------+----------+------+------------+ # | StoreA|2023-01-02| 150| 1| # | StoreA|2023-01-01| 100| 2| # | StoreA|2023-01-03| 80| 3| # | StoreB|2023-01-01| 200| 1| # | StoreB|2023-01-02| 120| 2| # +--------+----------+------+------------+ Tarea Ejercicios Pr\u00e1cticos relacionados con el tema 2.3 Crea un DataFrame de estudiantes con las columnas id , nombre , carrera y promedio_calificaciones . Datos de ejemplo: [(1, \"Juan\", \"Ingenier\u00eda\", 4.2), (2, \"Maria\", \"Medicina\", 3.8), (3, \"Pedro\", \"Ingenier\u00eda\", 4.5), (4, \"Ana\", \"Derecho\", 3.5)] . Registra este DataFrame como una vista temporal llamada estudiantes_temp . Luego, escribe una consulta SparkSQL que seleccione el nombre y la carrera de todos los estudiantes con un promedio_calificaciones superior a 4.0 . Usando la vista estudiantes_temp , escribe una consulta SparkSQL que seleccione el nombre , carrera y promedio_calificaciones de los estudiantes cuya carrera sea 'Ingenier\u00eda' Y cuyo nombre empiece con 'P'. Crea un DataFrame de ventas con columnas region , producto y cantidad . Datos de ejemplo: [(\"Norte\", \"A\", 10), (\"Norte\", \"B\", 15), (\"Sur\", \"A\", 20), (\"Norte\", \"A\", 5), (\"Sur\", \"B\", 12)] . Registra este DataFrame como ventas_temp . Escribe una consulta SparkSQL que calcule la SUM de cantidad por region y producto , pero solo para aquellos grupos donde la cantidad total sea mayor o igual a 25 . Crea un DataFrame de departamentos con id_departamento y nombre_departamento . Datos de ejemplo: [(1, \"Ventas\"), (2, \"Marketing\"), (3, \"Recursos Humanos\")] . Registra este DataFrame como departamentos_temp . Realiza un INNER JOIN entre estudiantes_temp (asume que carrera se puede unir con nombre_departamento ) y departamentos_temp para mostrar el nombre del estudiante y el nombre_departamento al que pertenecen. Nota: Para simplificar, puedes asumir que carrera de estudiantes_temp corresponde a nombre_departamento en departamentos_temp . Crea un DataFrame de productos_disponibles con producto_id y nombre_producto . Datos de ejemplo: [(101, \"Laptop\"), (102, \"Mouse\"), (103, \"Teclado\"), (104, \"Monitor\")] . Registra como productos_disponibles_temp . Crea otro DataFrame de productos_vendidos con producto_id y fecha_venta . Datos de ejemplo: [(101, \"2023-01-01\"), (103, \"2023-01-05\")] . Registra como productos_vendidos_temp . Escribe una consulta SparkSQL que encuentre los nombre_producto de los productos que nunca han sido vendidos. Utilizando las vistas productos_disponibles_temp y productos_vendidos_temp del ejercicio anterior, escribe una consulta SparkSQL usando una subconsulta con IN para seleccionar los nombre_producto de los productos que s\u00ed han sido vendidos. Crea un DataFrame de empleados_salarios con id_empleado , departamento y salario . Datos de ejemplo: [(1, \"IT\", 60000), (2, \"IT\", 70000), (3, \"HR\", 50000), (4, \"HR\", 55000), (5, \"IT\", 65000)] . Registra como empleados_salarios_temp . Usa una CTE para calcular el salario_promedio_departamental para cada departamento . Luego, en la consulta principal, selecciona id_empleado , departamento , salario y el salario_promedio_departamental de su departamento. Crea un DataFrame de puntajes_examen con clase , estudiante y puntaje . Datos de ejemplo: [(\"A\", \"Alice\", 90), (\"A\", \"Bob\", 85), (\"A\", \"Charlie\", 92), (\"B\", \"David\", 78), (\"B\", \"Eve\", 95)] . Registra como puntajes_examen_temp . Usa una funci\u00f3n de ventana para calcular el ranking de puntaje para cada estudiante dentro de cada clase , ordenando de mayor a menor puntaje. Muestra clase , estudiante , puntaje y ranking . Utilizando la vista ventas_temp del ejercicio 3 (a\u00f1ade una columna fecha_venta si no la tienes, ej. \"2023-01-01\" para todos por simplicidad, o mejor, crea nuevos datos con fechas distintas para cada venta en la misma regi\u00f3n): [(\"Norte\", \"A\", 10, \"2023-01-01\"), (\"Norte\", \"B\", 15, \"2023-01-02\"), (\"Norte\", \"A\", 5, \"2023-01-03\")] . Registra como ventas_fecha_temp . Calcula la cantidad_acumulada de ventas por region , ordenada por fecha_venta . Crea dos DataFrames: usuarios ( user_id , name ) y compras ( purchase_id , user_id , amount , purchase_date ). usuarios ejemplo: [(1, \"User A\"), (2, \"User B\")] compras ejemplo: [(101, 1, 50.0, \"2023-01-01\"), (102, 1, 75.0, \"2023-01-10\"), (103, 2, 120.0, \"2023-01-15\")] Registra ambos como vistas temporales ( users_temp , purchases_temp ). Usa una CTE para encontrar el total_comprado por cada user_id . Luego, en la consulta principal, une la CTE con users_temp para mostrar name y su total_comprado .","title":"Consultas y SQL en Spark"},{"location":"tema23/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema23/#tema-23-consultas-y-sql-en-spark","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de comprender los fundamentos de SparkSQL como una interfaz declarativa para el procesamiento de datos distribuidos, ejecutar consultas SQL directamente sobre DataFrames y fuentes de datos, crear y gestionar vistas temporales, y aplicar sentencias SQL avanzadas para realizar an\u00e1lisis complejos y transformaciones eficientes de grandes vol\u00famenes de datos. Introducci\u00f3n : Spark SQL es un m\u00f3dulo de Apache Spark para trabajar con datos estructurados. Proporciona una interfaz unificada para interactuar con datos utilizando consultas SQL est\u00e1ndar, lo que lo convierte en una herramienta invaluable para analistas de datos, ingenieros y cient\u00edficos que ya est\u00e1n familiarizados con el lenguaje SQL. Permite a los usuarios consultar datos almacenados en DataFrames, as\u00ed como en diversas fuentes de datos como Parquet, ORC, JSON, CSV, bases de datos JDBC y Hive, aprovechando al mismo tiempo el motor de ejecuci\u00f3n optimizado de Spark para lograr un rendimiento excepcional en escala de Big Data. Desarrollo : Este tema explorar\u00e1 c\u00f3mo Spark SQL integra la potencia de SQL con la escalabilidad de Spark. Iniciaremos con los fundamentos de Spark SQL, comprendiendo c\u00f3mo los DataFrames pueden ser vistos y consultados como tablas relacionales. Luego, avanzaremos a la ejecuci\u00f3n de consultas SQL b\u00e1sicas y la creaci\u00f3n y gesti\u00f3n de vistas temporales, que son esenciales para estructurar flujos de trabajo SQL. Finalmente, nos sumergiremos en consultas SQL avanzadas, incluyendo uniones complejas, subconsultas, funciones de ventana y CTEs (Common Table Expressions), demostrando c\u00f3mo Spark SQL puede manejar escenarios de an\u00e1lisis y transformaci\u00f3n de datos altamente sofisticados.","title":"Tema 2.3 Consultas y SQL en Spark"},{"location":"tema23/#231-fundamentos-de-sparksql","text":"Spark SQL permite la ejecuci\u00f3n de consultas SQL sobre datos estructurados o semi-estructurados. Esencialmente, act\u00faa como un motor SQL distribuido, permitiendo a los usuarios interactuar con DataFrames como si fueran tablas de bases de datos tradicionales, combinando la familiaridad de SQL con el poder de procesamiento de Spark.","title":"2.3.1 Fundamentos de SparkSQL"},{"location":"tema23/#la-relacion-entre-dataframes-y-tablasvistas-en-sparksql","text":"La clave de SparkSQL reside en su capacidad para mapear DataFrames a estructuras relacionales como tablas o vistas. Esto permite que los datos en un DataFrame sean consultados usando sintaxis SQL est\u00e1ndar, lo que facilita la integraci\u00f3n para usuarios con experiencia en bases de datos relacionales. DataFrames como la base de SparkSQL: Todos los datos en SparkSQL se manejan como DataFrames. Cuando se ejecuta una consulta SQL, Spark la analiza, la optimiza y la ejecuta sobre los DataFrames subyacentes. El resultado de una consulta SQL es siempre un DataFrame. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLFundamentals\").getOrCreate() # Crear un DataFrame data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\"), (\"Charlie\", 22, \"CHI\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) # El DataFrame es la representaci\u00f3n en memoria df.show() # +-------+---+----+ # | name|age|city| # +-------+---+----+ # | Alice| 25| NY| # | Bob| 30| LA| # |Charlie| 22| CHI| # +-------+---+----+ # Sin una vista temporal, no podemos consultarlo directamente con spark.sql try: spark.sql(\"SELECT * FROM my_table\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_table; spark.sql() para ejecutar consultas SQL: Esta es la funci\u00f3n principal para ejecutar consultas SQL directamente en el contexto de Spark. Las consultas se escriben como cadenas de texto. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SparkSQLQuery\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) # Crear una vista temporal para poder consultar el DataFrame con SQL df.createOrReplaceTempView(\"people\") # Ejecutar una consulta SQL result_df = spark.sql(\"SELECT name, age FROM people WHERE age > 25\") result_df.show() # Resultado: # +----+---+ # |name|age| # +----+---+ # | Bob| 30| # +----+---+ Optimizador Catalyst y Generaci\u00f3n de C\u00f3digo Tungsten: Spark SQL utiliza dos componentes clave para la optimizaci\u00f3n y ejecuci\u00f3n de consultas: Catalyst Optimizer: Un optimizador de consulta basado en reglas y costos que genera planes de ejecuci\u00f3n eficientes. Puede realizar optimizaciones como predicado pushdown, column pruning y reordenamiento de joins. Tungsten: Un motor de ejecuci\u00f3n que genera c\u00f3digo optimizado en tiempo de ejecuci\u00f3n para DataFrames, mejorando la eficiencia de la CPU y el uso de memoria a trav\u00e9s de t\u00e9cnicas como la eliminaci\u00f3n de punteros y la gesti\u00f3n expl\u00edcita de memoria. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CatalystTungsten\").getOrCreate() data = [(\"Laptop\", 1200, \"Electronics\", 10), (\"Mouse\", 25, \"Electronics\", 50), (\"Book\", 15, \"Books\", 100)] df = spark.createDataFrame(data, [\"product_name\", \"price\", \"category\", \"stock\"]) df.createOrReplaceTempView(\"products\") # Observar el plan de ejecuci\u00f3n l\u00f3gico y f\u00edsico (Catalyst y Tungsten en acci\u00f3n) # Explain muestra c\u00f3mo Spark traduce la consulta SQL a una serie de operaciones optimizadas spark.sql(\"SELECT product_name, price FROM products WHERE category = 'Electronics' AND price > 100\").explain(extended=True) # La salida mostrar\u00e1 el plan l\u00f3gico (Parsed, Analyzed, Optimized) y el plan f\u00edsico. # Notar \"PushedFilters\" y \"PushedProjections\" que son optimizaciones de Catalyst. # Los operadores f\u00edsicos son implementaciones optimizadas por Tungsten.","title":"La relaci\u00f3n entre DataFrames y Tablas/Vistas en SparkSQL"},{"location":"tema23/#232-consultas-basicas-con-sparksql","text":"Una vez que un DataFrame se ha registrado como una vista temporal, se pueden realizar las operaciones de SQL m\u00e1s comunes sobre \u00e9l. Estas operaciones son equivalentes a las transformaciones de DataFrame API, pero expresadas en un lenguaje declarativo.","title":"2.3.2 Consultas b\u00e1sicas con SparkSQL"},{"location":"tema23/#select-y-from","text":"La base de cualquier consulta SQL, permitiendo especificar qu\u00e9 columnas se quieren recuperar y de qu\u00e9 fuente de datos. Selecci\u00f3n de todas las columnas ( SELECT * ): Recupera todas las columnas de una vista o tabla. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25), (\"Bob\", 30)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"people\") spark.sql(\"SELECT * FROM people\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # |Alice| 25| # | Bob| 30| # +-----+---+ Selecci\u00f3n de columnas espec\u00edficas ( SELECT column1, column2 ): Permite especificar las columnas que se desean recuperar, optimizando el rendimiento al no leer datos innecesarios. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"Alice\", 25, \"NY\"), (\"Bob\", 30, \"LA\")] df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"]) df.createOrReplaceTempView(\"users\") spark.sql(\"SELECT name, city FROM users\").show() # Resultado: # +-----+----+ # | name|city| # +-----+----+ # |Alice| NY| # | Bob| LA| # +-----+----+ Renombrar columnas con AS : Asigna un alias a una columna para hacer el resultado m\u00e1s legible o para evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQL\").getOrCreate() data = [(\"ProductA\", 100), (\"ProductB\", 200)] df = spark.createDataFrame(data, [\"item_name\", \"item_price\"]) df.createOrReplaceTempView(\"items\") spark.sql(\"SELECT item_name AS product, item_price AS price FROM items\").show() # Resultado: # +--------+-----+ # | product|price| # +--------+-----+ # |ProductA| 100| # |ProductB| 200| # +--------+-----+","title":"SELECT y FROM"},{"location":"tema23/#where-filtrado","text":"La cl\u00e1usula WHERE se utiliza para filtrar filas bas\u00e1ndose en una o m\u00e1s condiciones, al igual que en SQL tradicional. Condiciones de igualdad y desigualdad ( = , != , < , > , <= , >= ): Filtra filas donde una columna cumple una condici\u00f3n de comparaci\u00f3n. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhere\").getOrCreate() data = [(\"Juan\", 30), (\"Maria\", 25), (\"Pedro\", 35)] df = spark.createDataFrame(data, [\"name\", \"age\"]) df.createOrReplaceTempView(\"employees\") spark.sql(\"SELECT name, age FROM employees WHERE age > 28\").show() # Resultado: # +-----+---+ # | name|age| # +-----+---+ # | Juan| 30| # |Pedro| 35| # +-----+---+ Operadores l\u00f3gicos ( AND , OR , NOT ): Combina m\u00faltiples condiciones de filtrado. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereLogic\").getOrCreate() data = [(\"Shirt\", \"Blue\", \"Small\"), (\"Pants\", \"Black\", \"Medium\"), (\"T-Shirt\", \"Red\", \"Large\")] df = spark.createDataFrame(data, [\"item\", \"color\", \"size\"]) df.createOrReplaceTempView(\"apparel\") spark.sql(\"SELECT item, color FROM apparel WHERE color = 'Blue' OR size = 'Large'\").show() # Resultado: # +-------+-----+ # | item|color| # +-------+-----+ # | Shirt| Blue| # |T-Shirt| Red| # +-------+-----+ LIKE , IN , BETWEEN , IS NULL / IS NOT NULL : Funciones de filtrado comunes para patrones, listas de valores, rangos y valores nulos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLWhereSpecial\").getOrCreate() data = [(\"Apple\", 1.0), (\"Banana\", 0.5), (\"Cherry\", 2.0), (\"Date\", None)] df = spark.createDataFrame(data, [\"fruit\", \"price\"]) df.createOrReplaceTempView(\"fruits\") # LIKE spark.sql(\"SELECT fruit FROM fruits WHERE fruit LIKE 'B%'\").show() # Resultado: # +------+ # | fruit| # +------+ # |Banana| # +------+ # IN spark.sql(\"SELECT fruit FROM fruits WHERE fruit IN ('Apple', 'Cherry')\").show() # Resultado: # +------+ # | fruit| # +------+ # | Apple| # |Cherry| # +------+ # BETWEEN spark.sql(\"SELECT fruit, price FROM fruits WHERE price BETWEEN 0.8 AND 1.5\").show() # Resultado: # +-----+-----+ # |fruit|price| # +-----+-----+ # |Apple| 1.0| # +-----+-----+ # IS NULL / IS NOT NULL spark.sql(\"SELECT fruit, price FROM fruits WHERE price IS NULL\").show() # Resultado: # +----+-----+ # |fruit|price| # +----+-----+ # |Date| null| # +----+-----+","title":"WHERE (Filtrado)"},{"location":"tema23/#group-by-y-having-agregacion","text":"GROUP BY se utiliza para agrupar filas que tienen los mismos valores en una o m\u00e1s columnas, permitiendo aplicar funciones de agregaci\u00f3n. HAVING se usa para filtrar los resultados de las agregaciones. Funciones de agregaci\u00f3n ( COUNT , SUM , AVG , MIN , MAX ): Permiten resumir datos dentro de cada grupo. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLAggregation\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") spark.sql(\"SELECT department, SUM(salary) AS total_salary, COUNT(*) AS num_employees FROM salaries GROUP BY department\").show() # Resultado: # +----------+------------+-------------+ # |department|total_salary|num_employees| # +----------+------------+-------------+ # | DeptA| 300| 2| # | DeptB| 250| 2| # | DeptC| 50| 1| # +----------+------------+-------------+ HAVING para filtrar resultados de agregaci\u00f3n: Aplica condiciones de filtrado despu\u00e9s de que las agregaciones se han calculado, a diferencia de WHERE que filtra antes. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"BasicSQLHaving\").getOrCreate() data = [(\"DeptA\", 100), (\"DeptB\", 150), (\"DeptA\", 200), (\"DeptC\", 50), (\"DeptB\", 100)] df = spark.createDataFrame(data, [\"department\", \"salary\"]) df.createOrReplaceTempView(\"salaries\") # Sumar salarios por departamento, pero solo para aquellos departamentos con un total de salarios > 200 spark.sql(\"SELECT department, SUM(salary) AS total_salary FROM salaries GROUP BY department HAVING SUM(salary) > 200\").show() # Resultado: # +----------+------------+ # |department|total_salary| # +----------+------------+ # | DeptA| 300| # +----------+------------+","title":"GROUP BY y HAVING (Agregaci\u00f3n)"},{"location":"tema23/#233-creacion-y-uso-de-vistas-temporales","text":"Las vistas temporales en SparkSQL son referencias a DataFrames que se registran en el cat\u00e1logo de Spark como si fueran tablas de bases de datos. Son \"temporales\" porque solo duran mientras la SparkSession est\u00e9 activa, o hasta que se eliminen expl\u00edcitamente. Son esenciales para permitir que las consultas SQL interact\u00faen con los DataFrames.","title":"2.3.3 Creaci\u00f3n y uso de vistas temporales"},{"location":"tema23/#vistas-temporales","text":"Las vistas temporales son la forma principal de exponer un DataFrame para ser consultado mediante SQL. createTempView() vs. createOrReplaceTempView() : createTempView(view_name) : Crea una vista temporal con el nombre especificado. Si ya existe una vista con ese nombre, lanzar\u00e1 un error. createOrReplaceTempView(view_name) : Crea o reemplaza una vista temporal. Si una vista con el mismo nombre ya existe, la reemplazar\u00e1 sin lanzar un error. Esta es la m\u00e1s com\u00fanmente utilizada. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"TempViews\").getOrCreate() data1 = [(\"Laptop\", 1200)] df1 = spark.createDataFrame(data1, [\"item\", \"price\"]) data2 = [(\"Monitor\", 300)] df2 = spark.createDataFrame(data2, [\"item\", \"price\"]) # Usar createTempView (fallar\u00e1 si se ejecuta dos veces sin borrar) df1.createTempView(\"products_v1\") spark.sql(\"SELECT * FROM products_v1\").show() # Usar createOrReplaceTempView (permite sobrescribir) df2.createOrReplaceTempView(\"products_v1\") # Reemplaza la vista existente spark.sql(\"SELECT * FROM products_v1\").show() # Resultado: # +-------+-----+ # | item|price| # +-------+-----+ # |Monitor| 300| # +-------+-----+ Alcance de las vistas temporales (sesi\u00f3n): Las vistas temporales son locales a la SparkSession en la que se crearon. No son visibles para otras SparkSession s ni persisten despu\u00e9s de que la SparkSession finaliza. from pyspark.sql import SparkSession spark1 = SparkSession.builder.appName(\"Session1\").getOrCreate() spark2 = SparkSession.builder.appName(\"Session2\").getOrCreate() df_session1 = spark1.createDataFrame([(\"Data1\",)], [\"col\"]) df_session1.createOrReplaceTempView(\"my_data_session1\") # Consulta en la misma sesi\u00f3n (spark1) spark1.sql(\"SELECT * FROM my_data_session1\").show() # Intentar consultar desde otra sesi\u00f3n (spark2) - esto fallar\u00e1 try: spark2.sql(\"SELECT * FROM my_data_session1\").show() except Exception as e: print(f\"Error esperado en spark2: {e}\") # Resultado: Error esperado en spark2: Table or view not found: my_data_session1; Eliminaci\u00f3n de vistas temporales ( dropTempView() ): Aunque son temporales, es buena pr\u00e1ctica eliminarlas expl\u00edcitamente si ya no se necesitan para liberar recursos o evitar conflictos de nombres. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropView\").getOrCreate() df = spark.createDataFrame([(\"test\",)], [\"col\"]) df.createOrReplaceTempView(\"my_test_view\") spark.sql(\"SELECT * FROM my_test_view\").show() spark.catalog.dropTempView(\"my_test_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM my_test_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_test_view;","title":"Vistas Temporales"},{"location":"tema23/#vistas-globales-temporales","text":"A diferencia de las vistas temporales, las vistas globales temporales son visibles a trav\u00e9s de todas las SparkSession s dentro de la misma aplicaci\u00f3n Spark. Esto las hace \u00fatiles para compartir datos entre diferentes m\u00f3dulos o usuarios de una misma aplicaci\u00f3n. createGlobalTempView() vs. createOrReplaceGlobalTempView() : createGlobalTempView(view_name) : Crea una vista global temporal. Lanza un error si ya existe. createOrReplaceGlobalTempView(view_name) : Crea o reemplaza una vista global temporal. Las vistas globales temporales se acceden con el prefijo global_temp . view_name . from pyspark.sql import SparkSession # Session 1 spark1 = SparkSession.builder.appName(\"GlobalViewSession1\").getOrCreate() df_global = spark1.createDataFrame([(\"GlobalData\",)], [\"data_col\"]) df_global.createOrReplaceGlobalTempView(\"shared_data\") spark1.sql(\"SELECT * FROM global_temp.shared_data\").show() # Session 2 spark2 = SparkSession.builder.appName(\"GlobalViewSession2\").getOrCreate() # La vista global es accesible desde otra SparkSession spark2.sql(\"SELECT * FROM global_temp.shared_data\").show() # Resultado (desde spark2): # +----------+ # | data_col| # +----------+ # |GlobalData| # +----------+ Alcance de las vistas globales temporales (aplicaci\u00f3n Spark): Persisten mientras la aplicaci\u00f3n Spark est\u00e9 activa (es decir, el proceso SparkContext est\u00e9 en ejecuci\u00f3n). Se eliminan cuando la aplicaci\u00f3n finaliza. El ejemplo anterior demuestra el alcance a trav\u00e9s de sesiones, una vez que el script o la aplicaci\u00f3n Spark finalizan, la vista global temporal se destruye. Eliminaci\u00f3n de vistas globales temporales ( dropGlobalTempView() ): Se pueden eliminar expl\u00edcitamente si ya no se necesitan. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"DropGlobalView\").getOrCreate() df = spark.createDataFrame([(\"global test\",)], [\"col\"]) df.createOrReplaceGlobalTempView(\"my_global_view\") spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() spark.catalog.dropGlobalTempView(\"my_global_view\") # Intentar consultar despu\u00e9s de eliminar - esto fallar\u00e1 try: spark.sql(\"SELECT * FROM global_temp.my_global_view\").show() except Exception as e: print(f\"Error esperado: {e}\") # Resultado: Error esperado: Table or view not found: my_global_view;","title":"Vistas Globales Temporales"},{"location":"tema23/#234-consultas-avanzadas-con-sparksql","text":"SparkSQL no se limita a consultas b\u00e1sicas; soporta caracter\u00edsticas SQL avanzadas que son fundamentales para an\u00e1lisis complejos y transformaciones de datos, como uniones complejas, subconsultas, funciones de ventana y Common Table Expressions (CTEs).","title":"2.3.4 Consultas avanzadas con SparkSQL"},{"location":"tema23/#uniones-joins-avanzadas","text":"M\u00e1s all\u00e1 del INNER JOIN , SparkSQL soporta una gama completa de tipos de uniones, incluyendo LEFT OUTER , RIGHT OUTER , FULL OUTER , LEFT SEMI y LEFT ANTI JOIN . Tipos de JOINs ( INNER , LEFT OUTER , RIGHT OUTER , FULL OUTER ): Comprender las diferencias entre los tipos de uniones es crucial para obtener los resultados deseados. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"AdvancedJoins\").getOrCreate() # Tabla de empleados employees_data = [(1, \"Alice\", 101), (2, \"Bob\", 102), (3, \"Charlie\", 103)] employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"emp_name\", \"dept_id\"]) employees_df.createOrReplaceTempView(\"employees\") # Tabla de departamentos departments_data = [(101, \"Sales\"), (102, \"HR\"), (104, \"IT\")] departments_df = spark.createDataFrame(departments_data, [\"dept_id\", \"dept_name\"]) departments_df.createOrReplaceTempView(\"departments\") # LEFT OUTER JOIN (todas las filas de la izquierda, coincidencias de la derecha o NULLs) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e LEFT OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Alice| Sales| # | Bob| HR| # | Charlie| null| # +---------+---------+ # FULL OUTER JOIN (todas las filas de ambas tablas, con NULLs donde no hay coincidencia) spark.sql(\"\"\" SELECT e.emp_name, d.dept_name FROM employees e FULL OUTER JOIN departments d ON e.dept_id = d.dept_id \"\"\").show() # Resultado: # +---------+---------+ # | emp_name|dept_name| # +---------+---------+ # | Charlie| null| # | Alice| Sales| # | Bob| HR| # | null| IT| # +---------+---------+ LEFT SEMI JOIN y LEFT ANTI JOIN (Subconsultas optimizadas): LEFT SEMI JOIN : Retorna las filas del lado izquierdo (primera tabla) que tienen una coincidencia en el lado derecho (segunda tabla). No incluye columnas de la tabla derecha. Es similar a un INNER JOIN pero solo retorna columnas de la tabla izquierda y es m\u00e1s eficiente. LEFT ANTI JOIN : Retorna las filas del lado izquierdo que no tienen una coincidencia en el lado derecho. \u00datil para encontrar registros hu\u00e9rfanos. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"SemiAntiJoins\").getOrCreate() # Tabla de clientes customers_data = [(1, \"Ana\"), (2, \"Luis\"), (3, \"Marta\"), (4, \"Pedro\")] customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\"]) customers_df.createOrReplaceTempView(\"customers\") # Tabla de pedidos orders_data = [(101, 1, 50), (102, 2, 75), (103, 1, 120)] # Marta y Pedro no tienen pedidos orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\"]) orders_df.createOrReplaceTempView(\"orders\") # LEFT SEMI JOIN (clientes que tienen al menos un pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT SEMI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Ana| # | Luis| # +-------------+ # LEFT ANTI JOIN (clientes que NO tienen ning\u00fan pedido) spark.sql(\"\"\" SELECT c.customer_name FROM customers c LEFT ANTI JOIN orders o ON c.customer_id = o.customer_id \"\"\").show() # Resultado: # +-------------+ # |customer_name| # +-------------+ # | Marta| # | Pedro| # +-------------+","title":"Uniones (JOINs) avanzadas"},{"location":"tema23/#subconsultas-y-ctes-common-table-expressions","text":"Las subconsultas y CTEs permiten dividir consultas complejas en partes m\u00e1s manejables, mejorando la legibilidad y, en algunos casos, la optimizaci\u00f3n. Subconsultas en WHERE (IN, EXISTS): Permiten filtrar resultados bas\u00e1ndose en los valores de otra consulta. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Subqueries\").getOrCreate() # Tabla de productos products_data = [(1, \"Laptop\", 1200), (2, \"Mouse\", 25), (3, \"Keyboard\", 75), (4, \"Monitor\", 300)] products_df = spark.createDataFrame(products_data, [\"product_id\", \"product_name\", \"price\"]) products_df.createOrReplaceTempView(\"products\") # Tabla de stock stock_data = [(1, 10), (3, 5), (5, 20)] # Producto 2 y 4 no tienen stock stock_df = spark.createDataFrame(stock_data, [\"product_id\", \"quantity\"]) stock_df.createOrReplaceTempView(\"stock\") # Seleccionar productos que tienen stock (usando IN) spark.sql(\"\"\" SELECT product_name, price FROM products WHERE product_id IN (SELECT product_id FROM stock) \"\"\").show() # Resultado: # +------------+-----+ # |product_name|price| # +------------+-----+ # | Laptop| 1200| # | Keyboard| 75| # +------------+-----+ CTEs (Common Table Expressions) con WITH : Las CTEs definen un conjunto de resultados temporal y nombrado que se puede referenciar dentro de una \u00fanica sentencia SELECT , INSERT , UPDATE o DELETE . Mejoran la legibilidad y la modularidad de las consultas complejas. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"CTEs\").getOrCreate() # Tabla de ventas sales_data = [(\"RegionA\", \"Q1\", 1000), (\"RegionA\", \"Q2\", 1200), (\"RegionB\", \"Q1\", 800), (\"RegionB\", \"Q2\", 900), (\"RegionA\", \"Q3\", 1500)] sales_df = spark.createDataFrame(sales_data, [\"region\", \"quarter\", \"revenue\"]) sales_df.createOrReplaceTempView(\"sales\") # Calcular el promedio de ingresos por regi\u00f3n usando una CTE spark.sql(\"\"\" WITH RegionalRevenue AS ( SELECT region, SUM(revenue) AS total_region_revenue FROM sales GROUP BY region ) SELECT r.region, r.total_region_revenue, (SELECT AVG(total_region_revenue) FROM RegionalRevenue) AS overall_avg_revenue FROM RegionalRevenue r \"\"\").show() # Resultado: # +-------+------------------+-------------------+ # | region|total_region_revenue|overall_avg_revenue| # +-------+------------------+-------------------+ # |RegionA| 3700| 2200.0| # |RegionB| 1700| 2200.0| # +-------+------------------+-------------------+ Funciones de Ventana (Window Functions): Permiten realizar c\u00e1lculos sobre un conjunto de filas relacionadas con la fila actual ( PARTITION BY , ORDER BY , ROWS BETWEEN / RANGE BETWEEN ). Son muy potentes para c\u00e1lculos anal\u00edticos como promedios m\u00f3viles, rankings, sumas acumuladas, etc. from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate() # Tabla de transacciones transactions_data = [(\"StoreA\", \"2023-01-01\", 100), (\"StoreA\", \"2023-01-02\", 150), (\"StoreA\", \"2023-01-03\", 80), (\"StoreB\", \"2023-01-01\", 200), (\"StoreB\", \"2023-01-02\", 120)] transactions_df = spark.createDataFrame(transactions_data, [\"store_id\", \"sale_date\", \"amount\"]) transactions_df.createOrReplaceTempView(\"transactions\") # Calcular la suma acumulada de ventas por tienda, ordenada por fecha spark.sql(\"\"\" SELECT store_id, sale_date, amount, SUM(amount) OVER (PARTITION BY store_id ORDER BY sale_date) AS running_total FROM transactions ORDER BY store_id, sale_date \"\"\").show() # Resultado: # +--------+----------+------+-------------+ # |store_id| sale_date|amount|running_total| # +--------+----------+------+-------------+ # | StoreA|2023-01-01| 100| 100| # | StoreA|2023-01-02| 150| 250| # | StoreA|2023-01-03| 80| 330| # | StoreB|2023-01-01| 200| 200| # | StoreB|2023-01-02| 120| 320| # +--------+----------+------+-------------+ # Ranking de transacciones por tienda spark.sql(\"\"\" SELECT store_id, sale_date, amount, ROW_NUMBER() OVER (PARTITION BY store_id ORDER BY amount DESC) AS rank_by_amount FROM transactions ORDER BY store_id, rank_by_amount \"\"\").show() # Resultado: # +--------+----------+------+------------+ # |store_id| sale_date|amount|rank_by_amount| # +--------+----------+------+------------+ # | StoreA|2023-01-02| 150| 1| # | StoreA|2023-01-01| 100| 2| # | StoreA|2023-01-03| 80| 3| # | StoreB|2023-01-01| 200| 1| # | StoreB|2023-01-02| 120| 2| # +--------+----------+------+------------+","title":"Subconsultas y CTEs (Common Table Expressions)"},{"location":"tema23/#tarea","text":"","title":"Tarea"},{"location":"tema23/#ejercicios-practicos-relacionados-con-el-tema-23","text":"Crea un DataFrame de estudiantes con las columnas id , nombre , carrera y promedio_calificaciones . Datos de ejemplo: [(1, \"Juan\", \"Ingenier\u00eda\", 4.2), (2, \"Maria\", \"Medicina\", 3.8), (3, \"Pedro\", \"Ingenier\u00eda\", 4.5), (4, \"Ana\", \"Derecho\", 3.5)] . Registra este DataFrame como una vista temporal llamada estudiantes_temp . Luego, escribe una consulta SparkSQL que seleccione el nombre y la carrera de todos los estudiantes con un promedio_calificaciones superior a 4.0 . Usando la vista estudiantes_temp , escribe una consulta SparkSQL que seleccione el nombre , carrera y promedio_calificaciones de los estudiantes cuya carrera sea 'Ingenier\u00eda' Y cuyo nombre empiece con 'P'. Crea un DataFrame de ventas con columnas region , producto y cantidad . Datos de ejemplo: [(\"Norte\", \"A\", 10), (\"Norte\", \"B\", 15), (\"Sur\", \"A\", 20), (\"Norte\", \"A\", 5), (\"Sur\", \"B\", 12)] . Registra este DataFrame como ventas_temp . Escribe una consulta SparkSQL que calcule la SUM de cantidad por region y producto , pero solo para aquellos grupos donde la cantidad total sea mayor o igual a 25 . Crea un DataFrame de departamentos con id_departamento y nombre_departamento . Datos de ejemplo: [(1, \"Ventas\"), (2, \"Marketing\"), (3, \"Recursos Humanos\")] . Registra este DataFrame como departamentos_temp . Realiza un INNER JOIN entre estudiantes_temp (asume que carrera se puede unir con nombre_departamento ) y departamentos_temp para mostrar el nombre del estudiante y el nombre_departamento al que pertenecen. Nota: Para simplificar, puedes asumir que carrera de estudiantes_temp corresponde a nombre_departamento en departamentos_temp . Crea un DataFrame de productos_disponibles con producto_id y nombre_producto . Datos de ejemplo: [(101, \"Laptop\"), (102, \"Mouse\"), (103, \"Teclado\"), (104, \"Monitor\")] . Registra como productos_disponibles_temp . Crea otro DataFrame de productos_vendidos con producto_id y fecha_venta . Datos de ejemplo: [(101, \"2023-01-01\"), (103, \"2023-01-05\")] . Registra como productos_vendidos_temp . Escribe una consulta SparkSQL que encuentre los nombre_producto de los productos que nunca han sido vendidos. Utilizando las vistas productos_disponibles_temp y productos_vendidos_temp del ejercicio anterior, escribe una consulta SparkSQL usando una subconsulta con IN para seleccionar los nombre_producto de los productos que s\u00ed han sido vendidos. Crea un DataFrame de empleados_salarios con id_empleado , departamento y salario . Datos de ejemplo: [(1, \"IT\", 60000), (2, \"IT\", 70000), (3, \"HR\", 50000), (4, \"HR\", 55000), (5, \"IT\", 65000)] . Registra como empleados_salarios_temp . Usa una CTE para calcular el salario_promedio_departamental para cada departamento . Luego, en la consulta principal, selecciona id_empleado , departamento , salario y el salario_promedio_departamental de su departamento. Crea un DataFrame de puntajes_examen con clase , estudiante y puntaje . Datos de ejemplo: [(\"A\", \"Alice\", 90), (\"A\", \"Bob\", 85), (\"A\", \"Charlie\", 92), (\"B\", \"David\", 78), (\"B\", \"Eve\", 95)] . Registra como puntajes_examen_temp . Usa una funci\u00f3n de ventana para calcular el ranking de puntaje para cada estudiante dentro de cada clase , ordenando de mayor a menor puntaje. Muestra clase , estudiante , puntaje y ranking . Utilizando la vista ventas_temp del ejercicio 3 (a\u00f1ade una columna fecha_venta si no la tienes, ej. \"2023-01-01\" para todos por simplicidad, o mejor, crea nuevos datos con fechas distintas para cada venta en la misma regi\u00f3n): [(\"Norte\", \"A\", 10, \"2023-01-01\"), (\"Norte\", \"B\", 15, \"2023-01-02\"), (\"Norte\", \"A\", 5, \"2023-01-03\")] . Registra como ventas_fecha_temp . Calcula la cantidad_acumulada de ventas por region , ordenada por fecha_venta . Crea dos DataFrames: usuarios ( user_id , name ) y compras ( purchase_id , user_id , amount , purchase_date ). usuarios ejemplo: [(1, \"User A\"), (2, \"User B\")] compras ejemplo: [(101, 1, 50.0, \"2023-01-01\"), (102, 1, 75.0, \"2023-01-10\"), (103, 2, 120.0, \"2023-01-15\")] Registra ambos como vistas temporales ( users_temp , purchases_temp ). Usa una CTE para encontrar el total_comprado por cada user_id . Luego, en la consulta principal, une la CTE con users_temp para mostrar name y su total_comprado .","title":"Ejercicios Pr\u00e1cticos relacionados con el tema 2.3"},{"location":"tema24/","text":"2. PySpark y SparkSQL Tema 2.4 Optimizaci\u00f3n y Rendimiento Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de identificar y aplicar t\u00e9cnicas avanzadas de optimizaci\u00f3n y gesti\u00f3n del rendimiento en aplicaciones Spark, comprendiendo el funcionamiento interno del motor y las estrategias para el manejo eficiente de datos distribuidos y operaciones complejas, con el fin de mejorar significativamente la eficiencia y escalabilidad de los pipelines de procesamiento de Big Data. Introducci\u00f3n : En el vasto universo del procesamiento de Big Data con Apache Spark, la capacidad de escribir c\u00f3digo funcional es solo la primera parte de la ecuaci\u00f3n. La verdadera maestr\u00eda reside en optimizar ese c\u00f3digo para que se ejecute de manera eficiente, consumiendo menos recursos y completando las tareas en el menor tiempo posible. Este tema se adentra en el coraz\u00f3n del rendimiento de Spark, explorando las herramientas y los principios subyacentes que permiten transformar una aplicaci\u00f3n de datos masivos en una soluci\u00f3n robusta y escalable. Desarrollo : La optimizaci\u00f3n en Spark es un proceso multifac\u00e9tico que abarca desde la gesti\u00f3n inteligente de la memoria y el disco hasta la comprensi\u00f3n profunda de c\u00f3mo Spark planifica y ejecuta las operaciones. Se explorar\u00e1n conceptos fundamentales como la persistencia de DataFrames, la arquitectura de optimizaci\u00f3n de Spark (Catalyst y Tungsten), y estrategias avanzadas para el manejo de joins y la reducci\u00f3n de operaciones costosas como los shuffles. Finalmente, se abordar\u00e1n consideraciones clave para escalar y mantener el desempe\u00f1o en entornos de producci\u00f3n, proporcionando al estudiante las herramientas para diagnosticar cuellos de botella y aplicar soluciones efectivas en sus proyectos de Big Data. 2.4.1 Persistencia y cach\u00e9 (persist() y cache()) La persistencia en Spark es una caracter\u00edstica fundamental para optimizar el rendimiento de las operaciones sobre DataFrames o RDDs que se reutilizan m\u00faltiples veces. Cuando una operaci\u00f3n se ejecuta sobre un DataFrame, Spark recalcula el DataFrame desde el origen cada vez que se invoca una acci\u00f3n. Esto puede ser ineficiente si el mismo DataFrame es utilizado en varias operaciones subsiguientes. Al aplicar persist() o cache() , Spark almacena el DataFrame o RDD resultante de una transformaci\u00f3n en memoria, en disco o una combinaci\u00f3n de ambos, evitando rec\u00e1lculos innecesarios y acelerando las operaciones posteriores. Niveles de almacenamiento en persist() Spark ofrece diferentes niveles de almacenamiento para persist() , lo que permite controlar d\u00f3nde y c\u00f3mo se guardan los datos para optimizar el equilibrio entre memoria, disco y replicaci\u00f3n. La elecci\u00f3n del nivel adecuado depende de la cantidad de memoria disponible, la necesidad de tolerancia a fallos y la frecuencia de acceso a los datos. MEMORY_ONLY : Es el nivel por defecto para cache() . Los RDDs/DataFrames se almacenan como objetos deserializados de Python (o Java/Scala si se usa Scala/Java) en la JVM. Si no cabe en memoria, algunas particiones se recalculan cuando son necesarias. Conjuntos de datos peque\u00f1os y medianos : Cuando se tiene un DataFrame que cabe completamente en la memoria de los ejecutores y se va a utilizar repetidamente en m\u00faltiples transformaciones. Por ejemplo, un cat\u00e1logo de productos que se une frecuentemente con transacciones. Operaciones iterativas : En algoritmos de machine learning (como K-Means o PageRank) donde un conjunto de datos se itera sobre muchas veces, manteniendo los datos en memoria reduce dr\u00e1sticamente el tiempo de ejecuci\u00f3n de cada iteraci\u00f3n. MEMORY_AND_DISK : Almacena las particiones en memoria. Si no hay suficiente memoria, las particiones que no caben se almacenan en disco. Las particiones en disco se leen y deserializan bajo demanda. DataFrames grandes con uso frecuente : Cuando se trabaja con un DataFrame que es demasiado grande para caber completamente en memoria, pero a\u00fan se necesita un acceso r\u00e1pido. Por ejemplo, un historial de eventos de usuario que se procesa a diario pero no cabe 100% en RAM. Reducir rec\u00e1lculos en fallos : Si se necesita persistir un DataFrame para su uso posterior y se busca una combinaci\u00f3n de rendimiento y resiliencia b\u00e1sica sin replicaci\u00f3n. MEMORY_ONLY_SER (Serialized) : Similar a MEMORY_ONLY , pero los objetos se almacenan en su forma serializada (bytes). Esto reduce el uso de memoria (hasta un 10x) a expensas de un mayor costo de CPU para deserializar los datos. Optimizaci\u00f3n de memoria en cl\u00fasteres limitados : Cuando la memoria es un recurso escaso y se prefiere sacrificar un poco de tiempo de CPU por un uso de memoria mucho m\u00e1s eficiente. \u00datil para DataFrames muy grandes que a\u00fan se quieren mantener mayormente en memoria. Uso de Kryo Serialization : Combinado con la serializaci\u00f3n Kryo personalizada, puede ser extremadamente eficiente en memoria y a\u00fan ofrecer buen rendimiento de acceso. MEMORY_AND_DISK_SER : Igual que MEMORY_ONLY_SER , pero las particiones que no caben en memoria se almacenan en disco. Grandes conjuntos de datos con memoria limitada : La opci\u00f3n m\u00e1s robusta para conjuntos de datos que exceden la memoria pero necesitan persistencia sin replicaci\u00f3n. Ofrece un buen equilibrio entre uso de memoria, rendimiento y fiabilidad. DISK_ONLY : Almacena las particiones solo en disco. Es el m\u00e1s lento de los niveles de persistencia ya que implica operaciones de E/S de disco. Debugging y auditor\u00eda : Cuando se quiere guardar el estado intermedio de un DataFrame para inspecci\u00f3n o para reiniciar un proceso sin tener que recalcular todo desde el principio, pero no se necesita el rendimiento de la memoria. Tolerancia a fallos en el estado intermedio : En casos donde la memoria es extremadamente limitada y se necesita garantizar que los resultados intermedios no se pierdan en caso de fallo de un ejecutor, aunque el acceso sea m\u00e1s lento. Diferencia entre persist() y cache() La funci\u00f3n cache() es simplemente un alias para persist() con el nivel de almacenamiento por defecto MEMORY_ONLY . Esto significa que df.cache() es equivalente a df.persist(StorageLevel.MEMORY_ONLY) . Generalmente, cache() se usa para la persistencia m\u00e1s com\u00fan y r\u00e1pida (en memoria), mientras que persist() se utiliza cuando se necesita un control m\u00e1s granular sobre c\u00f3mo se almacenan los datos. Es importante recordar que la persistencia es \"lazy\", es decir, los datos no se almacenan hasta que se ejecuta una acci\u00f3n sobre el DataFrame persistido por primera vez. Para despersistir un DataFrame, se utiliza unpersist() . 2.4.2 Optimizaci\u00f3n con Catalyst y Tungsten Apache Spark se basa en dos motores de optimizaci\u00f3n clave: Catalyst Optimizer y Project Tungsten . Juntos, estos componentes son responsables de la eficiencia y el alto rendimiento que Spark logra en el procesamiento de datos a gran escala, transformando las operaciones de DataFrames y SQL en planes de ejecuci\u00f3n optimizados y utilizando la memoria y la CPU de manera extremadamente eficiente. Catalyst Optimizer: El Cerebro de la Planificaci\u00f3n Catalyst Optimizer es el motor de optimizaci\u00f3n de consultas de Spark SQL (y DataFrames). Funciona en varias fases para traducir las transformaciones de alto nivel que el usuario escribe en un plan de ejecuci\u00f3n de bajo nivel y altamente optimizado. Su dise\u00f1o modular y extensible permite incorporar nuevas t\u00e9cnicas de optimizaci\u00f3n y fuentes de datos. Fase 1: An\u00e1lisis (Analysis) : Spark SQL analiza la consulta (DataFrame API o SQL) para resolver referencias, verificar la sintaxis y el esquema. Convierte el \u00e1rbol l\u00f3gico no resuelto (unresolved logical plan) en un \u00e1rbol l\u00f3gico resuelto (resolved logical plan). Es decir, mapea los nombres de columnas y tablas a sus respectivas fuentes de datos. Identificaci\u00f3n de errores de esquema : Si una columna referenciada no existe en el esquema de un DataFrame, Catalyst lo detectar\u00e1 en esta fase y lanzar\u00e1 una excepci\u00f3n. Resoluci\u00f3n de ambig\u00fcedades : Si una columna existe en m\u00faltiples tablas en un join, Catalyst requiere que se califique con el nombre de la tabla para resolver la ambig\u00fcedad. Fase 2: Optimizaci\u00f3n L\u00f3gica (Logical Optimization) : En esta fase, Catalyst aplica un conjunto de reglas de optimizaci\u00f3n sobre el plan l\u00f3gico resuelto para reducir la cantidad de datos a procesar o el n\u00famero de operaciones. Estas optimizaciones son independientes del tipo de motor de ejecuci\u00f3n. Predicado Pushdown (Predicate Pushdown) : Si se aplica un filtro ( .where() ) a un DataFrame que se lee de una fuente de datos (como Parquet), Catalyst empujar\u00e1 este filtro a la fuente de datos. Esto significa que la fuente de datos leer\u00e1 solo los registros que cumplan con la condici\u00f3n, reduciendo la cantidad de datos que se transfieren a Spark. Column Pruning : Si solo se seleccionan algunas columnas ( .select() ) de un DataFrame, Catalyst se asegura de que solo esas columnas se lean del origen de datos, en lugar de todo el conjunto de columnas. Combinaci\u00f3n de filtros : Si se tienen m\u00faltiples condiciones filter() o where() , Catalyst puede combinarlas en una sola expresi\u00f3n para una evaluaci\u00f3n m\u00e1s eficiente. Fase 3: Planificaci\u00f3n F\u00edsica (Physical Planning) : El plan l\u00f3gico optimizado se convierte en uno o m\u00e1s planes f\u00edsicos. Aqu\u00ed, Catalyst considera el entorno de ejecuci\u00f3n (tama\u00f1o del cl\u00faster, datos en cach\u00e9, etc.) y elige la mejor estrategia de ejecuci\u00f3n para cada operaci\u00f3n, generando c\u00f3digo ejecutable para el motor Tungsten. Elecci\u00f3n de estrategia de Join : Catalyst decide si usar un Broadcast Join , Shuffle Hash Join , Sort Merge Join , etc., bas\u00e1ndose en el tama\u00f1o de las tablas y la configuraci\u00f3n. Manejo de agregaciones : Decide si realizar agregaciones parciales (partial aggregations) en cada partici\u00f3n antes de combinarlas para reducir el shuffle. Fase 4: Generaci\u00f3n de C\u00f3digo (Code Generation) : La fase final donde se genera c\u00f3digo Java bytecode din\u00e1micamente en tiempo de ejecuci\u00f3n para ejecutar el plan f\u00edsico. Esto evita la sobrecarga de la interpretaci\u00f3n y permite que las operaciones se ejecuten a velocidades cercanas a las de c\u00f3digo nativo. Evaluaci\u00f3n de expresiones : Genera c\u00f3digo altamente optimizado para la evaluaci\u00f3n de expresiones complejas en lugar de usar llamadas a funciones gen\u00e9ricas, lo que reduce la sobrecarga de la JVM. Operaciones vectorizadas : Permite la ejecuci\u00f3n de operaciones por lotes (vectorizadas) en lugar de una fila a la vez, lo que es mucho m\u00e1s eficiente para operaciones como filtros y proyecciones. Tungsten: El Motor de Ejecuci\u00f3n de Bajo Nivel Project Tungsten es una iniciativa de optimizaci\u00f3n de bajo nivel en Spark que se enfoca en mejorar el uso de la memoria y la eficiencia de la CPU. Su objetivo principal es cerrar la brecha de rendimiento entre el c\u00f3digo Java/Scala y el c\u00f3digo nativo, utilizando t\u00e9cnicas como la gesti\u00f3n de memoria off-heap (fuera del heap de la JVM), la serializaci\u00f3n eficiente y la generaci\u00f3n de c\u00f3digo justo a tiempo (JIT). Gesti\u00f3n de Memoria Off-heap : Tungsten permite a Spark almacenar datos directamente en la memoria fuera del heap de la JVM, en formato binario y compactado. Esto reduce la sobrecarga de la recolecci\u00f3n de basura (Garbage Collection) de la JVM, que puede ser un cuello de botella significativo en cargas de trabajo de Big Data. Agregaciones y Joins con mucha memoria : Operaciones como groupBy o join que requieren mantener grandes tablas hash en memoria pueden beneficiarse enormemente al almacenar estas estructuras off-heap, evitando pausas prolongadas de GC. Ordenamiento (Sorting) : La clasificaci\u00f3n de grandes vol\u00famenes de datos puede ser m\u00e1s eficiente al manejar los datos directamente en memoria off-heap, reduciendo la presi\u00f3n sobre el heap de la JVM. Vectorizaci\u00f3n y Generaci\u00f3n de C\u00f3digo : Tungsten trabaja en conjunto con Catalyst para generar c\u00f3digo optimizado que procesa los datos de forma vectorial (por lotes) en lugar de fila por fila. Esto minimiza el costo de las llamadas a funciones y permite una mejor utilizaci\u00f3n del cach\u00e9 de la CPU. Procesamiento de columnas : Al leer datos en formato columnar (como Parquet), Tungsten puede procesar m\u00faltiples valores de una columna a la vez, aplicando operaciones de forma m\u00e1s eficiente. Operaciones de expresi\u00f3n : Para expresiones complejas que involucran m\u00faltiples funciones (ej. col1 + col2 * 5 - length(col3) ), Tungsten genera un \u00fanico bloque de c\u00f3digo que eval\u00faa toda la expresi\u00f3n de una vez. Serializaci\u00f3n Mejorada (Unsafe Row Format) : Tungsten introduce un formato de fila binario llamado \"Unsafe Row\", que es muy compacto y permite un acceso a datos basado en punteros, similar a c\u00f3mo se accede a los datos en C++. Esto elimina la necesidad de serializaci\u00f3n/deserializaci\u00f3n costosa entre pasos. Reducci\u00f3n de I/O en shuffles : Cuando los datos necesitan ser enviados a trav\u00e9s de la red durante un shuffle, el formato Unsafe Row minimiza el volumen de datos a transferir, reduciendo el cuello de botella de la red. ** Cach\u00e9 de datos eficiente *: Al almacenar datos en cach\u00e9, el formato Unsafe Row permite que los datos se almacenen de manera m\u00e1s compacta y se accedan directamente sin deserializaci\u00f3n completa, mejorando el rendimiento de las lecturas. 2.4.3 Broadcast joins y estrategias para evitar shuffles El \"shuffle\" es una operaci\u00f3n costosa en Spark que implica la reorganizaci\u00f3n de datos a trav\u00e9s de la red entre los ejecutores. Ocurre en operaciones como groupBy , join , orderBy , y repartition . Minimizar los shuffles es una de las estrategias m\u00e1s importantes para optimizar el rendimiento en Spark. Una t\u00e9cnica clave para evitar shuffles en joins es el uso de Broadcast Joins . Broadcast Join Un Broadcast Join es una estrategia de join en Spark donde una de las tablas (la m\u00e1s peque\u00f1a) se \"broadcast\" (transmite) a todos los nodos del cl\u00faster que participan en la operaci\u00f3n de join. Esto significa que cada ejecutor obtiene una copia completa de la tabla peque\u00f1a en su memoria local. Al tener la tabla peque\u00f1a localmente, cada ejecutor puede realizar el join con las particiones de la tabla grande sin necesidad de un shuffle, ya que no necesita intercambiar datos con otros ejecutores para encontrar las claves coincidentes. Spark detecta autom\u00e1ticamente si una tabla es lo suficientemente peque\u00f1a (o si se le indica expl\u00edcitamente con broadcast() ) para ser transmitida. La tabla peque\u00f1a se colecta al driver, se serializa y luego se env\u00eda a cada ejecutor. Los ejecutores pueden entonces realizar un Hash Join con las particiones de la tabla grande. Uni\u00f3n de una tabla de dimensiones peque\u00f1a con una tabla de hechos grande : Por ejemplo, unir una tabla de clientes (miles o cientos de miles de registros) con una tabla de transacciones (miles de millones de registros). Si la tabla de clientes es menor que el umbral de broadcast (por defecto 10 MB en Spark 3.x, configurable con spark.sql.autoBroadcastJoinThreshold ), Spark autom\u00e1ticamente realizar\u00e1 un Broadcast Join. Filtros complejos con Lookups : Cuando se tiene un conjunto de IDs de referencia (ej. una lista de c\u00f3digos de productos a excluir) que es peque\u00f1o y se necesita filtrar o enriquecer un DataFrame muy grande. Se puede crear un peque\u00f1o DataFrame con estos IDs y luego hacer un Broadcast Join. Estrategias para Evitar o Minimizar Shuffles M\u00e1s all\u00e1 de los Broadcast Joins, existen otras estrategias para reducir la necesidad de shuffles o mitigar su impacto en el rendimiento. Predicado Pushdown y Column Pruning : Estas optimizaciones (explicadas en la secci\u00f3n de Catalyst) reducen la cantidad de datos que se leen del origen y se procesan, lo que indirectamente reduce la cantidad de datos que potencialmente necesitar\u00edan ser shufflados. Al filtrar o seleccionar columnas tempranamente, se trabaja con un conjunto de datos m\u00e1s peque\u00f1o desde el principio. Filtrado por fecha antes del join : Si se va a unir una tabla de transacciones de varios a\u00f1os con una tabla de productos, y solo se necesitan transacciones del \u00faltimo mes, aplicar un filter(\"fecha >= '2025-01-01'\") antes del join reducir\u00e1 significativamente el volumen de datos que participan en el join y, por lo tanto, en cualquier shuffle subsiguiente. Seleccionar solo columnas necesarias : Si un DataFrame tiene 50 columnas pero solo se necesitan 5 para un an\u00e1lisis, realizar un .select('col1', 'col2', ...) al inicio reduce la cantidad de datos en memoria y en disco si hay shuffles. Co-ubicaci\u00f3n de Datos (Co-location) : Si los datos que se van a unir o agrupar est\u00e1n particionados de manera compatible en el almacenamiento subyacente (por ejemplo, en Hive o Parquet, utilizando la misma clave de partici\u00f3n que se usar\u00e1 para el join/group by), Spark puede aprovechar esto para realizar un Sort-Merge Join o Hash Join con menos o ning\u00fan shuffle. Esto requiere que las tablas se hayan escrito previamente con la misma estrategia de partici\u00f3n. Joins entre tablas particionadas por la misma clave : Si la tabla de pedidos y la tabla de \u00edtems_pedido est\u00e1n ambas particionadas por id_pedido , un join entre ellas por id_pedido ser\u00e1 mucho m\u00e1s eficiente ya que Spark puede simplemente unir las particiones coincidentes localmente en cada nodo. Agregaciones en datos pre-particionados : Si se agrupan datos por una columna que ya es la clave de partici\u00f3n de la tabla, Spark puede realizar agregaciones locales en cada partici\u00f3n antes de combinar resultados, reduciendo la cantidad de datos shufflados. Acumuladores y Broadcast Variables (para datos peque\u00f1os) : Aunque no evitan directamente un shuffle en DataFrames en la misma medida que un Broadcast Join, los acumuladores y las broadcast variables (a nivel de RDD, pero tambi\u00e9n \u00fatiles para datos peque\u00f1os en Spark) son herramientas para compartir datos peque\u00f1os de manera eficiente entre tareas. Las broadcast variables permiten enviar un valor de solo lectura a todos los nodos, \u00fatil para tablas de b\u00fasqueda o configuraciones. Listas de bloqueo o mapeos : Transmitir una lista peque\u00f1a de IDs prohibidos o un mapa de c\u00f3digos a descripciones a todos los ejecutores para filtrar o enriquecer datos sin realizar un join formal. Par\u00e1metros de configuraci\u00f3n din\u00e1micos : Si un algoritmo necesita un conjunto de par\u00e1metros que cambian din\u00e1micamente pero es peque\u00f1o, se puede transmitir usando una broadcast variable. Uso de repartition y coalesce con cuidado : Ambas funciones se utilizan para cambiar el n\u00famero de particiones de un DataFrame. repartition siempre implica un shuffle completo de los datos, mientras que coalesce intenta reducir el n\u00famero de particiones sin un shuffle completo si es posible (solo combina particiones existentes dentro del mismo nodo). Util\u00edzalas solo cuando sea estrictamente necesario (ej. para uniones eficientes con datos de gran tama\u00f1o, o para reducir el n\u00famero de archivos de salida). Reducir archivos de salida (small files problem) : Si un procesamiento genera miles de archivos peque\u00f1os (problema de \"small files\"), un df.coalesce(N).write.parquet(...) puede combinarlos en N archivos m\u00e1s grandes al final de la operaci\u00f3n, aunque coalesce tambi\u00e9n puede implicar un shuffle si se reduce el n\u00famero de particiones dr\u00e1sticamente. Preparaci\u00f3n para operaciones posteriores : En algunos escenarios, re-particionar un DataFrame por la clave de join antes del join puede ser beneficioso si se planean m\u00faltiples joins o agregaciones sobre la misma clave, aunque es una decisi\u00f3n que debe tomarse con base en un an\u00e1lisis de rendimiento. 2.4.4 Consideraciones de escalabilidad y desempe\u00f1o La escalabilidad y el desempe\u00f1o en Spark van m\u00e1s all\u00e1 de la optimizaci\u00f3n de c\u00f3digo individual; involucran la configuraci\u00f3n del cl\u00faster, la gesti\u00f3n de recursos y la elecci\u00f3n de las arquitecturas de datos adecuadas. Comprender estos aspectos es fundamental para dise\u00f1ar aplicaciones Spark robustas que puedan manejar vol\u00famenes de datos crecientes y mantener un rendimiento \u00f3ptimo en entornos de producci\u00f3n. Configuraci\u00f3n del Cl\u00faster y Asignaci\u00f3n de Recursos Una configuraci\u00f3n adecuada de Spark y la asignaci\u00f3n de recursos a los ejecutores son cr\u00edticas para el desempe\u00f1o. Un cl\u00faster mal configurado puede llevar a cuellos de botella incluso con el c\u00f3digo m\u00e1s optimizado. Tama\u00f1o de los Ejecutores ( spark.executor.cores , spark.executor.memory ) : Estos par\u00e1metros controlan cu\u00e1ntos n\u00facleos de CPU y cu\u00e1nta memoria se asignan a cada ejecutor. Un n\u00famero adecuado de n\u00facleos permite el paralelismo, mientras que suficiente memoria evita derrames a disco y optimiza el cach\u00e9. Un error com\u00fan es tener ejecutores muy grandes (pocos ejecutores con muchos n\u00facleos/memoria) o muy peque\u00f1os (muchos ejecutores con pocos recursos). Cl\u00faster con nodos de 64GB RAM, 16 n\u00facleos : Una configuraci\u00f3n com\u00fan podr\u00eda ser spark.executor.cores=5 y spark.executor.memory=20GB . Esto permite tener 2 ejecutores por nodo y deja memoria para el sistema operativo y el driver, maximizando la utilizaci\u00f3n de recursos sin sobrecargar. Tareas con mucha memoria (e.g., joins grandes sin broadcast) : Aumentar spark.executor.memory puede ser necesario para evitar derrames a disco durante operaciones intensivas en memoria. Memoria del Driver ( spark.driver.memory ) : La memoria asignada al nodo driver. El driver coordina las tareas, almacena metadatos y, en algunos casos, colecta resultados (como collect() ). Si se realizan operaciones que recolectan grandes cantidades de datos al driver, o si se manejan muchas broadcast variables, se necesita m\u00e1s memoria para el driver. Usar collect() en un DataFrame grande : Si se intenta df.collect() sobre un DataFrame con millones de filas, el driver podr\u00eda quedarse sin memoria. Ajustar spark.driver.memory o reestructurar el c\u00f3digo para evitar collect() en grandes vol\u00famenes. Broadcast de m\u00faltiples tablas peque\u00f1as : Si se transmiten muchas tablas peque\u00f1as, la memoria del driver podr\u00eda verse afectada. Configuraci\u00f3n de Shuffle ( spark.shuffle.service.enabled , spark.shuffle.file.buffer ) : Estos par\u00e1metros afectan c\u00f3mo Spark maneja los datos durante las operaciones de shuffle. Habilitar el servicio de shuffle externo ( spark.shuffle.service.enabled=true ) permite que los datos shufflados persistan incluso si un ejecutor falla, mejorando la fiabilidad. Ajustar el tama\u00f1o del buffer ( spark.shuffle.file.buffer ) puede optimizar las escrituras a disco durante el shuffle. Estabilidad en shuffles grandes : En entornos de producci\u00f3n con shuffles frecuentes y grandes, habilitar el shuffle service es crucial para la estabilidad y resiliencia. Rendimiento de I/O en shuffles : Para tareas con mucha escritura a disco durante shuffles, aumentar el buffer puede reducir la cantidad de peque\u00f1as escrituras y mejorar el rendimiento. Monitoreo y Diagn\u00f3stico Monitorear las aplicaciones Spark es esencial para identificar cuellos de botella y comprender el comportamiento del rendimiento. Spark UI es la herramienta principal para esto. Spark UI (Stages, Tasks, DAG Visualization) : La interfaz de usuario de Spark (accesible generalmente en http://<driver-ip>:4040 ) proporciona una visi\u00f3n detallada de las etapas (Stages), tareas (Tasks), y el Grafo Ac\u00edclico Dirigido (DAG) de la aplicaci\u00f3n. Permite identificar qu\u00e9 etapas son lentas, si hay desequilibrio de datos (skew), o si los ejecutores est\u00e1n infrautilizados/sobrecargados. Identificar Stage lento : Si una etapa particular (e.g., un join o groupBy ) toma mucho tiempo, se puede profundizar en esa etapa para ver si alguna tarea est\u00e1 tardando m\u00e1s de lo normal (indicando skew) o si hay problemas de I/O. An\u00e1lisis de Shuffles : La Spark UI muestra el tama\u00f1o de los datos shufflados y el tiempo que toma. Un shuffle excesivo es una se\u00f1al de que las estrategias de optimizaci\u00f3n (como Broadcast Join) podr\u00edan ser necesarias. Revisar el plan de ejecuci\u00f3n : En la pesta\u00f1a \"SQL\" de la Spark UI, se puede ver el plan de ejecuci\u00f3n generado por Catalyst, lo que ayuda a entender c\u00f3mo Spark est\u00e1 procesando la consulta y si se est\u00e1n aplicando las optimizaciones esperadas (e.g., Predicate Pushdown). M\u00e9tricas de Recursos (CPU, Memoria, Disk I/O) : Adem\u00e1s de Spark UI, es importante monitorear las m\u00e9tricas a nivel de cl\u00faster (CPU, uso de memoria, E/S de disco, red) para cada nodo ejecutor. Esto ayuda a identificar si el problema es de Spark en s\u00ed o si hay una limitaci\u00f3n de recursos a nivel de infraestructura. CPU subutilizada : Si la CPU de los ejecutores est\u00e1 consistentemente baja durante una etapa que se esperaba intensiva en CPU, podr\u00eda indicar un problema de paralelismo o un cuello de botella en I/O. Memoria agotada : Si los ejecutores est\u00e1n reportando errores OOM (Out Of Memory) o si hay mucho spill a disco, es una se\u00f1al de que los DataFrames son demasiado grandes para la memoria disponible o que la configuraci\u00f3n de persistencia es ineficiente. Consideraciones de Dise\u00f1o de Datos La forma en que se almacenan y estructuran los datos tiene un impacto directo en el rendimiento de Spark. Formato de Archivos (Parquet, ORC) : Optar por formatos de archivo columnares como Parquet u ORC es crucial. Estos formatos son auto-descriptivos, comprimidos y permiten optimizaciones como \"Predicate Pushdown\" y \"Column Pruning\" de forma nativa, lo que reduce dr\u00e1sticamente la cantidad de datos le\u00eddos del disco. Reemplazar CSV/JSON por Parquet : Migrar conjuntos de datos de formatos basados en filas (CSV, JSON) a Parquet para obtener mejoras significativas en el rendimiento de lectura y la eficiencia del almacenamiento. Beneficios del Predicate Pushdown : Si se tiene una tabla Parquet de logs y se filtra por timestamp > 'X' , Spark solo leer\u00e1 los bloques de datos relevantes, sin necesidad de escanear todo el archivo. Particionamiento de Datos : Organizar los datos en el sistema de archivos (HDFS, S3) en directorios basados en valores de columna (ej. data/year=2024/month=01/ ). Esto permite a Spark (y otras herramientas de Big Data) \"podar\" particiones (partition pruning), es decir, escanear solo los directorios relevantes para una consulta, evitando la lectura de datos innecesarios. Consulta de datos por fecha : Si los datos de ventas est\u00e1n particionados por a\u00f1o/mes/d\u00eda , una consulta para las ventas de un d\u00eda espec\u00edfico solo escanear\u00e1 el directorio correspondiente a ese d\u00eda, no toda la tabla. Optimizando Joins con partici\u00f3n por clave : Si se va a unir por una clave y ambas tablas est\u00e1n particionadas por esa misma clave, se puede evitar un shuffle completo (co-ubicaci\u00f3n, como se mencion\u00f3 anteriormente). Tama\u00f1o de los Archivos (Small Files Problem) : Tener un gran n\u00famero de archivos peque\u00f1os (ej. miles de archivos de unos pocos KB) en un sistema de archivos distribuido (como HDFS) puede degradar seriamente el rendimiento. Esto se debe a la sobrecarga de gestionar metadatos para cada archivo y la ineficiencia de la lectura de muchos archivos peque\u00f1os. Compactaci\u00f3n de datos hist\u00f3ricos : Si se ingieren datos en peque\u00f1os lotes, peri\u00f3dicamente se debe ejecutar un proceso de \"compactaci\u00f3n\" que combine estos peque\u00f1os archivos en unos pocos archivos m\u00e1s grandes (e.g., 128 MB a 1 GB) para optimizar las lecturas futuras. Ajustar el n\u00famero de particiones de salida : Al escribir resultados, se puede usar repartition() o coalesce() para controlar el n\u00famero de archivos de salida y evitar generar demasiados peque\u00f1os. Tarea Ejercicio de Persistencia con diferentes niveles : Crea un DataFrame con 10 millones de filas y varias columnas. Realiza una serie de transformaciones sobre \u00e9l. Luego, persiste el DataFrame utilizando MEMORY_ONLY , MEMORY_AND_DISK , y DISK_ONLY (en diferentes ejecuciones). Mide el tiempo de ejecuci\u00f3n de las operaciones posteriores a la primera acci\u00f3n para cada nivel de persistencia y explica las diferencias observadas en el rendimiento y el uso de memoria/disco (usando Spark UI). Identificaci\u00f3n de un Shuffle Costoso : Dise\u00f1a un escenario donde se genere un DataFrame grande y se realice una operaci\u00f3n que cause un shuffle significativo (ej. un groupBy por una columna de alta cardinalidad o un join entre dos DataFrames grandes sin una estrategia de optimizaci\u00f3n). Utiliza Spark UI para identificar la etapa del shuffle, el volumen de datos shufflados y el tiempo que consume. Implementaci\u00f3n de Broadcast Join : Toma el escenario del ejercicio anterior (o crea uno similar con un join entre una tabla grande y una peque\u00f1a). Implementa un Broadcast Join utilizando F.broadcast() y compara el tiempo de ejecuci\u00f3n y la ausencia/reducci\u00f3n del shuffle en Spark UI con respecto a un join normal. Predicado Pushdown y Column Pruning en acci\u00f3n : Lee un archivo Parquet con m\u00faltiples columnas y filtra por una columna indexada (si el formato lo permite) y selecciona solo un subconjunto de columnas. Observa el plan de ejecuci\u00f3n en Spark UI (pesta\u00f1a SQL -> Details) y explica c\u00f3mo Catalyst aplica el Predicado Pushdown y el Column Pruning. Simulaci\u00f3n de \"Small Files Problem\" y Soluci\u00f3n : Escribe un DataFrame grande en 1000 archivos peque\u00f1os. Luego, lee esos 1000 archivos y escribe el resultado en 10 archivos m\u00e1s grandes utilizando coalesce() o repartition() . Mide y compara los tiempos de lectura y escritura. Configuraci\u00f3n de Ejecutores : Experimenta con la configuraci\u00f3n de spark.executor.cores y spark.executor.memory en una aplicaci\u00f3n Spark. Ejecuta una carga de trabajo intensiva (ej. un join complejo o una agregaci\u00f3n grande) con diferentes configuraciones (ej. pocos ejecutores grandes vs. muchos ejecutores peque\u00f1os, manteniendo el mismo n\u00famero total de n\u00facleos y memoria para el cl\u00faster). Analiza el impacto en el rendimiento y la utilizaci\u00f3n de recursos en Spark UI. UDFs vs. Funciones Nativas para Optimizaci\u00f3n : Crea una funci\u00f3n para manipular una columna (ej. concatenar strings, transformar un valor num\u00e9rico). Primero, implementa la l\u00f3gica como una UDF. Luego, re-implementa la misma l\u00f3gica utilizando solo funciones nativas de Spark SQL (ej. concat_ws , when , lit ). Compara el rendimiento de ambas implementaciones y explica por qu\u00e9 una es m\u00e1s eficiente. An\u00e1lisis de Derrames a Disco (Spill) : Ejecuta una operaci\u00f3n Spark (ej. un groupBy con una clave de muy alta cardinalidad y un agregado complejo) con memoria de ejecutor limitada, forzando que Spark \"derrame\" datos al disco (spill). Monitorea el evento en Spark UI y explica qu\u00e9 significa el \"spill\" y c\u00f3mo afecta el rendimiento. Optimizaci\u00f3n de Sort : Genera un DataFrame con datos aleatorios y ord\u00e9nalo utilizando orderBy . Luego, compara el plan de ejecuci\u00f3n y el rendimiento si los datos ya estuvieran pre-ordenados o si se aplicara una partici\u00f3n adecuada antes del ordenamiento (esto puede ser un ejercicio conceptual o simulado). Lectura con Particionamiento (Partition Pruning) : Crea un DataFrame grande y escr\u00edbelo en Parquet particionando por una columna (ej. df.write.partitionBy(\"ciudad\").parquet(\"path/to/data\") ). Luego, lee los datos filtrando por esa columna particionada (ej. spark.read.parquet(\"path/to/data\").filter(\"ciudad = 'Bogota'\") ). Observa en Spark UI c\u00f3mo Spark solo escanea los directorios relevantes, demostrando el \"partition pruning\".","title":"Optimizaci\u00f3n y Rendimiento"},{"location":"tema24/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema24/#tema-24-optimizacion-y-rendimiento","text":"Objetivo : Al finalizar este tema, el estudiante ser\u00e1 capaz de identificar y aplicar t\u00e9cnicas avanzadas de optimizaci\u00f3n y gesti\u00f3n del rendimiento en aplicaciones Spark, comprendiendo el funcionamiento interno del motor y las estrategias para el manejo eficiente de datos distribuidos y operaciones complejas, con el fin de mejorar significativamente la eficiencia y escalabilidad de los pipelines de procesamiento de Big Data. Introducci\u00f3n : En el vasto universo del procesamiento de Big Data con Apache Spark, la capacidad de escribir c\u00f3digo funcional es solo la primera parte de la ecuaci\u00f3n. La verdadera maestr\u00eda reside en optimizar ese c\u00f3digo para que se ejecute de manera eficiente, consumiendo menos recursos y completando las tareas en el menor tiempo posible. Este tema se adentra en el coraz\u00f3n del rendimiento de Spark, explorando las herramientas y los principios subyacentes que permiten transformar una aplicaci\u00f3n de datos masivos en una soluci\u00f3n robusta y escalable. Desarrollo : La optimizaci\u00f3n en Spark es un proceso multifac\u00e9tico que abarca desde la gesti\u00f3n inteligente de la memoria y el disco hasta la comprensi\u00f3n profunda de c\u00f3mo Spark planifica y ejecuta las operaciones. Se explorar\u00e1n conceptos fundamentales como la persistencia de DataFrames, la arquitectura de optimizaci\u00f3n de Spark (Catalyst y Tungsten), y estrategias avanzadas para el manejo de joins y la reducci\u00f3n de operaciones costosas como los shuffles. Finalmente, se abordar\u00e1n consideraciones clave para escalar y mantener el desempe\u00f1o en entornos de producci\u00f3n, proporcionando al estudiante las herramientas para diagnosticar cuellos de botella y aplicar soluciones efectivas en sus proyectos de Big Data.","title":"Tema 2.4 Optimizaci\u00f3n y Rendimiento"},{"location":"tema24/#241-persistencia-y-cache-persist-y-cache","text":"La persistencia en Spark es una caracter\u00edstica fundamental para optimizar el rendimiento de las operaciones sobre DataFrames o RDDs que se reutilizan m\u00faltiples veces. Cuando una operaci\u00f3n se ejecuta sobre un DataFrame, Spark recalcula el DataFrame desde el origen cada vez que se invoca una acci\u00f3n. Esto puede ser ineficiente si el mismo DataFrame es utilizado en varias operaciones subsiguientes. Al aplicar persist() o cache() , Spark almacena el DataFrame o RDD resultante de una transformaci\u00f3n en memoria, en disco o una combinaci\u00f3n de ambos, evitando rec\u00e1lculos innecesarios y acelerando las operaciones posteriores.","title":"2.4.1 Persistencia y cach\u00e9 (persist() y cache())"},{"location":"tema24/#niveles-de-almacenamiento-en-persist","text":"Spark ofrece diferentes niveles de almacenamiento para persist() , lo que permite controlar d\u00f3nde y c\u00f3mo se guardan los datos para optimizar el equilibrio entre memoria, disco y replicaci\u00f3n. La elecci\u00f3n del nivel adecuado depende de la cantidad de memoria disponible, la necesidad de tolerancia a fallos y la frecuencia de acceso a los datos. MEMORY_ONLY : Es el nivel por defecto para cache() . Los RDDs/DataFrames se almacenan como objetos deserializados de Python (o Java/Scala si se usa Scala/Java) en la JVM. Si no cabe en memoria, algunas particiones se recalculan cuando son necesarias. Conjuntos de datos peque\u00f1os y medianos : Cuando se tiene un DataFrame que cabe completamente en la memoria de los ejecutores y se va a utilizar repetidamente en m\u00faltiples transformaciones. Por ejemplo, un cat\u00e1logo de productos que se une frecuentemente con transacciones. Operaciones iterativas : En algoritmos de machine learning (como K-Means o PageRank) donde un conjunto de datos se itera sobre muchas veces, manteniendo los datos en memoria reduce dr\u00e1sticamente el tiempo de ejecuci\u00f3n de cada iteraci\u00f3n. MEMORY_AND_DISK : Almacena las particiones en memoria. Si no hay suficiente memoria, las particiones que no caben se almacenan en disco. Las particiones en disco se leen y deserializan bajo demanda. DataFrames grandes con uso frecuente : Cuando se trabaja con un DataFrame que es demasiado grande para caber completamente en memoria, pero a\u00fan se necesita un acceso r\u00e1pido. Por ejemplo, un historial de eventos de usuario que se procesa a diario pero no cabe 100% en RAM. Reducir rec\u00e1lculos en fallos : Si se necesita persistir un DataFrame para su uso posterior y se busca una combinaci\u00f3n de rendimiento y resiliencia b\u00e1sica sin replicaci\u00f3n. MEMORY_ONLY_SER (Serialized) : Similar a MEMORY_ONLY , pero los objetos se almacenan en su forma serializada (bytes). Esto reduce el uso de memoria (hasta un 10x) a expensas de un mayor costo de CPU para deserializar los datos. Optimizaci\u00f3n de memoria en cl\u00fasteres limitados : Cuando la memoria es un recurso escaso y se prefiere sacrificar un poco de tiempo de CPU por un uso de memoria mucho m\u00e1s eficiente. \u00datil para DataFrames muy grandes que a\u00fan se quieren mantener mayormente en memoria. Uso de Kryo Serialization : Combinado con la serializaci\u00f3n Kryo personalizada, puede ser extremadamente eficiente en memoria y a\u00fan ofrecer buen rendimiento de acceso. MEMORY_AND_DISK_SER : Igual que MEMORY_ONLY_SER , pero las particiones que no caben en memoria se almacenan en disco. Grandes conjuntos de datos con memoria limitada : La opci\u00f3n m\u00e1s robusta para conjuntos de datos que exceden la memoria pero necesitan persistencia sin replicaci\u00f3n. Ofrece un buen equilibrio entre uso de memoria, rendimiento y fiabilidad. DISK_ONLY : Almacena las particiones solo en disco. Es el m\u00e1s lento de los niveles de persistencia ya que implica operaciones de E/S de disco. Debugging y auditor\u00eda : Cuando se quiere guardar el estado intermedio de un DataFrame para inspecci\u00f3n o para reiniciar un proceso sin tener que recalcular todo desde el principio, pero no se necesita el rendimiento de la memoria. Tolerancia a fallos en el estado intermedio : En casos donde la memoria es extremadamente limitada y se necesita garantizar que los resultados intermedios no se pierdan en caso de fallo de un ejecutor, aunque el acceso sea m\u00e1s lento.","title":"Niveles de almacenamiento en persist()"},{"location":"tema24/#diferencia-entre-persist-y-cache","text":"La funci\u00f3n cache() es simplemente un alias para persist() con el nivel de almacenamiento por defecto MEMORY_ONLY . Esto significa que df.cache() es equivalente a df.persist(StorageLevel.MEMORY_ONLY) . Generalmente, cache() se usa para la persistencia m\u00e1s com\u00fan y r\u00e1pida (en memoria), mientras que persist() se utiliza cuando se necesita un control m\u00e1s granular sobre c\u00f3mo se almacenan los datos. Es importante recordar que la persistencia es \"lazy\", es decir, los datos no se almacenan hasta que se ejecuta una acci\u00f3n sobre el DataFrame persistido por primera vez. Para despersistir un DataFrame, se utiliza unpersist() .","title":"Diferencia entre persist() y cache()"},{"location":"tema24/#242-optimizacion-con-catalyst-y-tungsten","text":"Apache Spark se basa en dos motores de optimizaci\u00f3n clave: Catalyst Optimizer y Project Tungsten . Juntos, estos componentes son responsables de la eficiencia y el alto rendimiento que Spark logra en el procesamiento de datos a gran escala, transformando las operaciones de DataFrames y SQL en planes de ejecuci\u00f3n optimizados y utilizando la memoria y la CPU de manera extremadamente eficiente.","title":"2.4.2 Optimizaci\u00f3n con Catalyst y Tungsten"},{"location":"tema24/#catalyst-optimizer-el-cerebro-de-la-planificacion","text":"Catalyst Optimizer es el motor de optimizaci\u00f3n de consultas de Spark SQL (y DataFrames). Funciona en varias fases para traducir las transformaciones de alto nivel que el usuario escribe en un plan de ejecuci\u00f3n de bajo nivel y altamente optimizado. Su dise\u00f1o modular y extensible permite incorporar nuevas t\u00e9cnicas de optimizaci\u00f3n y fuentes de datos. Fase 1: An\u00e1lisis (Analysis) : Spark SQL analiza la consulta (DataFrame API o SQL) para resolver referencias, verificar la sintaxis y el esquema. Convierte el \u00e1rbol l\u00f3gico no resuelto (unresolved logical plan) en un \u00e1rbol l\u00f3gico resuelto (resolved logical plan). Es decir, mapea los nombres de columnas y tablas a sus respectivas fuentes de datos. Identificaci\u00f3n de errores de esquema : Si una columna referenciada no existe en el esquema de un DataFrame, Catalyst lo detectar\u00e1 en esta fase y lanzar\u00e1 una excepci\u00f3n. Resoluci\u00f3n de ambig\u00fcedades : Si una columna existe en m\u00faltiples tablas en un join, Catalyst requiere que se califique con el nombre de la tabla para resolver la ambig\u00fcedad. Fase 2: Optimizaci\u00f3n L\u00f3gica (Logical Optimization) : En esta fase, Catalyst aplica un conjunto de reglas de optimizaci\u00f3n sobre el plan l\u00f3gico resuelto para reducir la cantidad de datos a procesar o el n\u00famero de operaciones. Estas optimizaciones son independientes del tipo de motor de ejecuci\u00f3n. Predicado Pushdown (Predicate Pushdown) : Si se aplica un filtro ( .where() ) a un DataFrame que se lee de una fuente de datos (como Parquet), Catalyst empujar\u00e1 este filtro a la fuente de datos. Esto significa que la fuente de datos leer\u00e1 solo los registros que cumplan con la condici\u00f3n, reduciendo la cantidad de datos que se transfieren a Spark. Column Pruning : Si solo se seleccionan algunas columnas ( .select() ) de un DataFrame, Catalyst se asegura de que solo esas columnas se lean del origen de datos, en lugar de todo el conjunto de columnas. Combinaci\u00f3n de filtros : Si se tienen m\u00faltiples condiciones filter() o where() , Catalyst puede combinarlas en una sola expresi\u00f3n para una evaluaci\u00f3n m\u00e1s eficiente. Fase 3: Planificaci\u00f3n F\u00edsica (Physical Planning) : El plan l\u00f3gico optimizado se convierte en uno o m\u00e1s planes f\u00edsicos. Aqu\u00ed, Catalyst considera el entorno de ejecuci\u00f3n (tama\u00f1o del cl\u00faster, datos en cach\u00e9, etc.) y elige la mejor estrategia de ejecuci\u00f3n para cada operaci\u00f3n, generando c\u00f3digo ejecutable para el motor Tungsten. Elecci\u00f3n de estrategia de Join : Catalyst decide si usar un Broadcast Join , Shuffle Hash Join , Sort Merge Join , etc., bas\u00e1ndose en el tama\u00f1o de las tablas y la configuraci\u00f3n. Manejo de agregaciones : Decide si realizar agregaciones parciales (partial aggregations) en cada partici\u00f3n antes de combinarlas para reducir el shuffle. Fase 4: Generaci\u00f3n de C\u00f3digo (Code Generation) : La fase final donde se genera c\u00f3digo Java bytecode din\u00e1micamente en tiempo de ejecuci\u00f3n para ejecutar el plan f\u00edsico. Esto evita la sobrecarga de la interpretaci\u00f3n y permite que las operaciones se ejecuten a velocidades cercanas a las de c\u00f3digo nativo. Evaluaci\u00f3n de expresiones : Genera c\u00f3digo altamente optimizado para la evaluaci\u00f3n de expresiones complejas en lugar de usar llamadas a funciones gen\u00e9ricas, lo que reduce la sobrecarga de la JVM. Operaciones vectorizadas : Permite la ejecuci\u00f3n de operaciones por lotes (vectorizadas) en lugar de una fila a la vez, lo que es mucho m\u00e1s eficiente para operaciones como filtros y proyecciones.","title":"Catalyst Optimizer: El Cerebro de la Planificaci\u00f3n"},{"location":"tema24/#tungsten-el-motor-de-ejecucion-de-bajo-nivel","text":"Project Tungsten es una iniciativa de optimizaci\u00f3n de bajo nivel en Spark que se enfoca en mejorar el uso de la memoria y la eficiencia de la CPU. Su objetivo principal es cerrar la brecha de rendimiento entre el c\u00f3digo Java/Scala y el c\u00f3digo nativo, utilizando t\u00e9cnicas como la gesti\u00f3n de memoria off-heap (fuera del heap de la JVM), la serializaci\u00f3n eficiente y la generaci\u00f3n de c\u00f3digo justo a tiempo (JIT). Gesti\u00f3n de Memoria Off-heap : Tungsten permite a Spark almacenar datos directamente en la memoria fuera del heap de la JVM, en formato binario y compactado. Esto reduce la sobrecarga de la recolecci\u00f3n de basura (Garbage Collection) de la JVM, que puede ser un cuello de botella significativo en cargas de trabajo de Big Data. Agregaciones y Joins con mucha memoria : Operaciones como groupBy o join que requieren mantener grandes tablas hash en memoria pueden beneficiarse enormemente al almacenar estas estructuras off-heap, evitando pausas prolongadas de GC. Ordenamiento (Sorting) : La clasificaci\u00f3n de grandes vol\u00famenes de datos puede ser m\u00e1s eficiente al manejar los datos directamente en memoria off-heap, reduciendo la presi\u00f3n sobre el heap de la JVM. Vectorizaci\u00f3n y Generaci\u00f3n de C\u00f3digo : Tungsten trabaja en conjunto con Catalyst para generar c\u00f3digo optimizado que procesa los datos de forma vectorial (por lotes) en lugar de fila por fila. Esto minimiza el costo de las llamadas a funciones y permite una mejor utilizaci\u00f3n del cach\u00e9 de la CPU. Procesamiento de columnas : Al leer datos en formato columnar (como Parquet), Tungsten puede procesar m\u00faltiples valores de una columna a la vez, aplicando operaciones de forma m\u00e1s eficiente. Operaciones de expresi\u00f3n : Para expresiones complejas que involucran m\u00faltiples funciones (ej. col1 + col2 * 5 - length(col3) ), Tungsten genera un \u00fanico bloque de c\u00f3digo que eval\u00faa toda la expresi\u00f3n de una vez. Serializaci\u00f3n Mejorada (Unsafe Row Format) : Tungsten introduce un formato de fila binario llamado \"Unsafe Row\", que es muy compacto y permite un acceso a datos basado en punteros, similar a c\u00f3mo se accede a los datos en C++. Esto elimina la necesidad de serializaci\u00f3n/deserializaci\u00f3n costosa entre pasos. Reducci\u00f3n de I/O en shuffles : Cuando los datos necesitan ser enviados a trav\u00e9s de la red durante un shuffle, el formato Unsafe Row minimiza el volumen de datos a transferir, reduciendo el cuello de botella de la red. ** Cach\u00e9 de datos eficiente *: Al almacenar datos en cach\u00e9, el formato Unsafe Row permite que los datos se almacenen de manera m\u00e1s compacta y se accedan directamente sin deserializaci\u00f3n completa, mejorando el rendimiento de las lecturas.","title":"Tungsten: El Motor de Ejecuci\u00f3n de Bajo Nivel"},{"location":"tema24/#243-broadcast-joins-y-estrategias-para-evitar-shuffles","text":"El \"shuffle\" es una operaci\u00f3n costosa en Spark que implica la reorganizaci\u00f3n de datos a trav\u00e9s de la red entre los ejecutores. Ocurre en operaciones como groupBy , join , orderBy , y repartition . Minimizar los shuffles es una de las estrategias m\u00e1s importantes para optimizar el rendimiento en Spark. Una t\u00e9cnica clave para evitar shuffles en joins es el uso de Broadcast Joins .","title":"2.4.3 Broadcast joins y estrategias para evitar shuffles"},{"location":"tema24/#broadcast-join","text":"Un Broadcast Join es una estrategia de join en Spark donde una de las tablas (la m\u00e1s peque\u00f1a) se \"broadcast\" (transmite) a todos los nodos del cl\u00faster que participan en la operaci\u00f3n de join. Esto significa que cada ejecutor obtiene una copia completa de la tabla peque\u00f1a en su memoria local. Al tener la tabla peque\u00f1a localmente, cada ejecutor puede realizar el join con las particiones de la tabla grande sin necesidad de un shuffle, ya que no necesita intercambiar datos con otros ejecutores para encontrar las claves coincidentes. Spark detecta autom\u00e1ticamente si una tabla es lo suficientemente peque\u00f1a (o si se le indica expl\u00edcitamente con broadcast() ) para ser transmitida. La tabla peque\u00f1a se colecta al driver, se serializa y luego se env\u00eda a cada ejecutor. Los ejecutores pueden entonces realizar un Hash Join con las particiones de la tabla grande. Uni\u00f3n de una tabla de dimensiones peque\u00f1a con una tabla de hechos grande : Por ejemplo, unir una tabla de clientes (miles o cientos de miles de registros) con una tabla de transacciones (miles de millones de registros). Si la tabla de clientes es menor que el umbral de broadcast (por defecto 10 MB en Spark 3.x, configurable con spark.sql.autoBroadcastJoinThreshold ), Spark autom\u00e1ticamente realizar\u00e1 un Broadcast Join. Filtros complejos con Lookups : Cuando se tiene un conjunto de IDs de referencia (ej. una lista de c\u00f3digos de productos a excluir) que es peque\u00f1o y se necesita filtrar o enriquecer un DataFrame muy grande. Se puede crear un peque\u00f1o DataFrame con estos IDs y luego hacer un Broadcast Join.","title":"Broadcast Join"},{"location":"tema24/#estrategias-para-evitar-o-minimizar-shuffles","text":"M\u00e1s all\u00e1 de los Broadcast Joins, existen otras estrategias para reducir la necesidad de shuffles o mitigar su impacto en el rendimiento. Predicado Pushdown y Column Pruning : Estas optimizaciones (explicadas en la secci\u00f3n de Catalyst) reducen la cantidad de datos que se leen del origen y se procesan, lo que indirectamente reduce la cantidad de datos que potencialmente necesitar\u00edan ser shufflados. Al filtrar o seleccionar columnas tempranamente, se trabaja con un conjunto de datos m\u00e1s peque\u00f1o desde el principio. Filtrado por fecha antes del join : Si se va a unir una tabla de transacciones de varios a\u00f1os con una tabla de productos, y solo se necesitan transacciones del \u00faltimo mes, aplicar un filter(\"fecha >= '2025-01-01'\") antes del join reducir\u00e1 significativamente el volumen de datos que participan en el join y, por lo tanto, en cualquier shuffle subsiguiente. Seleccionar solo columnas necesarias : Si un DataFrame tiene 50 columnas pero solo se necesitan 5 para un an\u00e1lisis, realizar un .select('col1', 'col2', ...) al inicio reduce la cantidad de datos en memoria y en disco si hay shuffles. Co-ubicaci\u00f3n de Datos (Co-location) : Si los datos que se van a unir o agrupar est\u00e1n particionados de manera compatible en el almacenamiento subyacente (por ejemplo, en Hive o Parquet, utilizando la misma clave de partici\u00f3n que se usar\u00e1 para el join/group by), Spark puede aprovechar esto para realizar un Sort-Merge Join o Hash Join con menos o ning\u00fan shuffle. Esto requiere que las tablas se hayan escrito previamente con la misma estrategia de partici\u00f3n. Joins entre tablas particionadas por la misma clave : Si la tabla de pedidos y la tabla de \u00edtems_pedido est\u00e1n ambas particionadas por id_pedido , un join entre ellas por id_pedido ser\u00e1 mucho m\u00e1s eficiente ya que Spark puede simplemente unir las particiones coincidentes localmente en cada nodo. Agregaciones en datos pre-particionados : Si se agrupan datos por una columna que ya es la clave de partici\u00f3n de la tabla, Spark puede realizar agregaciones locales en cada partici\u00f3n antes de combinar resultados, reduciendo la cantidad de datos shufflados. Acumuladores y Broadcast Variables (para datos peque\u00f1os) : Aunque no evitan directamente un shuffle en DataFrames en la misma medida que un Broadcast Join, los acumuladores y las broadcast variables (a nivel de RDD, pero tambi\u00e9n \u00fatiles para datos peque\u00f1os en Spark) son herramientas para compartir datos peque\u00f1os de manera eficiente entre tareas. Las broadcast variables permiten enviar un valor de solo lectura a todos los nodos, \u00fatil para tablas de b\u00fasqueda o configuraciones. Listas de bloqueo o mapeos : Transmitir una lista peque\u00f1a de IDs prohibidos o un mapa de c\u00f3digos a descripciones a todos los ejecutores para filtrar o enriquecer datos sin realizar un join formal. Par\u00e1metros de configuraci\u00f3n din\u00e1micos : Si un algoritmo necesita un conjunto de par\u00e1metros que cambian din\u00e1micamente pero es peque\u00f1o, se puede transmitir usando una broadcast variable. Uso de repartition y coalesce con cuidado : Ambas funciones se utilizan para cambiar el n\u00famero de particiones de un DataFrame. repartition siempre implica un shuffle completo de los datos, mientras que coalesce intenta reducir el n\u00famero de particiones sin un shuffle completo si es posible (solo combina particiones existentes dentro del mismo nodo). Util\u00edzalas solo cuando sea estrictamente necesario (ej. para uniones eficientes con datos de gran tama\u00f1o, o para reducir el n\u00famero de archivos de salida). Reducir archivos de salida (small files problem) : Si un procesamiento genera miles de archivos peque\u00f1os (problema de \"small files\"), un df.coalesce(N).write.parquet(...) puede combinarlos en N archivos m\u00e1s grandes al final de la operaci\u00f3n, aunque coalesce tambi\u00e9n puede implicar un shuffle si se reduce el n\u00famero de particiones dr\u00e1sticamente. Preparaci\u00f3n para operaciones posteriores : En algunos escenarios, re-particionar un DataFrame por la clave de join antes del join puede ser beneficioso si se planean m\u00faltiples joins o agregaciones sobre la misma clave, aunque es una decisi\u00f3n que debe tomarse con base en un an\u00e1lisis de rendimiento.","title":"Estrategias para Evitar o Minimizar Shuffles"},{"location":"tema24/#244-consideraciones-de-escalabilidad-y-desempeno","text":"La escalabilidad y el desempe\u00f1o en Spark van m\u00e1s all\u00e1 de la optimizaci\u00f3n de c\u00f3digo individual; involucran la configuraci\u00f3n del cl\u00faster, la gesti\u00f3n de recursos y la elecci\u00f3n de las arquitecturas de datos adecuadas. Comprender estos aspectos es fundamental para dise\u00f1ar aplicaciones Spark robustas que puedan manejar vol\u00famenes de datos crecientes y mantener un rendimiento \u00f3ptimo en entornos de producci\u00f3n.","title":"2.4.4 Consideraciones de escalabilidad y desempe\u00f1o"},{"location":"tema24/#configuracion-del-cluster-y-asignacion-de-recursos","text":"Una configuraci\u00f3n adecuada de Spark y la asignaci\u00f3n de recursos a los ejecutores son cr\u00edticas para el desempe\u00f1o. Un cl\u00faster mal configurado puede llevar a cuellos de botella incluso con el c\u00f3digo m\u00e1s optimizado. Tama\u00f1o de los Ejecutores ( spark.executor.cores , spark.executor.memory ) : Estos par\u00e1metros controlan cu\u00e1ntos n\u00facleos de CPU y cu\u00e1nta memoria se asignan a cada ejecutor. Un n\u00famero adecuado de n\u00facleos permite el paralelismo, mientras que suficiente memoria evita derrames a disco y optimiza el cach\u00e9. Un error com\u00fan es tener ejecutores muy grandes (pocos ejecutores con muchos n\u00facleos/memoria) o muy peque\u00f1os (muchos ejecutores con pocos recursos). Cl\u00faster con nodos de 64GB RAM, 16 n\u00facleos : Una configuraci\u00f3n com\u00fan podr\u00eda ser spark.executor.cores=5 y spark.executor.memory=20GB . Esto permite tener 2 ejecutores por nodo y deja memoria para el sistema operativo y el driver, maximizando la utilizaci\u00f3n de recursos sin sobrecargar. Tareas con mucha memoria (e.g., joins grandes sin broadcast) : Aumentar spark.executor.memory puede ser necesario para evitar derrames a disco durante operaciones intensivas en memoria. Memoria del Driver ( spark.driver.memory ) : La memoria asignada al nodo driver. El driver coordina las tareas, almacena metadatos y, en algunos casos, colecta resultados (como collect() ). Si se realizan operaciones que recolectan grandes cantidades de datos al driver, o si se manejan muchas broadcast variables, se necesita m\u00e1s memoria para el driver. Usar collect() en un DataFrame grande : Si se intenta df.collect() sobre un DataFrame con millones de filas, el driver podr\u00eda quedarse sin memoria. Ajustar spark.driver.memory o reestructurar el c\u00f3digo para evitar collect() en grandes vol\u00famenes. Broadcast de m\u00faltiples tablas peque\u00f1as : Si se transmiten muchas tablas peque\u00f1as, la memoria del driver podr\u00eda verse afectada. Configuraci\u00f3n de Shuffle ( spark.shuffle.service.enabled , spark.shuffle.file.buffer ) : Estos par\u00e1metros afectan c\u00f3mo Spark maneja los datos durante las operaciones de shuffle. Habilitar el servicio de shuffle externo ( spark.shuffle.service.enabled=true ) permite que los datos shufflados persistan incluso si un ejecutor falla, mejorando la fiabilidad. Ajustar el tama\u00f1o del buffer ( spark.shuffle.file.buffer ) puede optimizar las escrituras a disco durante el shuffle. Estabilidad en shuffles grandes : En entornos de producci\u00f3n con shuffles frecuentes y grandes, habilitar el shuffle service es crucial para la estabilidad y resiliencia. Rendimiento de I/O en shuffles : Para tareas con mucha escritura a disco durante shuffles, aumentar el buffer puede reducir la cantidad de peque\u00f1as escrituras y mejorar el rendimiento.","title":"Configuraci\u00f3n del Cl\u00faster y Asignaci\u00f3n de Recursos"},{"location":"tema24/#monitoreo-y-diagnostico","text":"Monitorear las aplicaciones Spark es esencial para identificar cuellos de botella y comprender el comportamiento del rendimiento. Spark UI es la herramienta principal para esto. Spark UI (Stages, Tasks, DAG Visualization) : La interfaz de usuario de Spark (accesible generalmente en http://<driver-ip>:4040 ) proporciona una visi\u00f3n detallada de las etapas (Stages), tareas (Tasks), y el Grafo Ac\u00edclico Dirigido (DAG) de la aplicaci\u00f3n. Permite identificar qu\u00e9 etapas son lentas, si hay desequilibrio de datos (skew), o si los ejecutores est\u00e1n infrautilizados/sobrecargados. Identificar Stage lento : Si una etapa particular (e.g., un join o groupBy ) toma mucho tiempo, se puede profundizar en esa etapa para ver si alguna tarea est\u00e1 tardando m\u00e1s de lo normal (indicando skew) o si hay problemas de I/O. An\u00e1lisis de Shuffles : La Spark UI muestra el tama\u00f1o de los datos shufflados y el tiempo que toma. Un shuffle excesivo es una se\u00f1al de que las estrategias de optimizaci\u00f3n (como Broadcast Join) podr\u00edan ser necesarias. Revisar el plan de ejecuci\u00f3n : En la pesta\u00f1a \"SQL\" de la Spark UI, se puede ver el plan de ejecuci\u00f3n generado por Catalyst, lo que ayuda a entender c\u00f3mo Spark est\u00e1 procesando la consulta y si se est\u00e1n aplicando las optimizaciones esperadas (e.g., Predicate Pushdown). M\u00e9tricas de Recursos (CPU, Memoria, Disk I/O) : Adem\u00e1s de Spark UI, es importante monitorear las m\u00e9tricas a nivel de cl\u00faster (CPU, uso de memoria, E/S de disco, red) para cada nodo ejecutor. Esto ayuda a identificar si el problema es de Spark en s\u00ed o si hay una limitaci\u00f3n de recursos a nivel de infraestructura. CPU subutilizada : Si la CPU de los ejecutores est\u00e1 consistentemente baja durante una etapa que se esperaba intensiva en CPU, podr\u00eda indicar un problema de paralelismo o un cuello de botella en I/O. Memoria agotada : Si los ejecutores est\u00e1n reportando errores OOM (Out Of Memory) o si hay mucho spill a disco, es una se\u00f1al de que los DataFrames son demasiado grandes para la memoria disponible o que la configuraci\u00f3n de persistencia es ineficiente.","title":"Monitoreo y Diagn\u00f3stico"},{"location":"tema24/#consideraciones-de-diseno-de-datos","text":"La forma en que se almacenan y estructuran los datos tiene un impacto directo en el rendimiento de Spark. Formato de Archivos (Parquet, ORC) : Optar por formatos de archivo columnares como Parquet u ORC es crucial. Estos formatos son auto-descriptivos, comprimidos y permiten optimizaciones como \"Predicate Pushdown\" y \"Column Pruning\" de forma nativa, lo que reduce dr\u00e1sticamente la cantidad de datos le\u00eddos del disco. Reemplazar CSV/JSON por Parquet : Migrar conjuntos de datos de formatos basados en filas (CSV, JSON) a Parquet para obtener mejoras significativas en el rendimiento de lectura y la eficiencia del almacenamiento. Beneficios del Predicate Pushdown : Si se tiene una tabla Parquet de logs y se filtra por timestamp > 'X' , Spark solo leer\u00e1 los bloques de datos relevantes, sin necesidad de escanear todo el archivo. Particionamiento de Datos : Organizar los datos en el sistema de archivos (HDFS, S3) en directorios basados en valores de columna (ej. data/year=2024/month=01/ ). Esto permite a Spark (y otras herramientas de Big Data) \"podar\" particiones (partition pruning), es decir, escanear solo los directorios relevantes para una consulta, evitando la lectura de datos innecesarios. Consulta de datos por fecha : Si los datos de ventas est\u00e1n particionados por a\u00f1o/mes/d\u00eda , una consulta para las ventas de un d\u00eda espec\u00edfico solo escanear\u00e1 el directorio correspondiente a ese d\u00eda, no toda la tabla. Optimizando Joins con partici\u00f3n por clave : Si se va a unir por una clave y ambas tablas est\u00e1n particionadas por esa misma clave, se puede evitar un shuffle completo (co-ubicaci\u00f3n, como se mencion\u00f3 anteriormente). Tama\u00f1o de los Archivos (Small Files Problem) : Tener un gran n\u00famero de archivos peque\u00f1os (ej. miles de archivos de unos pocos KB) en un sistema de archivos distribuido (como HDFS) puede degradar seriamente el rendimiento. Esto se debe a la sobrecarga de gestionar metadatos para cada archivo y la ineficiencia de la lectura de muchos archivos peque\u00f1os. Compactaci\u00f3n de datos hist\u00f3ricos : Si se ingieren datos en peque\u00f1os lotes, peri\u00f3dicamente se debe ejecutar un proceso de \"compactaci\u00f3n\" que combine estos peque\u00f1os archivos en unos pocos archivos m\u00e1s grandes (e.g., 128 MB a 1 GB) para optimizar las lecturas futuras. Ajustar el n\u00famero de particiones de salida : Al escribir resultados, se puede usar repartition() o coalesce() para controlar el n\u00famero de archivos de salida y evitar generar demasiados peque\u00f1os.","title":"Consideraciones de Dise\u00f1o de Datos"},{"location":"tema24/#tarea","text":"Ejercicio de Persistencia con diferentes niveles : Crea un DataFrame con 10 millones de filas y varias columnas. Realiza una serie de transformaciones sobre \u00e9l. Luego, persiste el DataFrame utilizando MEMORY_ONLY , MEMORY_AND_DISK , y DISK_ONLY (en diferentes ejecuciones). Mide el tiempo de ejecuci\u00f3n de las operaciones posteriores a la primera acci\u00f3n para cada nivel de persistencia y explica las diferencias observadas en el rendimiento y el uso de memoria/disco (usando Spark UI). Identificaci\u00f3n de un Shuffle Costoso : Dise\u00f1a un escenario donde se genere un DataFrame grande y se realice una operaci\u00f3n que cause un shuffle significativo (ej. un groupBy por una columna de alta cardinalidad o un join entre dos DataFrames grandes sin una estrategia de optimizaci\u00f3n). Utiliza Spark UI para identificar la etapa del shuffle, el volumen de datos shufflados y el tiempo que consume. Implementaci\u00f3n de Broadcast Join : Toma el escenario del ejercicio anterior (o crea uno similar con un join entre una tabla grande y una peque\u00f1a). Implementa un Broadcast Join utilizando F.broadcast() y compara el tiempo de ejecuci\u00f3n y la ausencia/reducci\u00f3n del shuffle en Spark UI con respecto a un join normal. Predicado Pushdown y Column Pruning en acci\u00f3n : Lee un archivo Parquet con m\u00faltiples columnas y filtra por una columna indexada (si el formato lo permite) y selecciona solo un subconjunto de columnas. Observa el plan de ejecuci\u00f3n en Spark UI (pesta\u00f1a SQL -> Details) y explica c\u00f3mo Catalyst aplica el Predicado Pushdown y el Column Pruning. Simulaci\u00f3n de \"Small Files Problem\" y Soluci\u00f3n : Escribe un DataFrame grande en 1000 archivos peque\u00f1os. Luego, lee esos 1000 archivos y escribe el resultado en 10 archivos m\u00e1s grandes utilizando coalesce() o repartition() . Mide y compara los tiempos de lectura y escritura. Configuraci\u00f3n de Ejecutores : Experimenta con la configuraci\u00f3n de spark.executor.cores y spark.executor.memory en una aplicaci\u00f3n Spark. Ejecuta una carga de trabajo intensiva (ej. un join complejo o una agregaci\u00f3n grande) con diferentes configuraciones (ej. pocos ejecutores grandes vs. muchos ejecutores peque\u00f1os, manteniendo el mismo n\u00famero total de n\u00facleos y memoria para el cl\u00faster). Analiza el impacto en el rendimiento y la utilizaci\u00f3n de recursos en Spark UI. UDFs vs. Funciones Nativas para Optimizaci\u00f3n : Crea una funci\u00f3n para manipular una columna (ej. concatenar strings, transformar un valor num\u00e9rico). Primero, implementa la l\u00f3gica como una UDF. Luego, re-implementa la misma l\u00f3gica utilizando solo funciones nativas de Spark SQL (ej. concat_ws , when , lit ). Compara el rendimiento de ambas implementaciones y explica por qu\u00e9 una es m\u00e1s eficiente. An\u00e1lisis de Derrames a Disco (Spill) : Ejecuta una operaci\u00f3n Spark (ej. un groupBy con una clave de muy alta cardinalidad y un agregado complejo) con memoria de ejecutor limitada, forzando que Spark \"derrame\" datos al disco (spill). Monitorea el evento en Spark UI y explica qu\u00e9 significa el \"spill\" y c\u00f3mo afecta el rendimiento. Optimizaci\u00f3n de Sort : Genera un DataFrame con datos aleatorios y ord\u00e9nalo utilizando orderBy . Luego, compara el plan de ejecuci\u00f3n y el rendimiento si los datos ya estuvieran pre-ordenados o si se aplicara una partici\u00f3n adecuada antes del ordenamiento (esto puede ser un ejercicio conceptual o simulado). Lectura con Particionamiento (Partition Pruning) : Crea un DataFrame grande y escr\u00edbelo en Parquet particionando por una columna (ej. df.write.partitionBy(\"ciudad\").parquet(\"path/to/data\") ). Luego, lee los datos filtrando por esa columna particionada (ej. spark.read.parquet(\"path/to/data\").filter(\"ciudad = 'Bogota'\") ). Observa en Spark UI c\u00f3mo Spark solo escanea los directorios relevantes, demostrando el \"partition pruning\".","title":"Tarea"},{"location":"tema31/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.1. Dise\u00f1o de pipelines ETL Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Dise\u00f1o de pipelines ETL"},{"location":"tema31/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema31/#tema-31-diseno-de-pipelines-etl","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 3.1. Dise\u00f1o de pipelines ETL"},{"location":"tema32/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.2. Conexi\u00f3n a m\u00faltiples fuentes Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Conexi\u00f3n a m\u00faltiples fuentes"},{"location":"tema32/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema32/#tema-32-conexion-a-multiples-fuentes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 3.2. Conexi\u00f3n a m\u00faltiples fuentes"},{"location":"tema33/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.3. Procesos escalables y particionamiento Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Procesos escalables y particionamiento"},{"location":"tema33/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema33/#tema-33-procesos-escalables-y-particionamiento","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 3.3. Procesos escalables y particionamiento"},{"location":"tema34/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.4. Buenas pr\u00e1cticas para ETL en Spark Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Buenas pr\u00e1cticas para ETL en Spark"},{"location":"tema34/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema34/#tema-34-buenas-practicas-para-etl-en-spark","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 3.4. Buenas pr\u00e1cticas para ETL en Spark"},{"location":"tema35/","text":"3. Arquitectura y Dise\u00f1o de Flujos ETL Tema 3.5. Introducci\u00f3n a Apache Airflow Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Introducci\u00f3n a Apache Airflow"},{"location":"tema35/#3-arquitectura-y-diseno-de-flujos-etl","text":"","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"tema35/#tema-35-introduccion-a-apache-airflow","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 3.5. Introducci\u00f3n a Apache Airflow"},{"location":"tema41/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Arquitectura y componentes de Airflow"},{"location":"tema41/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema41/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema42/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Instalaci\u00f3n local usando Docker"},{"location":"tema42/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema42/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema44/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Uso de `BashOperator`, `PythonOperator` y `SparkSubmitOperator`"},{"location":"tema44/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema44/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema45/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Monitoreo y manejo de dependencias"},{"location":"tema45/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema45/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema51/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Desarrollo del proyecto integrador"},{"location":"tema51/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema51/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema52/","text":"2. PySpark y SparkSQL Tema 2.1. Operaciones con DataFrames Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"2. PySpark y SparkSQL"},{"location":"tema52/#2-pyspark-y-sparksql","text":"","title":"2. PySpark y SparkSQL"},{"location":"tema52/#tema-21-operaciones-con-dataframes","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos.","title":"Tema 2.1. Operaciones con DataFrames"},{"location":"tema_databricks/","text":"\u2190 Volver al Inicio Configuraci\u00f3n Cuenta en Databricks Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos. \u2190 Volver al Inicio","title":"Tema databricks"},{"location":"tema_databricks/#configuracion-cuenta-en-databricks","text":"Lorem ipsum dolor sit amet consectetur adipiscing elit. Quisque faucibus ex sapien vitae pellentesque sem placerat. In id cursus mi pretium tellus duis convallis. Tempus leo eu aenean sed diam urna tempor. Pulvinar vivamus fringilla lacus nec metus bibendum egestas. Iaculis massa nisl malesuada lacinia integer nunc posuere. Ut hendrerit semper vel class aptent taciti sociosqu. Ad litora torquent per conubia nostra inceptos himenaeos. \u2190 Volver al Inicio","title":"Configuraci\u00f3n Cuenta en Databricks"},{"location":"tema_docker/","text":"\u2190 Volver al Inicio Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2 Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ). 1. Instalaci\u00f3n de WSL2 Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a. 2. Instalaci\u00f3n de Docker Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version 3. Ejecutando un contenedor de ejemplo Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios. \u2190 Volver al Inicio","title":"Tema docker"},{"location":"tema_docker/#instalacion-y-configuracion-de-docker-y-wsl2","text":"Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ).","title":"Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2"},{"location":"tema_docker/#1-instalacion-de-wsl2","text":"Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a.","title":"1. Instalaci\u00f3n de WSL2"},{"location":"tema_docker/#2-instalacion-de-docker","text":"Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version","title":"2. Instalaci\u00f3n de Docker"},{"location":"tema_docker/#3-ejecutando-un-contenedor-de-ejemplo","text":"Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios. \u2190 Volver al Inicio","title":"3. Ejecutando un contenedor de ejemplo"},{"location":"tema_python/","text":"\u2190 Volver al Inicio Dominio b\u00e1sico de Python Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave 1. Entorno de Trabajo: Jupyter y Markdown Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown 2. Fundamentos de Python Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos 3. An\u00e1lisis de Datos con Pandas Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas \u2190 Volver al Inicio","title":"Tema python"},{"location":"tema_python/#dominio-basico-de-python","text":"Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave","title":"Dominio b\u00e1sico de Python"},{"location":"tema_python/#1-entorno-de-trabajo-jupyter-y-markdown","text":"Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown","title":"1. Entorno de Trabajo: Jupyter y Markdown"},{"location":"tema_python/#2-fundamentos-de-python","text":"Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos","title":"2. Fundamentos de Python"},{"location":"tema_python/#3-analisis-de-datos-con-pandas","text":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas \u2190 Volver al Inicio","title":"3. An\u00e1lisis de Datos con Pandas"},{"location":"tema_sql/","text":"\u2190 Volver al Inicio Repaso de SQL 1. Consultas B\u00e1sicas (SELECT) El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC; 2. Filtrado de Datos (WHERE) La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia'); 3. Joins (Uniones de Tablas) Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre; 4. Funciones de Agregaci\u00f3n Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5; 5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING) GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000; 6. Inserci\u00f3n de Datos (INSERT) INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75); 7. Actualizaci\u00f3n de Datos (UPDATE) UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 ); 8. Eliminaci\u00f3n de Datos (DELETE) DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) ); 9. Subconsultas Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento ); 10. \u00cdndices y Optimizaci\u00f3n Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20; \u2190 Volver al Inicio","title":"Tema sql"},{"location":"tema_sql/#repaso-de-sql","text":"","title":"Repaso de SQL"},{"location":"tema_sql/#1-consultas-basicas-select","text":"El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC;","title":"1. Consultas B\u00e1sicas (SELECT)"},{"location":"tema_sql/#2-filtrado-de-datos-where","text":"La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia');","title":"2. Filtrado de Datos (WHERE)"},{"location":"tema_sql/#3-joins-uniones-de-tablas","text":"Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre;","title":"3. Joins (Uniones de Tablas)"},{"location":"tema_sql/#4-funciones-de-agregacion","text":"Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5;","title":"4. Funciones de Agregaci\u00f3n"},{"location":"tema_sql/#5-agrupacion-y-filtrado-de-grupos-group-by-y-having","text":"GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000;","title":"5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING)"},{"location":"tema_sql/#6-insercion-de-datos-insert","text":"INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75);","title":"6. Inserci\u00f3n de Datos (INSERT)"},{"location":"tema_sql/#7-actualizacion-de-datos-update","text":"UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 );","title":"7. Actualizaci\u00f3n de Datos (UPDATE)"},{"location":"tema_sql/#8-eliminacion-de-datos-delete","text":"DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) );","title":"8. Eliminaci\u00f3n de Datos (DELETE)"},{"location":"tema_sql/#9-subconsultas","text":"Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento );","title":"9. Subconsultas"},{"location":"tema_sql/#10-indices-y-optimizacion","text":"Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20; \u2190 Volver al Inicio","title":"10. \u00cdndices y Optimizaci\u00f3n"}]}