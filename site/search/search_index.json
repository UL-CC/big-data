{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos. Requisitos Previos Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition 1. Introducci\u00f3n Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark 2. PySpark y SparkSQL Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Operaciones con DataFrames Funciones integradas y definidas por el usuario Consultas con SparkSQL Esquemas y tipos de datos complejos Optimizaci\u00f3n con Catalyst y Tungsten 3. Arquitectura y Dise\u00f1o de Flujos ETL Objetivo : Al terminar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables y eficientes utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye la habilidad de conectar diversas fuentes de datos, gestionar el particionamiento para el rendimiento y aplicar las mejores pr\u00e1cticas de la industria para asegurar la integridad y la calidad de los datos. Dise\u00f1o de pipelines ETL Conexi\u00f3n a m\u00faltiples fuentes Procesos escalables y particionamiento Buenas pr\u00e1cticas para ETL en Spark Introducci\u00f3n a Apache Airflow 4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow Instalaci\u00f3n local usando Docker DAGs, operadores y tareas Uso de BashOperator , PythonOperator y SparkSubmitOperator Monitoreo y manejo de dependencias 5. Proyecto Integrador y Despliegue Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"Inicio"},{"location":"#metodos-de-procesamiento-y-analisis-de-big-data","text":"Objetivo General : Al finalizar el m\u00f3dulo, cada estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y ejecutar flujos de procesamiento para grandes vol\u00famenes de datos, utilizando herramientas como Apache Spark, Airflow, y servicios Big Data de Databricks, para extraer informaci\u00f3n valiosa y apoyar la toma de decisiones basada en datos.","title":"M\u00e9todos de Procesamiento y An\u00e1lisis de Big Data"},{"location":"#requisitos-previos","text":"Dominio b\u00e1sico de Python Conocimientos en SQL Es necesario tener instalado Docker/WSL2 Es necesario tener acceso a Databricks Community Edition","title":"Requisitos Previos"},{"location":"#1-introduccion","text":"Objetivo : Al finalizar esta unidad, el estudiante comprender\u00e1 los conceptos fundamentales del Big Data, sus desaf\u00edos y oportunidades, y ser\u00e1 capaz de configurar un entorno b\u00e1sico de Apache Spark con PySpark para realizar operaciones iniciales de procesamiento de datos utilizando RDDs, DataFrames y Datasets. Fundamentos de Big Data Introducci\u00f3n al ecosistema Spark RDD, DataFrame y Dataset Instalaci\u00f3n y configuraci\u00f3n de Spark Primeros pasos con PySpark","title":"1. Introducci\u00f3n"},{"location":"#2-pyspark-y-sparksql","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de manipular, transformar y consultar grandes vol\u00famenes de datos utilizando DataFrames de Apache Spark y SparkSQL, aplicando funciones integradas y personalizadas, gestionando esquemas complejos y comprendiendo los principios de optimizaci\u00f3n subyacentes para mejorar el rendimiento de las operaciones. Operaciones con DataFrames Funciones integradas y definidas por el usuario Consultas con SparkSQL Esquemas y tipos de datos complejos Optimizaci\u00f3n con Catalyst y Tungsten","title":"2. PySpark y SparkSQL"},{"location":"#3-arquitectura-y-diseno-de-flujos-etl","text":"Objetivo : Al terminar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y optimizar pipelines ETL (Extracci\u00f3n, Transformaci\u00f3n y Carga) escalables y eficientes utilizando herramientas como Apache Spark y Apache Airflow. Esto incluye la habilidad de conectar diversas fuentes de datos, gestionar el particionamiento para el rendimiento y aplicar las mejores pr\u00e1cticas de la industria para asegurar la integridad y la calidad de los datos. Dise\u00f1o de pipelines ETL Conexi\u00f3n a m\u00faltiples fuentes Procesos escalables y particionamiento Buenas pr\u00e1cticas para ETL en Spark Introducci\u00f3n a Apache Airflow","title":"3. Arquitectura y Dise\u00f1o de Flujos ETL"},{"location":"#4-automatizacion-y-orquestacion-con-apache-airflow","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de dise\u00f1ar, implementar y monitorear flujos de trabajo de datos complejos utilizando Apache Airflow, automatizando la ejecuci\u00f3n de tareas distribuidas y gestionando sus dependencias para asegurar la eficiencia y fiabilidad de los procesos de Big Data. Arquitectura y componentes de Airflow Instalaci\u00f3n local usando Docker DAGs, operadores y tareas Uso de BashOperator , PythonOperator y SparkSubmitOperator Monitoreo y manejo de dependencias","title":"4. Automatizaci\u00f3n y Orquestaci\u00f3n con Apache Airflow"},{"location":"#5-proyecto-integrador-y-despliegue","text":"Objetivo : Al finalizar esta unidad, el estudiante ser\u00e1 capaz de implementar un proyecto completo de Big Data, aplicando las metodolog\u00edas y herramientas aprendidas a lo largo del m\u00f3dulo para resolver un problema de negocio real, asegurando la escalabilidad y la eficiencia de la soluci\u00f3n. Desarrollo del proyecto integrador Despliegue en nube","title":"5. Proyecto Integrador y Despliegue"},{"location":"ejemplo_json/","text":"Ejemplo JSON { \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_json/#ejemplo-json","text":"{ \"hospital\": { \"id\": \"HSP-001\", \"nombre\": \"Hospital Universitario del Valle\", \"ubicacion\": \"Cali, Colombia\" }, \"sesion_monitoreo\": { \"id\": \"SES-20250530-001\", \"fecha_inicio\": \"2025-05-30T08:00:00Z\", \"fecha_fin\": \"2025-05-30T20:00:00Z\", \"unidad\": \"UCI\", \"habitacion\": \"UCI-205\" }, \"paciente\": { \"id\": \"P12345\", \"historia_clinica\": \"HC-789456\", \"edad\": 45, \"sexo\": \"M\", \"peso_kg\": 75.5, \"altura_cm\": 175, \"condicion_principal\": \"Post-operatorio cardiovascular\" }, \"dispositivos_monitoreados\": [ { \"dispositivo_id\": \"MON-CV-001\", \"fabricante\": \"Philips\", \"modelo\": \"IntelliVue MX800\", \"ubicacion\": \"Cabecera cama\", \"estado\": \"activo\", \"ultima_calibracion\": \"2025-05-29T14:30:00Z\" }, { \"dispositivo_id\": \"VENT-001\", \"fabricante\": \"Medtronic\", \"modelo\": \"Puritan Bennett 980\", \"ubicacion\": \"Soporte ventilatorio\", \"estado\": \"activo\", \"ultima_mantenimiento\": \"2025-05-25T09:00:00Z\" } ], \"registros_sensores\": [ { \"id\": \"REG-001\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:00Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"frecuencia_cardiaca\", \"valor\": 72, \"unidad\": \"bpm\", \"rango_normal\": { \"min\": 60, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"excelente\", \"metadatos\": { \"derivacion\": \"Lead II\", \"filtro_aplicado\": \"50Hz\", \"resolucion\": \"0.1bpm\" } }, { \"id\": \"REG-002\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:00:15Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 125, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"metodo_medicion\": \"oscilometrico\", \"brazalete_tamano\": \"adulto_standard\", \"ciclo_medicion\": \"automatico_5min\" }, \"medicion_completa\": { \"sistolica\": 125, \"diastolica\": 78, \"presion_media\": 94, \"presion_pulso\": 47 } }, { \"id\": \"REG-003\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:01:00Z\", \"tipo_sensor\": \"oximetria_pulso\", \"tipo_dato\": \"saturacion_oxigeno\", \"valor\": 98, \"unidad\": \"%\", \"rango_normal\": { \"min\": 95, \"max\": 100 }, \"estado_alerta\": \"normal\", \"calidad_senal\": \"buena\", \"metadatos\": { \"longitud_onda\": \"660nm/940nm\", \"ubicacion_sensor\": \"dedo_indice_derecho\", \"perfusion_index\": 2.1, \"frecuencia_pulso\": 73 } }, { \"id\": \"REG-004\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:02:00Z\", \"tipo_sensor\": \"temperatura\", \"tipo_dato\": \"temperatura_corporal\", \"valor\": 36.8, \"unidad\": \"\u00b0C\", \"rango_normal\": { \"min\": 36.0, \"max\": 37.5 }, \"estado_alerta\": \"normal\", \"metadatos\": { \"ubicacion_medicion\": \"timpano\", \"sensor_tipo\": \"infrarrojo\", \"compensacion_ambiental\": true } }, { \"id\": \"REG-005\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"VENT-001\", \"timestamp\": \"2025-05-30T10:03:00Z\", \"tipo_sensor\": \"ventilador_mecanico\", \"tipo_dato\": \"parametros_ventilatorios\", \"metadatos\": { \"modo_ventilacion\": \"SIMV\", \"trigger_sensibilidad\": \"-2cmH2O\" }, \"parametros\": { \"volumen_corriente\": { \"valor\": 450, \"unidad\": \"ml\", \"programado\": 450, \"medido\": 448 }, \"frecuencia_respiratoria\": { \"valor\": 16, \"unidad\": \"rpm\", \"programado\": 16, \"total\": 18, \"espontanea\": 2 }, \"presion_pico\": { \"valor\": 28, \"unidad\": \"cmH2O\", \"limite_alarma\": 35 }, \"presion_meseta\": { \"valor\": 22, \"unidad\": \"cmH2O\" }, \"peep\": { \"valor\": 8, \"unidad\": \"cmH2O\", \"programado\": 8 }, \"fio2\": { \"valor\": 45, \"unidad\": \"%\", \"programado\": 45 }, \"compliance\": { \"valor\": 35, \"unidad\": \"ml/cmH2O\" } }, \"estado_alerta\": \"normal\" }, { \"id\": \"REG-006\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:15:30Z\", \"tipo_sensor\": \"electrocardiograma\", \"tipo_dato\": \"arritmia_detectada\", \"valor\": \"PVC_aislado\", \"severidad\": \"leve\", \"estado_alerta\": \"atencion\", \"metadatos\": { \"derivacion\": \"Lead V1\", \"duracion_evento\": \"0.12s\", \"acoplamiento\": \"tardio\", \"morfologia\": \"unifocal\" }, \"contexto_evento\": { \"fc_antes\": 74, \"fc_despues\": 71, \"timestamp_anterior\": \"2025-05-30T10:15:25Z\" } }, { \"id\": \"REG-007\", \"paciente_id\": \"P12345\", \"dispositivo_id\": \"MON-CV-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo_sensor\": \"presion_arterial\", \"tipo_dato\": \"presion_arterial_sistolica\", \"valor\": 155, \"unidad\": \"mmHg\", \"rango_normal\": { \"min\": 90, \"max\": 140 }, \"estado_alerta\": \"alerta_alta\", \"nivel_prioridad\": \"media\", \"medicion_completa\": { \"sistolica\": 155, \"diastolica\": 92, \"presion_media\": 113, \"presion_pulso\": 63 }, \"acciones_automaticas\": [ \"notificacion_enfermeria\", \"registro_tendencia\", \"sugerencia_remedicion_5min\" ], \"confirmacion_requerida\": true } ], \"alertas_activas\": [ { \"id\": \"ALT-001\", \"timestamp\": \"2025-05-30T10:45:00Z\", \"tipo\": \"presion_arterial_elevada\", \"prioridad\": \"media\", \"parametro_afectado\": \"presion_arterial_sistolica\", \"valor_actual\": 155, \"valor_umbral\": 140, \"estado\": \"activa\", \"acciones_tomadas\": [ \"notificacion_enfermera_turno\", \"registro_evento_historial\" ], \"tiempo_desde_activacion\": \"00:00:30\" } ], \"estadisticas_sesion\": { \"total_registros\": 7, \"registros_por_tipo\": { \"frecuencia_cardiaca\": 1, \"presion_arterial\": 2, \"saturacion_oxigeno\": 1, \"temperatura\": 1, \"ventilacion\": 1, \"eventos_especiales\": 1 }, \"alertas_generadas\": 1, \"calidad_datos\": { \"excelente\": 4, \"buena\": 2, \"regular\": 1 }, \"tiempo_monitoreo_minutos\": 45 }, \"configuracion_sistema\": { \"frecuencia_muestreo\": { \"signos_vitales\": \"15s\", \"ventilacion\": \"60s\", \"ecg_continuo\": \"1s\" }, \"umbrales_personalizados\": { \"fc_min\": 55, \"fc_max\": 110, \"pa_sistolica_max\": 150, \"spo2_min\": 92 }, \"notificaciones\": { \"audio\": true, \"visual\": true, \"remota\": true } } }","title":"Ejemplo JSON"},{"location":"ejemplo_log/","text":"Ejemplo LOG de acceso web 192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_log/#ejemplo-log-de-acceso-web","text":"192.168.1.45 - - [30/May/2025:08:15:23 +0000] \"GET /index.html HTTP/1.1\" 200 4521 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.12 - - [30/May/2025:08:15:45 +0000] \"POST /api/login HTTP/1.1\" 200 156 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.78 - - [30/May/2025:08:16:02 +0000] \"GET /css/styles.css HTTP/1.1\" 200 12458 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 192.168.1.67 - - [30/May/2025:08:16:18 +0000] \"GET /images/logo.png HTTP/1.1\" 200 8742 \"https://example.com/index.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.34 - - [30/May/2025:08:16:35 +0000] \"GET /products/catalog HTTP/1.1\" 200 15632 \"https://example.com/index.html\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 172.16.0.89 - - [30/May/2025:08:16:52 +0000] \"POST /api/search HTTP/1.1\" 200 2847 \"https://example.com/products\" \"Mozilla/5.0 (Android 11; Mobile; rv:68.0) Gecko/68.0 Firefox/88.0\" 10.0.0.156 - - [30/May/2025:08:17:08 +0000] \"GET /admin/dashboard HTTP/1.1\" 401 891 \"-\" \"curl/7.68.0\" 192.168.1.23 - - [30/May/2025:08:17:25 +0000] \"GET /js/main.js HTTP/1.1\" 200 34521 \"https://example.com/products\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.145 - - [30/May/2025:08:17:41 +0000] \"GET /about-us HTTP/1.1\" 200 6789 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 198.51.100.67 - - [30/May/2025:08:17:58 +0000] \"GET /contact HTTP/1.1\" 200 3456 \"https://example.com/about-us\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 172.16.0.201 - - [30/May/2025:08:18:14 +0000] \"POST /api/newsletter HTTP/1.1\" 201 78 \"https://example.com/contact\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 192.168.1.78 - - [30/May/2025:08:18:31 +0000] \"GET /favicon.ico HTTP/1.1\" 200 1150 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.87 - - [30/May/2025:08:18:47 +0000] \"GET /products/item/123 HTTP/1.1\" 200 9876 \"https://example.com/products/catalog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.92 - - [30/May/2025:08:19:04 +0000] \"GET /images/product-123.jpg HTTP/1.1\" 200 45632 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 198.51.100.156 - - [30/May/2025:08:19:20 +0000] \"POST /api/cart/add HTTP/1.1\" 200 234 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Android 12; SM-G991B) AppleWebKit/537.36\" 172.16.0.45 - - [30/May/2025:08:19:37 +0000] \"GET /cart HTTP/1.1\" 200 5678 \"https://example.com/products/item/123\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\" 192.168.1.134 - - [30/May/2025:08:19:53 +0000] \"GET /checkout HTTP/1.1\" 200 7890 \"https://example.com/cart\" \"Mozilla/5.0 (iPad; CPU OS 15_0 like Mac OS X) AppleWebKit/605.1.15\" 10.0.0.198 - - [30/May/2025:08:20:10 +0000] \"POST /api/payment HTTP/1.1\" 200 445 \"https://example.com/checkout\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" 203.0.113.34 - - [30/May/2025:08:20:26 +0000] \"GET /order/confirmation/987654 HTTP/1.1\" 200 2345 \"https://example.com/checkout\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 198.51.100.89 - - [30/May/2025:08:20:43 +0000] \"GET /robots.txt HTTP/1.1\" 200 156 \"-\" \"Googlebot/2.1 (+http://www.google.com/bot.html)\" 172.16.0.123 - - [30/May/2025:08:20:59 +0000] \"GET /sitemap.xml HTTP/1.1\" 200 3456 \"-\" \"Bingbot/2.0 (+http://www.bing.com/bingbot.htm)\" 192.168.1.99 - - [30/May/2025:08:21:16 +0000] \"GET /blog HTTP/1.1\" 200 12345 \"https://example.com/index.html\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 10.0.0.67 - - [30/May/2025:08:21:32 +0000] \"GET /blog/post/latest-trends HTTP/1.1\" 200 8765 \"https://example.com/blog\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\" 203.0.113.178 - - [30/May/2025:08:21:49 +0000] \"GET /images/blog/trends.jpg HTTP/1.1\" 200 67890 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15\" 198.51.100.123 - - [30/May/2025:08:22:05 +0000] \"POST /api/comments HTTP/1.1\" 201 89 \"https://example.com/blog/post/latest-trends\" \"Mozilla/5.0 (Android 11; Pixel 5) AppleWebKit/537.36\" 172.16.0.67 - - [30/May/2025:08:22:22 +0000] \"GET /support HTTP/1.1\" 200 4567 \"https://example.com/index.html\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" 192.168.1.156 - - [30/May/2025:08:22:38 +0000] \"GET /support/faq HTTP/1.1\" 200 9876 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 10.0.0.234 - - [30/May/2025:08:22:55 +0000] \"POST /api/ticket HTTP/1.1\" 201 156 \"https://example.com/support\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\" 203.0.113.67 - - [30/May/2025:08:23:11 +0000] \"GET /downloads HTTP/1.1\" 200 2345 \"https://example.com/support\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\" 198.51.100.45 - - [30/May/2025:08:23:28 +0000] \"GET /downloads/manual.pdf HTTP/1.1\" 200 1234567 \"https://example.com/downloads\" \"Mozilla/5.0 (iPad; CPU OS 14_8 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.189 - - [30/May/2025:08:23:44 +0000] \"GET /api/status HTTP/1.1\" 200 56 \"-\" \"curl/7.74.0\" 192.168.1.67 - - [30/May/2025:08:24:01 +0000] \"GET /search?q=laptop HTTP/1.1\" 200 15678 \"https://example.com/index.html\" \"Mozilla/5.0 (Android 12; SM-A515F) AppleWebKit/537.36\" 10.0.0.123 - - [30/May/2025:08:24:17 +0000] \"GET /products/item/456 HTTP/1.1\" 200 8900 \"https://example.com/search?q=laptop\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" 203.0.113.156 - - [30/May/2025:08:24:34 +0000] \"GET /api/reviews/456 HTTP/1.1\" 200 3456 \"https://example.com/products/item/456\" \"Mozilla/5.0 (X11; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\" 198.51.100.78 - - [30/May/2025:08:24:50 +0000] \"GET /login HTTP/1.1\" 200 2345 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15\" 172.16.0.234 - - [30/May/2025:08:25:07 +0000] \"POST /api/auth HTTP/1.1\" 401 234 \"https://example.com/login\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" 192.168.1.89 - - [30/May/2025:08:25:23 +0000] \"GET /password-reset HTTP/1.1\" 200 1789 \"https://example.com/login\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0\" 10.0.0.156 - - [30/May/2025:08:25:40 +0000] \"POST /api/password-reset HTTP/1.1\" 200 67 \"https://example.com/password-reset\" \"Mozilla/5.0 (Android 11; Pixel 4a) AppleWebKit/537.36\" 203.0.113.99 - - [30/May/2025:08:25:56 +0000] \"GET /profile HTTP/1.1\" 302 0 \"https://example.com/index.html\" \"Mozilla/5.0 (iPad; CPU OS 15_1","title":"Ejemplo LOG de acceso web"},{"location":"ejemplo_xml/","text":"Ejemplo XML <?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"ejemplo_xml/#ejemplo-xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <FacturaElectronica xmlns=\"http://www.facturacion.gov.co/schema\" version=\"2.1\"> <!-- Informaci\u00f3n de la factura --> <InformacionGeneral> <NumeroFactura>FE-2024-001234</NumeroFactura> <FechaEmision>2024-05-30</FechaEmision> <HoraEmision>14:30:00</HoraEmision> <TipoDocumento>01</TipoDocumento> <!-- 01 = Factura de Venta --> <Moneda>COP</Moneda> <CUFE>a1b2c3d4e5f6789012345678901234567890abcd</CUFE> </InformacionGeneral> <!-- Datos del emisor --> <Emisor> <RazonSocial>Tecnolog\u00eda y Soluciones SAS</RazonSocial> <NIT>900123456-1</NIT> <RegimenFiscal>49</RegimenFiscal> <!-- Responsable del IVA --> <Direccion> <Calle>Carrera 15 # 93-45</Calle> <Ciudad>Bogot\u00e1</Ciudad> <Departamento>Cundinamarca</Departamento> <CodigoPostal>110221</CodigoPostal> </Direccion> <Telefono>+57-1-2345678</Telefono> <Email>facturacion@tecysol.com.co</Email> </Emisor> <!-- Datos del receptor --> <Receptor> <RazonSocial>Comercializadora del Valle LTDA</RazonSocial> <NIT>800987654-2</NIT> <RegimenFiscal>48</RegimenFiscal> <Direccion> <Calle>Avenida 6N # 25-80</Calle> <Ciudad>Cali</Ciudad> <Departamento>Valle del Cauca</Departamento> <CodigoPostal>760001</CodigoPostal> </Direccion> <Email>compras@comervalle.com.co</Email> </Receptor> <!-- Detalle de productos/servicios --> <DetalleFactura> <Item> <NumeroLinea>1</NumeroLinea> <CodigoProducto>SOFT-001</CodigoProducto> <Descripcion>Licencia de Software ERP - M\u00f3dulo Contable</Descripcion> <Cantidad>2</Cantidad> <UnidadMedida>UND</UnidadMedida> <ValorUnitario>2500000.00</ValorUnitario> <ValorTotal>5000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>950000.00</Valor> </IVA> </Impuestos> </Item> <Item> <NumeroLinea>2</NumeroLinea> <CodigoProducto>SERV-001</CodigoProducto> <Descripcion>Servicio de Implementaci\u00f3n y Capacitaci\u00f3n</Descripcion> <Cantidad>20</Cantidad> <UnidadMedida>HOR</UnidadMedida> <ValorUnitario>150000.00</ValorUnitario> <ValorTotal>3000000.00</ValorTotal> <Impuestos> <IVA> <Porcentaje>19.00</Porcentaje> <Valor>570000.00</Valor> </IVA> </Impuestos> </Item> </DetalleFactura> <!-- Totales de la factura --> <Totales> <SubTotal>8000000.00</SubTotal> <TotalImpuestos>1520000.00</TotalImpuestos> <TotalDescuentos>0.00</TotalDescuentos> <Total>9520000.00</Total> <TotalEnLetras>NUEVE MILLONES QUINIENTOS VEINTE MIL PESOS M/CTE</TotalEnLetras> </Totales> <!-- Medios de pago --> <MediosPago> <MedioPago> <Codigo>10</Codigo> <!-- Efectivo --> <Descripcion>Efectivo</Descripcion> <FechaPago>2024-06-15</FechaPago> </MedioPago> </MediosPago> <!-- Informaci\u00f3n adicional --> <InformacionAdicional> <CampoAdicional nombre=\"Orden de Compra\">OC-2024-0567</CampoAdicional> <CampoAdicional nombre=\"Vendedor\">Juan P\u00e9rez</CampoAdicional> <CampoAdicional nombre=\"Observaciones\">Entrega en 15 d\u00edas h\u00e1biles</CampoAdicional> </InformacionAdicional> <!-- Firma digital (representaci\u00f3n simplificada) --> <FirmaDigital> <Certificado>MIICertificadoDigital...</Certificado> <Algoritmo>SHA256withRSA</Algoritmo> <FechaFirma>2024-05-30T14:30:00Z</FechaFirma> </FirmaDigital> </FacturaElectronica>","title":"Ejemplo XML"},{"location":"tema11/","text":"1. Introducci\u00f3n Tema 1.1. Fundamentos de Big Data Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados. 1.1.1 \u00bfQu\u00e9 es Big Data? Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales. 1.1.2 Las 5 V s del Big Data: Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data: Volumen Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad. Velocidad Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones. Variedad Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas. Veracidad Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados. Valor Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha. 1.1.3 Desaf\u00edos del Big Data: El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos. 1.1.4 Oportunidades del Big Data: A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros. 1.1.5 Tipos de Datos en Big Data: Datos Estructurados Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo . Datos Semi-estructurados Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor. Datos No Estructurados Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre. 1.1.6 Modelos de Procesamiento de Datos Procesamiento Batch (por Lotes) Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa. Procesamiento en Tiempo Real (Streaming) Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real. Procesamiento Interactivo (Ad-Hoc/Exploratorio) Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido. Tarea Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Fundamentos de Big Data"},{"location":"tema11/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema11/#tema-11-fundamentos-de-big-data","text":"Objetivo : Al finalizar este tema, el estudiante comprender\u00e1 qu\u00e9 es Big Data, identificar\u00e1 sus caracter\u00edsticas principales (las 'Vs'), reconocer\u00e1 los desaf\u00edos y oportunidades que presenta, y diferenciar\u00e1 los tipos de datos y los modelos de procesamiento de datos m\u00e1s comunes en el contexto del Big Data. Introducci\u00f3n : En la era digital actual, la cantidad de datos generados crece exponencialmente cada segundo. Desde nuestras interacciones en redes sociales hasta transacciones comerciales y sensores IoT, todo produce datos. Sin embargo, no es solo el volumen lo que define el \"Big Data\", sino tambi\u00e9n la velocidad a la que se generan y la variedad de sus formatos. Este tema sentar\u00e1 las bases para entender este fen\u00f3meno, explorando sus dimensiones, las problem\u00e1ticas que resuelve y las nuevas oportunidades de negocio y an\u00e1lisis que habilita, preparando al estudiante para adentrarse en las herramientas dise\u00f1adas para manejar este desaf\u00edo. Desarrollo : El concepto de \"Big Data\" ha evolucionado para describir conjuntos de datos tan grandes y complejos que los m\u00e9todos tradicionales de procesamiento y an\u00e1lisis de datos no son adecuados. Va m\u00e1s all\u00e1 del tama\u00f1o, abarcando la complejidad y la velocidad con la que los datos son generados, procesados y analizados.","title":"Tema 1.1. Fundamentos de Big Data"},{"location":"tema11/#111-que-es-big-data","text":"Big Data se refiere al conjunto de tecnolog\u00edas, arquitecturas y metodolog\u00edas orientadas al manejo de grandes vol\u00famenes de datos que, por su tama\u00f1o, velocidad de generaci\u00f3n y variedad de formatos, exceden las capacidades de los sistemas tradicionales de gesti\u00f3n y procesamiento de datos, para realizar la labor en un tiempo razonable. Este paradigma implica trabajar con datos estructurados, semi-estructurados y no estructurados que son generados de manera continua desde m\u00faltiples fuentes, como sensores IoT, redes sociales, logs de sistemas, transacciones financieras, aplicaciones m\u00f3viles, entre otros. El objetivo de Big Data no es solo almacenar grandes cantidades de informaci\u00f3n, sino habilitar su an\u00e1lisis a trav\u00e9s de herramientas y plataformas distribuidas como Apache Hadoop, Apache Spark, y sistemas en la nube (AWS, Azure, GCP), que permiten descubrir patrones, correlaciones, comportamientos y tendencias en tiempo casi real. Esto proporciona una base s\u00f3lida para la toma de decisiones basada en datos (data-driven decision making) en contextos empresariales, cient\u00edficos, industriales y sociales.","title":"1.1.1 \u00bfQu\u00e9 es Big Data?"},{"location":"tema11/#112-las-5-vs-del-big-data","text":"Estas cinco caracter\u00edsticas definen intr\u00ednsecamente lo que consideramos Big Data:","title":"1.1.2 Las 5 Vs del Big Data:"},{"location":"tema11/#volumen","text":"Se refiere a la cantidad masiva de datos generados por fuentes heterog\u00e9neas a una escala que supera la capacidad de almacenamiento, procesamiento y an\u00e1lisis de los sistemas tradicionales. El volumen en Big Data se mide en terabytes (TB), petabytes (PB), exabytes (EB) y m\u00e1s, y exige arquitecturas distribuidas para su gesti\u00f3n eficiente. Esta dimensi\u00f3n implica dise\u00f1ar soluciones capaces de almacenar datos a gran escala (como HDFS, S3, Snowflake o BigQuery) y realizar operaciones anal\u00edticas paralelas mediante frameworks como Apache Spark o Hive. Walmart procesa m\u00e1s de un mill\u00f3n de transacciones de clientes por hora, lo que se traduce en m\u00e1s de 2.5 petabytes de datos generados diariamente. El Large Hadron Collider (LHC) del CERN genera aproximadamente 1 petabyte de datos por segundo durante sus experimentos, de los cuales solo una fracci\u00f3n se conserva para an\u00e1lisis posterior. Plataformas como Facebook almacenan y gestionan m\u00e1s de 300 petabytes de datos generados por interacciones de usuarios, contenido multimedia, mensajes y logs de actividad.","title":"Volumen"},{"location":"tema11/#velocidad","text":"Hace referencia a la rapidez con la que los datos son generados, transmitidos, recibidos y procesados . En aplicaciones modernas, como monitoreo en tiempo real, detecci\u00f3n de fraude, an\u00e1lisis de redes sociales o telemetr\u00eda industrial, los datos deben ser procesados en milisegundos o segundos. Esto requiere sistemas capaces de ingerir flujos de datos continuos (streaming) utilizando tecnolog\u00edas como Apache Kafka, Apache Flink, Spark Streaming o Google Dataflow, combinadas con almacenamiento en memoria y mecanismos de baja latencia. Twitter genera m\u00e1s de 500 millones de tweets al d\u00eda, que deben ser indexados y disponibles casi instant\u00e1neamente para b\u00fasquedas y an\u00e1lisis en tiempo real. Los sistemas de detecci\u00f3n de fraude bancario procesan miles de transacciones por segundo, aplicando modelos de machine learning en tiempo real para identificar comportamientos sospechosos. En veh\u00edculos aut\u00f3nomos , los sensores LIDAR, c\u00e1maras y radares generan flujos de datos que deben ser analizados al instante para tomar decisiones de navegaci\u00f3n y evitar colisiones.","title":"Velocidad"},{"location":"tema11/#variedad","text":"Describe la diversidad de formatos, fuentes y estructuras de los datos disponibles en los entornos Big Data. Incluye datos estructurados (por ejemplo, registros en bases de datos relacionales), semi-estructurados (como JSON, XML, YAML), y no estructurados (texto libre, im\u00e1genes, audio, video, logs, etc.). Esta heterogeneidad plantea desaf\u00edos en la integraci\u00f3n, transformaci\u00f3n y an\u00e1lisis de datos, lo que requiere pipelines ETL flexibles y herramientas capaces de manejar m\u00faltiples formatos, como Apache NiFi, Databricks, o soluciones basadas en el modelo de Data Lake. Un sistema de salud digital recolecta datos de pacientes de diversas fuentes: registros m\u00e9dicos electr\u00f3nicos (estructurados), notas de doctores (no estructurados), im\u00e1genes de resonancia magn\u00e9tica (no estructurados), y datos de dispositivos wearables (semi-estructurados). Una empresa de medios digitales puede manejar simult\u00e1neamente streams de video (no estructurados), logs de visualizaci\u00f3n (estructurados), comentarios de usuarios (semi-estructurados) y datos de interacci\u00f3n en redes sociales. En el an\u00e1lisis de ciberseguridad , se integran eventos de red (estructurados), reportes t\u00e9cnicos (no estructurados), y registros de acceso de usuarios (semi-estructurados) para detectar amenazas complejas.","title":"Variedad"},{"location":"tema11/#veracidad","text":"Se refiere al grado de confiabilidad, calidad y precisi\u00f3n de los datos . Dado que los datos provienen de m\u00faltiples fuentes, muchas veces no controladas, pueden estar incompletos, duplicados, inconsistentes o sesgados. La veracidad es cr\u00edtica para garantizar que los an\u00e1lisis y modelos derivados sean v\u00e1lidos y \u00fatiles. Requiere t\u00e9cnicas de limpieza, validaci\u00f3n, reconciliaci\u00f3n de fuentes, y gobernanza de datos, apoyadas en cat\u00e1logos de datos, reglas de calidad y mecanismos de trazabilidad. Un an\u00e1lisis de sentimiento en redes sociales puede verse afectado por bots o noticias falsas, introduciendo ruido y reduciendo la veracidad de los hallazgos. En el sector financiero, datos inconsistentes entre diferentes fuentes bancarias pueden provocar errores en modelos predictivos de riesgo crediticio si no se valida adecuadamente la calidad de los datos. Los sistemas de sensores industriales (IoT), lecturas defectuosas o con interferencias pueden generar falsas alarmas o decisiones err\u00f3neas si no se filtran y validan correctamente los datos recolectados.","title":"Veracidad"},{"location":"tema11/#valor","text":"Representa la capacidad de los datos para generar conocimiento accionable y ventaja competitiva . El verdadero objetivo del Big Data no es acumular informaci\u00f3n, sino transformar grandes vol\u00famenes de datos en insights que soporten decisiones estrat\u00e9gicas, mejoren procesos operativos o creen nuevos productos y servicios. Obtener valor requiere combinar ingenier\u00eda de datos, anal\u00edtica avanzada, inteligencia artificial y visualizaci\u00f3n de datos, siempre alineado con objetivos de negocio o cient\u00edficos claramente definidos. El an\u00e1lisis de los patrones de compra de clientes en una tienda en l\u00ednea permite a la empresa optimizar el inventario, personalizar ofertas y mejorar la experiencia del usuario, lo que se traduce en mayores ventas. En el sector energ\u00e9tico, el an\u00e1lisis de datos de consumo el\u00e9ctrico permite implementar modelos de tarificaci\u00f3n din\u00e1mica , optimizar la distribuci\u00f3n de energ\u00eda y prevenir apagones. En agricultura de precisi\u00f3n, el uso de im\u00e1genes satelitales y sensores en campo permite maximizar el rendimiento de cultivos , reducir el uso de insumos y tomar decisiones m\u00e1s informadas sobre riego y cosecha.","title":"Valor"},{"location":"tema11/#113-desafios-del-big-data","text":"El manejo de Big Data presenta varios desaf\u00edos significativos: Almacenamiento : La necesidad de infraestructuras escalables y distribuidas para almacenar vol\u00famenes masivos de datos de diferentes formatos. Procesamiento : Desarrollar sistemas capaces de procesar r\u00e1pidamente datos en constante movimiento y en diferentes formatos. An\u00e1lisis : Dise\u00f1ar algoritmos y t\u00e9cnicas que puedan extraer patrones significativos de conjuntos de datos complejos y heterog\u00e9neos. Seguridad y Privacidad : Proteger la informaci\u00f3n sensible y cumplir con las regulaciones de privacidad de datos (como GDPR o CCPA) en entornos distribuidos. Gobernanza de Datos : Establecer pol\u00edticas y procedimientos para la gesti\u00f3n de la disponibilidad, usabilidad, integridad y seguridad de los datos.","title":"1.1.3 Desaf\u00edos del Big Data:"},{"location":"tema11/#114-oportunidades-del-big-data","text":"A pesar de los desaf\u00edos, Big Data abre un abanico de oportunidades en diversas industrias: Toma de Decisiones Mejorada : Basadas en an\u00e1lisis de datos en tiempo real y predictivos. Personalizaci\u00f3n y Experiencia del Cliente : Entender mejor el comportamiento del cliente para ofrecer productos y servicios m\u00e1s relevantes. Optimizaci\u00f3n de Operaciones : Mejora de la eficiencia en la cadena de suministro, log\u00edstica, mantenimiento predictivo, etc. Nuevos Productos y Servicios : Desarrollo de innovaciones basadas en la informaci\u00f3n obtenida de los datos. Detecci\u00f3n de Fraude y Riesgos : Identificaci\u00f3n de patrones an\u00f3malos que indican actividades fraudulentas o riesgos. Investigaci\u00f3n Cient\u00edfica : Avances en medicina, astronom\u00eda, climatolog\u00eda, entre otros.","title":"1.1.4 Oportunidades del Big Data:"},{"location":"tema11/#115-tipos-de-datos-en-big-data","text":"","title":"1.1.5 Tipos de Datos en Big Data:"},{"location":"tema11/#datos-estructurados","text":"Son datos que se encuentran organizados en un esquema r\u00edgido y predefinido , generalmente en forma de filas y columnas, lo que permite su almacenamiento y consulta eficiente mediante sistemas de gesti\u00f3n de bases de datos relacionales (RDBMS) como MySQL, PostgreSQL u Oracle. Est\u00e1n gobernados por modelos tabulares (normalizados o no) con tipos de datos espec\u00edficos, claves primarias/for\u00e1neas y reglas de integridad. Su estructura facilita la indexaci\u00f3n, consultas SQL, an\u00e1lisis OLAP, y migraciones entre sistemas. Tablas de clientes en una base de datos bancaria ( tipo_doc , num_doc , nombre_completo , tipo_cta , num_cta , saldo ). Registros de ventas diarias en un sistema ERP ( FechaTx , IdProducto , Cantidad , Precio ). Inventario de una tienda en formato CSV con columnas bien definidas ( ID , descripci\u00f3n , categor\u00eda , stock ). La base de datos de pacientes con campos como ID_Paciente , Nombre , Fecha_Nacimiento , Grupo_Sanguineo .","title":"Datos Estructurados"},{"location":"tema11/#datos-semi-estructurados","text":"Son datos que no se ajustan completamente a un modelo relacional tradicional , pero contienen etiquetas, delimitadores o estructuras jer\u00e1rquicas que permiten cierta organizaci\u00f3n y comprensi\u00f3n automatizada. A menudo se representan en formatos legibles por m\u00e1quinas como JSON, XML o YAML , que pueden modelar relaciones complejas (an\u00e1logas a objetos o diccionarios) y son comunes en APIs, configuraciones y flujos de integraci\u00f3n. Su an\u00e1lisis requiere herramientas capaces de interpretar dicha estructura, como motores NoSQL (MongoDB, Elasticsearch) o lenguajes con soporte nativo para parsing (Python, Java). Documentos XML que describen transacciones electr\u00f3nicas entre sistemas: facturaci\u00f3n electr\u00f3nica Logs de acceso web en formato Apache/Nginx donde cada l\u00ednea sigue una estructura repetitiva (IP, timestamp, recurso, c\u00f3digo de estado). Registros de sensores de equipos m\u00e9dicos ( monitores de signos vitales ) en formato JSON, donde cada registro puede tener diferentes campos dependiendo del tipo de sensor.","title":"Datos Semi-estructurados"},{"location":"tema11/#datos-no-estructurados","text":"Son datos que carecen de un modelo predefinido o estructura fija , lo que los hace dif\u00edciles de organizar y analizar mediante m\u00e9todos tradicionales. Representan la mayor parte del volumen de datos generados a nivel global , y suelen requerir t\u00e9cnicas avanzadas de procesamiento como an\u00e1lisis de texto (NLP), reconocimiento de im\u00e1genes, extracci\u00f3n de entidades, o clasificaci\u00f3n mediante machine learning. Su almacenamiento se realiza com\u00fanmente en sistemas distribuidos o Data Lakes. Publicaciones en redes sociales que combinan texto libre, emojis, hashtags e im\u00e1genes. Archivos de video provenientes de c\u00e1maras de seguridad sin etiquetas asociadas. Grabaciones de audio de llamadas en centros de atenci\u00f3n al cliente. Im\u00e1genes de rayos X, grabaciones de voz de consultas m\u00e9dicas, informes m\u00e9dicos en formato de texto libre.","title":"Datos No Estructurados"},{"location":"tema11/#116-modelos-de-procesamiento-de-datos","text":"","title":"1.1.6 Modelos de Procesamiento de Datos"},{"location":"tema11/#procesamiento-batch-por-lotes","text":"Es un enfoque de procesamiento de datos que consiste en acumular grandes vol\u00famenes de informaci\u00f3n durante un intervalo de tiempo determinado para luego ser procesados de forma masiva y secuencial , sin requerir intervenci\u00f3n humana durante la ejecuci\u00f3n. Este tipo de procesamiento es ideal para cargas de trabajo donde la latencia no es cr\u00edtica , y permite realizar tareas computacionalmente intensivas como agregaciones, transformaciones, limpieza y carga de datos hist\u00f3ricos. Se implementa com\u00fanmente con herramientas como Apache Hadoop, Apache Spark en modo batch, AWS Glue, o Azure Data Factory . Procesamiento nocturno de transacciones bancarias para generar extractos y actualizar balances. Carga diaria de datos hist\u00f3ricos desde un sistema transaccional a un data warehouse para an\u00e1lisis BI. Generaci\u00f3n mensual de reportes de n\u00f3mina y deducciones en una empresa.","title":"Procesamiento Batch (por Lotes)"},{"location":"tema11/#procesamiento-en-tiempo-real-streaming","text":"Este modelo se refiere al procesamiento de flujos de datos de forma continua e inmediata , a medida que los datos son generados o ingresan al sistema. Est\u00e1 dise\u00f1ado para manejar eventos de alto volumen y baja latencia, permitiendo a los sistemas reaccionar casi en tiempo real. Utiliza frameworks especializados como Apache Kafka, Apache Flink, Apache Spark Streaming, Google Dataflow o AWS Kinesis , y se aplica en contextos donde la inmediatez en la toma de decisiones es cr\u00edtica . Detecci\u00f3n de fraudes en tarjetas de cr\u00e9dito, analizando patrones de transacciones mientras ocurren. Monitoreo de infraestructura tecnol\u00f3gica (DevOps/Observabilidad) para detectar errores o picos de carga en servidores. Recomendaciones personalizadas en plataformas de streaming o e-commerce, basadas en el comportamiento del usuario en tiempo real.","title":"Procesamiento en Tiempo Real (Streaming)"},{"location":"tema11/#procesamiento-interactivo-ad-hocexploratorio","text":"Se refiere a la capacidad de los usuarios para ejecutar consultas din\u00e1micas y obtener resultados r\u00e1pidamente , con el objetivo de explorar, analizar o visualizar datos en forma directa , sin depender de procesos predefinidos o programaci\u00f3n previa. Este tipo de procesamiento requiere sistemas optimizados para baja latencia y acceso aleatorio eficiente , como motores SQL distribuidos o motores de exploraci\u00f3n columnar (por ejemplo, Presto, Trino, Google BigQuery, Snowflake, Dremio, Databricks SQL ). Es fundamental en entornos anal\u00edticos donde los cient\u00edficos o analistas de datos realizan exploraci\u00f3n iterativa. Consultas ad-hoc sobre un data lake para identificar patrones de ventas por regi\u00f3n y temporada. Exploraci\u00f3n de grandes vol\u00famenes de logs en tiempo casi real para diagnosticar errores de aplicaciones. An\u00e1lisis interactivo de cohortes de usuarios en herramientas BI como Tableau o Power BI conectadas a un motor distribuido.","title":"Procesamiento Interactivo (Ad-Hoc/Exploratorio)"},{"location":"tema11/#tarea","text":"Para consolidar tu comprensi\u00f3n sobre los fundamentos de Big Data, investiga y responde las siguientes preguntas. Documenta tus respuestas y las fuentes consultadas. Explora las \"V\" adicionales : Adem\u00e1s de Volumen, Velocidad, Variedad, Veracidad y Valor, algunos expertos proponen otras \"V\" (como Variabilidad, Visualizaci\u00f3n, Viabilidad, etc.). Elige al menos dos \"V\" adicionales y explica su relevancia en el contexto del Big Data actual. Tecnolog\u00edas emergentes para Big Data : Investiga una tecnolog\u00eda o paradigma emergente (aparte de Spark, que veremos en el siguiente tema) que est\u00e9 ganando tracci\u00f3n en el \u00e1mbito del Big Data (ej. Lakehouses, Data Meshes, Procesamiento Serverless, etc.). Describe brevemente qu\u00e9 problema resuelve y c\u00f3mo se integra con el ecosistema Big Data existente. Comparaci\u00f3n de arquitecturas Big Data : Investiga las diferencias fundamentales entre una arquitectura tradicional de Data Warehouse y una arquitectura de Data Lake. \u00bfCu\u00e1ndo ser\u00eda m\u00e1s apropiado usar una u otra, o una combinaci\u00f3n de ambas (Data Lakehouse)?","title":"Tarea"},{"location":"tema12/","text":"1. Introducci\u00f3n Tema 1.2 Introducci\u00f3n al ecosistema Spark Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo. 1.2.1 \u00bfQu\u00e9 es Apache Spark? Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos. Procesamiento en memoria Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria. Resilient Distributed Datasets (RDDs) Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave. 1.2.2 Componentes principales de Spark Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero. Spark Core Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia. Spark SQL Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional. Spark Streaming Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos. MLlib MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos. GraphX GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n. 1.2.3 Arquitectura de Spark La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva. Spark Driver El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones. Spark Executor Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos. Cluster Manager El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias. 1.2.4 Interacci\u00f3n con Spark La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente. Spark Shell El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande. Spark Submit spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL. Spark UI La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria. 1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas. Particionamiento de datos El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes. Persistencia de datos (Caching) La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido. Lazy Evaluation (Evaluaci\u00f3n Perezosa) La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia. 1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas. Spark vs. Hadoop MapReduce Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente. Tarea Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema12/#tema-12-introduccion-al-ecosistema-spark","text":"Objetivo : Comprender la arquitectura, componentes principales y modos de operaci\u00f3n de Apache Spark, as\u00ed como sus ventajas y casos de uso en el procesamiento de Big Data, para sentar las bases de su aplicaci\u00f3n pr\u00e1ctica. Introducci\u00f3n : En la era del Big Data, el procesamiento eficiente de grandes vol\u00famenes de informaci\u00f3n es crucial. Apache Spark ha emergido como una de las herramientas m\u00e1s potentes y vers\u00e1tiles para esta tarea. Este tema proporcionar\u00e1 una visi\u00f3n integral del ecosistema Spark, desde su concepci\u00f3n hasta sus principales componentes y c\u00f3mo interact\u00faa con otras tecnolog\u00edas del Big Data, preparando el terreno para un uso efectivo en el an\u00e1lisis y la ingenier\u00eda de datos. Desarrollo : Apache Spark es un motor unificado de an\u00e1lisis para el procesamiento de datos a gran escala. A diferencia de sus predecesores, como Hadoop MapReduce, Spark se distingue por su capacidad para procesar datos en memoria, lo que resulta en una velocidad significativamente mayor. Su dise\u00f1o modular y extensible permite manejar una amplia variedad de cargas de trabajo, desde el procesamiento por lotes hasta el an\u00e1lisis en tiempo real, aprendizaje autom\u00e1tico y procesamiento de grafos, todo ello dentro de un \u00fanico ecosistema cohesivo.","title":"Tema 1.2 Introducci\u00f3n al ecosistema Spark"},{"location":"tema12/#121-que-es-apache-spark","text":"Apache Spark es un motor de procesamiento de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para el an\u00e1lisis r\u00e1pido de grandes vol\u00famenes de datos. Se desarroll\u00f3 en la Universidad de California, Berkeley, en el AMPLab, y fue donado a la Apache Software Foundation. Su principal fortaleza radica en su capacidad para realizar operaciones en memoria, lo que lo hace considerablemente m\u00e1s r\u00e1pido que otros sistemas de procesamiento distribuido que dependen en gran medida del disco, como Hadoop MapReduce. Spark est\u00e1 optimizado para flujos de trabajo iterativos y consultas interactivas, lo que lo convierte en una opci\u00f3n ideal para Machine Learning y an\u00e1lisis de datos complejos.","title":"1.2.1 \u00bfQu\u00e9 es Apache Spark?"},{"location":"tema12/#procesamiento-en-memoria","text":"Se refiere a la capacidad de Apache Spark para retener los datos en la memoria RAM de los nodos del cl\u00faster mientras realiza operaciones, en lugar de escribirlos y leerlos repetidamente del disco. Esta caracter\u00edstica es la principal raz\u00f3n de la alta velocidad de Spark, ya que el acceso a la memoria es \u00f3rdenes de magnitud m\u00e1s r\u00e1pido que el acceso al disco. Permite operaciones iterativas r\u00e1pidas, algo esencial para algoritmos de Machine Learning y operaciones ETL complejas que requieren m\u00faltiples pasadas sobre los mismos datos. Un algoritmo de Machine Learning que requiere varias iteraciones para optimizar un modelo. Spark mantiene los datos en memoria a trav\u00e9s de las iteraciones, evitando la sobrecarga de I/O de disco. Un proceso de ETL (Extracci\u00f3n, Transformaci\u00f3n, Carga) que realiza m\u00faltiples transformaciones sobre un conjunto de datos. En lugar de guardar resultados intermedios en disco, Spark los gestiona en memoria. Consultas interactivas en un data lake . Los analistas pueden ejecutar consultas complejas con tiempos de respuesta muy bajos, ya que los datos relevantes pueden ser cacheados en memoria.","title":"Procesamiento en memoria"},{"location":"tema12/#resilient-distributed-datasets-rdds","text":"Los RDDs son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Son la abstracci\u00f3n de datos principal en Spark Core. Un RDD es una colecci\u00f3n inmutable y particionada de registros que se puede operar en paralelo. Los RDDs pueden ser creados a partir de fuentes externas (como HDFS o S3) o de colecciones existentes en Scala, Python o Java. Son \"resilientes\" porque pueden reconstruirse autom\u00e1ticamente en caso de fallo de un nodo, y \"distribuidos\" porque se extienden a trav\u00e9s de m\u00faltiples nodos en un cl\u00faster. Cargar un archivo CSV grande de 1 TB desde HDFS en un RDD para su procesamiento. Realizar una operaci\u00f3n map sobre un RDD para transformar cada elemento (ej: convertir una cadena a un n\u00famero entero). Aplicar una operaci\u00f3n reduceByKey a un RDD de pares clave-valor para sumar los valores por cada clave.","title":"Resilient Distributed Datasets (RDDs)"},{"location":"tema12/#122-componentes-principales-de-spark","text":"Spark no es una herramienta monol\u00edtica; es un ecosistema compuesto por varios m\u00f3dulos integrados que extienden sus capacidades. Estos componentes se construyen sobre Spark Core y proporcionan APIs de alto nivel para diferentes tipos de procesamiento de datos, lo que permite a los desarrolladores elegir la herramienta adecuada para su tarea sin tener que manejar las complejidades del procesamiento distribuido desde cero.","title":"1.2.2 Componentes principales de Spark"},{"location":"tema12/#spark-core","text":"Es el motor subyacente de todas las funcionalidades de Spark. Proporciona la funcionalidad b\u00e1sica de E/S, la planificaci\u00f3n de tareas y la gesti\u00f3n de memoria. La abstracci\u00f3n fundamental de Spark Core es el RDD, que permite operaciones de procesamiento distribuido. Es la base sobre la cual se construyen todos los dem\u00e1s componentes de Spark. Leer un conjunto de datos sin estructura espec\u00edfica (por ejemplo, logs de servidores) y aplicar transformaciones b\u00e1sicas con RDDs. Realizar operaciones de bajo nivel y personalizadas que no est\u00e1n f\u00e1cilmente disponibles en las APIs de alto nivel (como Spark SQL). Implementar un algoritmo de procesamiento de datos altamente espec\u00edfico donde se necesita control granular sobre las operaciones de particionamiento y persistencia.","title":"Spark Core"},{"location":"tema12/#spark-sql","text":"Spark SQL es un m\u00f3dulo para trabajar con datos estructurados y semiestructurados. Proporciona una interfaz de programaci\u00f3n unificada para ejecutar consultas SQL y operaciones de manipulaci\u00f3n de datos sobre estructuras como DataFrames y Datasets. Permite integrar c\u00f3digo Spark con consultas SQL, facilitando la interacci\u00f3n con bases de datos relacionales, Hive, JSON, Parquet, etc. Su optimizador Catalyst es clave para su alto rendimiento. Cargar un archivo Parquet en un DataFrame y ejecutar una consulta SQL est\u00e1ndar como SELECT * FROM tabla WHERE columna > 100 . Unir dos DataFrames basados en una clave com\u00fan para combinar informaci\u00f3n de diferentes fuentes de datos. Leer un conjunto de datos JSON y transformarlo en un DataFrame para luego exportarlo a una base de datos relacional.","title":"Spark SQL"},{"location":"tema12/#spark-streaming","text":"Spark Streaming es una extensi\u00f3n de la API principal de Spark que permite el procesamiento de flujos de datos en tiempo real. Recibe flujos de datos de diversas fuentes (Kafka, Flume, Kinesis, TCP Sockets, etc.) y los divide en peque\u00f1os lotes que luego son procesados por el motor Spark Core. Esto permite aplicar las mismas transformaciones de datos que se usan para el procesamiento por lotes a los datos en tiempo real. Analizar el clickstream de un sitio web en tiempo real para detectar patrones de navegaci\u00f3n o anomal\u00edas. Monitorear datos de sensores de IoT para detectar fallos o eventos cr\u00edticos instant\u00e1neamente. Procesar mensajes de Twitter en vivo para realizar an\u00e1lisis de sentimiento sobre temas espec\u00edficos.","title":"Spark Streaming"},{"location":"tema12/#mllib","text":"MLlib es la biblioteca de aprendizaje autom\u00e1tico de Spark. Proporciona una colecci\u00f3n de algoritmos de Machine Learning de alto rendimiento y escalables, como clasificaci\u00f3n, regresi\u00f3n, clustering, filtrado colaborativo, entre otros. Est\u00e1 dise\u00f1ada para integrarse perfectamente con los DataFrames de Spark SQL, lo que permite a los usuarios construir pipelines de ML complejos. Entrenar un modelo de clasificaci\u00f3n para predecir si un cliente abandonar\u00e1 un servicio (churn prediction) usando datos de transacciones. Aplicar un algoritmo de clustering para segmentar clientes basado en su comportamiento de compra. Construir un sistema de recomendaci\u00f3n de productos utilizando datos de interacciones de usuarios con art\u00edculos.","title":"MLlib"},{"location":"tema12/#graphx","text":"GraphX es la API de Spark para el procesamiento de grafos y el c\u00e1lculo de grafos en paralelo. Combina las propiedades de los RDDs de Spark con las operaciones de grafos para proporcionar un marco flexible y eficiente para trabajar con estructuras de datos de grafos. Permite construir y manipular grafos, y ejecutar algoritmos de grafos como PageRank o Connected Components. Calcular el PageRank de nodos en una red social para identificar los usuarios m\u00e1s influyentes. Identificar las conexiones m\u00e1s cortas entre dos puntos en una red de transporte. Detectar comunidades o grupos de usuarios en una red de colaboraci\u00f3n.","title":"GraphX"},{"location":"tema12/#123-arquitectura-de-spark","text":"La arquitectura de Spark es clave para su capacidad de procesamiento distribuido y tolerancia a fallos. Se basa en un modelo maestro-esclavo, donde un Driver coordina las operaciones entre los Executors distribuidos en el cl\u00faster, con la ayuda de un Cluster Manager . Entender estos roles es fundamental para desplegar y gestionar aplicaciones Spark de manera efectiva.","title":"1.2.3 Arquitectura de Spark"},{"location":"tema12/#spark-driver","text":"El Spark Driver es el programa principal que coordina y gestiona la ejecuci\u00f3n de una aplicaci\u00f3n Spark. Contiene el main de la aplicaci\u00f3n Spark y crea el SparkContext (o SparkSession en versiones m\u00e1s recientes). El Driver es responsable de convertir el c\u00f3digo de la aplicaci\u00f3n Spark en una serie de tareas, programarlas en los Executors y monitorear su ejecuci\u00f3n. Es el punto de entrada para cualquier aplicaci\u00f3n Spark. Un programa Python que inicializa una SparkSession , lee un archivo CSV y ejecuta algunas transformaciones de datos. El Driver se encarga de dividir el trabajo y enviarlo a los Executors. Cuando se utiliza spark-submit para lanzar una aplicaci\u00f3n, el comando invoca el Driver en el nodo especificado (o en el cluster manager ). En un Jupyter Notebook con un kernel Spark, el Driver se ejecuta en el proceso del notebook o en un nodo configurado, orquestando todas las operaciones.","title":"Spark Driver"},{"location":"tema12/#spark-executor","text":"Un Spark Executor es un proceso que se ejecuta en los nodos worker del cl\u00faster de Spark. Son responsables de ejecutar las tareas individuales asignadas por el Driver y de almacenar los datos que se cachean o se persisten. Cada Executor tiene un cierto n\u00famero de cores y una cantidad de memoria RAM asignada para ejecutar tareas en paralelo y almacenar datos. Un Executor recibe una tarea del Driver para filtrar un subconjunto de filas de un DataFrame. M\u00faltiples Executors procesan diferentes particiones del mismo RDD en paralelo. Un Executor almacena en cach\u00e9 una porci\u00f3n de un DataFrame en su memoria local para acelerar futuras operaciones sobre esos datos.","title":"Spark Executor"},{"location":"tema12/#cluster-manager","text":"El Cluster Manager es el componente responsable de asignar recursos del cl\u00faster (CPU, memoria) a la aplicaci\u00f3n Spark. Act\u00faa como intermediario entre el Driver y los Executors, gestionando la asignaci\u00f3n de nodos y la disponibilidad de recursos. Spark puede trabajar con varios tipos de Cluster Managers . YARN (Yet Another Resource Negotiator) : Es el Cluster Manager m\u00e1s com\u00fan en entornos Hadoop. Spark lo utiliza para solicitar recursos en un cl\u00faster Hadoop existente. Apache Mesos : Un gestor de recursos de prop\u00f3sito general que puede ejecutar Spark junto con otras aplicaciones distribuidas. Spark Standalone : El propio Cluster Manager de Spark, ideal para entornos de desarrollo y pruebas o cl\u00fasteres dedicados a Spark sin otras dependencias.","title":"Cluster Manager"},{"location":"tema12/#124-interaccion-con-spark","text":"La interacci\u00f3n con Apache Spark puede realizarse de diversas maneras, dependiendo del prop\u00f3sito, ya sea para desarrollo interactivo, ejecuci\u00f3n de trabajos programados o monitoreo. Comprender c\u00f3mo interactuar con Spark permite a los desarrolladores y operadores gestionar sus aplicaciones de forma eficiente.","title":"1.2.4 Interacci\u00f3n con Spark"},{"location":"tema12/#spark-shell","text":"El Spark Shell es una herramienta interactiva basada en la consola que permite a los usuarios experimentar con Spark directamente. Proporciona un entorno REPL (Read-Eval-Print Loop) donde se pueden escribir y ejecutar comandos Spark en Scala, Python o R. Es ideal para prototipado, pruebas r\u00e1pidas y exploraci\u00f3n de datos. Iniciar pyspark en la terminal para abrir el Spark Shell con soporte para Python. Escribir sc.parallelize([1, 2, 3]).map(lambda x: x*2).collect() en el Spark Shell para ver el resultado de una operaci\u00f3n simple. Probar la lectura de un archivo de datos peque\u00f1o y las primeras transformaciones antes de integrarlas en un script m\u00e1s grande.","title":"Spark Shell"},{"location":"tema12/#spark-submit","text":"spark-submit es el comando de l\u00ednea de comandos principal utilizado para enviar aplicaciones Spark (escritas en Scala, Java, Python o R) a un cl\u00faster Spark. Permite especificar la ubicaci\u00f3n del c\u00f3digo de la aplicaci\u00f3n, los recursos a asignar (memoria del driver, memoria de los executors, n\u00famero de cores, etc.) y el Cluster Manager a utilizar. Es la forma est\u00e1ndar de ejecutar trabajos Spark en producci\u00f3n. spark-submit --class com.example.MyApp --master yarn --deploy-mode cluster myapp.jar para enviar una aplicaci\u00f3n Java/Scala a un cl\u00faster YARN. spark-submit --master local[*] my_python_script.py para ejecutar un script Python en modo local (\u00fatil para desarrollo y pruebas en una sola m\u00e1quina). spark-submit --driver-memory 4g --executor-memory 8g --num-executors 10 my_etl_job.py para asignar recursos espec\u00edficos a una aplicaci\u00f3n ETL.","title":"Spark Submit"},{"location":"tema12/#spark-ui","text":"La Spark UI (User Interface) es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Permite a los usuarios ver el estado de los trabajos, las etapas, las tareas, el consumo de memoria de los Executors, los logs y otra informaci\u00f3n detallada sobre la ejecuci\u00f3n de la aplicaci\u00f3n. Es una herramienta invaluable para depurar, optimizar y comprender el rendimiento de las aplicaciones Spark. Acceder a http://localhost:4040 (o la direcci\u00f3n IP y puerto correspondientes) mientras una aplicaci\u00f3n Spark se est\u00e1 ejecutando para ver los DAGs de las etapas. Inspeccionar la pesta\u00f1a \"Stages\" para identificar qu\u00e9 partes de un trabajo est\u00e1n tardando m\u00e1s en ejecutarse o si hay skew en los datos (desequilibrio de carga). Revisar los logs de los Executors en la pesta\u00f1a \"Executors\" para diagnosticar errores o problemas de memoria.","title":"Spark UI"},{"location":"tema12/#125-conceptos-fundamentales-de-procesamiento-distribuido-en-spark","text":"El procesamiento distribuido en Spark se basa en varios conceptos clave que optimizan el rendimiento y la tolerancia a fallos. Entender c\u00f3mo Spark maneja la partici\u00f3n de datos, la persistencia y la evaluaci\u00f3n perezosa es crucial para escribir aplicaciones eficientes y robustas.","title":"1.2.5 Conceptos fundamentales de procesamiento distribuido en Spark"},{"location":"tema12/#particionamiento-de-datos","text":"El particionamiento de datos en Spark se refiere a c\u00f3mo los datos se dividen y se distribuyen entre los nodos de un cl\u00faster. Cada partici\u00f3n de un RDD o DataFrame es un conjunto l\u00f3gico de datos que puede ser procesado por una tarea individual en un Executor. El n\u00famero y la estrategia de particionamiento afectan directamente el paralelismo, la eficiencia de las operaciones de shuffle (reorganizaci\u00f3n de datos entre nodos) y el rendimiento general de la aplicaci\u00f3n. Al leer un archivo de texto grande, Spark lo divide autom\u00e1ticamente en particiones basadas en el tama\u00f1o de bloque del sistema de archivos subyacente (ej. HDFS). Despu\u00e9s de una operaci\u00f3n como groupByKey o join , Spark puede necesitar re-particionar los datos (esto se conoce como shuffle ) para asegurar que los datos relacionados est\u00e9n en el mismo nodo. Un desarrollador puede especificar el n\u00famero de particiones manualmente ( repartition o coalesce ) para optimizar el rendimiento, por ejemplo, para evitar demasiadas particiones peque\u00f1as o muy pocas particiones grandes.","title":"Particionamiento de datos"},{"location":"tema12/#persistencia-de-datos-caching","text":"La persistencia de datos o caching en Spark es la capacidad de almacenar en memoria o en disco los RDDs o DataFrames intermedios para acelerar futuras operaciones sobre ellos. Cuando se marca un RDD/DataFrame para persistencia, Spark intenta mantener sus particiones en la memoria RAM de los Executors. Esto es especialmente \u00fatil para flujos de trabajo iterativos o cuando se accede repetidamente al mismo conjunto de datos. Marcar un DataFrame como df.cache() despu\u00e9s de una costosa operaci\u00f3n de carga y limpieza, antes de ejecutar m\u00faltiples consultas sobre \u00e9l. En un algoritmo de Machine Learning iterativo, el conjunto de datos de entrenamiento se persiste ( persist(StorageLevel.MEMORY_AND_DISK) ) para evitar recalcularlo en cada iteraci\u00f3n. Un conjunto de datos de referencia (ej. una tabla de c\u00f3digos postales) que se une frecuentemente con otros DataFrames se puede persistir para un acceso r\u00e1pido.","title":"Persistencia de datos (Caching)"},{"location":"tema12/#lazy-evaluation-evaluacion-perezosa","text":"La Evaluaci\u00f3n Perezosa es un concepto fundamental en Spark que significa que las transformaciones (operaciones que producen un nuevo RDD/DataFrame a partir de uno existente, como map , filter , join ) no se ejecutan inmediatamente cuando se invocan. En su lugar, Spark construye un plan l\u00f3gico de las operaciones. La ejecuci\u00f3n real de estas transformaciones solo ocurre cuando se invoca una acci\u00f3n (operaci\u00f3n que devuelve un valor al Driver o escribe datos en un sistema externo, como count , collect , saveAsTextFile ). Cuando se escribe df.filter(\"edad > 30\").select(\"nombre\") , Spark no procesa los datos en ese instante; solo registra estas transformaciones en su plan. La ejecuci\u00f3n real del c\u00f3digo del ejemplo anterior solo se dispara cuando se a\u00f1ade una acci\u00f3n como .show() o .count() . La evaluaci\u00f3n perezosa permite a Spark optimizar el plan de ejecuci\u00f3n completo (DAG) antes de ejecutar cualquier c\u00e1lculo, eliminando operaciones innecesarias o reorden\u00e1ndolas para una mayor eficiencia.","title":"Lazy Evaluation (Evaluaci\u00f3n Perezosa)"},{"location":"tema12/#126-comparacion-de-spark-con-otras-herramientas-big-data","text":"Apache Spark, aunque muy potente, no es una soluci\u00f3n aislada. Se integra y a menudo complementa a otras herramientas en el ecosistema Big Data. Comprender su posici\u00f3n y c\u00f3mo se compara con otras soluciones es crucial para tomar decisiones arquitect\u00f3nicas informadas.","title":"1.2.6 Comparaci\u00f3n de Spark con otras herramientas Big Data"},{"location":"tema12/#spark-vs-hadoop-mapreduce","text":"Hadoop MapReduce es el motor de procesamiento original del ecosistema Hadoop. Opera en un modelo de dos fases (map y reduce), escribiendo resultados intermedios en disco. Spark , por otro lado, puede realizar operaciones multipase en memoria y ofrece una API m\u00e1s flexible. Spark es generalmente m\u00e1s r\u00e1pido para cargas de trabajo iterativas y para el procesamiento de datos en tiempo real, mientras que MapReduce puede ser adecuado para procesamientos por lotes masivos que no requieren mucha interacci\u00f3n o iteraciones. Para un proceso de ETL que involucra m\u00faltiples pasos de transformaci\u00f3n y limpieza de datos (ej. filter -> join -> groupBy ), Spark es significativamente m\u00e1s eficiente que MapReduce debido a su procesamiento en memoria. Un algoritmo de PageRank o K-Means que requiere muchas iteraciones sobre el mismo conjunto de datos se ejecuta mucho m\u00e1s r\u00e1pido en Spark. Para un an\u00e1lisis de datos que solo implica una operaci\u00f3n de conteo masiva y una sola pasada (ej. word count en archivos muy grandes), MapReduce podr\u00eda ser suficiente, aunque Spark tambi\u00e9n lo manejar\u00eda eficientemente.","title":"Spark vs. Hadoop MapReduce"},{"location":"tema12/#tarea","text":"Busca un ejemplo de c\u00f3digo en Python o Scala donde se utilice persist() con diferentes StorageLevel (por ejemplo, MEMORY_ONLY , DISK_ONLY , MEMORY_AND_DISK ) y explica cu\u00e1ndo ser\u00eda apropiado usar cada uno. Compara la resiliencia de los RDDs en Spark con la tolerancia a fallos en Hadoop HDFS . \u00bfCu\u00e1les son las similitudes y diferencias clave en c\u00f3mo manejan la p\u00e9rdida de datos o nodos? Identifica dos escenarios de negocio donde Spark Streaming ser\u00eda la soluci\u00f3n ideal y justifica por qu\u00e9.","title":"Tarea"},{"location":"tema13/","text":"1. Introducci\u00f3n Tema 1.3 RDD, DataFrame y Dataset Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas. 1.3.1 RDD (Resilient Distributed Datasets) Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje. Caracter\u00edsticas clave de los RDDs Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark. Operaciones de transformaci\u00f3n Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo. Operaciones de acci\u00f3n Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido. 1.3.2 DataFrame Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados. Ventajas sobre los RDDs Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos. Creaci\u00f3n y manipulaci\u00f3n de DataFrames Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) . 1.3.3 Dataset El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark. Seguridad de tipos (Type-safety) La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador. Encoders Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado. 1.3.4 Comparaci\u00f3n y Casos de Uso La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n. Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir. Tarea Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"RDD, DataFrame y Dataset"},{"location":"tema13/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema13/#tema-13-rdd-dataframe-y-dataset","text":"Objetivo : Comprender las diferencias, ventajas y casos de uso de las principales abstracciones de datos en Apache Spark: RDD, DataFrame y Dataset, permitiendo a los estudiantes seleccionar la herramienta adecuada para diversas tareas de procesamiento de datos. Introducci\u00f3n : Apache Spark ofrece diferentes abstracciones para trabajar con datos, cada una con sus propias caracter\u00edsticas y optimizaciones. Los Resilient Distributed Datasets (RDDs) fueron la abstracci\u00f3n original, proporcionando un control de bajo nivel. Posteriormente, surgieron los DataFrames para manejar datos estructurados y semiestructurados con optimizaciones de rendimiento significativas. Finalmente, los Datasets combinaron las ventajas de ambos, ofreciendo seguridad de tipos y las optimizaciones de los DataFrames. Dominar estas tres abstracciones es fundamental para explotar todo el potencial de Spark en el procesamiento de Big Data. Desarrollo : En este tema, exploraremos en detalle RDDs, DataFrames y Datasets, las tres principales formas de representar y manipular datos en Apache Spark. Cada una representa un paso evolutivo en la API de Spark, dise\u00f1ada para mejorar la facilidad de uso, el rendimiento y la seguridad de tipos. Analizaremos sus caracter\u00edsticas distintivas, c\u00f3mo interact\u00faan entre s\u00ed y en qu\u00e9 escenarios es m\u00e1s apropiado utilizar cada una, lo que te permitir\u00e1 construir aplicaciones Spark m\u00e1s eficientes y robustas.","title":"Tema 1.3 RDD, DataFrame y Dataset"},{"location":"tema13/#131-rdd-resilient-distributed-datasets","text":"Los RDDs (Resilient Distributed Datasets) son la colecci\u00f3n fundamental de elementos tolerantes a fallos en Spark que se ejecutan en paralelo. Fueron la abstracci\u00f3n de datos original de Spark y representan una colecci\u00f3n inmutable y particionada de registros. Los RDDs pueden ser creados a partir de fuentes de datos externas (como HDFS, S3, HBASE) o a partir de colecciones existentes en lenguajes de programaci\u00f3n como Scala, Python o Java. Su principal fortaleza radica en su naturaleza inmutable y en la capacidad de Spark para reconstruirlos autom\u00e1ticamente en caso de fallos de nodos, gracias a su mecanismo de linaje.","title":"1.3.1 RDD (Resilient Distributed Datasets)"},{"location":"tema13/#caracteristicas-clave-de-los-rdds","text":"Los RDDs son fundamentalmente colecciones de objetos inmutables distribuidas entre un cl\u00faster. Son \"resilientes\" porque pueden recuperarse de fallos reconstruyendo sus particiones a partir de las operaciones que los generaron (su linaje). Son \"distribuidos\" porque sus datos se reparten entre m\u00faltiples nodos, permitiendo el procesamiento en paralelo. Ofrecen una API de bajo nivel, lo que da un control granular sobre las operaciones de transformaci\u00f3n y acci\u00f3n, pero carecen de informaci\u00f3n de esquema inherente, lo que puede limitar las optimizaciones de Spark. Procesamiento de archivos de log sin formato donde cada l\u00ednea es un string y no se conoce una estructura fija. Implementaci\u00f3n de algoritmos de Machine Learning personalizados que requieren control detallado sobre las estructuras de datos y el procesamiento por bloques. Trabajar con datos binarios complejos o formatos propietarios para los cuales no existen parsers o esquemas predefinidos en Spark.","title":"Caracter\u00edsticas clave de los RDDs"},{"location":"tema13/#operaciones-de-transformacion","text":"Las transformaciones en RDDs son operaciones que crean un nuevo RDD a partir de uno existente. Son de naturaleza lazy (perezosa), lo que significa que no se ejecutan inmediatamente. En su lugar, Spark registra la transformaci\u00f3n en un linaje o DAG (Directed Acyclic Graph) de operaciones. Esto permite a Spark optimizar el plan de ejecuci\u00f3n antes de realizar cualquier c\u00e1lculo real. Algunos ejemplos comunes incluyen map , filter , flatMap , union , groupByKey . Usar rdd.map(lambda x: x.upper()) para convertir todas las cadenas de texto en un RDD a may\u00fasculas. Utilizar rdd.filter(lambda x: \"error\" in x) para seleccionar solo las l\u00edneas de un log que contienen la palabra \"error\". Aplicar rdd1.union(rdd2) para combinar dos RDDs en uno solo.","title":"Operaciones de transformaci\u00f3n"},{"location":"tema13/#operaciones-de-accion","text":"Las acciones en RDDs son operaciones que disparan la ejecuci\u00f3n de las transformaciones y devuelven un resultado al programa Driver o escriben datos en un sistema de almacenamiento externo. A diferencia de las transformaciones, las acciones son eager (\u00e1vidas), lo que significa que fuerzan la evaluaci\u00f3n del DAG de transformaciones. Ejemplos incluyen collect , count , reduce , saveAsTextFile , foreach . Usar rdd.collect() para obtener todos los elementos del RDD como una lista en el programa Driver (tener cuidado con RDDs muy grandes). Aplicar rdd.count() para obtener el n\u00famero de elementos en el RDD. Utilizar rdd.saveAsTextFile(\"ruta/salida\") para escribir el contenido del RDD en un archivo de texto en el sistema de archivos distribuido.","title":"Operaciones de acci\u00f3n"},{"location":"tema13/#132-dataframe","text":"Un DataFrame en Apache Spark es una colecci\u00f3n distribuida de datos organizada en columnas con nombre. Se puede pensar en un DataFrame como una tabla en una base de datos relacional o una tabla en R/Python, pero con la capacidad de escalar a terabytes de datos en un cl\u00faster. A diferencia de los RDDs, los DataFrames tienen un esquema (estructura) definido, lo que permite a Spark realizar optimizaciones de rendimiento significativas a trav\u00e9s de su optimizador Catalyst. Los DataFrames son la interfaz de programaci\u00f3n preferida para la mayor\u00eda de los casos de uso de Spark, especialmente cuando se trabaja con datos estructurados y semiestructurados.","title":"1.3.2 DataFrame"},{"location":"tema13/#ventajas-sobre-los-rdds","text":"Los DataFrames ofrecen varias ventajas clave sobre los RDDs, principalmente debido a su conocimiento del esquema de los datos. Esto permite optimizaciones de rendimiento a nivel de motor, una sintaxis m\u00e1s expresiva y familiar para usuarios de SQL o Pandas, y una mejor interoperabilidad con diferentes fuentes de datos y herramientas de an\u00e1lisis. Spark puede optimizar autom\u00e1ticamente las operaciones de un DataFrame (por ejemplo, el orden de los filtros o joins ) usando el optimizador Catalyst , algo que no es posible con los RDDs. La sintaxis de los DataFrames es mucho m\u00e1s intuitiva y menos propensa a errores que las operaciones de bajo nivel de los RDDs, especialmente para tareas comunes como filtrar, seleccionar columnas o agregar datos. Los DataFrames permiten ejecutar consultas SQL directamente sobre ellos, facilitando la integraci\u00f3n con herramientas de BI y la familiaridad para usuarios de bases de datos.","title":"Ventajas sobre los RDDs"},{"location":"tema13/#creacion-y-manipulacion-de-dataframes","text":"Los DataFrames se pueden crear a partir de una amplia variedad de fuentes de datos, incluyendo archivos CSV, JSON, Parquet, Hive tables, bases de datos JDBC, e incluso RDDs existentes. Una vez creados, Spark ofrece una API rica y expresiva para manipularlos, ya sea a trav\u00e9s de un DSL (Domain Specific Language) con funciones de alto nivel o mediante consultas SQL. Cargar un archivo Parquet en un DataFrame: spark.read.parquet(\"ruta/a/archivo.parquet\") . Seleccionar columnas y filtrar filas: df.select(\"nombre\", \"edad\").filter(df.edad > 30) . Realizar una agregaci\u00f3n: df.groupBy(\"departamento\").agg(avg(\"salario\").alias(\"salario_promedio\")) .","title":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames"},{"location":"tema13/#133-dataset","text":"El Dataset API fue introducido en Spark 1.6 como un intento de proporcionar lo mejor de ambos mundos: la eficiencia y optimizaciones de rendimiento de los DataFrames, junto con la seguridad de tipos y la capacidad de usar funciones lambda que caracterizan a los RDDs. Los Datasets son fuertemente tipados, lo que significa que los errores relacionados con el tipo de datos pueden detectarse en tiempo de compilaci\u00f3n (solo en Scala y Java), en lugar de en tiempo de ejecuci\u00f3n, lo que lleva a un c\u00f3digo m\u00e1s robusto. En esencia, un DataFrame es un Dataset[Row] , donde Row es un tipo gen\u00e9rico y no tiene seguridad de tipos en tiempo de compilaci\u00f3n. Los Datasets requieren un Encoder para serializar y deserializar los objetos entre el formato de JVM y el formato binario interno de Spark.","title":"1.3.3 Dataset"},{"location":"tema13/#seguridad-de-tipos-type-safety","text":"La seguridad de tipos es la principal ventaja de los Datasets sobre los DataFrames para los usuarios de Scala y Java. Permite a los desarrolladores trabajar con objetos fuertemente tipados, lo que significa que el compilador puede verificar los tipos de datos y detectar errores en tiempo de compilaci\u00f3n. Esto reduce la posibilidad de errores en tiempo de ejecuci\u00f3n que podr\u00edan surgir al intentar acceder a campos inexistentes o realizar operaciones con tipos incompatibles, algo com\u00fan con DataFrames (donde tales errores solo se manifiestan al ejecutar el c\u00f3digo). En Scala, si se tiene un Dataset[Person] , donde Person es una case class con campos name y age , el compilador detectar\u00e1 un error si se intenta acceder a person.address si address no es un campo de la clase Person . Al realizar transformaciones en un Dataset[Product] , las operaciones se aplican directamente sobre los objetos Product , aprovechando la autocompletaci\u00f3n y las verificaciones del IDE. La refactorizaci\u00f3n de c\u00f3digo es m\u00e1s segura y sencilla con Datasets, ya que los cambios en el esquema se detectan de inmediato por el compilador.","title":"Seguridad de tipos (Type-safety)"},{"location":"tema13/#encoders","text":"Los Encoders son un mecanismo de serializaci\u00f3n que Spark utiliza para convertir objetos de JVM (como las case classes de Scala o los POJOs de Java) en el formato binario interno de Spark (formato Tungsten) y viceversa. Los Encoders son m\u00e1s eficientes que la serializaci\u00f3n de Java u otros mecanismos porque generan c\u00f3digo para serializar y deserializar datos de forma compacta y r\u00e1pida, permitiendo a Spark realizar operaciones directamente sobre el formato binario optimizado, lo que contribuye a las mejoras de rendimiento de los Datasets. Al crear un Dataset[Long] , Spark utiliza un Encoder optimizado para los tipos Long , que sabe c\u00f3mo representar y operar sobre estos n\u00fameros de manera eficiente en formato binario. Si tienes una case class Person(name: String, age: Int) , el Encoder para Person sabr\u00e1 c\u00f3mo convertir una lista de objetos Person en un formato de columnas de Spark y viceversa. Los Encoders permiten que las operaciones de Datasets sean tan eficientes como las de DataFrames, ya que ambos utilizan el mismo formato de almacenamiento y motor de ejecuci\u00f3n optimizado.","title":"Encoders"},{"location":"tema13/#134-comparacion-y-casos-de-uso","text":"La elecci\u00f3n entre RDD, DataFrame y Dataset depende en gran medida del tipo de datos con el que se est\u00e1 trabajando, las necesidades de rendimiento, la seguridad de tipos deseada y el lenguaje de programaci\u00f3n que se utiliza. Aunque las APIs de DataFrame y Dataset son las m\u00e1s recomendadas para la mayor\u00eda de los casos de uso modernos, entender las capacidades de los RDDs sigue siendo importante, especialmente para escenarios de bajo nivel o depuraci\u00f3n.","title":"1.3.4 Comparaci\u00f3n y Casos de Uso"},{"location":"tema13/#tabla-comparativa-detallada-rdd-vs-dataframe-vs-dataset","text":"Caracter\u00edstica RDD DataFrame Dataset Abstracci\u00f3n Colecci\u00f3n distribuida de objetos Colecci\u00f3n distribuida de Row objetos con esquema Colecci\u00f3n distribuida de objetos fuertemente tipados con esquema Optimizaci\u00f3n Manual (sin optimizador) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Catalyst Optimizer (optimizaci\u00f3n autom\u00e1tica) Seguridad de Tipos No (colecci\u00f3n de Object ) No (en tiempo de compilaci\u00f3n, Row es gen\u00e9rico) S\u00ed (en tiempo de compilaci\u00f3n, para Scala/Java) API Bajo nivel, funcional Alto nivel, SQL-like (DSL, SQL) Alto nivel, funcional y SQL-like Serializaci\u00f3n Java Serialization / Kryo Tungsten (binario optimizado) Tungsten (binario optimizado con Encoders) Rendimiento Bueno, pero puede ser menor que DataFrame/Dataset Alto (optimizaci\u00f3n autom\u00e1tica) Alto (optimizaci\u00f3n autom\u00e1tica + Encoders) Lenguajes Scala, Java, Python, R Scala, Java, Python, R Scala, Java (principalmente) Mutabilidad Inmutable Inmutable Inmutable Para un an\u00e1lisis de logs complejos y no estructurados donde necesitas un control muy granular sobre cada l\u00ednea y las operaciones de bajo nivel, los RDDs son la opci\u00f3n adecuada. Para consultas anal\u00edticas sobre datos de ventas estructurados almacenados en Parquet, donde se busca eficiencia y facilidad de uso con sintaxis SQL, los DataFrames son la mejor elecci\u00f3n. Si est\u00e1s desarrollando una aplicaci\u00f3n de procesamiento de datos en Scala o Java que requiere la m\u00e1xima seguridad de tipos en tiempo de compilaci\u00f3n y las optimizaciones de rendimiento de Spark, un Dataset es el camino a seguir.","title":"Tabla comparativa detallada: RDD vs. DataFrame vs. Dataset"},{"location":"tema13/#tarea","text":"Explica la diferencia entre una transformaci\u00f3n y una acci\u00f3n en Spark. Proporciona un ejemplo de c\u00f3digo para cada una y describe c\u00f3mo la evaluaci\u00f3n perezosa afecta su ejecuci\u00f3n. Considera un escenario donde tienes una tabla de clientes y otra de pedidos. Describe c\u00f3mo usar\u00edas DataFrames para unir estas dos tablas y calcular el monto total de pedidos por cliente, utilizando tanto la API DSL como una consulta SQL. Imagina que est\u00e1s depurando una aplicaci\u00f3n Spark y notas que un RDD particular se est\u00e1 recalculando varias veces. \u00bfC\u00f3mo usar\u00edas el concepto de persistencia (caching) para optimizar el rendimiento en este escenario? Proporciona un ejemplo de c\u00f3digo.","title":"Tarea"},{"location":"tema14/","text":"1. Introducci\u00f3n Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente. 1.4.1 Requisitos previos para la instalaci\u00f3n Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo. Java Development Kit (JDK) Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n. Python (para PySpark) Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior. Apache Hadoop (opcional, para HDFS y YARN) Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop). 1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento. Descarga de Spark El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/ Configuraci\u00f3n del modo Standalone El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado. Configuraci\u00f3n con YARN (Yet Another Resource Negotiator) YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos. Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo) La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental. 1.4.3 Spark en entornos de nube (AWS, Azure, GCP) La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos. Servicios gestionados de Spark en la nube Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres. Ventajas de los servicios gestionados Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones. Configuraci\u00f3n de acceso a datos en la nube Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP. 1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas) Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows. Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark) Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal. Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs) Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888 Tarea Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema14/#tema-14-instalacion-y-configuracion-de-spark","text":"Objetivo : Instalar y configurar entornos de Apache Spark tanto en escenarios locales (para desarrollo y pruebas) como en configuraciones de cl\u00faster on-premise y plataformas de nube, asegurando la capacidad de ejecutar aplicaciones Spark de manera eficiente. Introducci\u00f3n : Para aprovechar el poder de Apache Spark, es fundamental comprender c\u00f3mo instalarlo y configurarlo correctamente. Este tema cubrir\u00e1 los pasos necesarios para establecer un entorno Spark, desde los requisitos b\u00e1sicos hasta la configuraci\u00f3n de cl\u00fasteres a gran escala en diferentes modos de despliegue. Abordaremos tanto las instalaciones on-premise, que te dan un control total, como los servicios gestionados en la nube, que simplifican la operaci\u00f3n. Finalmente, nos centraremos en la configuraci\u00f3n de un entorno de desarrollo en Windows utilizando Docker y WSL2, lo que te permitir\u00e1 realizar las pr\u00e1cticas del curso de manera efectiva y sin complicaciones. Desarrollo : La instalaci\u00f3n y configuraci\u00f3n de Spark puede variar significativamente dependiendo del entorno de despliegue. Ya sea que busques construir un cl\u00faster dedicado en tus propios servidores, aprovechar la elasticidad de los servicios en la nube, o simplemente configurar un entorno local para tus pr\u00e1cticas de desarrollo, cada escenario tiene sus particularidades. En este tema, desglosaremos los requisitos previos, los pasos detallados para las instalaciones on-premise, las consideraciones clave al trabajar con Spark en la nube, y una gu\u00eda pr\u00e1ctica para configurar tu estaci\u00f3n de trabajo Windows con Docker y WSL2 para una experiencia de desarrollo fluida y eficiente.","title":"Tema 1.4 Instalaci\u00f3n y configuraci\u00f3n de Spark"},{"location":"tema14/#141-requisitos-previos-para-la-instalacion","text":"Antes de sumergirte en la instalaci\u00f3n de Spark, es crucial asegurar que tu sistema cumpla con ciertos requisitos de software. Spark est\u00e1 construido sobre Java y se integra estrechamente con otros componentes, por lo que tener las versiones correctas de las dependencias es fundamental para evitar problemas de compatibilidad y asegurar un funcionamiento \u00f3ptimo.","title":"1.4.1 Requisitos previos para la instalaci\u00f3n"},{"location":"tema14/#java-development-kit-jdk","text":"Apache Spark requiere una instalaci\u00f3n de Java Development Kit (JDK) para funcionar, ya que el propio Spark est\u00e1 escrito en Scala y Java. Es esencial tener una versi\u00f3n de JDK compatible con la versi\u00f3n de Spark que planeas instalar. Generalmente, Spark es compatible con JDK 8 o superior, pero siempre es buena pr\u00e1ctica revisar la documentaci\u00f3n oficial para la versi\u00f3n espec\u00edfica de Spark que est\u00e9s utilizando. Verificar la versi\u00f3n de Java instalada ejecutando java -version en la terminal. Si no est\u00e1 instalada o la versi\u00f3n es incompatible, descargar e instalar el JDK apropiado (por ejemplo, OpenJDK 11 o Oracle JDK 8). Configurar la variable de entorno JAVA_HOME para que apunte al directorio de instalaci\u00f3n de tu JDK. Esto es crucial para que Spark encuentre la JVM. Asegurarse de que el directorio bin del JDK est\u00e9 en la variable PATH del sistema para poder ejecutar comandos Java desde cualquier ubicaci\u00f3n.","title":"Java Development Kit (JDK)"},{"location":"tema14/#python-para-pyspark","text":"Si planeas usar PySpark para escribir aplicaciones Spark en Python, necesitar\u00e1s una instalaci\u00f3n de Python en tu sistema. Spark utiliza el int\u00e9rprete de Python para ejecutar el c\u00f3digo PySpark. Es recomendable usar una versi\u00f3n de Python compatible con la versi\u00f3n de Spark que est\u00e1s instalando, generalmente Python 3.9 o superior.","title":"Python (para PySpark)"},{"location":"tema14/#apache-hadoop-opcional-para-hdfs-y-yarn","text":"Aunque Spark puede ejecutarse de forma independiente, a menudo se utiliza en conjunci\u00f3n con Apache Hadoop , especialmente para el sistema de archivos distribuido (HDFS) y el gestor de recursos (YARN). Si planeas usar Spark con HDFS o YARN, necesitar\u00e1s una instalaci\u00f3n de Hadoop. La versi\u00f3n de Spark que descargues deber\u00eda estar precompilada con la versi\u00f3n de Hadoop que planeas usar para evitar problemas de compatibilidad. Si tu cl\u00faster ya tiene Hadoop instalado, asegurarte de que HADOOP_HOME y HADOOP_CONF_DIR est\u00e9n configurados correctamente para que Spark pueda interactuar con \u00e9l. Si no tienes Hadoop, puedes descargar una distribuci\u00f3n precompilada de Spark que incluya los binaries de Hadoop, lo que te permitir\u00e1 usar funcionalidades b\u00e1sicas de Hadoop sin una instalaci\u00f3n completa. En entornos de nube, esta dependencia se maneja t\u00edpicamente por el servicio gestionado (ej., EMR ya incluye Hadoop).","title":"Apache Hadoop (opcional, para HDFS y YARN)"},{"location":"tema14/#142-instalacion-de-un-cluster-spark-on-premise","text":"Configurar un cl\u00faster Spark on-premise te brinda el m\u00e1ximo control y flexibilidad, aunque requiere una inversi\u00f3n significativa en hardware, configuraci\u00f3n y mantenimiento. Es una opci\u00f3n com\u00fan para organizaciones con centros de datos existentes o necesidades espec\u00edficas de seguridad y rendimiento.","title":"1.4.2 Instalaci\u00f3n de un cl\u00faster Spark On-Premise"},{"location":"tema14/#descarga-de-spark","text":"El primer paso para una instalaci\u00f3n on-premise es obtener la distribuci\u00f3n de Spark. Debes elegir la versi\u00f3n precompilada que mejor se adapte a tu entorno de Hadoop (si lo usas) y tu versi\u00f3n de Scala. La descarga se realiza desde el sitio web oficial de Apache Spark. Navegar a la secci\u00f3n de descargas de Apache Spark y seleccionar la versi\u00f3n m\u00e1s reciente compatible con tu JDK y la versi\u00f3n de Hadoop deseada (ej., \"Spark 3.5.1 for Hadoop 3.3 and later\"). Descargar el archivo .tgz (tar.gz) a cada nodo del cl\u00faster o a un servidor central para su distribuci\u00f3n. Descomprimir el archivo en un directorio accesible, por ejemplo, /opt/spark en sistemas Linux: tar -xzf spark-<version>-bin-hadoop<version>.tgz -C /opt/","title":"Descarga de Spark"},{"location":"tema14/#configuracion-del-modo-standalone","text":"El modo Standalone es el gestor de cl\u00fasteres aut\u00f3nomo de Spark. Es el m\u00e1s f\u00e1cil de configurar y es \u00fatil para pruebas r\u00e1pidas o cl\u00fasteres dedicados a Spark sin otras dependencias de gestores de recursos. Implica configurar un Master y varios Workers . En el nodo que ser\u00e1 el Master, editar spark/conf/spark-env.sh (si no existe, copiar spark-env.sh.template ) y a\u00f1adir export SPARK_MASTER_HOST=<IP_DEL_MASTER> . Tambi\u00e9n puedes configurar SPARK_MASTER_PORT y SPARK_MASTER_WEBUI_PORT . En cada nodo Worker, editar su spark/conf/spark-env.sh para definir export SPARK_MASTER_URL=spark://<IP_DEL_MASTER>:<PUERTO_DEL_MASTER> . Iniciar el Master y los Workers utilizando los scripts sbin/start-master.sh y sbin/start-workers.sh (o sbin/start-all.sh si usas conf/slaves ). Verifica la UI del Master en http://<IP_DEL_MASTER>:8080 . Comentario para la Nube : En entornos de nube, el modo Standalone raramente se usa para producci\u00f3n. Los servicios gestionados (AWS EMR, Azure Databricks, GCP Dataproc) lo abstraen o emplean gestores de cl\u00fasteres m\u00e1s robustos como YARN o Kubernetes. Sin embargo, puedes replicar esta configuraci\u00f3n manualmente en VMs en la nube si necesitas un control granular y no quieres usar un servicio gestionado.","title":"Configuraci\u00f3n del modo Standalone"},{"location":"tema14/#configuracion-con-yarn-yet-another-resource-negotiator","text":"YARN es el gestor de recursos de Hadoop y es la forma m\u00e1s com\u00fan de desplegar Spark en cl\u00fasteres de Hadoop existentes. Permite a Spark compartir recursos din\u00e1micamente con otras aplicaciones Hadoop. Para configurar Spark con YARN, es necesario que Spark tenga acceso a los archivos de configuraci\u00f3n de Hadoop. Asegurarse de que la variable de entorno HADOOP_CONF_DIR est\u00e9 configurada en spark/conf/spark-env.sh en todos los nodos y apunte al directorio que contiene core-site.xml y yarn-site.xml de tu instalaci\u00f3n de Hadoop. Verificar que el cl\u00faster Hadoop con YARN est\u00e9 en funcionamiento (ResourceManager, NodeManagers, etc.). Enviar una aplicaci\u00f3n Spark a YARN utilizando spark-submit --master yarn --deploy-mode cluster <your-app.jar> . Spark utilizar\u00e1 YARN para asignar recursos y ejecutar la aplicaci\u00f3n. Comentario para la Nube : YARN es el Cluster Manager predeterminado en muchos servicios gestionados de Spark en la nube, como AWS EMR y GCP Dataproc. En estos casos, la integraci\u00f3n con YARN es autom\u00e1tica y no requiere configuraci\u00f3n manual de HADOOP_CONF_DIR . Solo necesitas especificar --master yarn al enviar tus trabajos.","title":"Configuraci\u00f3n con YARN (Yet Another Resource Negotiator)"},{"location":"tema14/#optimizacion-de-la-configuracion-memoria-cores-paralelismo","text":"La optimizaci\u00f3n del rendimiento de Spark depende en gran medida de una configuraci\u00f3n adecuada de sus recursos. Esto implica ajustar la memoria asignada al driver y a los ejecutores, el n\u00famero de cores por ejecutor, y el paralelismo de las tareas. Una configuraci\u00f3n incorrecta puede llevar a errores de memoria, subutilizaci\u00f3n de recursos o ejecuciones lentas. Ajustar la memoria del driver ( spark.driver.memory ) si el programa principal necesita cargar muchos datos en memoria o manejar una gran cantidad de metadatos. Por ejemplo: --driver-memory 4g . Configurar la memoria y los cores por ejecutor ( spark.executor.memory , spark.executor.cores ) para balancear el n\u00famero de tareas concurrentes por nodo y la cantidad de datos que puede procesar cada tarea. Por ejemplo: --executor-memory 8g --executor-cores 4 . Controlar el paralelismo de las operaciones de shuffle ( spark.sql.shuffle.partitions ) para evitar la creaci\u00f3n de demasiadas particiones peque\u00f1as o muy pocas particiones grandes, lo que puede impactar el rendimiento. Un valor de 200 o m\u00e1s es com\u00fan para cl\u00fasteres grandes. Comentario para la Nube : La optimizaci\u00f3n de la configuraci\u00f3n es igualmente cr\u00edtica en la nube. Los servicios gestionados ofrecen la flexibilidad de ajustar estos par\u00e1metros a trav\u00e9s de la consola o la CLI. Adem\u00e1s, algunos servicios como Databricks y Dataproc ofrecen caracter\u00edsticas avanzadas como el autoescalado y la optimizaci\u00f3n autom\u00e1tica del motor que pueden simplificar este proceso, aunque entender los par\u00e1metros b\u00e1sicos sigue siendo fundamental.","title":"Optimizaci\u00f3n de la configuraci\u00f3n (memoria, cores, paralelismo)"},{"location":"tema14/#143-spark-en-entornos-de-nube-aws-azure-gcp","text":"La computaci\u00f3n en la nube ha revolucionado la forma en que se despliegan y gestionan los cl\u00fasteres de Spark. Los proveedores de la nube ofrecen servicios gestionados que abstraen la complejidad de la infraestructura subyacente, permiti\u00e9ndote enfocarte en el desarrollo de tus aplicaciones de datos.","title":"1.4.3 Spark en entornos de nube (AWS, Azure, GCP)"},{"location":"tema14/#servicios-gestionados-de-spark-en-la-nube","text":"Estos servicios ofrecen una experiencia \"llave en mano\" para Spark, donde el proveedor se encarga del aprovisionamiento de m\u00e1quinas, la instalaci\u00f3n del software Spark y Hadoop, la configuraci\u00f3n de red y el monitoreo b\u00e1sico. Esto reduce significativamente la carga operativa y el tiempo de configuraci\u00f3n. AWS EMR (Elastic MapReduce) : Un servicio de cl\u00fasteres gestionados que facilita el despliegue y la ejecuci\u00f3n de frameworks de Big Data como Spark, Hadoop, Hive, Presto, etc. Se integra nativamente con S3 para almacenamiento y Kinesis para streaming. Azure HDInsight : El servicio de an\u00e1lisis de Big Data de Microsoft Azure, que ofrece cl\u00fasteres gestionados para Spark, Hadoop, Kafka y otros. Se integra con Azure Data Lake Storage (ADLS), Cosmos DB y Azure Synapse Analytics. GCP Dataproc : El servicio de Google Cloud para Spark y Hadoop. Se caracteriza por su r\u00e1pido aprovisionamiento de cl\u00fasteres y escalado autom\u00e1tico, con fuerte integraci\u00f3n con Google Cloud Storage (GCS) y BigQuery. Databricks : Una plataforma unificada para datos y IA, construida sobre Spark y disponible en AWS, Azure y GCP. Ofrece un entorno de desarrollo colaborativo (notebooks), optimizaciones de rendimiento a nivel de motor y gesti\u00f3n simplificada de cl\u00fasteres.","title":"Servicios gestionados de Spark en la nube"},{"location":"tema14/#ventajas-de-los-servicios-gestionados","text":"Las soluciones de Spark en la nube ofrecen beneficios sustanciales en comparaci\u00f3n con las instalaciones on-premise, principalmente en t\u00e9rminos de escalabilidad, flexibilidad de costos y reducci\u00f3n de la sobrecarga de gesti\u00f3n. Un cl\u00faster EMR o Dataproc puede escalar autom\u00e1ticamente el n\u00famero de nodos hacia arriba o hacia abajo en funci\u00f3n de la demanda de carga de trabajo, lo que optimiza el uso de recursos y el costo. Solo pagas por los recursos computacionales y de almacenamiento que consumes, sin la necesidad de una inversi\u00f3n inicial en hardware. Puedes apagar los cl\u00fasteres cuando no los uses. El proveedor de la nube se encarga de las actualizaciones de software, parches de seguridad, mantenimiento de infraestructura y recuperaci\u00f3n de fallos, liberando a tu equipo para centrarse en el desarrollo de aplicaciones.","title":"Ventajas de los servicios gestionados"},{"location":"tema14/#configuracion-de-acceso-a-datos-en-la-nube","text":"Un aspecto clave al usar Spark en la nube es la configuraci\u00f3n del acceso a los sistemas de almacenamiento de objetos nativos de la nube (como S3 en AWS, ADLS en Azure o GCS en GCP). Estos sistemas son altamente escalables, duraderos y rentables, y Spark se integra muy bien con ellos. Para acceder a datos en AWS S3 desde EMR, solo necesitas especificar la ruta s3a://<bucket-name>/<path-to-data> en tu c\u00f3digo Spark, y EMR gestionar\u00e1 autom\u00e1ticamente las credenciales de autenticaci\u00f3n si el cl\u00faster tiene los roles IAM correctos. En Azure HDInsight , puedes leer y escribir datos en Azure Data Lake Storage (ADLS) Gen2 especificando rutas como abfss://<filesystem>@<accountname>.dfs.core.windows.net/<path> . La autenticaci\u00f3n se maneja a trav\u00e9s de las identidades de Azure. Con GCP Dataproc , el acceso a Google Cloud Storage (GCS) es directo usando rutas gs://<bucket-name>/<path-to-data> , ya que los servicios de Google Cloud est\u00e1n configurados para interoperar con la seguridad del proyecto de GCP.","title":"Configuraci\u00f3n de acceso a datos en la nube"},{"location":"tema14/#144-instalacion-de-spark-en-windows-para-practicas","text":"Para el desarrollo y las pr\u00e1cticas locales en Windows, la instalaci\u00f3n nativa de Spark puede ser compleja debido a dependencias de Hadoop y Path variables. La soluci\u00f3n m\u00e1s robusta y recomendada es utilizar Docker y el Windows Subsystem for Linux 2 (WSL2) , que te permite ejecutar un entorno Linux con Spark de manera ligera y eficiente en tu m\u00e1quina Windows.","title":"1.4.4 Instalaci\u00f3n de Spark en Windows (para pr\u00e1cticas)"},{"location":"tema14/#uso-de-imagenes-docker-para-spark-ej-bitnamispark","text":"Una vez que Docker Desktop y WSL2 est\u00e1n listos, puedes utilizar im\u00e1genes Docker preconstruidas que ya contienen Spark. Esto elimina la necesidad de instalar Java, Scala o Spark manualmente en tu entorno WSL, simplificando enormemente el setup para las pr\u00e1cticas. Desde tu terminal de WSL2 (o PowerShell/CMD), descargar la imagen de Spark deseada, con el comando: docker pull bitnami/spark:latest Ejecutar un contenedor Spark en modo Standalone. Primero el Master: docker run -d --name spark-master -p 8080:8080 -p 7077:7077 bitnami/spark:latest start_master.sh Luego, un Worker: docker run -d --name spark-worker1 --link spark-master:spark-master bitnami/spark:latest start_worker.sh spark://spark-master:7077 Acceder al Spark Shell (PySpark o Scala) dentro del contenedor Master: docker exec -it spark-master pyspark Ahora puedes escribir y ejecutar c\u00f3digo Spark directamente en tu terminal.","title":"Uso de im\u00e1genes Docker para Spark (ej. bitnami/spark)"},{"location":"tema14/#configuracion-de-entorno-de-desarrollo-jupyter-ides","text":"Para una experiencia de desarrollo m\u00e1s completa, puedes configurar un entorno como Jupyter Notebook o integrar Spark con tu IDE favorito, conect\u00e1ndote al cl\u00faster Spark que se ejecuta en Docker/WSL2. Dentro de tu distribuci\u00f3n WSL2, instala Jupyter: pip install jupyter Luego, para interactuar con PySpark, puedes a\u00f1adir un kernel de PySpark al Jupyter: # En tu .bashrc o .zshrc en WSL2 export SPARK_HOME=/opt/bitnami/spark # Si usas la imagen bitnami/spark export PATH=$SPARK_HOME/bin:$PATH export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH export PYSPARK_SUBMIT_ARGS=\"--master spark://localhost:7077 pyspark-shell\" Luego, desde WSL2, lanza Jupyter: jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 Accede desde tu navegador Windows a http://localhost:8888","title":"Configuraci\u00f3n de entorno de desarrollo (Jupyter, IDEs)"},{"location":"tema14/#tarea","text":"Imagina que tu equipo tiene un centro de datos on-premise y est\u00e1 decidiendo si migrar su cl\u00faster Spark a la nube o mantenerlo local. Detalla al menos cinco pros y cinco contras de cada enfoque, considerando aspectos como costos, escalabilidad, seguridad y complejidad operativa. Explica con tus propias palabras qu\u00e9 es un Cluster Manager en Spark y por qu\u00e9 es una pieza tan fundamental en la arquitectura distribuida de Spark. Proporciona un ejemplo de c\u00f3mo YARN y Spark Standalone difieren en su gesti\u00f3n de recursos. Investiga el concepto de autoescalado (autoscaling) en los servicios gestionados de Spark en la nube (ej., Dataproc o EMR). Describe c\u00f3mo funciona y qu\u00e9 beneficios aporta en comparaci\u00f3n con la gesti\u00f3n manual de recursos. \u00bfCu\u00e1les son los pasos clave para conectar un Jupyter Notebook (ejecut\u00e1ndose en tu entorno WSL2) a un cl\u00faster Spark que est\u00e1 en un contenedor Docker? Proporciona un pseudoc\u00f3digo o un ejemplo de configuraci\u00f3n de variables de entorno necesario.","title":"Tarea"},{"location":"tema15/","text":"1. Introducci\u00f3n Tema 1.5 Primeros pasos con PySpark Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados. 1.5.1 Entorno de desarrollo para PySpark Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE. Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv) Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows). Instalaci\u00f3n de PySpark ( pip install pyspark ) Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa. Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm) Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente. Acceso a la Spark UI La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores. 1.5.2 Inicializaci\u00f3n de SparkSession La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz. El papel de SparkSession SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL). Creaci\u00f3n de una SparkSession La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts. 1.5.3 Carga de datos con PySpark Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos. Lectura de archivos CSV (inferSchema, header, delimiter) El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema) Lectura de archivos JSON Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea. 1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames. Visualizaci\u00f3n de datos ( show() , printSchema() , describe() ) Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50 Selecci\u00f3n de columnas ( select() ) La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show() Filtrado de filas ( filter() / where() ) Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show() 1.5.5 Operaciones comunes de transformaci\u00f3n Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos. Renombrar y eliminar columnas ( withColumnRenamed() , drop() ) Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show() A\u00f1adir y modificar columnas ( withColumn() ) El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show() Operaciones de agregaci\u00f3n ( groupBy() , agg() , sum() , avg() , min() , max() ) Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show() 1.5.6 Escritura de datos con PySpark Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes. Guardar DataFrames en formato CSV, JSON, Parquet PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\") Modos de escritura (append, overwrite, ignore, errorIfExists) PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists Particionamiento de la salida ( partitionBy() ) El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True) Tarea Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Primeros pasos con PySpark"},{"location":"tema15/#1-introduccion","text":"","title":"1. Introducci\u00f3n"},{"location":"tema15/#tema-15-primeros-pasos-con-pyspark","text":"Objetivo : Adquirir las habilidades fundamentales para configurar un entorno de desarrollo PySpark, inicializar una sesi\u00f3n Spark, cargar y explorar datos, y realizar transformaciones b\u00e1sicas de DataFrames, sentando las bases para el an\u00e1lisis y procesamiento de Big Data. Introducci\u00f3n : PySpark es la API de Python para Apache Spark, que permite a los desarrolladores y cient\u00edficos de datos aprovechar el poder computacional distribuido de Spark utilizando la familiaridad y versatilidad del lenguaje Python. Es una herramienta indispensable para el procesamiento de datos a gran escala, Machine Learning y an\u00e1lisis en entornos Big Data. Este tema te guiar\u00e1 a trav\u00e9s de los primeros pasos esenciales con PySpark, desde la configuraci\u00f3n de tu entorno hasta la ejecuci\u00f3n de tus primeras operaciones con DataFrames. Desarrollo : En este tema, exploraremos c\u00f3mo empezar a trabajar con PySpark de forma pr\u00e1ctica. Comenzaremos configurando un entorno de desarrollo Python adecuado e instalando las librer\u00edas necesarias. Luego, aprenderemos a inicializar una SparkSession , que es el punto de entrada principal para cualquier aplicaci\u00f3n Spark. Una vez que tengamos un contexto Spark, nos centraremos en c\u00f3mo cargar datos desde diversas fuentes en DataFrames y c\u00f3mo realizar operaciones b\u00e1sicas de exploraci\u00f3n para entender la estructura y el contenido de nuestros datos. Finalmente, cubriremos las transformaciones m\u00e1s comunes que te permitir\u00e1n manipular y preparar tus datos para an\u00e1lisis m\u00e1s avanzados.","title":"Tema 1.5 Primeros pasos con PySpark"},{"location":"tema15/#151-entorno-de-desarrollo-para-pyspark","text":"Un entorno de desarrollo bien configurado es crucial para trabajar eficientemente con PySpark. Esto implica tener una instalaci\u00f3n de Python gestionada, PySpark instalado como una librer\u00eda de Python, y un entorno para escribir y ejecutar c\u00f3digo, como Jupyter Notebooks o un IDE.","title":"1.5.1 Entorno de desarrollo para PySpark"},{"location":"tema15/#configuracion-de-un-entorno-python-anacondaminiconda-virtualenv","text":"Para gestionar las dependencias de Python y evitar conflictos entre proyectos, es altamente recomendable utilizar entornos virtuales. Anaconda/Miniconda son distribuciones de Python que vienen con su propio gestor de paquetes ( conda ) y facilitan la creaci\u00f3n y gesti\u00f3n de entornos. virtualenv es otra herramienta est\u00e1ndar de Python para crear entornos virtuales aislados. Crear un nuevo entorno conda para PySpark: conda create -n pyspark_env python=3.9 . Activar el entorno reci\u00e9n creado: conda activate pyspark_env . Usar virtualenv para crear un entorno: python -m venv pyspark_venv y activarlo con source pyspark_venv/bin/activate (Linux/macOS) o pyspark_venv\\Scripts\\activate (Windows).","title":"Configuraci\u00f3n de un entorno Python (Anaconda/Miniconda, virtualenv)"},{"location":"tema15/#instalacion-de-pyspark-pip-install-pyspark","text":"Una vez que tu entorno Python est\u00e1 activado, la instalaci\u00f3n de PySpark es tan sencilla como usar pip . Esto descargar\u00e1 la librer\u00eda de PySpark y sus dependencias, permiti\u00e9ndote importar pyspark en tus scripts. Instalar la \u00faltima versi\u00f3n de PySpark: pip install pyspark . Instalar una versi\u00f3n espec\u00edfica de PySpark para asegurar compatibilidad: pip install pyspark==3.5.0 . Verificar la instalaci\u00f3n abriendo un int\u00e9rprete de Python y ejecutando import pyspark . Si no hay errores, la instalaci\u00f3n fue exitosa.","title":"Instalaci\u00f3n de PySpark (pip install pyspark)"},{"location":"tema15/#integracion-con-jupyter-notebooks-o-ides-vs-code-pycharm","text":"Para escribir y ejecutar c\u00f3digo PySpark de forma interactiva y con herramientas de desarrollo avanzadas, puedes integrar PySpark con Jupyter Notebooks o IDEs como VS Code o PyCharm. Para usar PySpark en Jupyter Notebooks , instala jupyter ( pip install jupyter ). Luego, al iniciar un notebook, puedes importar SparkSession y usarlo directamente. # En una celda de Jupyter from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"MyFirstPySparkApp\").getOrCreate() En VS Code , instala la extensi\u00f3n de Python y abre una carpeta de proyecto. Puedes configurar el int\u00e9rprete de Python para que sea el de tu entorno virtual de PySpark. Para ejecutar scripts .py , configura SPARK_HOME y PYTHONPATH en tu terminal antes de ejecutar spark-submit . En PyCharm , puedes configurar un int\u00e9rprete de Python basado en tu entorno virtual. Para ejecutar aplicaciones Spark, puedes configurar una \"Run Configuration\" que utilice spark-submit internamente.","title":"Integraci\u00f3n con Jupyter Notebooks o IDEs (VS Code, PyCharm)"},{"location":"tema15/#acceso-a-la-spark-ui","text":"La Spark UI es una interfaz web que proporciona monitoreo en tiempo real de las aplicaciones Spark en ejecuci\u00f3n. Es invaluable para depurar, optimizar y entender el rendimiento de tus aplicaciones PySpark. Cuando ejecutas una aplicaci\u00f3n Spark, el Driver de Spark lanza un servidor web para la UI. Al ejecutar una aplicaci\u00f3n PySpark localmente, la Spark UI suele estar disponible en http://localhost:4040 . Si ya hay una aplicaci\u00f3n ejecut\u00e1ndose, el puerto puede incrementarse (ej. 4041, 4042). Acceder a la pesta\u00f1a \"Jobs\" para ver el DAG de ejecuci\u00f3n, las etapas y las tareas, y cu\u00e1nto tiempo tard\u00f3 cada una. Utilizar la pesta\u00f1a \"Executors\" para monitorear el uso de memoria y CPU de los ejecutores y revisar sus logs en busca de errores.","title":"Acceso a la Spark UI"},{"location":"tema15/#152-inicializacion-de-sparksession","text":"La SparkSession es el punto de entrada principal para programar Spark con la API de DataFrame y Dataset. Sustituy\u00f3 a SparkContext y SQLContext a partir de Spark 2.0, unificando todas las funcionalidades en una sola interfaz.","title":"1.5.2 Inicializaci\u00f3n de SparkSession"},{"location":"tema15/#el-papel-de-sparksession","text":"SparkSession es el objeto unificado para interactuar con Spark. Proporciona acceso a todas las funcionalidades de Spark, incluyendo la creaci\u00f3n de DataFrames, la ejecuci\u00f3n de SQL, la lectura y escritura de datos, y el acceso al SparkContext subyacente. Se encarga de la comunicaci\u00f3n con el cl\u00faster y la gesti\u00f3n de recursos. Crear una SparkSession con un nombre de aplicaci\u00f3n espec\u00edfico y el modo de ejecuci\u00f3n local: from pyspark.sql import SparkSession spark = SparkSession.builder \\ .appName(\"MiPrimeraAppPySpark\") \\ .master(\"local[*]\") \\ .getOrCreate() Si intentas crear una segunda SparkSession en la misma aplicaci\u00f3n, getOrCreate() devolver\u00e1 la instancia existente, asegurando que solo haya una activa. Utilizar el objeto spark para acceder a funcionalidades como spark.read (para cargar datos) o spark.sql (para ejecutar consultas SQL).","title":"El papel de SparkSession"},{"location":"tema15/#creacion-de-una-sparksession","text":"La SparkSession se crea utilizando el patr\u00f3n builder . Puedes encadenar m\u00e9todos para configurar diferentes aspectos de la sesi\u00f3n antes de llamar a getOrCreate() para obtener la instancia. Crear una SparkSession simple para desarrollo local: from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"LocalTestApp\").master(\"local[*]\").getOrCreate() Configurar la memoria del driver y los ejecutores al crear la SparkSession : spark = SparkSession.builder \\ .appName(\"BigDataJob\") \\ .master(\"yarn\") \\ .config(\"spark.driver.memory\", \"4g\") \\ .config(\"spark.executor.memory\", \"8g\") \\ .config(\"spark.executor.cores\", \"4\") \\ .getOrCreate() Detener la SparkSession al finalizar la aplicaci\u00f3n para liberar recursos: spark.stop() . Esto es importante, especialmente en entornos de producci\u00f3n o scripts.","title":"Creaci\u00f3n de una SparkSession"},{"location":"tema15/#153-carga-de-datos-con-pyspark","text":"Una de las tareas m\u00e1s comunes en el procesamiento de datos es cargar informaci\u00f3n desde diversas fuentes. PySpark proporciona APIs flexibles y potentes para leer datos en DataFrames desde una variedad de formatos y sistemas de archivos distribuidos.","title":"1.5.3 Carga de datos con PySpark"},{"location":"tema15/#lectura-de-archivos-csv-inferschema-header-delimiter","text":"El formato CSV es uno de los m\u00e1s utilizados para el intercambio de datos. PySpark ofrece opciones robustas para leer archivos CSV, incluyendo la inferencia autom\u00e1tica del esquema, el manejo de encabezados y la especificaci\u00f3n de delimitadores. Cargar un archivo CSV, indicando que la primera fila es el encabezado y que Spark debe inferir el esquema (tipos de datos de las columnas): df_csv = spark.read.csv(\"data/clientes.csv\", header=True, inferSchema=True) df_csv.show() Cargar un CSV con un delimitador diferente (ej. ; ) y sin encabezado: df_semicolon = spark.read.csv(\"data/productos.txt\", sep=\";\", header=False) df_semicolon.printSchema() # Mostrar\u00e1 _c0, _c1, etc. Deshabilitar la inferencia de esquema para mayor control y especificar el esquema manualmente para un CSV grande (mejora el rendimiento): from pyspark.sql.types import StructType, StructField, StringType, IntegerType schema = StructType([ StructField(\"id\", IntegerType(), True), StructField(\"nombre\", StringType(), True), StructField(\"edad\", IntegerType(), True) ]) df_manual_schema = spark.read.csv(\"data/usuarios.csv\", header=True, schema=schema)","title":"Lectura de archivos CSV (inferSchema, header, delimiter)"},{"location":"tema15/#lectura-de-archivos-json","text":"Los archivos JSON son comunes para datos semiestructurados. PySpark puede leer JSON de forma sencilla, trat\u00e1ndolos como objetos anidados y creando un esquema basado en su estructura. Cargar un archivo JSON (cada l\u00ednea es un objeto JSON v\u00e1lido): df_json = spark.read.json(\"data/eventos.json\") df_json.show() df_json.printSchema() # Muestra la estructura inferida Cargar m\u00faltiples archivos JSON de un directorio: df_multi_json = spark.read.json(\"data/json_logs/*.json\") Si los archivos JSON tienen un formato m\u00e1s complejo o se distribuyen en m\u00faltiples l\u00edneas, Spark puede necesitar una configuraci\u00f3n adicional, aunque por defecto asume un objeto JSON por l\u00ednea.","title":"Lectura de archivos JSON"},{"location":"tema15/#154-exploracion-basica-de-dataframes","text":"Una vez que los datos se han cargado en un DataFrame, el siguiente paso es explorarlos para entender su estructura, contenido y calidad. PySpark proporciona m\u00e9todos intuitivos para visualizar, inspeccionar y obtener estad\u00edsticas descriptivas de tus DataFrames.","title":"1.5.4 Exploraci\u00f3n b\u00e1sica de DataFrames"},{"location":"tema15/#visualizacion-de-datos-show-printschema-describe","text":"Estos m\u00e9todos son esenciales para obtener una primera impresi\u00f3n r\u00e1pida de tu DataFrame. show() muestra las primeras filas, printSchema() revela la estructura de las columnas y sus tipos de datos, y describe() proporciona estad\u00edsticas resumidas para columnas num\u00e9ricas y de cadena. Mostrar las primeras 5 filas del DataFrame: df.show(5) # Por defecto muestra 20 filas, show(n) para n filas, show(n, truncate=False) para no truncar cadenas largas Imprimir el esquema del DataFrame para ver nombres de columnas y tipos de datos: df.printSchema() # Output: # root # |-- id: integer (nullable = true) # |-- nombre: string (nullable = true) # |-- edad: integer (nullable = true) Obtener estad\u00edsticas descriptivas para todas las columnas num\u00e9ricas y de cadena: df.describe().show() # Output (ejemplo para 'edad' y 'nombre'): # summary id nombre edad # -------- -------- -------- ---- # count 100 100 100 # mean 50.5 null 35.0 # stddev 29.01 null 10.0 # min 1 Alice 20 # max 100 Zoe 50","title":"Visualizaci\u00f3n de datos (show(), printSchema(), describe())"},{"location":"tema15/#seleccion-de-columnas-select","text":"La operaci\u00f3n select() te permite elegir un subconjunto de columnas de un DataFrame, o incluso crear nuevas columnas basadas en transformaciones de las existentes. Es fundamental para reducir el volumen de datos o preparar datos para an\u00e1lisis espec\u00edficos. Seleccionar una o varias columnas por su nombre: df_selected = df.select(\"nombre\", \"edad\") df_selected.show() Renombrar una columna mientras se selecciona: from pyspark.sql.functions import col df_renamed = df.select(col(\"nombre\").alias(\"nombre_completo\"), \"edad\") df_renamed.show() Crear una nueva columna aplicando una funci\u00f3n a una columna existente: df_with_new_col = df.select(\"nombre\", \"edad\", (col(\"edad\") * 12).alias(\"edad_meses\")) df_with_new_col.show()","title":"Selecci\u00f3n de columnas (select())"},{"location":"tema15/#filtrado-de-filas-filter-where","text":"Las operaciones filter() y where() son equivalentes y se utilizan para seleccionar filas que satisfacen una o m\u00e1s condiciones. Son cruciales para limpiar datos o para concentrarse en subconjuntos espec\u00edficos de informaci\u00f3n. Filtrar filas donde la edad sea mayor de 30: df_adultos = df.filter(df.edad > 30) df_adultos.show() Aplicar m\u00faltiples condiciones de filtrado usando operadores l\u00f3gicos ( & para AND, | para OR, ~ para NOT): df_filtered = df.filter((df.edad >= 25) & (df.nombre.contains(\"a\"))) df_filtered.show() Usar una expresi\u00f3n SQL para el filtrado: df_sql_filter = df.where(\"edad < 30 AND id % 2 = 0\") df_sql_filter.show()","title":"Filtrado de filas (filter() / where())"},{"location":"tema15/#155-operaciones-comunes-de-transformacion","text":"Las transformaciones son el coraz\u00f3n del procesamiento de datos en Spark. Permiten modificar DataFrames de diversas maneras, desde a\u00f1adir o eliminar columnas hasta realizar agregaciones y uniones, preparando los datos para an\u00e1lisis m\u00e1s complejos.","title":"1.5.5 Operaciones comunes de transformaci\u00f3n"},{"location":"tema15/#renombrar-y-eliminar-columnas-withcolumnrenamed-drop","text":"Estas operaciones son fundamentales para limpiar y organizar tu DataFrame, haci\u00e9ndolo m\u00e1s legible y adecuado para los an\u00e1lisis posteriores. Renombrar una columna: df_renamed_col = df.withColumnRenamed(\"nombre\", \"nombre_del_cliente\") df_renamed_col.show() Eliminar una o varias columnas: df_dropped_col = df.drop(\"id\", \"nombre_del_cliente\") # Si se renombro antes df_dropped_col.show() Renombrar una columna y luego eliminar otra en una secuencia: df_processed = df.withColumnRenamed(\"edad\", \"age\").drop(\"id\") df_processed.show()","title":"Renombrar y eliminar columnas (withColumnRenamed(), drop())"},{"location":"tema15/#anadir-y-modificar-columnas-withcolumn","text":"El m\u00e9todo withColumn() es extremadamente vers\u00e1til. Permite a\u00f1adir una nueva columna a un DataFrame o modificar una existente, bas\u00e1ndose en expresiones o funciones. A\u00f1adir una nueva columna calculada, por ejemplo, es_mayor_edad basada en edad : from pyspark.sql.functions import when df_with_flag = df.withColumn(\"es_mayor_edad\", when(df.edad >= 18, \"S\u00ed\").otherwise(\"No\")) df_with_flag.show() Modificar una columna existente, por ejemplo, convertir nombre a may\u00fasculas: from pyspark.sql.functions import upper df_upper_name = df.withColumn(\"nombre\", upper(df.nombre)) df_upper_name.show() Crear una columna a partir de un valor literal: from pyspark.sql.functions import lit df_with_constant = df.withColumn(\"fuente\", lit(\"sistema_A\")) df_with_constant.show()","title":"A\u00f1adir y modificar columnas (withColumn())"},{"location":"tema15/#operaciones-de-agregacion-groupby-agg-sum-avg-min-max","text":"Las agregaciones son fundamentales para resumir datos. groupBy() se utiliza para agrupar filas que tienen el mismo valor en una o m\u00e1s columnas, y agg() se utiliza para aplicar funciones de agregaci\u00f3n (como suma, promedio, conteo) a los grupos resultantes. Calcular el promedio de edad por sexo: df_agg = df.groupBy(\"sexo\").agg({\"edad\": \"avg\"}).show() # Alternativa m\u00e1s expl\u00edcita con funciones: # from pyspark.sql.functions import avg # df.groupBy(\"sexo\").agg(avg(\"edad\").alias(\"edad_promedio\")).show() Contar el n\u00famero de clientes por ciudad y la edad m\u00e1xima en cada ciudad: from pyspark.sql.functions import count, max df.groupBy(\"ciudad\").agg(count(\"*\").alias(\"num_clientes\"), max(\"edad\").alias(\"edad_maxima\")).show() Agregaci\u00f3n de m\u00faltiples columnas y funciones: from pyspark.sql.functions import sum, min df.groupBy(\"departamento\").agg( sum(\"ventas\").alias(\"total_ventas\"), min(\"fecha_pedido\").alias(\"primer_pedido\") ).show()","title":"Operaciones de agregaci\u00f3n (groupBy(), agg(), sum(), avg(), min(), max())"},{"location":"tema15/#156-escritura-de-datos-con-pyspark","text":"Una vez que has procesado y transformado tus datos con PySpark, el paso final es guardar los resultados en un formato y ubicaci\u00f3n deseados. PySpark soporta la escritura en varios formatos y ofrece modos para controlar c\u00f3mo se manejan los datos existentes.","title":"1.5.6 Escritura de datos con PySpark"},{"location":"tema15/#guardar-dataframes-en-formato-csv-json-parquet","text":"PySpark permite guardar DataFrames en diversos formatos de archivo, como CSV, JSON y Parquet. El formato Parquet es generalmente preferido en el ecosistema Big Data por su eficiencia en el almacenamiento y el rendimiento de las consultas, ya que es un formato columnar optimizado. Guardar un DataFrame en formato CSV: df_resultado.write.csv(\"output/clientes_procesados.csv\", header=True, mode=\"overwrite\") # Esto crear\u00e1 un directorio con m\u00faltiples archivos CSV (uno por partici\u00f3n) Guardar un DataFrame en formato JSON: df_resultado.write.json(\"output/eventos_limpios.json\", mode=\"append\") Guardar un DataFrame en formato Parquet (recomendado para eficiencia): df_resultado.write.parquet(\"output/datos_analiticos.parquet\", mode=\"overwrite\")","title":"Guardar DataFrames en formato CSV, JSON, Parquet"},{"location":"tema15/#modos-de-escritura-append-overwrite-ignore-errorifexists","text":"PySpark ofrece diferentes modos para manejar la situaci\u00f3n cuando un archivo o directorio de salida ya existe, lo que te da control sobre la persistencia de tus datos. overwrite : Sobrescribe el directorio de salida si ya existe. \u00a1\u00datil pero peligroso si no se usa con cuidado! df.write.mode(\"overwrite\").parquet(\"output/mi_data\") append : Si el directorio de salida ya existe, los nuevos datos se a\u00f1adir\u00e1n a los datos existentes. df.write.mode(\"append\").csv(\"output/registros.csv\") ignore : Si el directorio de salida ya existe, la operaci\u00f3n de escritura no har\u00e1 nada y los datos existentes permanecer\u00e1n intactos. df.write.mode(\"ignore\").json(\"output/datos_seguros.json\") errorIfExists (por defecto): Si el directorio de salida ya existe, lanzar\u00e1 una excepci\u00f3n, evitando la sobrescritura accidental. # df.write.mode(\"errorIfExists\").csv(\"output/error.csv\") # Esto fallar\u00e1 si el directorio existe df.write.csv(\"output/nuevo_csv.csv\") # El modo por defecto es errorIfExists","title":"Modos de escritura (append, overwrite, ignore, errorIfExists)"},{"location":"tema15/#particionamiento-de-la-salida-partitionby","text":"El m\u00e9todo partitionBy() permite organizar la salida de tu DataFrame en subdirectorios basados en los valores de una o m\u00e1s columnas. Esto mejora el rendimiento de lectura para consultas que filtran por esas columnas, ya que Spark puede escanear solo los subdirectorios relevantes. Particionar los datos de ventas por a\u00f1o y mes: df_ventas.write.partitionBy(\"anio\", \"mes\").parquet(\"output/ventas_particionadas\") # Esto crear\u00e1 una estructura como: ventas_particionadas/anio=2023/mes=01/part-*.parquet Guardar datos de usuarios particionados por pa\u00eds: df_usuarios.write.mode(\"overwrite\").partitionBy(\"pais\").json(\"output/usuarios_por_pais\") Combinar particionamiento con un formato de archivo espec\u00edfico: df_logs.write.partitionBy(\"fecha\").csv(\"output/logs_diarios\", header=True)","title":"Particionamiento de la salida (partitionBy())"},{"location":"tema15/#tarea","text":"Aqu\u00ed tienes 8 ejercicios de programaci\u00f3n PySpark para practicar los conceptos aprendidos: Inicializaci\u00f3n y Carga B\u00e1sica : Crea una SparkSession llamada \"MiPrimeraAppPySpark\". Crea una lista de tuplas en Python que represente datos de empleados (ej. [(1, \"Alice\", 30, \"IT\"), (2, \"Bob\", 24, \"HR\"), (3, \"Charlie\", 35, \"IT\")] ). Define un esquema expl\u00edcito para este DataFrame. Crea un DataFrame a partir de esta lista y el esquema. Muestra el DataFrame y su esquema. Lectura de CSV y Exploraci\u00f3n : Descarga un archivo CSV p\u00fablico, como \"data_sales.csv\" (puedes buscarlo en Kaggle o crear uno simple con datos de ventas: ID_Venta,Producto,Cantidad,Precio,Fecha,Ciudad ). Carga este archivo CSV en un DataFrame, asegur\u00e1ndote de que el encabezado sea reconocido y el esquema sea inferido autom\u00e1ticamente. Muestra las primeras 10 filas del DataFrame. Imprime el esquema inferido. Genera estad\u00edsticas descriptivas para el DataFrame y mu\u00e9stralas. Selecci\u00f3n y Filtrado : Usando el DataFrame de ventas del ejercicio 2, selecciona las columnas Producto , Cantidad y Precio . Filtra el DataFrame para mostrar solo las ventas donde la Cantidad sea mayor que 5. Filtra el DataFrame para mostrar las ventas de \"Producto_A\" realizadas en la \"Ciudad_X\" (ajusta a tus datos de prueba). A\u00f1adir y Modificar Columnas : En el DataFrame de ventas, a\u00f1ade una nueva columna llamada Total_Venta que sea el producto de Cantidad por Precio . Modifica la columna Producto para que todos los nombres de los productos est\u00e9n en may\u00fasculas. A\u00f1ade una columna llamada Es_Gran_Venta que sea \"S\u00ed\" si Total_Venta es mayor que 100 y \"No\" en caso contrario. Agregaciones : Calcula la Cantidad total vendida por cada Producto . Encuentra el Precio promedio de los productos por cada Ciudad . Determina el n\u00famero de ventas ( ID_Venta o conteo de filas) y el Total_Venta m\u00e1ximo por cada Fecha . Uniones de DataFrames : Crea un segundo DataFrame llamado df_productos con la siguiente estructura: ID_Producto, Nombre_Producto, Categoria (ej. [(1, \"Laptop\", \"Electr\u00f3nica\"), (2, \"Mouse\", \"Accesorios\")] ). Aseg\u00farate de que Nombre_Producto coincida con algunos nombres en tu DataFrame de ventas. Une el DataFrame de ventas con el DataFrame de productos usando el Producto (o Nombre_Producto ) como clave com\u00fan. Muestra las ventas junto con la categor\u00eda del producto. Escritura de Datos y Modos : Guarda el DataFrame de ventas procesado (con Total_Venta y Es_Gran_Venta ) en un nuevo directorio llamado output/ventas_analisis_parquet en formato Parquet, usando el modo overwrite . Intenta guardar el mismo DataFrame en el mismo directorio usando el modo errorIfExists . Observa el error. Cambia el modo a ignore y reintenta la operaci\u00f3n. Particionamiento de Salida : Utilizando el DataFrame de ventas procesado, guarda los datos en formato CSV en el directorio output/ventas_particionadas_por_ciudad y partici\u00f3nalos por la columna Ciudad . Verifica la estructura de directorios creada en output/ventas_particionadas_por_ciudad . Carga solo los datos de una Ciudad espec\u00edfica (ej. \"Madrid\" o \"Bogota\") usando la ruta particionada y verifica que solo se carguen esas filas.","title":"Tarea"},{"location":"tema_docker/","text":"Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2 Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ). 1. Instalaci\u00f3n de WSL2 Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a. 2. Instalaci\u00f3n de Docker Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version 3. Ejecutando un contenedor de ejemplo Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios.","title":"Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2"},{"location":"tema_docker/#instalacion-y-configuracion-de-docker-y-wsl2","text":"Para trabajar c\u00f3modamente con Spark y Jupyter , es recomendable usar Docker en combinaci\u00f3n con WSL2 (Windows Subsystem for Linux). Esto garantizar\u00e1 un entorno flexible y eficiente para el desarrollo. Docker en Windows requiere virtualizaci\u00f3n por hardware habilitada en el BIOS, una versi\u00f3n superior a Windows 10 1607 + o soporte para Hyper-V, La forma m\u00e1s f\u00e1cil para correr Docker en Windows, es configurarlo para utilizar WSL2 ( Windows Subsystem para Linux ).","title":"Instalaci\u00f3n y Configuraci\u00f3n de Docker y WSL2"},{"location":"tema_docker/#1-instalacion-de-wsl2","text":"Para Habilitar WSL2, abre una ventana de PowerShell como administrador y ejecuta: wsl --install Asegurate que WSL2 est\u00e1 ejecutando la \u00faltima versi\u00f3n con el comando: wsl --update Abre una ventana de Ubuntu desde el men\u00fa de inicio, y configura usuario y contrase\u00f1a.","title":"1. Instalaci\u00f3n de WSL2"},{"location":"tema_docker/#2-instalacion-de-docker","text":"Docker permitir\u00e1 ejecutar Spark y Jupyter en contenedores sin complicaciones. Descarga Docker Desktop para Windows desde el sitio web oficial y sigue las instrucciones. Activa el uso de WSL2, abriendo el programa Docker Desktop , ir al men\u00fa Settings , pesta\u00f1a General y activar Use the WSL 2 based engine , luego haz click en Apply & Restart y listo. Verifica la instalaci\u00f3n con este comando: docker --version","title":"2. Instalaci\u00f3n de Docker"},{"location":"tema_docker/#3-ejecutando-un-contenedor-de-ejemplo","text":"Ya tienes instalado Docker , es el momento de ejecutar tu primer contenedor, usando el siguiente comando: docker run hello-world Ese contenedor solo sirve para probar que Docker funciona, hay m\u00e1s im\u00e1genes preconstruidas en Docker Hub que puedes utilizar para ejecutar otros servicios.","title":"3. Ejecutando un contenedor de ejemplo"},{"location":"tema_python/","text":"Dominio b\u00e1sico de Python Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave 1. Entorno de Trabajo: Jupyter y Markdown Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown 2. Fundamentos de Python Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos 3. An\u00e1lisis de Datos con Pandas Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas","title":"Dominio b\u00e1sico de Python"},{"location":"tema_python/#dominio-basico-de-python","text":"Antes de sumergirse en es m\u00f3dulo, es importante que cada estudiante tenga una base s\u00f3lida en Python y herramientas esenciales para el an\u00e1lisis de datos. Esta gu\u00eda ofrece una serie de temas para repasar, junto con referencias a videos clave","title":"Dominio b\u00e1sico de Python"},{"location":"tema_python/#1-entorno-de-trabajo-jupyter-y-markdown","text":"Jupyter Notebooks, Google Colab y otros derivados Uso de celdas de c\u00f3digo y celdas de texto Sintaxis de Markdown para documentaci\u00f3n en Jupyter Video: Cuadernos Jupyter, Markdown","title":"1. Entorno de Trabajo: Jupyter y Markdown"},{"location":"tema_python/#2-fundamentos-de-python","text":"Tipos de datos: listas, diccionarios, tuplas y conjuntos Control de flujo: condicionales ( if / else ) y ciclos ( for / while ) Funciones y m\u00f3dulos Video: El Lenguaje de Programaci\u00f3n Python Cuaderno: Lenguaje Python Libro: Python para Todos","title":"2. Fundamentos de Python"},{"location":"tema_python/#3-analisis-de-datos-con-pandas","text":"Creaci\u00f3n y manipulaci\u00f3n de DataFrames Filtrado y selecci\u00f3n de datos Agrupaciones y operaciones estad\u00edsticas b\u00e1sicas Limpieza y transformaci\u00f3n de datos EDA: An\u00e1lisis Exploratorio de Datos Video: An\u00e1lisis de Datos con Pandas Cuaderno: Pandas Sitio: Manual de Pandas","title":"3. An\u00e1lisis de Datos con Pandas"},{"location":"tema_sql/","text":"Repaso de SQL 1. Consultas B\u00e1sicas (SELECT) El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC; 2. Filtrado de Datos (WHERE) La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia'); 3. Joins (Uniones de Tablas) Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre; 4. Funciones de Agregaci\u00f3n Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5; 5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING) GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000; 6. Inserci\u00f3n de Datos (INSERT) INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75); 7. Actualizaci\u00f3n de Datos (UPDATE) UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 ); 8. Eliminaci\u00f3n de Datos (DELETE) DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) ); 9. Subconsultas Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento ); 10. \u00cdndices y Optimizaci\u00f3n Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20;","title":"Repaso de SQL"},{"location":"tema_sql/#repaso-de-sql","text":"","title":"Repaso de SQL"},{"location":"tema_sql/#1-consultas-basicas-select","text":"El comando SELECT es la base de toda consulta SQL. Permite recuperar datos de una o m\u00e1s tablas especificando qu\u00e9 columnas mostrar y de qu\u00e9 tablas. Selecci\u00f3n b\u00e1sica SELECT nombre, email, edad FROM usuarios WHERE edad > 25; Selecci\u00f3n con alias y ordenamiento SELECT nombre AS nombre_completo, salario * 12 AS salario_anual FROM empleados ORDER BY salario_anual DESC;","title":"1. Consultas B\u00e1sicas (SELECT)"},{"location":"tema_sql/#2-filtrado-de-datos-where","text":"La cl\u00e1usula WHERE permite filtrar registros bas\u00e1ndose en condiciones espec\u00edficas. Es esencial para obtener exactamente los datos que necesitas. Filtros con operadores l\u00f3gicos SELECT * FROM productos WHERE precio BETWEEN 100 AND 500 AND categoria = 'Electr\u00f3nicos' AND stock > 0; Filtros con patrones y listas SELECT cliente_id, nombre FROM clientes WHERE nombre LIKE 'Juan%' OR ciudad IN ('Madrid', 'Barcelona', 'Valencia');","title":"2. Filtrado de Datos (WHERE)"},{"location":"tema_sql/#3-joins-uniones-de-tablas","text":"Los joins permiten combinar datos de m\u00faltiples tablas relacionadas. Son fundamentales para trabajar con bases de datos normalizadas. INNER JOIN SELECT u.nombre, p.titulo, p.fecha_publicacion FROM usuarios u INNER JOIN posts p ON u.id = p.usuario_id WHERE p.fecha_publicacion > '2024-01-01'; LEFT JOIN SELECT c.nombre AS cliente, COUNT(p.id) AS total_pedidos FROM clientes c LEFT JOIN pedidos p ON c.id = p.cliente_id GROUP BY c.id, c.nombre;","title":"3. Joins (Uniones de Tablas)"},{"location":"tema_sql/#4-funciones-de-agregacion","text":"Las funciones de agregaci\u00f3n realizan c\u00e1lculos sobre conjuntos de filas y devuelven un \u00fanico valor: COUNT, SUM, AVG, MIN, MAX. Funciones b\u00e1sicas de agregaci\u00f3n SELECT COUNT(*) AS total_productos, AVG(precio) AS precio_promedio, MAX(precio) AS precio_maximo, MIN(stock) AS stock_minimo FROM productos WHERE categoria = 'Libros'; Agregaci\u00f3n con GROUP BY SELECT categoria, COUNT(*) AS cantidad_productos, SUM(precio * stock) AS valor_inventario FROM productos GROUP BY categoria HAVING COUNT(*) > 5;","title":"4. Funciones de Agregaci\u00f3n"},{"location":"tema_sql/#5-agrupacion-y-filtrado-de-grupos-group-by-y-having","text":"GROUP BY agrupa filas con valores similares, mientras que HAVING filtra grupos (no filas individuales como WHERE). Agrupaci\u00f3n simple SELECT departamento, COUNT(*) AS num_empleados, AVG(salario) AS salario_promedio FROM empleados GROUP BY departamento ORDER BY salario_promedio DESC; Agrupaci\u00f3n con filtrado de grupos SELECT YEAR(fecha_pedido) AS a\u00f1o, MONTH(fecha_pedido) AS mes, SUM(total) AS ventas_mensuales FROM pedidos GROUP BY YEAR(fecha_pedido), MONTH(fecha_pedido) HAVING SUM(total) > 10000;","title":"5. Agrupaci\u00f3n y Filtrado de Grupos (GROUP BY y HAVING)"},{"location":"tema_sql/#6-insercion-de-datos-insert","text":"INSERT permite agregar nuevos registros a las tablas. Es crucial dominar tanto inserciones simples como m\u00faltiples. Inserci\u00f3n simple INSERT INTO usuarios (nombre, email, fecha_registro, activo) VALUES ('Mar\u00eda Garc\u00eda', 'maria@email.com', '2024-05-31', TRUE); Inserci\u00f3n m\u00faltiple INSERT INTO productos (nombre, precio, categoria, stock) VALUES ('iPhone 15', 999.99, 'Electr\u00f3nicos', 50), ('MacBook Pro', 1999.99, 'Electr\u00f3nicos', 25), ('iPad Air', 599.99, 'Electr\u00f3nicos', 75);","title":"6. Inserci\u00f3n de Datos (INSERT)"},{"location":"tema_sql/#7-actualizacion-de-datos-update","text":"UPDATE modifica registros existentes. Siempre debe usarse con WHERE para evitar actualizar toda la tabla accidentalmente. Actualizaci\u00f3n condicional UPDATE empleados SET salario = salario * 1.10, fecha_actualizacion = NOW() WHERE departamento = 'Ventas' AND fecha_contratacion < '2023-01-01'; Actualizaci\u00f3n con subconsulta UPDATE productos SET precio = precio * 0.90 WHERE categoria = 'Ropa' AND id IN ( SELECT producto_id FROM inventario WHERE stock > 100 );","title":"7. Actualizaci\u00f3n de Datos (UPDATE)"},{"location":"tema_sql/#8-eliminacion-de-datos-delete","text":"DELETE elimina registros de una tabla. Como UPDATE, siempre debe incluir WHERE para evitar eliminar todos los registros. Eliminaci\u00f3n condicional DELETE FROM pedidos WHERE estado = 'cancelado' AND fecha_pedido < DATE_SUB(NOW(), INTERVAL 1 YEAR); Eliminaci\u00f3n con subconsulta DELETE FROM usuarios WHERE activo = FALSE AND id NOT IN ( SELECT DISTINCT usuario_id FROM pedidos WHERE fecha_pedido > DATE_SUB(NOW(), INTERVAL 6 MONTH) );","title":"8. Eliminaci\u00f3n de Datos (DELETE)"},{"location":"tema_sql/#9-subconsultas","text":"Las subconsultas son consultas anidadas dentro de otras consultas. Permiten realizar operaciones complejas y comparaciones din\u00e1micas. Subconsulta en WHERE SELECT nombre, salario FROM empleados WHERE salario > ( SELECT AVG(salario) FROM empleados WHERE departamento = 'IT' ); Subconsulta correlacionada SELECT e1.nombre, e1.departamento, e1.salario FROM empleados e1 WHERE e1.salario = ( SELECT MAX(e2.salario) FROM empleados e2 WHERE e2.departamento = e1.departamento );","title":"9. Subconsultas"},{"location":"tema_sql/#10-indices-y-optimizacion","text":"Los \u00edndices mejoran el rendimiento de las consultas, especialmente en tablas grandes. Es importante saber cu\u00e1ndo y c\u00f3mo usarlos. Creaci\u00f3n de \u00edndices -- \u00cdndice simple para b\u00fasquedas frecuentes CREATE INDEX idx_usuarios_email ON usuarios(email); -- \u00cdndice compuesto para consultas multi-columna CREATE INDEX idx_pedidos_fecha_cliente ON pedidos(fecha_pedido, cliente_id); Optimizaci\u00f3n de consultas -- Consulta optimizada usando \u00edndices SELECT * FROM pedidos WHERE cliente_id = 123 AND fecha_pedido BETWEEN '2024-01-01' AND '2024-12-31' ORDER BY fecha_pedido DESC LIMIT 10; -- Uso de EXPLAIN para analizar el plan de ejecuci\u00f3n EXPLAIN SELECT * FROM productos WHERE categoria = 'Libros' AND precio > 20;","title":"10. \u00cdndices y Optimizaci\u00f3n"}]}